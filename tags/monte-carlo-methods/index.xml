<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Monte Carlo Methods on Followb1ind1y</title>
    <link>https://followb1ind1y.github.io/tags/monte-carlo-methods/</link>
    <description>Recent content in Monte Carlo Methods on Followb1ind1y</description>
    <image>
      <url>https://followb1ind1y.github.io/papermod-cover.png</url>
      <link>https://followb1ind1y.github.io/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 04 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://followb1ind1y.github.io/tags/monte-carlo-methods/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Monte Carlo Methods</title>
      <link>https://followb1ind1y.github.io/posts/reinforcement_learning/05_-monte_carlo_methods/</link>
      <pubDate>Sat, 04 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://followb1ind1y.github.io/posts/reinforcement_learning/05_-monte_carlo_methods/</guid>
      <description>Monte Carlo methods require only experience—sample sequences of states, actions, and rewards from actual or simulated interaction with an environment. Learning from actual experience is striking because it requires no prior knowledge of the environment’s dynamics, yet can still attain optimal behavior. Learning from simulated experience is also powerful. Although a model is required, the model need only generate sample transitions, not the complete probability distributions of all possible transitions that is required for dynamic programming (DP).</description>
    </item>
    
  </channel>
</rss>
