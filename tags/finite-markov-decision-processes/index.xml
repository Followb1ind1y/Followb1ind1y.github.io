<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Finite Markov Decision Processes on Followb1ind1y</title>
    <link>https://followb1ind1y.github.io/tags/finite-markov-decision-processes/</link>
    <description>Recent content in Finite Markov Decision Processes on Followb1ind1y</description>
    <image>
      <url>https://followb1ind1y.github.io/papermod-cover.png</url>
      <link>https://followb1ind1y.github.io/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 24 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://followb1ind1y.github.io/tags/finite-markov-decision-processes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Finite Markov Decision Processes</title>
      <link>https://followb1ind1y.github.io/posts/reinforcement_learning/03_finite_markov_decision_processes/</link>
      <pubDate>Tue, 24 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://followb1ind1y.github.io/posts/reinforcement_learning/03_finite_markov_decision_processes/</guid>
      <description>MDPs are a mathematically idealized form of the reinforcement learning problem for which precise theoretical statements can be made. We introduce key elements of the problem&amp;rsquo;s mathematical structure, such as returns, value functions, and Bellman equations. We try to convey the wide range of applications that can be formulated as finite MDPs. As in all of artificial intelligence, there is a tension between breadth of applicability and mathematical tractability.
The Agentâ€“Environment Interface MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.</description>
    </item>
    
  </channel>
</rss>
