<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/posts/04_lda_and_qda_for_classification/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="LDA and QDA for Classification"><meta property="og:description" content="Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-13T00:00:00+00:00"><meta property="article:modified_time" content="2021-05-13T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="LDA"><meta property="article:tag" content="QDA"><title>LDA and QDA for Classification | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/posts/04_lda_and_qda_for_classification/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.75584a09e5db45a75c5b5cc34b0b915aba12db7c38cd3362219013456d7628fe.js integrity="sha256-dVhKCeXbRadcW1zDSwuRWroS23w4zTNiIZATRW12KP4=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><span>Common Libraries</span><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a><ul></ul></li><li><input type=checkbox id=section-1d03ca2abc7a4a1911fc57e2cecd1bcf class=toggle>
<label for=section-1d03ca2abc7a4a1911fc57e2cecd1bcf class="flex justify-between"><a href=/docs/deep-learning/computer-vision/>Computer Vision</a></label><ul></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>LDA and QDA for Classification</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#linear-discriminant-analysis-for-classification>Linear Discriminant Analysis for Classification</a></li><li><a href=#quadratic-discrimination-analysis-for-classification>Quadratic Discrimination Analysis for Classification</a><ul><li><a href=#case-1--when-hahahugoshortcode27s43hbhb>Case 1 : When HAHAHUGOSHORTCODE27s43HBHB</a></li><li><a href=#case-2--when-hahahugoshortcode27s51hbhb>Case 2 : When HAHAHUGOSHORTCODE27s51HBHB</a></li></ul></li><li><a href=#lda-and-qda-in-practice>LDA and QDA in practice</a></li><li><a href=#reference>Reference</a></li></ul></nav></aside></header><article class="markdown book-post"><h2>LDA and QDA for Classification</h2><div class="flex align-center text-small book-post-date"><img src=/svg/calendar.svg class=book-icon alt>
<span>May 13, 2021</span></div><div class=text-small><a href=/categories/data-science/>Data Science</a>,
<a href=/categories/machine-learning/>Machine Learning</a></div><div class=text-small><a href=/tags/machine-learning/>Machine Learning</a>,
<a href=/tags/lda/>LDA</a>,
<a href=/tags/qda/>QDA</a></div><div class=book-post-content><p>Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. <strong>Linear discriminant analysis (LDA)</strong> is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. <strong>Quadratic discriminant analysis (QDA)</strong> is a variant of LDA that allows for non-linear separation of data.</p><br><h2 id=linear-discriminant-analysis-for-classification>Linear Discriminant Analysis for Classification
<a class=anchor href=#linear-discriminant-analysis-for-classification>#</a></h2><p>LDA assumes that all classes are linearly separable and according to this multiple linear discrimination function representing several hyperplanes in the feature space are created to distinguish the classes. If there are two classes then the LDA draws one hyperplane and projects the data onto this hyperplane in such a way as to maximize the separation of the two categories. This hyperplane is created according to the two criteria considered simultaneously:</p><ul><li>Maximizing the distance between the means of two classes;</li><li>Minimizing the variation between each category.</li></ul><p>Suppose that
<link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\(Y \in \{1, ..., K\}\)
</span>is assigned a prior <span>\(\hat{\pi}_{k}\)
</span>such that <span>\(\sum_{i=1}^k \hat{\pi}_{k} = 1\)
</span>. According to Bayes’ rule, the posterior probability is</p><span>\[
P(Y=k |X=x)=\frac{f_{k}(x)\pi_{k}}{\sum_{i=1}^{K}f_{i}(x)\pi_{i}} \\
\]</span><p>where <span>\(f_{k}(x)\)
</span>is the density of <span>\(X\)
</span>conditioned on <span>\(k\)
</span>. The Bayes Classifier can be expessed as:</p><span>\[
h^{*}(x) = \arg\max_{k}\{P(Y=k|X=x)\} = \arg\max_{k}\delta_{k}(x) \\
\]</span><p>For we assume that the random variable <span>\(X\)
</span>is a vector <span>\(X=(X_1,X_2,...,X_k)\)
</span>which is drawn from a <em>multivariate Gaussian</em> with class-specific mean vector and a common covariance matrix <span>\(\Sigma \ (i.e. \Sigma_{k} = \Sigma, \forall k)\)
</span>. In other words the covariance matrix is common to all K classes: <span>\(Cov(X)=\Sigma\)
</span>of shape <span>\(d \times d\)
</span>.</p><p>Since <span>\(x\)
</span>follows a multivariate Gaussian distribution, the probability <span>\(P(X=x|Y=k)\)
</span>is given by: (<span>
\(\mu_k\)
</span>is the mean of inputs for category <span>\(k\)
</span>)</p><span>\[
f_k(x)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k)) \\
\]</span><p>Then we can find the posterior distribution as:</p><span>\[
P(Y=k |X=x)=\frac{f_{k}(x)\pi_{k}}{P(X=x)} = C \cdot f_{k}(x)\pi_{k} \\
\]</span><p>Since <span>\(P(X=x)\)
</span>does not depend on <span>\(k\)
</span>so we write it as a constant. We will now proceed to expand and simplify the algebra, putting all constant terms into <span>\(C,C^{'},C{''}\)
</span>etc..</p><span>\[
\begin{align*}
p_{k}(x) = &P(Y=k |X=x)=\frac{f_{k}(x)\pi_{k}}{P(X=x)} = C \cdot f_{k}(x)\pi_{k} \\
&=C \cdot \pi_{k} \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma^{-1}(x-\mu_k)) \\
&=C^{'} \cdot \pi_{k} exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma^{-1}(x-\mu_k)) \\
\end{align*}
\]</span><p>Take the log of both sides:</p><span>\[
\begin{align*}
logp_{k}(x) &=log(C^{'} \cdot \pi_{k} exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma^{-1}(x-\mu_k))) \\
&= logC^{'} + log\pi_k -\frac{1}{2}(x-\mu_k)^{T}\Sigma^{-1}(x-\mu_k) \\
&= logC^{'} + log\pi_k -\frac{1}{2}[(x^{T}\Sigma^{-1}x+\mu_{k}^{T}\Sigma^{-1}\mu_{k}]+x^{T}\Sigma^{-1}\mu_{k} \\
&= C^{''} + log\pi_k -\frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k}+x^{T}\Sigma^{-1}\mu_{k} \\
\end{align*}
\]</span><p>And so the objective function, sometimes called the linear discriminant function or linear score function is:</p><span>\[
\delta_{k} = log\pi_k -\frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k}+x^{T}\Sigma^{-1}\mu_{k} \\
\]</span><p>Which means that given an input <span>\(x\)
</span>we predict the class with the highest value of <span>\(\delta_{k}(x)\)
</span>.</p><p>To find the Dicision Boundary, we will set <span>\(P(Y=k|X=x) = P(Y=l|X=x)\)
</span>which is <span>\(\delta_{k}(x) = \delta_{l}(x)\)
</span>:</p><span>\[
log\pi_k -\frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k}+x^{T}\Sigma^{-1}\mu_{k} =
log\pi_l -\frac{1}{2}\mu_{l}^{T}\Sigma^{-1}\mu_{l}+x^{T}\Sigma^{-1}\mu_{l} \\
log\frac{\pi_{k}}{\pi_{l}} -\underbrace{\frac{1}{2}(\mu_{k}-\mu_{l})^{T}\Sigma^{-1}(\mu_{k}-\mu_{l})}_{\mathrm{Constant}}+\underbrace{x^{T}\Sigma^{-1}(\mu_{k}-\mu_{l})}_{\mathrm{Linear \ in} \ x} = 0 \\
\Rightarrow a^{T}x + b = 0 \\
\]</span><p>Which is a linear function in <span>\(x\)
</span>- this explains why the decision boundaries are linear - hence the name Linear Discriminant Analysis.</p><br><h2 id=quadratic-discrimination-analysis-for-classification>Quadratic Discrimination Analysis for Classification
<a class=anchor href=#quadratic-discrimination-analysis-for-classification>#</a></h2><p>LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector, but a covariance matrix that is common to all <span>\(K\)
</span>classes. <strong>Quadratic discriminant analysis</strong> provides an alternative approach by assuming that each class has its <strong>own covariance matrix <span>\(\Sigma_{k}\)
</span></strong>.</p><p>To derive the quadratic score function, we return to the previous derivation, but now <span>\(\Sigma_{k}\)
</span>is a function of <span>\(k\)
</span>, so we cannot push it into the constant anymore.</p><span>\[
p_{k}(x) = \pi_{k}\frac{1}{(2\pi)^{d/2}|\Sigma_{k}|^{1/2}}exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k)) \\
\]
</span><span>\[
\begin{align*}
logp_{k}(x) &= C +log\pi_{k} - \frac{1}{2}log|\Sigma_{k}|-\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k) \\
&= C +log\pi_{k} - \frac{1}{2}log|\Sigma_{k}| -\frac{1}{2}x^{T}\Sigma_{k}^{-1}x +x^{T}\Sigma_{k}^{-1}\mu_{k} -\frac{1}{2}\mu_{k}^{T}\Sigma_{k}^{-1}\mu_{k} \\
\end{align*}
\]</span><p>Which is a quadratic function of <span>\(x\)
</span>. Under this less restrictive assumption, the classifier assigns an observation <span>\(X=x\)
</span>to the class for which the quadratic score function is the largest:</p><span>\[
\delta_{k}(x) = log\pi_k- \frac{1}{2}log|\Sigma_{k}| -\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k) \\
\]</span><p>To find the Quadratic Dicision Boundary, we will set <span>\(P(Y=k|X=x) = P(Y=l|X=x)\)
</span>which is <span>\(\delta_{k}(x) = \delta_{l}(x)\)
</span>:</p><span>\[
log\pi_k- \frac{1}{2}log|\Sigma_{k}| -\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k) =
log\pi_l- \frac{1}{2}log|\Sigma_{l}| -\frac{1}{2}(x-\mu_l)^{T}\Sigma_{l}^{-1}(x-\mu_l) \\
\frac{1}{2}x^{T}\underbrace{(\Sigma_{l}-\Sigma_{k})}_{A}x+\underbrace{(\mu_{k}^{T}\Sigma_{k}^{-1}-\mu_{l}^{T}\Sigma_{l}^{-1})}_{b^{T}}x +\underbrace{\frac{1}{2}(\mu_{k}-\mu_{l})^{T}\Sigma^{-1}(\mu_{k}-\mu_{l}) + log(\frac{\pi_{l}}{\pi_{k}}) + log(\frac{|\Sigma_{k}|^{1/2}}{|\Sigma_{l}|^{1/2}})}_{c} = 0 \\
\Rightarrow x^{T}Ax + b^{T}x + c = 0 \\
\]</span><h3 id=case-1--when-hahahugoshortcode27s43hbhb>Case 1 : When <span>\(\Sigma_{k} = I\)
</span><a class=anchor href=#case-1--when-hahahugoshortcode27s43hbhb>#</a></h3><p>We first concider the case that <span>\(\Sigma_{k} = I, \forall k\)
</span>. This is the case where each distribution is spherical, around
the mean point. Then we can have:</p><span>\[
\delta_{k}(x) = - \frac{1}{2}log|I| + log\pi_k -\frac{1}{2}(x-\mu_k)^{T}I(x-\mu_k) \\
\]</span><p>Where <span>\(log(|I|) = log(1) = 0\)
</span>and <span>\((x-\mu_k)^{T}I(x-\mu_k) = (x-\mu_k)^{T}(x-\mu_k)\)
</span>is the <strong>Squared Euclidean Distance</strong> between two points <span>\(x\)
</span>and <span>\(\mu_{k}\)
</span>.</p><p>Thus under this condition (i.e. <span>\(\Sigma = I\)
</span>) , a new point can be classified by <strong>its distance from the center of a class</strong>, adjusted by some prior. Further, for two-class problem with equal prior, the discriminating function would be the perpendicular bisector of the 2-class’s means.</p><h3 id=case-2--when-hahahugoshortcode27s51hbhb>Case 2 : When <span>\(\Sigma_{k} \neq I\)
</span><a class=anchor href=#case-2--when-hahahugoshortcode27s51hbhb>#</a></h3><p>Since <span>\(\Sigma_{k}\)
</span>is a symmetric matrix <span>\(\Sigma_{k} = \Sigma_{k}^{T}\)
</span>, by using the <strong>Singular Value Decomposition (SVD)</strong> of <span>\(\Sigma_{k}\)
</span>, we can get:</p><span>\[
\Sigma_{k} = U_{k}S_{k}U_{k}^{T} \\
\Sigma_{k}^{-1} = (U_{k}S_{k}U_{k}^{T})^{-1} = U_{k}S_{k}^{-1}U_{k}^{T}\\
\]</span><p>Then,</p><span>\[
\begin{align*}
(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k) &= (x-\mu_k)^{T}U_{k}S_{k}^{-1}U_{k}^{T}(x-\mu_k) \\
&= (U_{k}^{T}x-U_{k}^{T}\mu_k)^{T}S_{k}^{-1}(U_{k}^{T}x-U_{k}^{T}\mu_k) \\
&= (U_{k}^{T}x-U_{k}^{T}\mu_k)^{T}S_{k}^{-\frac{1}{2}}S_{k}^{-\frac{1}{2}}(U_{k}^{T}x-U_{k}^{T}\mu_k) \\
&= (S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k)^{T}I(S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k) \\
&= (S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k)^{T}(S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k) \\
\end{align*}
\]</span><p>Which is also known as the <strong>Mahalanobis distance</strong>.</p><p>Think of <span>\(S_{k}^{-\frac{1}{2}}U_{k}^{T}\)
</span>as a linear transformation that takes points in class <span>\(k\)
</span>and distributes them spherically around a point, like in Case 1. Thus when we are given a new point, we can apply the modified <span>\(\delta_{k}\)
</span>values to calculate <span>\(h^{*}(x)\)
</span>. After applying the singular value decomposition, <span>\(\Sigma_{k}^{-1}\)
</span>is considered to be an identity matrix such that:</p><span>\[
\delta_{k}(x) = - \frac{1}{2}log|I| + log\pi_k -\frac{1}{2}(S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k)^{T}(S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k) \\
\]</span><p>Where <span>\(log(|I|) = log(1) = 0\)</span></p><p>The difference between Case 1 and Case 2 (i.e. the difference between using the <strong>Euclidean and Mahalanobis distance</strong>) can be seen in the illustration below:</p><div align=center><img src=/posts/images/4_Euclidean_and_Mahalanobis_distance.gif width=700px/></div><br><h2 id=lda-and-qda-in-practice>LDA and QDA in practice
<a class=anchor href=#lda-and-qda-in-practice>#</a></h2><p>In practice we don’t know the parameters of Gaussian and will need to estimate them using our training data.</p><span>\[
\hat{\pi}_{k} = \hat{P}(y=k) = \frac{n_{k}}{n} \\
\]</span><p>where <span>\(n_{k}\)
</span>is the number of class <span>\(k\)
</span>observations.</p><span>\[
\hat{\mu}_{k} = \frac{1}{n_{k}}\sum_{i:y_{i}=k}x_{i} \\
\]
</span><span>\[
\hat{\Sigma}_{k} = \frac{1}{n_{k}-k}\sum_{i:y_{i}=k}(x_{i}-\hat{\mu}_{k})(x_{i}-\hat{\mu}_{k})^{T} \\
\]</span><p>If we wish to use LDA we must calculate a <strong>common covariance</strong>, so we average all the covariances, e.g.</p><span>\[
\Sigma = \frac{\sum_{r=1}^k(n_{r}\Sigma_{r})}{\sum_{r=1}^k n_{r}} \\
\]</span><p>Where:</p><ul><li><p><span>\(n_{r}\)
</span>is the number of data points in class <span>\(r\)
</span>.</p></li><li><p><span>\(\Sigma_{r}\)
</span>is the covariance of class <span>\(r\)
</span>and <span>\(n\)
</span>is the total number of data points.</p></li><li><p><span>\(k\)
</span>is the number of classes.</p></li></ul><br><h2 id=reference>Reference
<a class=anchor href=#reference>#</a></h2><p>[1] Sicotte, X. B. (2018, June 22). Xavier Bourret Sicotte. Linear and Quadratic Discriminant Analysis - Data Blog. <a href=https://xavierbourretsicotte.github.io/LDA_QDA.html>https://xavierbourretsicotte.github.io/LDA_QDA.html</a>.</p></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#linear-discriminant-analysis-for-classification>Linear Discriminant Analysis for Classification</a></li><li><a href=#quadratic-discrimination-analysis-for-classification>Quadratic Discrimination Analysis for Classification</a><ul><li><a href=#case-1--when-hahahugoshortcode27s43hbhb>Case 1 : When HAHAHUGOSHORTCODE27s43HBHB</a></li><li><a href=#case-2--when-hahahugoshortcode27s51hbhb>Case 2 : When HAHAHUGOSHORTCODE27s51HBHB</a></li></ul></li><li><a href=#lda-and-qda-in-practice>LDA and QDA in practice</a></li><li><a href=#reference>Reference</a></li></ul></nav></div></aside></main></body></html>