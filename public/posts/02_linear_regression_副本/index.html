<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Regression is a method of modelling a target value based on independent predictors. This method is mostly used for forecasting and finding out cause and effect relationship between variables. Regression techniques mostly differ based on the number of independent variables and the type of relationship between the independent and dependent variables. Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/posts/02_linear_regression_%E5%89%AF%E6%9C%AC/"><meta property="og:site_name" content="Followb1ind1y"><meta property="og:title" content="Linear Regression"><meta property="og:description" content="Regression is a method of modelling a target value based on independent predictors. This method is mostly used for forecasting and finding out cause and effect relationship between variables. Regression techniques mostly differ based on the number of independent variables and the type of relationship between the independent and dependent variables. Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-08T00:00:00+00:00"><meta property="article:modified_time" content="2021-05-08T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Linear Regression"><title>Linear Regression | Followb1ind1y</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/posts/02_linear_regression_%E5%89%AF%E6%9C%AC/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.0d6f237f4e565536813941a13653016e190434f60822b37c29a7ea2f3f393f4f.js integrity="sha256-DW8jf05WVTaBOUGhNlMBbhkENPYIIrN8KafqLz85P08=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followb1ind1y</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-dc4356e6935aa68f1720a2edf4a9ccc3 class=toggle>
<label for=section-dc4356e6935aa68f1720a2edf4a9ccc3 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><span>Common Libraries</span><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-4878b38f894495969abad566d2343967 class=toggle>
<label for=section-4878b38f894495969abad566d2343967 class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul></ul></li><li><input type=checkbox id=section-d2db63a80a5d441392b9c36090e0f444 class=toggle>
<label for=section-d2db63a80a5d441392b9c36090e0f444 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a><ul></ul></li><li><input type=checkbox id=section-3b88424afbd0729618ea13cc9d079d29 class=toggle>
<label for=section-3b88424afbd0729618ea13cc9d079d29 class="flex justify-between"><a href=/docs/deep-learning/computer-vision/>Computer Vision</a></label><ul></ul></li><li><input type=checkbox id=section-200826a85c564b19fbb46b42ac917f3d class=toggle>
<label for=section-200826a85c564b19fbb46b42ac917f3d class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-6760b597543079e12cda6e2f8d441ad1 class=toggle>
<label for=section-6760b597543079e12cda6e2f8d441ad1 class="flex justify-between"><a role=button>Others</a></label><ul></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Linear Regression</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#linear-regression>Linear Regression</a><ul><li><a href=#gradient-descent-for-linear-regression>Gradient Descent for Linear Regression</a></li><li><a href=#normal-equation-for-linear-regression>Normal Equation for Linear Regression</a></li><li><a href=#evaluating-the-performance-of-the-linear-regression-model>Evaluating the performance of the Linear Regression Model</a></li></ul></li><li><a href=#references>References:</a></li></ul></nav></aside></header><article class="markdown book-post"><h2>Linear Regression</h2><div class="flex align-center text-small book-post-date"><img src=/svg/calendar.svg class=book-icon alt>
<span>May 8, 2021</span></div><div class=text-small><a href=/categories/data-science/>Data Science</a>,
<a href=/categories/machine-learning/>Machine Learning</a></div><div class=text-small><a href=/tags/machine-learning/>Machine Learning</a>,
<a href=/tags/linear-regression/>Linear Regression</a></div><div class=book-post-content><p>Regression is a method of modelling a target value based on independent predictors. This method is mostly used for forecasting and finding out <strong>cause and effect relationship between variables</strong>. Regression techniques mostly differ based on the number of independent variables and the type of relationship between the independent and dependent variables. Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on.</p><div align=center><img src=/img_ML/2_Regression.SVG width=500px/></div><br><h2 id=linear-regression>Linear Regression
<a class=anchor href=#linear-regression>#</a></h2><p>Linear regression with multiple variables is also known as &ldquo;<strong>multivariate linear regression</strong>&rdquo;. The multivariable form of the hypothesis function accommodating these multiple features is as follows:</p><p><code>$$ \begin{align*} &\mathrm{Hypothesis}: h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} + \cdot\cdot\cdot + \theta_{n}x_{n} \\ &\mathrm{Cost \ Function}: J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2 \\ &\mathrm{Goal}: \min_{\theta}J(\theta) \\ \end{align*} $$</code></p><p>Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:</p><p><code>$$ h_{\theta}(x) = \begin{bmatrix} \theta_{0} & \theta_{1} & \cdot\cdot\cdot & \theta_{n} \end{bmatrix} \begin{bmatrix} x_{0} \\ x_{1} \\ \cdot\cdot\cdot \\ x_{n} \end{bmatrix} = \theta^{T}x \\ $$</code></p><h3 id=gradient-descent-for-linear-regression>Gradient Descent for Linear Regression
<a class=anchor href=#gradient-descent-for-linear-regression>#</a></h3><p>Gradient descent is a generic optimization algorithm used in many machine learning algorithms. It iteratively tweaks the parameters of the model in order to minimize the cost function. We do this is by taking the <strong>derivative</strong> (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter <strong><code>$\alpha$</code></strong>, which is called the <strong>learning rate</strong>. The <strong>gradient descent algorithm</strong> can be represented as:</p><p><code>$$\begin{align*} &\frac{\partial J(\theta)}{\partial \theta_{0}}= \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}) \\ &\frac{\partial J(\theta)}{\partial \theta_{j}}= \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} \\ \end{align*} $$</code></p><p><code>$$ \begin{bmatrix} \frac{\partial J(\theta)}{\partial \theta_{0}} \\ \frac{\partial J(\theta)}{\partial \theta_{1}} \\ \cdot\cdot\cdot \\ \frac{\partial J(\theta)}{\partial \theta_{n}} \end{bmatrix}=\frac{1}{m}x^{T}(h_{\theta}(x)-y) \\ $$</code></p><p><code>$$ \begin{bmatrix} \theta_{0} \\ \theta_{1} \\ \cdot\cdot\cdot \\ \theta_{n} \end{bmatrix}=\begin{bmatrix} \theta_{0} \\ \theta_{1} \\ \cdot\cdot\cdot \\ \theta_{n} \end{bmatrix}-\alpha\begin{bmatrix} \frac{\partial J(\theta)}{\partial \theta_{0}} \\ \frac{\partial J(\theta)}{\partial \theta_{1}} \\ \cdot\cdot\cdot \\ \frac{\partial J(\theta)}{\partial \theta_{n}} \end{bmatrix} \\ $$</code></p><p><code>$$\begin{align*} &\mathrm{Repect\ until \ convergence} \{ \\ &\ \ \ \ \theta_{j}^{new} :=\theta_{j}^{old} - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}) \ \ \ \ \mathrm{for} \ j = 0 \cdot\cdot\cdot n \\ &\} \\ \end{align*} $$</code></p><p>To demonstrate the gradient descent algorithm, we initialize the model parameters with 0. The equation becomes <code>$h_{\theta}(x) = 0$</code>. Gradient descent algorithm now tries to update the value of the parameters so that we arrive at the best fit line.</p><p>We should adjust our parameter <code>$\alpha$</code> to ensure that the gradient descent algorithm converges in a reasonable time. <strong>If <code>$\alpha$</code> is too small</strong>, gradient descent can be slow. <strong>If <code>$\alpha$</code> is too large</strong>, gradient descent can overshoot the minimum. It may fail to converge, or even diverge. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.</p><div align=center><img src=/img_ML/2_Gradient_Descent.PNG width=500px/></div><br><p>We can <strong>speed up gradient descent</strong> by having each of our input values in roughly the same range. This is because <code>$\theta$</code> will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p><p>Two techniques to help with this are feature scaling and mean normalization. <strong>Feature scaling</strong> involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. <strong>Mean normalization</strong> involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:</p><p><code>$$ x_{i} := \frac{(x_{i}-\mu_{i})}{s_{i}} \\ $$</code></p><p>Where <code>$\mu_{i}$</code> is the average of all the values for feature (i) and <code>$s_{i}$</code> is the range of values (max - min), or <code>$s_{i}$</code> is the standard deviation. Note that dividing by the range, or dividing by the standard deviation, give different results.</p><h3 id=normal-equation-for-linear-regression>Normal Equation for Linear Regression
<a class=anchor href=#normal-equation-for-linear-regression>#</a></h3><p>Gradient descent gives one way of minimizing J. In the &ldquo;<strong>Normal Equation</strong>&rdquo; method, we will minimize J by explicitly taking its derivatives with respect to the <code>$\theta_{j}$</code> ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:</p><p><code>$$ \theta = (X^{T}X)^{-1}X^{T}y \\ $$</code></p><p>The following is a comparison of gradient descent and the normal equation:</p><table><thead><tr><th style=text-align:left><strong>Gradient Descent</strong></th><th style=text-align:left><strong>Normal Equation</strong></th></tr></thead><tbody><tr><td style=text-align:left>Need to choose alpha</td><td style=text-align:left>No need to choose alpha</td></tr><tr><td style=text-align:left>Needs many iterations</td><td style=text-align:left>No need to iterate</td></tr><tr><td style=text-align:left><code>$O(kn^{2})$</code></td><td style=text-align:left><code>$O(n^{3})$</code>, need to calculate inverse of <code>$X^{T}X$</code></td></tr><tr><td style=text-align:left>Works well when n is large</td><td style=text-align:left>Slow if n is very large</td></tr></tbody></table><p>With the normal equation, computing the inversion has complexity <code>$O(n^{3})$</code>. So if we have a very large number of features, the normal equation will be <strong>slow</strong>. In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.</p><p>If <code>$X^{T}X$</code> is <strong>noninvertible</strong>, the common causes might be having :</p><ul><li>Redundant features, where two features are very closely related (i.e. they are linearly dependent)</li><li>Too many features (e.g. <code>$m ≤ n$</code>). In this case, delete some features or use &ldquo;regularization&rdquo;.</li></ul><p>Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.</p><h3 id=evaluating-the-performance-of-the-linear-regression-model>Evaluating the performance of the Linear Regression Model
<a class=anchor href=#evaluating-the-performance-of-the-linear-regression-model>#</a></h3><p>We will be using <strong>Root mean squared error(RMSE)</strong> and <strong>Coefficient of Determination(R² score)</strong> to evaluate our model. RMSE is the square root of the average of the sum of the squares of residuals. RMSE is defined by:</p><p><code>$$ RMSE = \sqrt{\frac{1}{m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2} \\ $$</code></p><p><code>$R^{2}$</code> score or the coefficient of determination explains how much the total variance of the dependent variable can be reduced by using the least square regression. It can be defined by:</p><p><code>$$ R^{2} = 1- \frac{SS_{r}}{SS{t}} \\ $$</code></p><p>Where <code>$SS_{t}$</code> is the total sum of errors if we take the mean of the observed values as the predicted value and <code>$SS_{r}$</code> is the sum of the square of residuals.</p><p><code>$$\begin{align*} SS_{t} &= \sum_{i=1}^{m}(y^{(i)}-\bar{y})^2 \\ SS_{r} &= \sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2 \\ \end{align*}$$</code></p><br><h2 id=references>References:
<a class=anchor href=#references>#</a></h2><p>[1] Agarwal, A. (2018, November 14). Linear Regression using Python. Medium. <a href=https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2>https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2</a>.</p><p>[2] Ng, A. (n.d.). Machine Learning. Coursera. <a href=https://www.coursera.org/learn/machine-learning>https://www.coursera.org/learn/machine-learning</a>.</p></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#linear-regression>Linear Regression</a><ul><li><a href=#gradient-descent-for-linear-regression>Gradient Descent for Linear Regression</a></li><li><a href=#normal-equation-for-linear-regression>Normal Equation for Linear Regression</a></li><li><a href=#evaluating-the-performance-of-the-linear-regression-model>Evaluating the performance of the Linear Regression Model</a></li></ul></li><li><a href=#references>References:</a></li></ul></nav></div></aside></main></body></html>