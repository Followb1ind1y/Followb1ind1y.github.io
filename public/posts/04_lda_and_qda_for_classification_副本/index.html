<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/posts/04_lda_and_qda_for_classification_%E5%89%AF%E6%9C%AC/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="LDA and QDA for Classification"><meta property="og:description" content="Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-13T00:00:00+00:00"><meta property="article:modified_time" content="2021-05-13T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="LDA"><meta property="article:tag" content="QDA"><title>LDA and QDA for Classification | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/posts/04_lda_and_qda_for_classification_%E5%89%AF%E6%9C%AC/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.e5192f029984d9eef6a6a5551f67dbbd3722078e598ae5b028339b884dea2a08.js integrity="sha256-5RkvApmE2e72pqVVH2fbvTciB45ZiuWwKDObiE3qKgg=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-dc4356e6935aa68f1720a2edf4a9ccc3 class=toggle>
<label for=section-dc4356e6935aa68f1720a2edf4a9ccc3 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><span>Common Libraries</span><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-4878b38f894495969abad566d2343967 class=toggle>
<label for=section-4878b38f894495969abad566d2343967 class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul></ul></li><li><input type=checkbox id=section-d2db63a80a5d441392b9c36090e0f444 class=toggle>
<label for=section-d2db63a80a5d441392b9c36090e0f444 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a><ul></ul></li><li><input type=checkbox id=section-3b88424afbd0729618ea13cc9d079d29 class=toggle>
<label for=section-3b88424afbd0729618ea13cc9d079d29 class="flex justify-between"><a href=/docs/deep-learning/computer-vision/>Computer Vision</a></label><ul></ul></li><li><input type=checkbox id=section-200826a85c564b19fbb46b42ac917f3d class=toggle>
<label for=section-200826a85c564b19fbb46b42ac917f3d class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-6760b597543079e12cda6e2f8d441ad1 class=toggle>
<label for=section-6760b597543079e12cda6e2f8d441ad1 class="flex justify-between"><a role=button>Others</a></label><ul></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>LDA and QDA for Classification</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#linear-discriminant-analysis-for-classification>Linear Discriminant Analysis for Classification</a></li><li><a href=#quadratic-discrimination-analysis-for-classification>Quadratic Discrimination Analysis for Classification</a><ul><li><a href=#case-1--when-sigma_k--i>Case 1 : When <code>$\Sigma_{k} = I$</code></a></li><li><a href=#case-2--when-sigma_k-neq-i>Case 2 : When <code>$\Sigma_{k} \neq I$</code></a></li></ul></li><li><a href=#lda-and-qda-in-practice>LDA and QDA in practice</a></li><li><a href=#reference>Reference</a></li></ul></nav></aside></header><article class="markdown book-post"><h2>LDA and QDA for Classification</h2><div class="flex align-center text-small book-post-date"><img src=/svg/calendar.svg class=book-icon alt>
<span>May 13, 2021</span></div><div class=text-small><a href=/categories/data-science/>Data Science</a>,
<a href=/categories/machine-learning/>Machine Learning</a></div><div class=text-small><a href=/tags/machine-learning/>Machine Learning</a>,
<a href=/tags/lda/>LDA</a>,
<a href=/tags/qda/>QDA</a></div><div class=book-post-content><p>Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. <strong>Linear discriminant analysis (LDA)</strong> is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. <strong>Quadratic discriminant analysis (QDA)</strong> is a variant of LDA that allows for non-linear separation of data.</p><br><h2 id=linear-discriminant-analysis-for-classification>Linear Discriminant Analysis for Classification
<a class=anchor href=#linear-discriminant-analysis-for-classification>#</a></h2><p>LDA assumes that all classes are linearly separable and according to this multiple linear discrimination function representing several hyperplanes in the feature space are created to distinguish the classes. If there are two classes then the LDA draws one hyperplane and projects the data onto this hyperplane in such a way as to maximize the separation of the two categories. This hyperplane is created according to the two criteria considered simultaneously:</p><ul><li>Maximizing the distance between the means of two classes;</li><li>Minimizing the variation between each category.</li></ul><p>Suppose that <code>$Y \in \{1, ..., K\}$</code> is assigned a prior <code>$\hat{\pi}_{k}$</code> such that <code>$\sum_{i=1}^k \hat{\pi}_{k} = 1$</code>. According to Bayes’ rule, the posterior probability is</p><p><code>$$ P(Y=k |X=x)=\frac{f_{k}(x)\pi_{k}}{\sum_{i=1}^{K}f_{i}(x)\pi_{i}} \\ $$</code></p><p>where <code>$f_{k}(x)$</code> is the density of <code>$X$</code> conditioned on <code>$k$</code>. The Bayes Classifier can be expessed as:</p><p><code>$$ h^{*}(x) = \arg\max_{k}\{P(Y=k|X=x)\} = \arg\max_{k}\delta_{k}(x) \\ $$</code></p><p>For we assume that the random variable <code>$X$</code> is a vector <code>$X=(X_1,X_2,...,X_k)$</code> which is drawn from a <em>multivariate Gaussian</em> with class-specific mean vector and a common covariance matrix <code>$\Sigma \ (i.e. \Sigma_{k} = \Sigma, \forall k)$</code>. In other words the covariance matrix is common to all K classes: <code>$Cov(X)=\Sigma$</code> of shape <code>$d \times d$</code>.</p><p>Since <code>$x$</code> follows a multivariate Gaussian distribution, the probability <code>$P(X=x|Y=k)$</code> is given by: (<code>$\mu_k$</code> is the mean of inputs for category <code>$k$</code>)</p><p><code>$$ f_k(x)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k)) \\ $$</code></p><p>Then we can find the posterior distribution as:</p><p><code>$$ P(Y=k |X=x)=\frac{f_{k}(x)\pi_{k}}{P(X=x)} = C \cdot f_{k}(x)\pi_{k} \\ $$</code></p><p>Since <code>$P(X=x)$</code> does not depend on <code>$k$</code> so we write it as a constant. We will now proceed to expand and simplify the algebra, putting all constant terms into <code>$C,C^{'},C{''}$</code> etc..</p><p><code>$$ \begin{align*} p_{k}(x) = &amp;P(Y=k |X=x)=\frac{f_{k}(x)\pi_{k}}{P(X=x)} = C \cdot f_{k}(x)\pi_{k} \\ &=C \cdot \pi_{k} \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma^{-1}(x-\mu_k)) \\ &=C^{'} \cdot \pi_{k} exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma^{-1}(x-\mu_k)) \\ \end{align*} $$</code></p><p>Take the log of both sides:</p><p><code>$$ \begin{align*} logp_{k}(x) &=log(C^{'} \cdot \pi_{k} exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma^{-1}(x-\mu_k))) \\ &= logC^{'} + log\pi_k -\frac{1}{2}(x-\mu_k)^{T}\Sigma^{-1}(x-\mu_k) \\ &= logC^{'} + log\pi_k -\frac{1}{2}[(x^{T}\Sigma^{-1}x+\mu_{k}^{T}\Sigma^{-1}\mu_{k}]+x^{T}\Sigma^{-1}\mu_{k} \\ &= C^{''} + log\pi_k -\frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k}+x^{T}\Sigma^{-1}\mu_{k} \\ \end{align*} $$</code></p><p>And so the objective function, sometimes called the linear discriminant function or linear score function is:</p><p><code>$$ \delta_{k} = log\pi_k -\frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k}+x^{T}\Sigma^{-1}\mu_{k} \\ $$</code></p><p>Which means that given an input <code>$x$</code> we predict the class with the highest value of <code>$\delta_{k}(x)$</code>.</p><p>To find the Dicision Boundary, we will set <code>$P(Y=k|X=x) = P(Y=l|X=x)$</code> which is <code>$\delta_{k}(x) = \delta_{l}(x)$</code>:</p><p><code>$$ log\pi_k -\frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k}+x^{T}\Sigma^{-1}\mu_{k} = log\pi_l -\frac{1}{2}\mu_{l}^{T}\Sigma^{-1}\mu_{l}+x^{T}\Sigma^{-1}\mu_{l} \\ log\frac{\pi_{k}}{\pi_{l}} -\underbrace{\frac{1}{2}(\mu_{k}-\mu_{l})^{T}\Sigma^{-1}(\mu_{k}-\mu_{l})}_{\mathrm{Constant}}+\underbrace{x^{T}\Sigma^{-1}(\mu_{k}-\mu_{l})}_{\mathrm{Linear \ in} \ x} = 0 \\ \Rightarrow a^{T}x + b = 0 \\ $$</code></p><p>Which is a linear function in <code>$x$</code> - this explains why the decision boundaries are linear - hence the name Linear Discriminant Analysis.</p><br><h2 id=quadratic-discrimination-analysis-for-classification>Quadratic Discrimination Analysis for Classification
<a class=anchor href=#quadratic-discrimination-analysis-for-classification>#</a></h2><p>LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector, but a covariance matrix that is common to all <code>$K$</code> classes. <strong>Quadratic discriminant analysis</strong> provides an alternative approach by assuming that each class has its <strong>own covariance matrix <code>$\Sigma_{k}$</code></strong>.</p><p>To derive the quadratic score function, we return to the previous derivation, but now <code>$\Sigma_{k}$</code> is a function of <code>$k$</code>, so we cannot push it into the constant anymore.</p><p><code>$$ p_{k}(x) = \pi_{k}\frac{1}{(2\pi)^{d/2}|\Sigma_{k}|^{1/2}}exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k)) \\ $$</code></p><p><code>$$ \begin{align*} logp_{k}(x) &= C +log\pi_{k} - \frac{1}{2}log|\Sigma_{k}|-\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k) \\ &= C +log\pi_{k} - \frac{1}{2}log|\Sigma_{k}| -\frac{1}{2}x^{T}\Sigma_{k}^{-1}x +x^{T}\Sigma_{k}^{-1}\mu_{k} -\frac{1}{2}\mu_{k}^{T}\Sigma_{k}^{-1}\mu_{k} \\ \end{align*} $$</code></p><p>Which is a quadratic function of <code>$x$</code>. Under this less restrictive assumption, the classifier assigns an observation <code>$X=x$</code> to the class for which the quadratic score function is the largest:</p><p><code>$$ \delta_{k}(x) = log\pi_k- \frac{1}{2}log|\Sigma_{k}| -\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k) \\ $$</code></p><p>To find the Quadratic Dicision Boundary, we will set <code>$P(Y=k|X=x) = P(Y=l|X=x)$</code> which is <code>$\delta_{k}(x) = \delta_{l}(x)$</code>:</p><p><code>$$ log\pi_k- \frac{1}{2}log|\Sigma_{k}| -\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k) = log\pi_l- \frac{1}{2}log|\Sigma_{l}| -\frac{1}{2}(x-\mu_l)^{T}\Sigma_{l}^{-1}(x-\mu_l) \\ \frac{1}{2}x^{T}\underbrace{(\Sigma_{l}-\Sigma_{k})}_{A}x+\underbrace{(\mu_{k}^{T}\Sigma_{k}^{-1}-\mu_{l}^{T}\Sigma_{l}^{-1})}_{b^{T}}x +\underbrace{\frac{1}{2}(\mu_{k}-\mu_{l})^{T}\Sigma^{-1}(\mu_{k}-\mu_{l}) + log(\frac{\pi_{l}}{\pi_{k}}) + log(\frac{|\Sigma_{k}|^{1/2}}{|\Sigma_{l}|^{1/2}})}_{c} = 0 \\ \Rightarrow x^{T}Ax + b^{T}x + c = 0 \\ $$</code></p><h3 id=case-1--when-sigma_k--i>Case 1 : When <code>$\Sigma_{k} = I$</code>
<a class=anchor href=#case-1--when-sigma_k--i>#</a></h3><p>We first concider the case that <code>$\Sigma_{k} = I, \forall k$</code>. This is the case where each distribution is spherical, around
the mean point. Then we can have:</p><p><code>$$ \delta_{k}(x) = - \frac{1}{2}log|I| + log\pi_k -\frac{1}{2}(x-\mu_k)^{T}I(x-\mu_k) \\ $$</code></p><p>Where <code>$log(|I|) = log(1) = 0$</code> and <code>$(x-\mu_k)^{T}I(x-\mu_k) = (x-\mu_k)^{T}(x-\mu_k)$</code> is the <strong>Squared Euclidean Distance</strong> between two points <code>$x$</code> and <code>$\mu_{k}$</code>.</p><p>Thus under this condition (i.e. <code>$\Sigma = I$</code>) , a new point can be classified by <strong>its distance from the center of a class</strong>, adjusted by some prior. Further, for two-class problem with equal prior, the discriminating function would be the perpendicular bisector of the 2-class’s means.</p><h3 id=case-2--when-sigma_k-neq-i>Case 2 : When <code>$\Sigma_{k} \neq I$</code>
<a class=anchor href=#case-2--when-sigma_k-neq-i>#</a></h3><p>Since <code>$\Sigma_{k}$</code> is a symmetric matrix <code>$\Sigma_{k} = \Sigma_{k}^{T}$</code>, by using the <strong>Singular Value Decomposition (SVD)</strong> of <code>$\Sigma_{k}$</code>, we can get:</p><p><code>$$ \Sigma_{k} = U_{k}S_{k}U_{k}^{T} \\ \Sigma_{k}^{-1} = (U_{k}S_{k}U_{k}^{T})^{-1} = U_{k}S_{k}^{-1}U_{k}^{T}\\ $$</code></p><p>Then,</p><p><code>$$ \begin{align*} (x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k) &= (x-\mu_k)^{T}U_{k}S_{k}^{-1}U_{k}^{T}(x-\mu_k) \\ &= (U_{k}^{T}x-U_{k}^{T}\mu_k)^{T}S_{k}^{-1}(U_{k}^{T}x-U_{k}^{T}\mu_k) \\ &= (U_{k}^{T}x-U_{k}^{T}\mu_k)^{T}S_{k}^{-\frac{1}{2}}S_{k}^{-\frac{1}{2}}(U_{k}^{T}x-U_{k}^{T}\mu_k) \\ &= (S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k)^{T}I(S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k) \\ &= (S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k)^{T}(S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k) \\ \end{align*} $$</code></p><p>Which is also known as the <strong>Mahalanobis distance</strong>.</p><p>Think of <code>$S_{k}^{-\frac{1}{2}}U_{k}^{T}$</code> as a linear transformation that takes points in class <code>$k$</code> and distributes them spherically around a point, like in Case 1. Thus when we are given a new point, we can apply the modified <code>$\delta_{k}$</code> values to calculate <code>$h^{*}(x)$</code>. After applying the singular value decomposition, <code>$\Sigma_{k}^{-1}$</code> is considered to be an identity matrix such that:</p><p><code>$$ \delta_{k}(x) = - \frac{1}{2}log|I| + log\pi_k -\frac{1}{2}(S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k)^{T}(S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k) \\ $$</code></p><p>Where <code>$log(|I|) = log(1) = 0$</code></p><p>The difference between Case 1 and Case 2 (i.e. the difference between using the <strong>Euclidean and Mahalanobis distance</strong>) can be seen in the illustration below:</p><div align=center><img src=/img_ML/4_Euclidean_and_Mahalanobis_distance.gif width=600px/></div><br><h2 id=lda-and-qda-in-practice>LDA and QDA in practice
<a class=anchor href=#lda-and-qda-in-practice>#</a></h2><p>In practice we don’t know the parameters of Gaussian and will need to estimate them using our training data.</p><p><code>$$ \hat{\pi}_{k} = \hat{P}(y=k) = \frac{n_{k}}{n} \\ $$</code></p><p>where <code>$n_{k}$</code> is the number of class <code>$k$</code> observations.</p><p><code>$$ \hat{\mu}_{k} = \frac{1}{n_{k}}\sum_{i:y_{i}=k}x_{i} \\ $$</code></p><p><code>$$ \hat{\Sigma}_{k} = \frac{1}{n_{k}-k}\sum_{i:y_{i}=k}(x_{i}-\hat{\mu}_{k})(x_{i}-\hat{\mu}_{k})^{T} \\ $$</code></p><p>If we wish to use LDA we must calculate a <strong>common covariance</strong>, so we average all the covariances, e.g.</p><p><code>$$ \Sigma = \frac{\sum_{r=1}^k(n_{r}\Sigma_{r})}{\sum_{r=1}^k n_{r}} \\ $$</code></p><p>Where:</p><ul><li><p><code>$n_{r}$</code> is the number of data points in class <code>$r$</code>.</p></li><li><p><code>$\Sigma_{r}$</code> is the covariance of class <code>$r$</code> and <code>$n$</code> is the total number of data points.</p></li><li><p><code>$k$</code> is the number of classes.</p></li></ul><br><h2 id=reference>Reference
<a class=anchor href=#reference>#</a></h2><p>[1] Sicotte, X. B. (2018, June 22). Xavier Bourret Sicotte. Linear and Quadratic Discriminant Analysis - Data Blog. <a href=https://xavierbourretsicotte.github.io/LDA_QDA.html>https://xavierbourretsicotte.github.io/LDA_QDA.html</a>.</p></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#linear-discriminant-analysis-for-classification>Linear Discriminant Analysis for Classification</a></li><li><a href=#quadratic-discrimination-analysis-for-classification>Quadratic Discrimination Analysis for Classification</a><ul><li><a href=#case-1--when-sigma_k--i>Case 1 : When <code>$\Sigma_{k} = I$</code></a></li><li><a href=#case-2--when-sigma_k-neq-i>Case 2 : When <code>$\Sigma_{k} \neq I$</code></a></li></ul></li><li><a href=#lda-and-qda-in-practice>LDA and QDA in practice</a></li><li><a href=#reference>Reference</a></li></ul></nav></div></aside></main></body></html>