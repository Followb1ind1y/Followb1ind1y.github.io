<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Regression is a method of modelling a target value based on independent predictors. This method is mostly used for forecasting and finding out cause and effect relationship between variables. Regression techniques mostly differ based on the number of independent variables and the type of relationship between the independent and dependent variables. Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/posts/02_linear_regression/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Linear Regression"><meta property="og:description" content="Regression is a method of modelling a target value based on independent predictors. This method is mostly used for forecasting and finding out cause and effect relationship between variables. Regression techniques mostly differ based on the number of independent variables and the type of relationship between the independent and dependent variables. Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-08T00:00:00+00:00"><meta property="article:modified_time" content="2021-05-08T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Linear Regression"><title>Linear Regression | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/posts/02_linear_regression/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.c1d5d3b1459293346cbdd74835ddb9c3cd955d0b54070652d7a2a892624fadb7.js integrity="sha256-wdXTsUWSkzRsvddINd25w82VXQtUBwZS16KokmJPrbc=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li><li><a href=/docs/machine-learning/computational-performance/>Computational Performance</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><input type=checkbox id=section-d0dd931d60033c220ecd4cd60b7c9170 class=toggle>
<label for=section-d0dd931d60033c220ecd4cd60b7c9170 class="flex justify-between"><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a></label><ul><li><a href=/docs/deep-learning/convolutional-neural-networks/modern-convolutional-neural-networks/>Modern Convolutional Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-a3019bfa8037cc33ed6405d1589b6219 class=toggle>
<label for=section-a3019bfa8037cc33ed6405d1589b6219 class="flex justify-between"><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a></label><ul><li><a href=/docs/deep-learning/recurrent-neural-networks/modern-recurrent-neural-networks/>Modern Recurrent Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-0a43584c16258b228ae9aa8d70efc320 class=toggle>
<label for=section-0a43584c16258b228ae9aa8d70efc320 class="flex justify-between"><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a></label><ul><li><a href=/docs/deep-learning/attention-and-transformers/tokenization-and-word-embeddings/>Tokenization and Word Embeddings</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/transformer-architecture/>Transformer Architecture</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/large-scale-pretraining-with-transformers/>Large-Scale Pretraining with Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/modern-large-language-models/>Modern Large Language Models</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/post-training-large-language-models/>Post-training Large Language Models</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/multimodal-large-language-models/>Multimodal Large Language Models</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92e8358c45c96009753cf4227e9daea8 class=toggle>
<label for=section-92e8358c45c96009753cf4227e9daea8 class="flex justify-between"><a href=/docs/deep-learning/llm-pipelines/>LLM Pipelines</a></label><ul><li><a href=/docs/deep-learning/llm-pipelines/llm-hardware-and-model-size/>LLM Hardware and Model Size</a><ul></ul></li><li><a href=/docs/deep-learning/llm-pipelines/llm-inference-and-deployment/>LLM Inference and Deployment</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul><li><a href=/docs/others/interview-preparation-guide/>Interview Preparation Guide</a><ul></ul></li></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Linear Regression</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#linear-regression><strong>Linear Regression</strong></a><ul><li><a href=#gradient-descent-for-linear-regression><strong>Gradient Descent for Linear Regression</strong></a></li><li><a href=#normal-equation-for-linear-regression><strong>Normal Equation for Linear Regression</strong></a></li><li><a href=#evaluating-the-performance-of-the-linear-regression-model><strong>Evaluating the performance of the Linear Regression Model</strong></a></li></ul></li><li><a href=#references><strong>References</strong></a></li></ul></nav></aside></header><article class="markdown book-post"><h2>Linear Regression</h2><div class="flex align-center text-small book-post-date"><img src=/svg/calendar.svg class=book-icon alt>
<span>May 8, 2021</span></div><div class=text-small><a href=/categories/data-science/>Data Science</a>,
<a href=/categories/machine-learning/>Machine Learning</a></div><div class=text-small><a href=/tags/machine-learning/>Machine Learning</a>,
<a href=/tags/linear-regression/>Linear Regression</a></div><div class=book-post-content><p>Regression is a method of modelling a target value based on independent predictors. This method is mostly used for forecasting and finding out <strong>cause and effect relationship between variables</strong>. Regression techniques mostly differ based on the number of independent variables and the type of relationship between the independent and dependent variables. Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on.</p><div align=center><img src=/posts/images/2_Regression.SVG width=500px/></div><br><h2 id=linear-regression><strong>Linear Regression</strong>
<a class=anchor href=#linear-regression>#</a></h2><p>Linear regression with multiple variables is also known as &ldquo;<strong>multivariate linear regression</strong>&rdquo;. The multivariable form of the hypothesis function accommodating these multiple features is as follows:</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[
\begin{align*}
&\mathrm{Hypothesis}: h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} + \cdot\cdot\cdot + \theta_{n}x_{n} \\
&\mathrm{Cost \ Function}: J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2 \\
&\mathrm{Goal}: \min_{\theta}J(\theta) \\
\end{align*}
\]</span><p>Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:</p><span>\[
h_{\theta}(x) =
\begin{bmatrix}
\theta_{0} & \theta_{1} & \cdot\cdot\cdot & \theta_{n}
\end{bmatrix}
\begin{bmatrix}
x_{0} \\
x_{1} \\
\cdot\cdot\cdot \\
x_{n}
\end{bmatrix}
= \theta^{T}x \\
\]</span><h3 id=gradient-descent-for-linear-regression><strong>Gradient Descent for Linear Regression</strong>
<a class=anchor href=#gradient-descent-for-linear-regression>#</a></h3><p>Gradient descent is a generic optimization algorithm used in many machine learning algorithms. It iteratively tweaks the parameters of the model in order to minimize the cost function. We do this is by taking the <strong>derivative</strong> (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter <strong><span>\(\alpha\)
</span></strong>, which is called the <strong>learning rate</strong>. The <strong>gradient descent algorithm</strong> can be represented as:</p><span>\[
\begin{align*}
&\frac{\partial J(\theta)}{\partial \theta_{0}}= \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}) \\
&\frac{\partial J(\theta)}{\partial \theta_{j}}= \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} \\
\end{align*}
\]
</span><span>\[
\begin{bmatrix}
\frac{\partial J(\theta)}{\partial \theta_{0}} \\
\frac{\partial J(\theta)}{\partial \theta_{1}} \\
\cdot\cdot\cdot \\
\frac{\partial J(\theta)}{\partial \theta_{n}}
\end{bmatrix}=\frac{1}{m}x^{T}(h_{\theta}(x)-y) \\
\]
</span><span>\[
\begin{bmatrix}
\theta_{0} \\
\theta_{1} \\
\cdot\cdot\cdot \\
\theta_{n}
\end{bmatrix}=\begin{bmatrix}
\theta_{0} \\
\theta_{1} \\
\cdot\cdot\cdot \\
\theta_{n}
\end{bmatrix}-\alpha\begin{bmatrix}
\frac{\partial J(\theta)}{\partial \theta_{0}} \\
\frac{\partial J(\theta)}{\partial \theta_{1}} \\
\cdot\cdot\cdot \\
\frac{\partial J(\theta)}{\partial \theta_{n}}
\end{bmatrix} \\
\]
</span><span>\[
\begin{align*}
&\mathrm{Repect\ until \ convergence} \{ \\
&\ \ \ \ \theta_{j}^{new} :=\theta_{j}^{old} - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}) \ \ \ \ \mathrm{for} \ j = 0 \cdot\cdot\cdot n \\
&\} \\
\end{align*}
\]</span><p>To demonstrate the gradient descent algorithm, we initialize the model parameters with 0. The equation becomes <span>\(h_{\theta}(x) = 0\)
</span>. Gradient descent algorithm now tries to update the value of the parameters so that we arrive at the best fit line.</p><p>We should adjust our parameter <span>\(\alpha\)
</span>to ensure that the gradient descent algorithm converges in a reasonable time. <strong>If <span>\(\alpha\)
</span>is too small</strong>, gradient descent can be slow. <strong>If <span>\(\alpha\)
</span>is too large</strong>, gradient descent can overshoot the minimum. It may fail to converge, or even diverge. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.</p><div align=center><img src=/posts/images/2_Gradient_Descent.PNG width=400px/></div><p>We can <strong>speed up gradient descent</strong> by having each of our input values in roughly the same range. This is because <span>\(\theta\)
</span>will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p><p>Two techniques to help with this are feature scaling and mean normalization. <strong>Feature scaling</strong> involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. <strong>Mean normalization</strong> involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:</p><span>\[
x_{i} := \frac{(x_{i}-\mu_{i})}{s_{i}} \\
\]</span><p>Where <span>\(\mu_{i}\)
</span>is the average of all the values for feature (i) and <span>\(s_{i}\)
</span>is the range of values (max - min), or <span>\(s_{i}\)
</span>is the standard deviation. Note that dividing by the range, or dividing by the standard deviation, give different results.</p><h3 id=normal-equation-for-linear-regression><strong>Normal Equation for Linear Regression</strong>
<a class=anchor href=#normal-equation-for-linear-regression>#</a></h3><p>Gradient descent gives one way of minimizing J. In the &ldquo;<strong>Normal Equation</strong>&rdquo; method, we will minimize J by explicitly taking its derivatives with respect to the <span>\(\theta_{j}\)
</span>’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:</p><span>\[
\theta = (X^{T}X)^{-1}X^{T}y \\
\]</span><p>The following is a comparison of gradient descent and the normal equation:</p><table><thead><tr><th style=text-align:left><strong>Gradient Descent</strong></th><th style=text-align:left><strong>Normal Equation</strong></th></tr></thead><tbody><tr><td style=text-align:left>Need to choose alpha</td><td style=text-align:left>No need to choose alpha</td></tr><tr><td style=text-align:left>Needs many iterations</td><td style=text-align:left>No need to iterate</td></tr><tr><td style=text-align:left><span>\(O(kn^{2})\)</span></td><td style=text-align:left><span>\(O(n^{3})\)
</span>, need to calculate inverse of <span>\(X^{T}X\)</span></td></tr><tr><td style=text-align:left>Works well when n is large</td><td style=text-align:left>Slow if n is very large</td></tr></tbody></table><p>With the normal equation, computing the inversion has complexity <span>\(O(n^{3})\)
</span>. So if we have a very large number of features, the normal equation will be <strong>slow</strong>. In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.</p><p>If <span>\(X^{T}X\)
</span>is <strong>noninvertible</strong>, the common causes might be having :</p><ul><li>Redundant features, where two features are very closely related (i.e. they are linearly dependent)</li><li>Too many features (e.g. <span>\(m ≤ n\)
</span>). In this case, delete some features or use &ldquo;regularization&rdquo;.</li></ul><p>Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.</p><h3 id=evaluating-the-performance-of-the-linear-regression-model><strong>Evaluating the performance of the Linear Regression Model</strong>
<a class=anchor href=#evaluating-the-performance-of-the-linear-regression-model>#</a></h3><p>We will be using <strong>Root mean squared error(RMSE)</strong> and <strong>Coefficient of Determination(R² score)</strong> to evaluate our model. RMSE is the square root of the average of the sum of the squares of residuals. RMSE is defined by:</p><span>\[
RMSE = \sqrt{\frac{1}{m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2} \\
\]</span><p><span>\(R^{2}\)
</span>score or the coefficient of determination explains how much the total variance of the dependent variable can be reduced by using the least square regression. It can be defined by:</p><span>\[
R^{2} = 1- \frac{SS_{r}}{SS{t}} \\
\]</span><p>Where <span>\(SS_{t}\)
</span>is the total sum of errors if we take the mean of the observed values as the predicted value and <span>\(SS_{r}\)
</span>is the sum of the square of residuals.</p><span>\[
\begin{align*}
SS_{t} &= \sum_{i=1}^{m}(y^{(i)}-\bar{y})^2 \\
SS_{r} &= \sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2 \\
\end{align*}
\]</span><br><h2 id=references><strong>References</strong>
<a class=anchor href=#references>#</a></h2><p>[1] Agarwal, A. (2018, November 14). Linear Regression using Python. Medium. <a href=https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2>https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2</a>.</p><p>[2] Ng, A. (n.d.). Machine Learning. Coursera. <a href=https://www.coursera.org/learn/machine-learning>https://www.coursera.org/learn/machine-learning</a>.</p><br><div align=center><a href=/posts/ class=book-btn>Blog</a>
<a href=/ class=book-btn>Home</a></div></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#linear-regression><strong>Linear Regression</strong></a><ul><li><a href=#gradient-descent-for-linear-regression><strong>Gradient Descent for Linear Regression</strong></a></li><li><a href=#normal-equation-for-linear-regression><strong>Normal Equation for Linear Regression</strong></a></li><li><a href=#evaluating-the-performance-of-the-linear-regression-model><strong>Evaluating the performance of the Linear Regression Model</strong></a></li></ul></li><li><a href=#references><strong>References</strong></a></li></ul></nav></div></aside></main></body></html>