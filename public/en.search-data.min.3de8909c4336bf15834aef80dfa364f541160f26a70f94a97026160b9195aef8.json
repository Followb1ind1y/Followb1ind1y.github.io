[{"id":0,"href":"/posts/02_linear_regression/","title":"Linear Regression","section":"Blog","content":"Regression is a method of modelling a target value based on independent predictors. This method is mostly used for forecasting and finding out cause and effect relationship between variables. Regression techniques mostly differ based on the number of independent variables and the type of relationship between the independent and dependent variables. Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on.\nLinear Regression # Linear regression with multiple variables is also known as \u0026ldquo;multivariate linear regression\u0026rdquo;. The multivariable form of the hypothesis function accommodating these multiple features is as follows:\n\\[ \\begin{align*} \u0026\\mathrm{Hypothesis}: h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\theta_{3}x_{3} + \\cdot\\cdot\\cdot + \\theta_{n}x_{n} \\\\ \u0026\\mathrm{Cost \\ Function}: J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2 \\\\ \u0026\\mathrm{Goal}: \\min_{\\theta}J(\\theta) \\\\ \\end{align*} \\] Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:\n\\[ h_{\\theta}(x) = \\begin{bmatrix} \\theta_{0} \u0026 \\theta_{1} \u0026 \\cdot\\cdot\\cdot \u0026 \\theta_{n} \\end{bmatrix} \\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ \\cdot\\cdot\\cdot \\\\ x_{n} \\end{bmatrix} = \\theta^{T}x \\\\ \\] Gradient Descent for Linear Regression # Gradient descent is a generic optimization algorithm used in many machine learning algorithms. It iteratively tweaks the parameters of the model in order to minimize the cost function. We do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter \\(\\alpha\\) , which is called the learning rate. The gradient descent algorithm can be represented as:\n\\[ \\begin{align*} \u0026\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}}= \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)}) \\\\ \u0026\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}= \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} \\\\ \\end{align*} \\] \\[ \\begin{bmatrix} \\frac{\\partial J(\\theta)}{\\partial \\theta_{0}} \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{1}} \\\\ \\cdot\\cdot\\cdot \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{n}} \\end{bmatrix}=\\frac{1}{m}x^{T}(h_{\\theta}(x)-y) \\\\ \\] \\[ \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\cdot\\cdot\\cdot \\\\ \\theta_{n} \\end{bmatrix}=\\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\cdot\\cdot\\cdot \\\\ \\theta_{n} \\end{bmatrix}-\\alpha\\begin{bmatrix} \\frac{\\partial J(\\theta)}{\\partial \\theta_{0}} \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{1}} \\\\ \\cdot\\cdot\\cdot \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{n}} \\end{bmatrix} \\\\ \\] \\[ \\begin{align*} \u0026\\mathrm{Repect\\ until \\ convergence} \\{ \\\\ \u0026\\ \\ \\ \\ \\theta_{j}^{new} :=\\theta_{j}^{old} - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}) \\ \\ \\ \\ \\mathrm{for} \\ j = 0 \\cdot\\cdot\\cdot n \\\\ \u0026\\} \\\\ \\end{align*} \\] To demonstrate the gradient descent algorithm, we initialize the model parameters with 0. The equation becomes \\(h_{\\theta}(x) = 0\\) . Gradient descent algorithm now tries to update the value of the parameters so that we arrive at the best fit line.\nWe should adjust our parameter \\(\\alpha\\) to ensure that the gradient descent algorithm converges in a reasonable time. If \\(\\alpha\\) is too small, gradient descent can be slow. If \\(\\alpha\\) is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.\nWe can speed up gradient descent by having each of our input values in roughly the same range. This is because \\(\\theta\\) will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\nTwo techniques to help with this are feature scaling and mean normalization. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:\n\\[ x_{i} := \\frac{(x_{i}-\\mu_{i})}{s_{i}} \\\\ \\] Where \\(\\mu_{i}\\) is the average of all the values for feature (i) and \\(s_{i}\\) is the range of values (max - min), or \\(s_{i}\\) is the standard deviation. Note that dividing by the range, or dividing by the standard deviation, give different results.\nNormal Equation for Linear Regression # Gradient descent gives one way of minimizing J. In the \u0026ldquo;Normal Equation\u0026rdquo; method, we will minimize J by explicitly taking its derivatives with respect to the \\(\\theta_{j}\\) ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:\n\\[ \\theta = (X^{T}X)^{-1}X^{T}y \\\\ \\] The following is a comparison of gradient descent and the normal equation:\nGradient Descent Normal Equation Need to choose alpha No need to choose alpha Needs many iterations No need to iterate \\(O(kn^{2})\\) \\(O(n^{3})\\) , need to calculate inverse of \\(X^{T}X\\) Works well when n is large Slow if n is very large With the normal equation, computing the inversion has complexity \\(O(n^{3})\\) . So if we have a very large number of features, the normal equation will be slow. In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.\nIf \\(X^{T}X\\) is noninvertible, the common causes might be having :\nRedundant features, where two features are very closely related (i.e. they are linearly dependent) Too many features (e.g. \\(m ≤ n\\) ). In this case, delete some features or use \u0026ldquo;regularization\u0026rdquo;. Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.\nEvaluating the performance of the Linear Regression Model # We will be using Root mean squared error(RMSE) and Coefficient of Determination(R² score) to evaluate our model. RMSE is the square root of the average of the sum of the squares of residuals. RMSE is defined by:\n\\[ RMSE = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2} \\\\ \\] \\(R^{2}\\) score or the coefficient of determination explains how much the total variance of the dependent variable can be reduced by using the least square regression. It can be defined by:\n\\[ R^{2} = 1- \\frac{SS_{r}}{SS{t}} \\\\ \\] Where \\(SS_{t}\\) is the total sum of errors if we take the mean of the observed values as the predicted value and \\(SS_{r}\\) is the sum of the square of residuals.\n\\[ \\begin{align*} SS_{t} \u0026= \\sum_{i=1}^{m}(y^{(i)}-\\bar{y})^2 \\\\ SS_{r} \u0026= \\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2 \\\\ \\end{align*} \\] References # [1] Agarwal, A. (2018, November 14). Linear Regression using Python. Medium. https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2.\n[2] Ng, A. (n.d.). Machine Learning. Coursera. https://www.coursera.org/learn/machine-learning.\nBlog Home "},{"id":1,"href":"/docs/machine-learning/machine-learning-basics/","title":"Machine Learning Basics","section":"Machine Learning","content":" Machine Learning Basics # 机器学习的定义与类型 # 定义 # Machine Learning（机器学习） 是一种人工智能技术，核心目标是使计算机能够从数据中自动学习，并根据这些数据改进对特定任务的性能，而无需明确的编程指令。\n机器学习通过以下三个要素实现：\n数据（Data）：输入的原始数据或特征数据。 模型（Model）：表示学习的假设空间，决定如何从数据中学习模式。 优化目标（Object）：定义如何评估模型好坏并改进其性能（如最小化误差）。 主要类型 # 监督学习 (Supervised Learning) # 监督学习 (Supervised Learning) 从标记数据中学习。在这种情况下，输入数据及其对应的输出值被提供给算法，算法学习输入和输出值之间的映射（the mapping between the input and output values）。监督学习的目标是对新的、看不见的输入数据做出准确的预测。\n输入（Input）：特征数据 \\(X\\) 和目标变量 \\(y\\) （如标签或真实值）。\n输出（Output）：预测模型，用于对新数据进行分类或回归。\n应用场景：\n分类问题（Classification）：将输入数据划分到预定义类别中。 回归问题（Regression）：预测连续数值的目标变量。 无监督学习 (Unsupervised Learning) # 无监督学习 (Unsupervised Learning) 从未标记的数据中学习。在这种情况下，输入数据没有标记，算法自己学习在数据中寻找模式和结构。无监督学习的目标是发现数据中隐藏的模式和结构。\n输入（Input）：仅有特征数据 \\(X\\) 。 输出（Output）：数据的潜在结构或表示。 应用场景： 聚类 (Clustering)：将数据分组到不同簇中。 降维 (Dimensionality Reduction)：简化数据表示，同时保留主要信息。 半监督学习 (Semi-Supervised Learning) # 半监督学习 (Semi-Supervised Learning) 从标记和未标记数据的组合中进行学习。在这种情况下，算法学习在未标记数据中寻找模式和结构，并使用标记数据来指导学习过程。\n输入（Input）：部分标注的数据和大量未标注的数据。 输出（Output）：用于分类或回归的预测模型。 应用场景： 在标注数据有限或获取标签成本高昂的情况下（如医学影像标注）。 强化学习 (Reinforcement Learning) # 强化学习 (Reinforcement Learning) 通过与 环境（environment） 交互进行学习。在这种情况下，算法学习采取 行动（action） 来最大化环境提供的奖励信号。强化学习的目标是学习最大化长期 奖励（reward） 的 策略（policy）。\n输入（Input）：状态 \\(S\\) 、动作 \\(A\\) 、奖励 \\(R\\) 。 输出（Output）：一个策略 \\(\\pi\\) ，指引在不同状态下的最佳行动。 应用场景： 游戏 AI：如 AlphaGo 使用强化学习在围棋中击败人类选手。 机器人导航：训练机器人在环境中找到最佳路径。 泛化能力（Generalization） # 机器学习的核心挑战在于，我们必须在新的、以前未见过的输入上表现良好——而不仅仅是我们的模型所训练的输入。在以前未观察到的输入上表现良好的能力称为泛化（generalization）。在训练过程中，我们通过降低 训练误差 (Training Error) 优化模型。但模型不仅需要在训练集上表现优异，还需要降低泛化误差 (Generalization Error)，即 测试误差（Test Error）。\n数据集（Datasets）分类和误差（Errors） # 训练集（Train Set） # 定义：训练集是模型学习的主要数据来源（占大多数），包括 输入特征 和对应的 目标输出（对于监督学习）。 功能：用于训练模型（的参数），通过优化算法最小化训练误差。 训练误差（Training Error）：训练误差是模型在训练数据上的错误率。它是通过测量每个训练示例的预测输出（predicted output）与实际输出（actual output）之间的差异来计算的。由于模型是在此数据上训练的，因此预计它会在此数据上表现良好，并且训练误差通常较低。\n验证集（Validation Set） # 定义：验证集是从训练数据中分离出来的一部分，用于评估模型在未见数据上的表现。 功能： 帮助调整超参数（如学习率、正则化系数、模型结构等）。 用于选择最佳模型，例如在多次训练后选择验证误差最低的模型。 验证误差（Validation Error）：验证误差是模型在验证数据上的错误率。用于评估训练期间模型的性能，目标是找到验证误差最低的模型。\n测试集（Test Set） # 定义：测试集是完全独立于训练和验证的数据，模型在训练和验证过程中从未接触过。 功能：用于评估模型的最终泛化性能，反映模型在实际场景中的表现。 测试误差（Test Error）：测试误差是模型在测试数据上的错误率。测试数据是与训练和验证数据完全独立的数据集，用于评估模型的最终性能。测试误差是 最重要的误差指标，因为它告诉我们模型在新的、未见过的数据上的表现如何。\n欠拟合 (Underfitting) # 定义：当模型过于简单而无法捕捉数据中的底层模式时，就会发生欠拟合。该模型具有高偏差和低方差，这意味着它在训练和测试数据上的表现都很差。 表现： 模型复杂度低，无法很好地拟合训练数据。 训练误差与测试误差都较大，模型性能较差。 原因： 模型过于简单，无法学习到数据中的复杂关系或特征。 特征不足，数据无法充分表达问题。 训练时间不足，模型未完全收敛。 解决方法： 增加模型复杂度：选择更复杂的模型（如从线性模型切换到非线性模型）。 增加特征：引入更多特征或通过特征工程提取更有效的特征。 延长训练时间：确保模型充分训练直到收敛。 过拟合 (Overfitting) # 定义：当模型过于复杂，与训练数据的拟合度过高时，就会发生过度拟合，从而捕获数据中的噪声和随机波动。因此，该模型在新的、未见过的数据上表现不佳。 表现： 模型对训练数据拟合良好，训练误差很低。 测试误差较高，模型泛化能力差。 原因： 模型复杂度过高，学习到了训练数据中的噪声或无意义模式。 训练数据过少，噪声占比高。 缺乏正则化约束，模型自由度太高。 解决方法： 减少模型复杂度：降低模型自由度（如减少神经网络层数或节点数）。 增加数据量：收集更多样本，减少模型对噪声的敏感性。 正则化： \\(L_1\\) 正则化：鼓励稀疏性，减少不重要的参数。 \\(L_2\\) 正则化：限制参数的幅度，防止过大权重。 交叉验证：通过交叉验证选择模型或超参数，避免过拟合。 偏差-方差权衡（Bias-Variance Tradeoﬀ） # 偏差 (Bias) # 定义：偏差衡量模型预测值的期望值与真实值之间的偏离程度。 公式： \\[ \\text{Bias} = E[f(x)] - f^*(x) \\] \\(f(x)\\) ：模型的预测值 \\(f^(x)\\) ：真实值或目标函数 方差 (Variance) # 定义：方差衡量模型在不同训练数据集上的预测值的变化幅度。 公式： \\[ \\text{Variance} = E[(f(x) - E[f(x)])^2] \\] 方差的平方根称为标准差，表示为 \\(SE(x)\\) 总误差分解 # 模型的总误差（Expected Error）可以分解为三部分：偏差、方差和噪声。在实际应用中，我们需要平衡 Bias 和 Variance 以此来找到最小的 Expected Error，目标是找到偏差和方差的最佳平衡点，既能保证低训练误差，又能有良好的泛化能力。\n\\[ \\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise} \\] \\(\\text{Bias}^2\\) : 表示系统误差，与模型的表达能力有关。 \\(\\text{Variance}\\) : 表示模型对训练数据的敏感程度。 \\(\\text{Irreducible Noise}\\) : 数据中固有的随机噪声，无法通过任何模型降低。 Bias-Variance Tradeoff 理解 # 偏差-方差权衡指的是模型很好地拟合训练数据的能力（低偏差） 与其推广到新数据的能力（低方差） 之间的权衡。\n随着模型复杂度的增加，偏差趋于减小，方差趋于增大。如果模型太简单，它可能具有高偏差，这意味着它无法捕捉数据中的潜在模式，训练误差和测试误差会很高。如果模型太复杂，它可能具有高方差，这意味着它对训练数据中的噪声过于敏感，并且可能无法很好地推广到新数据，从而导致过度拟合。为了在偏差和方差之间取得平衡，我们需要找到模型的最佳复杂度。\n低Bias但高Variance：复杂模型，过度拟合，表现为训练误差低但测试误差高。 高Bias但低Variance：简单模型，欠拟合，表现为训练误差和测试误差都高。 选择模型评估方法 # 交叉验证 (Cross-Validation)：通过分割数据集来更好地估计模型的偏差和方差。 学习曲线 (Learning Curve)：观察训练集误差和验证集误差随样本数量或模型复杂度变化的趋势，帮助分析模型的偏差和方差问题。 交叉验证（Cross Validation） # 交叉验证是机器学习中用于评估模型在独立数据集上性能的一种技术。交叉验证的基本思想是将可用数据分成两个或多个部分，其中一个部分用于训练模型，另一个部分用于验证模型。交叉验证用于通过提供模型对新数据的泛化程度的估计来防止过度拟合。它有效解决了仅用单一验证集或测试集可能导致的评估结果不稳定或偏差的问题。它也可以用于调整模型的超参数。\nK折交叉验证 (K-Fold Cross-Validation) # 将数据集划分为 K 个不重叠的子集（folds）。每次取一个子集作为验证集，其余 K-1 个子集作为训练集。重复 K 次，每次更换验证集，最终对所有验证结果取平均。 在 Cross-Validation 的框架内，调节模型的 hyperparameters，对使用不同参数的模型进行 Cross-Validation 验证。根据最终结果选择最佳模型。 在选择好最佳的 hyperparameters 和模型后，用全部的数据进行训练。 使用完全独立的 Testset 来评估模型的泛化能力。 优缺点： # 优点：可靠性高，适合数据量较大的情况。 缺点：当 K 较大时，计算成本较高。 K-Fold Cross-Validation 代码实现： # 留一法 (Leave-One-Out Cross-Validation, LOOCV) # 数据集中每个样本单独作为一次验证集，其余样本作为训练集。 模型训练次数等于样本数 N，最后计算所有验证集的误差平均值。 优缺点： # 优点：不浪费数据，最全面的评估方法。 缺点：计算代价极高，尤其是数据集较大时。 常见的机器学习流程 # "},{"id":2,"href":"/docs/deep-learning/perceptrons-and-neural-network/","title":"Perceptrons and Neural Network","section":"Deep Learning","content":" Perceptrons and Neural Network # "},{"id":3,"href":"/posts/03_logistic_regression/","title":"Logistic Regression","section":"Blog","content":"Binary Logistic Regression is one of the most simple and commonly used Machine Learning algorithms for two-class classification. It is easy to implement and can be used as the baseline for any binary classification problem. Its basic fundamental concepts are also constructive in deep learning. Logistic regression describes and estimates the relationship between one dependent binary variable and independent variables.\n\u0026ldquo;分类\u0026quot;是应用 逻辑回归(Logistic Regression) 的目的和结果, 但中间过程依旧是\u0026quot;回归\u0026rdquo;. 通过逻辑回归模型, 我们得到的计算结果是0-1之间的连续数字, 可以把它称为\u0026quot;可能性\u0026quot;（概率）. 然后, 给这个可能性加一个阈值, 就成了分类. 例如, 可能性大于 0.5 即记为 1, 可能性小于 0.5 则记为 0.\nThe classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which \\(y\\) can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.)\n\\[ log\\frac{p}{1-p} = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{3} + \\cdot\\cdot\\cdot + \\beta_{n}x_{n} = \\beta^{T}x \\\\ \\] \\[ \\begin{align*} P(y = 1) \u0026= p = \\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x}} \\\\ P(y = 0) \u0026= 1 - p = \\frac{1}{1+e^{\\beta^{T}x}} \\end{align*} \\] We could approach the classification problem ignoring the fact that \\(y\\) is discrete-valued, and use our old linear regression algorithm to try to predict \\(y\\) given \\(x\\) . However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn’t make sense for \\(h_{\\theta}(x)\\) to take values larger than 1 or smaller than 0 when we know that \\(y \\in {0, 1}\\) . To fix this, let’s change the form for our hypotheses \\(h_{\\theta}(x)\\) to satisfy \\(0 \\leq h_{\\theta}(x) \\leq 1\\) This is accomplished by plugging \\(\\theta^{T}x\\) into the Logistic Function. Our new form uses the \u0026ldquo;Sigmoid Function,\u0026rdquo; also called the \u0026ldquo;Logistic Function\u0026rdquo;:\n\\[ f(x) = \\frac{1}{1+e^{-(x)}} \\\\ \\] Logistic Regression # First we need to define a Probability Mass Function:\n\\[ \\begin{align*} \u0026\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P(Y=1|X=x) = \\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x}} \\\\ \u0026\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P(Y=0|X=x) = 1 - \\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x}} = \\frac{1}{1+e^{\\beta^{T}x}} \\\\ \u0026\\Rightarrow \\ \\ \\ \\ P(Y \\ |X=x_{i}) = (\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}})^{y_{i}} (\\frac{1}{1+e^{\\beta^{T}x_{i}}})^{1-y_{i}} \\\\ \\end{align*} \\] Naturally, we want to maximize the right-hand-side of the above statement. We will use Maximun Likelihood Estimation(MLE) to find \\(\\beta\\) :\n\\[ \\hat{\\beta}_{MLE}= \\arg\\max_{\\beta} L(\\beta) \\\\ \\] \\[ L(\\beta) = \\prod_{i=1}^n P(Y=y_{i} |x_{i}) = \\prod_{i=1}^n (\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}})^{y_{i}} (\\frac{1}{1+e^{\\beta^{T}x_{i}}})^{1-y_{i}} \\\\ \\] \\[ \\begin{align*} l(\\beta) = log\\ L(\\beta) \u0026= \\sum_{i=1}^n y_{i}\\left[\\beta^{T}x_{i} - log(1+e^{\\beta^{T}x_{i}})] + (1-y_{i})[-log(1+e^{\\beta^{T}x_{i}})\\right] \\\\ \u0026=\\sum_{i=1}^n y_{i}\\beta^{T}x_{i}- log(1+e^{\\beta^{T}x_{i}}) \\\\ \\end{align*} \\] Newton‐Raphson Method for Binary Logistic Regression # Newton’s Method is an iterative equation solver: it is an algorithm to find the roots of a convex function. Equivalently, the method solves for root of the derivative of the convex function. The idea behind this method is ro use a quadratic approximate of the convex function and solve for its minimum at each step. For a convex function \\(f(x)\\) , the step taken in each iteration is \\(-(\\nabla^{2}f(x))^{-1}\\nabla f(x)\\) . while \\(\\lVert\\nabla f(\\beta)\\rVert \u003e \\varepsilon\\) :\n\\[ \\beta^{new} = \\beta^{old}-(\\nabla^{2}f(x))^{-1}\\nabla f(x) \\\\ \\] Where \\(\\nabla f(x)\\) is the Gradient of \\(f(x)\\) and \\(\\nabla^{2} f(x)\\) is the Hessian Matrix of \\(f(x)\\) .\n\\[ \\begin{align*} \\nabla f(x) = \\frac{\\partial l}{\\partial \\beta} \u0026= \\sum_{i=1}^n y_{i}x_{i}- (\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}})\\cdot x_{i}^{T} \\\\ \u0026= \\sum_{i=1}^n (y_{i}- \\underbrace{\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}}}_{p_{i}})\\cdot x_{i}^{T} = X(y-p) \\\\ \\end{align*} \\] \\[ \\nabla^{2}f(x) = \\frac{\\partial^{2} l}{\\partial \\beta \\partial \\beta^{T}} = \\sum_{i=1}^n - \\underbrace{\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}}}_{p_{i}}\\cdot \\underbrace{\\frac{1}{1+e^{\\beta^{T}x_{i}}}}_{1-p_{i}}x_{i} \\cdot x_{i}^{T} = -XWX^{T} \\\\ \\] Where \\(W\\) is a diagonal \\((n,n)\\) matrix with the \\(i^{th}\\) diagonal element defined as\n\\[ W = \\begin{bmatrix} p_{i}(1-p_{i}) \u0026 \u0026 \\\\ \u0026 \\ddots \u0026 \u0026 \\\\ \u0026 \u0026 \u0026 \\\\ \\end{bmatrix}_{\\ n x n} \\\\ \\] The Newton‐Raphson algorithm can now be expressed as:\n\\[ \\begin{align*} \\beta^{new} \u0026= \\beta^{old}-(\\nabla^{2}f(x))^{-1}\\nabla f(x) \\\\ \u0026= \\beta^{old}+ (XWX^{T})^{-1}X(y-p) \\\\ \u0026= \\beta^{old}+ (XWX^{T})^{-1}[XWX^{T}\\beta^{t}+ X(y-p)] \\\\ \u0026= \\beta^{old}+ (XWX^{T})^{-1}XWZ \\\\ \\end{align*} \\] Where \\(Z\\) can be expressed as: \\(Z = X^{T}\\beta^{t}+ W^{-1}(y-p) \\) . This algorithm is also known as Iteratively Reweighted Least Squares(IRLS).\n\\[ \\beta^{t+1} = \\arg\\min_{\\beta}(Z - X\\beta)^{T}W(Z-X\\beta) \\\\ \\] Other types of Logistic Regression # Multinomial Logistic Regression # Three or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan).\nOrdinal Logistic Regression # Three or more categories with ordering. Example: Movie rating from 1 to 5.\nReferences # [1] K, D. (2021, April 8). Logistic Regression in Python using Scikit-learn. Medium. https://heartbeat.fritz.ai/logistic-regression-in-python-using-scikit-learn-d34e882eebb1.\n[2] Li, S. (2019, February 27). Building A Logistic Regression in Python, Step by Step. Medium. https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8.\n[3] Christian. (2020, September 17). Plotting the decision boundary of a logistic regression model. https://scipython.com/blog/plotting-the-decision-boundary-of-a-logistic-regression-model/.\nBlog Home "},{"id":4,"href":"/docs/deep-learning/convolutional-neural-networks/","title":"Convolutional Neural Networks","section":"Deep Learning","content":" Convolutional Neural Networks # "},{"id":5,"href":"/docs/machine-learning/data-preprocessing/","title":"Data Preprocessing","section":"Machine Learning","content":" Data Preprocessing # "},{"id":6,"href":"/docs/common-libraries/numpy/","title":"NumPy","section":"Common Libraries","content":" NumPy # "},{"id":7,"href":"/docs/python-basics/python-fundamentals/","title":"Python Fundamentals","section":"Python Basics","content":" Python Fundamentals # 列表 (Lists) # 列表 (Lists) 是 有序的 (ordered)、可变的 (mutable) 值集合，这些值以逗号分隔并用方括号括起来。列表可以由许多不同类型的变量组成。\n# Creating a list x = [3, \u0026#34;hello\u0026#34;, 1.2] print (x) [3, \u0026#39;hello\u0026#39;, 1.2] 我们可以使用 append 函数将新的值添加到 列表 (Lists) 中：\n# Adding to a list x.append(7) print (x) print (len(x)) [3, \u0026#39;hello\u0026#39;, 1.2, 7] 4 或者直接替换现有的值：\n# Replacing items in a list x[1] = \u0026#34;bye\u0026#34; print (x) [3, \u0026#39;bye\u0026#39;, 1.2, 7] 并可以直接对 列表 (list) 执行操作：\n# Operations y = [2.4, \u0026#34;world\u0026#34;] z = x + y print (z) [3, \u0026#39;bye\u0026#39;, 1.2, 7, 2.4, \u0026#39;world\u0026#39;] 元组 (Tuples) # 元组 (Tuples) 是 有序 (ordered) 且 不可变 (immutable) 的集合。我们将使用元组来存储 永远不会改变 的值。\n# Creating a tuple x = (3.0, \u0026#34;hello\u0026#34;) # tuples start and end with () print (x) (3.0, \u0026#39;hello\u0026#39;) # Adding values to a tuple x = x + (5.6, 4) print (x) (3.0, \u0026#39;hello\u0026#39;, 5.6, 4) # Try to change (it won\u0026#39;t work and we get an error) x[0] = 1.2 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ----\u0026gt; 1 x[0] = 1.2 TypeError: \u0026#39;tuple\u0026#39; object does not support item assignment 集合 (Sets) # 集合(sets) 是 无序(unordered) 且 可变(mutable) 的。但是，集合中的每个项目必须是 唯一(unique) 的。\n# Sets text = \u0026#34;Learn ML with Made With ML\u0026#34; print (set(text)) print (set(text.split(\u0026#34; \u0026#34;))) {\u0026#39;e\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39; \u0026#39;, \u0026#34;r\u0026#34;, \u0026#34;w\u0026#34;, \u0026#39;d\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;t\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;L\u0026#39;, \u0026#39;n\u0026#39;, \u0026#34;w\u0026#34;} {\u0026#39;with\u0026#39;, \u0026#39;Learn\u0026#39;, \u0026#39;ML\u0026#39;, \u0026#39;Made\u0026#39;, \u0026#39;With\u0026#39;} 字典 (Dictionaries) # 字典 (Dictionaries) 是 无序 (unordered) 且 可变 (mutable) 的 键值对(key-value pair) 集合。我们可以根据 键(key) 检索 值(value)，但字典不能有两个相同的键。\n# Creating a dictionary person = {\u0026#34;name\u0026#34;: \u0026#34;Goku\u0026#34;, \u0026#34;eye_color\u0026#34;: \u0026#34;brown\u0026#34;} print (person) print (person[\u0026#34;name\u0026#34;]) print (person[\u0026#34;eye_color\u0026#34;]) {\u0026#34;name\u0026#34;: \u0026#34;Goku\u0026#34;, \u0026#34;eye_color\u0026#34;: \u0026#34;brown\u0026#34;} Goku brown # Changing the value for a key person[\u0026#34;eye_color\u0026#34;] = \u0026#34;green\u0026#34; print (person) {\u0026#34;name\u0026#34;: \u0026#34;Goku\u0026#34;, \u0026#34;eye_color\u0026#34;: \u0026#34;green\u0026#34;} # Adding new key-value pairs person[\u0026#34;age\u0026#34;] = 24 print (person) {\u0026#34;name\u0026#34;: \u0026#34;Goku\u0026#34;, \u0026#34;eye_color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;age\u0026#34;: 24} # Length of a dictionary print (len(person)) 3 索引 (Indexing) # 通过列表的 索引(indexing) 和 切片 (slicing)，我们可以检索列表中的特定值。请注意，索引可以是正数（从 0 开始）或负数（-1 及以下，其中 -1 是列表中的最后一项）。\n# Indexing x = [3, \u0026#34;hello\u0026#34;, 1.2] print (\u0026#34;x[0]: \u0026#34;, x[0]) print (\u0026#34;x[1]: \u0026#34;, x[1]) print (\u0026#34;x[-1]: \u0026#34;, x[-1]) # the last item print (\u0026#34;x[-2]: \u0026#34;, x[-2]) # the second to last item x[0]: 3 x[1]: hello x[-1]: 1.2 x[-2]: hello # Slicing print (\u0026#34;x[:]: \u0026#34;, x[:]) # all indices print (\u0026#34;x[1:]: \u0026#34;, x[1:]) # index 1 to the end of the list print (\u0026#34;x[1:2]: \u0026#34;, x[1:2]) # index 1 to index 2 (not including index 2) print (\u0026#34;x[:-1]: \u0026#34;, x[:-1]) # index 0 to last index (not including last index) x[:]: [3, \u0026#39;hello\u0026#39;, 1.2] x[1:]: [\u0026#39;hello\u0026#39;, 1.2] x[1:2]: [\u0026#39;hello\u0026#39;] x[:-1]: [3, \u0026#39;hello\u0026#39;] if 语句 (if statements) # 我们可以使用 if 语句有条件地执行某项操作。条件由单词 if、elif（代表 else if）和 else 定义。我们可以根据需要使用任意数量的 elif 语句。每个条件下方的缩进代码是条件为 True 时将执行的代码。\n# If statement x = 4 if x \u0026lt; 1: score = \u0026#34;low\u0026#34; elif x \u0026lt;= 4: # elif = else if score = \u0026#34;medium\u0026#34; else: score = \u0026#34;high\u0026#34; print (score) medium # If statement with a boolean x = True if x: print (\u0026#34;it worked\u0026#34;) it worked 循环语句 (Loop) # For loops # for 循环可以迭代值集合（列表 (list)、元组 (tuple)、字典 (dictionaries)等）。缩进的代码针对值集合中的 每个项目 执行。\n# For loop veggies = [\u0026#34;carrots\u0026#34;, \u0026#34;broccoli\u0026#34;, \u0026#34;beans\u0026#34;] for veggie in veggies: print (veggie) carrots broccoli beans 当循环遇到 break 命令时，循环将立即终止。如果列表中还有更多项目，则不会处理它们。\n# `break` from a for loop veggies = [\u0026#34;carrots\u0026#34;, \u0026#34;broccoli\u0026#34;, \u0026#34;beans\u0026#34;] for veggie in veggies: if veggie == \u0026#34;broccoli\u0026#34;: break print (veggie) carrots 当循环遇到 continue 命令时，循环将仅跳过列表中该项目的所有其他操作。如果列表中还有更多项目，循环将正常继续。\n# `continue` to the next iteration veggies = [\u0026#34;carrots\u0026#34;, \u0026#34;broccoli\u0026#34;, \u0026#34;beans\u0026#34;] for veggie in veggies: if veggie == \u0026#34;broccoli\u0026#34;: continue print (veggie) carrots beans While loops # 只要条件为 True，while 循环就可以重复执行。我们也可以在 while 循环中使用 continue 和 break 命令。\n# While loop x = 3 while x \u0026gt; 0: x -= 1 # same as x = x - 1 print (x) 2 1 0 列表推导式 (list comprehension) 和生成器表达式 (generator expression) # 快速构建列表或生成器，尤其适合在数据处理或特征工程中清洗数据或生成特定序列。\n列表推导式 (list comprehension)：返回一个完整的列表。 生成器表达式 (generator expression)：返回一个惰性生成器，节省内存。 # 过滤列表中的偶数 numbers = [1, 2, 3, 4, 5, 6] even_numbers = [x for x in numbers if x % 2 == 0] print(even_numbers) # [2, 4, 6] # 生成器表达式（惰性求值） gen = (x**2 for x in range(5)) # 不占用大量内存 for val in gen: print(val) [2, 4, 6] 0 1 4 9 16 Lambda 函数和 map, filter 的使用 # Lambda函数：定义简洁的匿名函数，适合简单逻辑。(e.g. func = lambda var1 var2 : function =\u0026gt; func(var1, var2)) map：对可迭代对象中的每个元素应用函数。 filter：筛选符合条件的元素。 from functools import reduce # Lambda函数示例 add = lambda x, y: x + y print(add(3, 5)) # 8 # map: 计算平方 squares = list(map(lambda x: x**2, [1, 2, 3])) print(squares) # [1, 4, 9] # filter: 筛选出大于2的元素 filtered = list(filter(lambda x: x \u0026gt; 2, [1, 2, 3, 4])) print(filtered) # [3, 4] 8 [1, 4, 9] [3, 4] 函数 (Function) 封装 # 在ML项目中封装代码逻辑，便于维护和复用。例如，封装数据预处理步骤或模型训练流程。\ndef preprocess_data(data): preprocessed_data = data.dropna() return preprocessed_data # \u0026lt;---------- Usage ----------\u0026gt; # cleaned_data = preprocess_data(raw_data) def f(*args, **kwargs) 是另一种定义函数的方式，用来接收可变数量的参数。它允许函数在调用时传入任意数量的位置参数和关键字参数，从而使函数更加灵活。\n*args: 可变长度位置参数，接收任意数量的未命名参数 (arguments)，作为一个元组。 **kwargs: 可变长度关键字参数，接收任意数量的命名参数 (keyword arguments)，作为一个字典。 def f(*args, **kwargs): # 从位置参数中提取第一个值，赋值给变量 x x = args[0] # 从关键字参数中获取键 \u0026#34;y\u0026#34; 的值，若不存在返回 None y = kwargs.get(\u0026#34;y\u0026#34;) print (f\u0026#34;x: {x}, y: {y}\u0026#34;) # 调用函数 f，传入一个位置参数 5 和一个关键字参数 y=2 f(5, y=2) x: 5, y: 2 类 (Class) 封装 # class DataHandler: def __init__(self, filepath): self.filepath = filepath def load_data(self): print(f\u0026#34;Loading data from {self.filepath}...\u0026#34;) return {\u0026#34;data\u0026#34;: [1, 2, 3, 4]} def preprocess_data(self, data): return [x * 2 for x in data] # \u0026lt;---------- Usage ----------\u0026gt; # new_handler = DataHandler(\u0026#39;data.csv\u0026#39;) # new_data = new_handler.load_data() # preprocess_data = new_handler.preprocess_data(new_data[\u0026#39;data\u0026#39;]) 继承 (Inheritance) # 继承用于让一个类（子类）从另一个类（父类）中获得 属性(properties) 和 方法(methods)，从而实现代码复用和扩展。\n子类继承父类的方法和属性。 使用 super() 调用父类的方法。 方法重写：子类可重写父类的方法实现。 # 定义一个基础模型类 class BaseModel: def __init__(self, name): self.name = name def train(self): print(f\u0026#34;{self.name} is training...\u0026#34;) def test(self): print(f\u0026#34;{self.name} is testing...\u0026#34;) # 子类继承基础模型类 class RegressionModel(BaseModel): def __init__(self, name, num_features): super().__init__(name) self.num_features = num_features def train_1(self): super().train() print(f\u0026#34;Training a regression model with {self.num_features} features.\u0026#34;) # 使用子类 model = RegressionModel(\u0026#34;LinearRegression\u0026#34;, 10) model.train() model.test() LinearRegression is training... LinearRegression is testing... 方法 (Methods) # 实例方法 (Instance Method)：实例方法的第一个参数是 self，用于访问实例属性和其他实例方法。它需要通过实例对象调用，依赖于具体的实例状态。当方法需要操作实例的属性（如 self.name）或依赖于实例的状态时，实例方法是最合适的选择。 类方法 (Class Method)：类方法的第一个参数是 cls，表示类本身（而不是实例）。它使用 @classmethod 装饰器修饰，可以通过类对象或实例对象调用。当方法需要操作类级别的状态（如 total_models）而不依赖于任何具体实例时，类方法可以保持逻辑的清晰和一致性。 静态方法 (Static Method)：使用 @staticmethod，与类或实例 (self 或 cls)无绑定，仅实现功能逻辑。当方法的逻辑与类相关，但完全独立于类或实例的状态时，静态方法可以避免无意义的参数（如 self 或 cls），提高代码的简洁性。 class MLModel: total_models = 0 # 类属性 def __init__(self, name): self.name = name MLModel.total_models += 1 def display(self): # 实例方法 print(f\u0026#34;Model Name: {self.name}\u0026#34;) @classmethod def get_total_models(cls): # 类方法 print(f\u0026#34;Total models created: {cls.total_models}\u0026#34;) @staticmethod def utility_function(x): # 静态方法 return x**2 # 使用 model = MLModel(\u0026#34;RandomForest\u0026#34;) model.display() MLModel.get_total_models() print(MLModel.utility_function(5)) Model Name: RandomForest Total models created: 1 25 装饰器 (Decorators) # 装饰器是一种函数，接受另一个函数或方法作为输入并返回一个修改后的函数，用于动态扩展功能。\n@decorator def function(): pass 等价于：\ndef function(): pass function = decorator(function) import time # 定义一个装饰器 def timer_decorator(func): def wrapper(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print(f\u0026#34;Function \u0026#39;{func.__name__}\u0026#39; executed in {end_time - start_time:.4f}s\u0026#34;) return result return wrapper # 装饰训练函数 class MLModel: @timer_decorator def train(self, data): print(\u0026#34;Training model...\u0026#34;) time.sleep(1) # 模拟训练时间 return \u0026#34;Training Complete\u0026#34; model = MLModel() print(model.train([])) Training model... Function \u0026#39;train\u0026#39; executed in 1.0015s Training Complete 回调函数 (Callbacks) # 回调是一个函数作为参数传递给另一个函数，并在适当时调用。它在机器学习中常用于动态调整训练流程（如早停、学习率调整）。\nclass TrainingCallback: def on_epoch_start(self, epoch): print(f\u0026#34;Epoch {epoch} started.\u0026#34;) def on_epoch_end(self, epoch, loss): print(f\u0026#34;Epoch {epoch} ended with loss: {loss}.\u0026#34;) # 使用回调 class MLTrainer: def __init__(self, callback=None): self.callback = callback def train(self, epochs): for epoch in range(epochs): if self.callback: self.callback.on_epoch_start(epoch) # 模拟训练过程 loss = 0.01 * (epochs - epoch) if self.callback: self.callback.on_epoch_end(epoch, loss) # 测试回调 callback = TrainingCallback() trainer = MLTrainer(callback) trainer.train(3) Epoch 0 started. Epoch 0 ended with loss: 0.03. Epoch 1 started. Epoch 1 ended with loss: 0.02. Epoch 2 started. Epoch 2 ended with loss: 0.01. 模块 (Module) 封装 # project/ ├── data_processing.py # Contains functions for data processing ├── model_training.py # Contains model training logic ├── evaluation.py # Contains evaluation methods # \u0026lt;---------- Usage ----------\u0026gt; # \u0026lt;---------- data_processing.py ----------\u0026gt; # def clean_data(data): # \u0026#34;\u0026#34;\u0026#34;Clean the data by dropping missing values.\u0026#34;\u0026#34;\u0026#34; # print(\u0026#34;Cleaning data...\u0026#34;) # return data.dropna() # \u0026lt;---------- main.py ----------\u0026gt; # import data_processing as dp # data = dp.load_data(\u0026#34;data.csv\u0026#34;) # cleaned_data = dp.clean_data(data) 异步编程 (Asynchronous Programming) # 在训练、数据下载、或者与API通信时异步执行，提高性能。\nasync 用于定义一个异步函数。使用 async def 定义，表示该函数会返回一个协程对象。 await 用于调用另一个异步函数，并等待其执行完成，直到结果返回。 asyncio 库提供了一个框架来实现异步编程。 import nest_asyncio import asyncio nest_asyncio.apply() # Allows nested use of asyncio.run [Only for Jupyter Notebook] # 定义一个异步函数 async def say_hello(): print(\u0026#34;Hello\u0026#34;) await asyncio.sleep(1) # 模拟一个耗时的异步操作 print(\u0026#34;World\u0026#34;) # 运行异步任务 asyncio.run(say_hello()) Hello World import asyncio # 定义异步任务 async def fetch_data(): print(\u0026#34;Fetching data...\u0026#34;) await asyncio.sleep(2) # 模拟耗时的异步操作 return \u0026#34;Data fetched\u0026#34; async def process_data(): print(\u0026#34;Processing data...\u0026#34;) await asyncio.sleep(1) # 模拟耗时的异步操作 return \u0026#34;Data processed\u0026#34; # 主程序 async def main(): data_task = asyncio.create_task(fetch_data()) # 启动任务1 process_task = asyncio.create_task(process_data()) # 启动任务2 result1 = await data_task # 等待任务1完成 result2 = await process_task # 等待任务2完成 print(result1) print(result2) # 启动事件循环 asyncio.run(main()) Fetching data... Processing data... Data fetched Data processed "},{"id":8,"href":"/posts/04_lda_and_qda_for_classification/","title":"LDA and QDA for Classification","section":"Blog","content":"Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data.\nLinear Discriminant Analysis for Classification # LDA assumes that all classes are linearly separable and according to this multiple linear discrimination function representing several hyperplanes in the feature space are created to distinguish the classes. If there are two classes then the LDA draws one hyperplane and projects the data onto this hyperplane in such a way as to maximize the separation of the two categories. This hyperplane is created according to the two criteria considered simultaneously:\nMaximizing the distance between the means of two classes; Minimizing the variation between each category. Suppose that \\(Y \\in \\{1, ..., K\\}\\) is assigned a prior \\(\\hat{\\pi}_{k}\\) such that \\(\\sum_{i=1}^k \\hat{\\pi}_{k} = 1\\) . According to Bayes’ rule, the posterior probability is\n\\[ P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{\\sum_{i=1}^{K}f_{i}(x)\\pi_{i}} \\\\ \\] where \\(f_{k}(x)\\) is the density of \\(X\\) conditioned on \\(k\\) . The Bayes Classifier can be expessed as:\n\\[ h^{*}(x) = \\arg\\max_{k}\\{P(Y=k|X=x)\\} = \\arg\\max_{k}\\delta_{k}(x) \\\\ \\] For we assume that the random variable \\(X\\) is a vector \\(X=(X_1,X_2,...,X_k)\\) which is drawn from a multivariate Gaussian with class-specific mean vector and a common covariance matrix \\(\\Sigma \\ (i.e. \\Sigma_{k} = \\Sigma, \\forall k)\\) . In other words the covariance matrix is common to all K classes: \\(Cov(X)=\\Sigma\\) of shape \\(d \\times d\\) .\nSince \\(x\\) follows a multivariate Gaussian distribution, the probability \\(P(X=x|Y=k)\\) is given by: ( \\(\\mu_k\\) is the mean of inputs for category \\(k\\) )\n\\[ f_k(x)=\\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k)) \\\\ \\] Then we can find the posterior distribution as:\n\\[ P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{P(X=x)} = C \\cdot f_{k}(x)\\pi_{k} \\\\ \\] Since \\(P(X=x)\\) does not depend on \\(k\\) so we write it as a constant. We will now proceed to expand and simplify the algebra, putting all constant terms into \\(C,C^{'},C{''}\\) etc..\n\\[ \\begin{align*} p_{k}(x) = \u0026P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{P(X=x)} = C \\cdot f_{k}(x)\\pi_{k} \\\\ \u0026=C \\cdot \\pi_{k} \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k)) \\\\ \u0026=C^{'} \\cdot \\pi_{k} exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k)) \\\\ \\end{align*} \\] Take the log of both sides:\n\\[ \\begin{align*} logp_{k}(x) \u0026=log(C^{'} \\cdot \\pi_{k} exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k))) \\\\ \u0026= logC^{'} + log\\pi_k -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k) \\\\ \u0026= logC^{'} + log\\pi_k -\\frac{1}{2}[(x^{T}\\Sigma^{-1}x+\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}]+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ \u0026= C^{''} + log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ \\end{align*} \\] And so the objective function, sometimes called the linear discriminant function or linear score function is:\n\\[ \\delta_{k} = log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ \\] Which means that given an input \\(x\\) we predict the class with the highest value of \\(\\delta_{k}(x)\\) .\nTo find the Dicision Boundary, we will set \\(P(Y=k|X=x) = P(Y=l|X=x)\\) which is \\(\\delta_{k}(x) = \\delta_{l}(x)\\) :\n\\[ log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} = log\\pi_l -\\frac{1}{2}\\mu_{l}^{T}\\Sigma^{-1}\\mu_{l}+x^{T}\\Sigma^{-1}\\mu_{l} \\\\ log\\frac{\\pi_{k}}{\\pi_{l}} -\\underbrace{\\frac{1}{2}(\\mu_{k}-\\mu_{l})^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l})}_{\\mathrm{Constant}}+\\underbrace{x^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l})}_{\\mathrm{Linear \\ in} \\ x} = 0 \\\\ \\Rightarrow a^{T}x + b = 0 \\\\ \\] Which is a linear function in \\(x\\) - this explains why the decision boundaries are linear - hence the name Linear Discriminant Analysis.\nQuadratic Discrimination Analysis for Classification # LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector, but a covariance matrix that is common to all \\(K\\) classes. Quadratic discriminant analysis provides an alternative approach by assuming that each class has its own covariance matrix \\(\\Sigma_{k}\\) .\nTo derive the quadratic score function, we return to the previous derivation, but now \\(\\Sigma_{k}\\) is a function of \\(k\\) , so we cannot push it into the constant anymore.\n\\[ p_{k}(x) = \\pi_{k}\\frac{1}{(2\\pi)^{d/2}|\\Sigma_{k}|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k)) \\\\ \\] \\[ \\begin{align*} logp_{k}(x) \u0026= C +log\\pi_{k} - \\frac{1}{2}log|\\Sigma_{k}|-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \\\\ \u0026= C +log\\pi_{k} - \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}x^{T}\\Sigma_{k}^{-1}x +x^{T}\\Sigma_{k}^{-1}\\mu_{k} -\\frac{1}{2}\\mu_{k}^{T}\\Sigma_{k}^{-1}\\mu_{k} \\\\ \\end{align*} \\] Which is a quadratic function of \\(x\\) . Under this less restrictive assumption, the classifier assigns an observation \\(X=x\\) to the class for which the quadratic score function is the largest:\n\\[ \\delta_{k}(x) = log\\pi_k- \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \\\\ \\] To find the Quadratic Dicision Boundary, we will set \\(P(Y=k|X=x) = P(Y=l|X=x)\\) which is \\(\\delta_{k}(x) = \\delta_{l}(x)\\) :\n\\[ log\\pi_k- \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) = log\\pi_l- \\frac{1}{2}log|\\Sigma_{l}| -\\frac{1}{2}(x-\\mu_l)^{T}\\Sigma_{l}^{-1}(x-\\mu_l) \\\\ \\frac{1}{2}x^{T}\\underbrace{(\\Sigma_{l}-\\Sigma_{k})}_{A}x+\\underbrace{(\\mu_{k}^{T}\\Sigma_{k}^{-1}-\\mu_{l}^{T}\\Sigma_{l}^{-1})}_{b^{T}}x +\\underbrace{\\frac{1}{2}(\\mu_{k}-\\mu_{l})^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l}) + log(\\frac{\\pi_{l}}{\\pi_{k}}) + log(\\frac{|\\Sigma_{k}|^{1/2}}{|\\Sigma_{l}|^{1/2}})}_{c} = 0 \\\\ \\Rightarrow x^{T}Ax + b^{T}x + c = 0 \\\\ \\] Case 1 : When \\(\\Sigma_{k} = I\\) # We first concider the case that \\(\\Sigma_{k} = I, \\forall k\\) . This is the case where each distribution is spherical, around the mean point. Then we can have:\n\\[ \\delta_{k}(x) = - \\frac{1}{2}log|I| + log\\pi_k -\\frac{1}{2}(x-\\mu_k)^{T}I(x-\\mu_k) \\\\ \\] Where \\(log(|I|) = log(1) = 0\\) and \\((x-\\mu_k)^{T}I(x-\\mu_k) = (x-\\mu_k)^{T}(x-\\mu_k)\\) is the Squared Euclidean Distance between two points \\(x\\) and \\(\\mu_{k}\\) .\nThus under this condition (i.e. \\(\\Sigma = I\\) ) , a new point can be classified by its distance from the center of a class, adjusted by some prior. Further, for two-class problem with equal prior, the discriminating function would be the perpendicular bisector of the 2-class’s means.\nCase 2 : When \\(\\Sigma_{k} \\neq I\\) # Since \\(\\Sigma_{k}\\) is a symmetric matrix \\(\\Sigma_{k} = \\Sigma_{k}^{T}\\) , by using the Singular Value Decomposition (SVD) of \\(\\Sigma_{k}\\) , we can get:\n\\[ \\Sigma_{k} = U_{k}S_{k}U_{k}^{T} \\\\ \\Sigma_{k}^{-1} = (U_{k}S_{k}U_{k}^{T})^{-1} = U_{k}S_{k}^{-1}U_{k}^{T}\\\\ \\] Then,\n\\[ \\begin{align*} (x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \u0026= (x-\\mu_k)^{T}U_{k}S_{k}^{-1}U_{k}^{T}(x-\\mu_k) \\\\ \u0026= (U_{k}^{T}x-U_{k}^{T}\\mu_k)^{T}S_{k}^{-1}(U_{k}^{T}x-U_{k}^{T}\\mu_k) \\\\ \u0026= (U_{k}^{T}x-U_{k}^{T}\\mu_k)^{T}S_{k}^{-\\frac{1}{2}}S_{k}^{-\\frac{1}{2}}(U_{k}^{T}x-U_{k}^{T}\\mu_k) \\\\ \u0026= (S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}I(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ \u0026= (S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ \\end{align*} \\] Which is also known as the Mahalanobis distance.\nThink of \\(S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\) as a linear transformation that takes points in class \\(k\\) and distributes them spherically around a point, like in Case 1. Thus when we are given a new point, we can apply the modified \\(\\delta_{k}\\) values to calculate \\(h^{*}(x)\\) . After applying the singular value decomposition, \\(\\Sigma_{k}^{-1}\\) is considered to be an identity matrix such that:\n\\[ \\delta_{k}(x) = - \\frac{1}{2}log|I| + log\\pi_k -\\frac{1}{2}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ \\] Where \\(log(|I|) = log(1) = 0\\) The difference between Case 1 and Case 2 (i.e. the difference between using the Euclidean and Mahalanobis distance) can be seen in the illustration below:\nLDA and QDA in practice # In practice we don’t know the parameters of Gaussian and will need to estimate them using our training data.\n\\[ \\hat{\\pi}_{k} = \\hat{P}(y=k) = \\frac{n_{k}}{n} \\\\ \\] where \\(n_{k}\\) is the number of class \\(k\\) observations.\n\\[ \\hat{\\mu}_{k} = \\frac{1}{n_{k}}\\sum_{i:y_{i}=k}x_{i} \\\\ \\] \\[ \\hat{\\Sigma}_{k} = \\frac{1}{n_{k}-k}\\sum_{i:y_{i}=k}(x_{i}-\\hat{\\mu}_{k})(x_{i}-\\hat{\\mu}_{k})^{T} \\\\ \\] If we wish to use LDA we must calculate a common covariance, so we average all the covariances, e.g.\n\\[ \\Sigma = \\frac{\\sum_{r=1}^k(n_{r}\\Sigma_{r})}{\\sum_{r=1}^k n_{r}} \\\\ \\] Where:\n\\(n_{r}\\) is the number of data points in class \\(r\\) .\n\\(\\Sigma_{r}\\) is the covariance of class \\(r\\) and \\(n\\) is the total number of data points.\n\\(k\\) is the number of classes.\nReference # [1] Sicotte, X. B. (2018, June 22). Xavier Bourret Sicotte. Linear and Quadratic Discriminant Analysis - Data Blog. https://xavierbourretsicotte.github.io/LDA_QDA.html.\n"},{"id":9,"href":"/docs/deep-learning/computer-vision/","title":"Computer Vision","section":"Deep Learning","content":" Computer Vision # "},{"id":10,"href":"/docs/python-basics/leetcode/","title":"Leetcode Notes","section":"Python Basics","content":" Leetcode Interview Preparation Notes # Basic Data Structures # Arrays # In Python, arrays are typically represented using lists. While Python doesn\u0026rsquo;t have a native array type as seen in other languages like Java or C++, lists are versatile and can be used similarly to arrays.\n【Last Update: 2024-08-14】\narr = [] # O(1) arr = [1, 2, 3] # O(n), where n is the number of elements first_element = arr[0] # O(1) arr[1] = 10 # O(1) arr.append(6) # O(1) on average for appending arr.insert(2, 15) # O(n), where n is the number of elements after the insertion index arr.remove(15) # O(n), where n is the number of elements in the list [remove the first 15 in the array] del arr[2] # O(n), where n is the number of elements after the deleted index last_element = arr.pop() # O(1) arr.sort() # 原地排序 sorted_arr = sorted(arr) # 返回排序后的数组 arr[::-1] # arr 倒序 ## Counter() 的常用语法和使用情况 from collections import Counter arr = [1, 2, 2, 3, 3, 3] counts = Counter(arr) # 结果：Counter({3: 3, 2: 2, 1: 1}) ## 找到出现次数最多的元素 most_common_element = counts.most_common(1)[0] # 结果：(3, 3) ## 判断出现的元素是否相同 arr1 = [1, 2, 3] arr2 = [3, 2, 1] is_anagram = Counter(arr1) == Counter(arr2) # 结果：True ## set() 的常用语法和使用情况 arr = [1, 2, 2, 3, 4, 4] ## 快速查找 seen = set(arr) if 3 in seen: print(\u0026#34;3 is in array\u0026#34;) ## 去重 unique_elements = list(set(arr)) # 结果：[1, 2, 3, 4] ## 两个数组的交集 arr1 = [1, 2, 2, 3] arr2 = [2, 3, 4] intersection = list(set(arr1) \u0026amp; set(arr2)) # 结果：[2, 3] Strings # Strings in Python are immutable sequences of characters. You can perform various operations on strings using built-in methods and operators.\n【Last Update: 2024-08-14】\ns = \u0026#34;Hello, World!\u0026#34; # O(n), where n is the length of the string first_char = s[0] # O(1) substring = s[7:12] # O(k), where k is the length of the substring combined = s + \u0026#34; Python\u0026#34; # O(n + m), where n and m are the lengths of the two strings repeated = s * 2 # O(n * k), where k is the number of repetitions upper_s = s.upper() # O(n), where n is the length of the string lower_s = s.lower() # O(n), where n is the length of the string starts_with_hello = s.startswith(\u0026#34;Hello\u0026#34;) # O(n), where n is the length of the prefix contains_world = \u0026#34;World\u0026#34; in s # O(n * m), where n is the length of the string and m is the length of the substring replaced_s = s.replace(\u0026#34;World\u0026#34;, \u0026#34;Python\u0026#34;) # O(n * m), where n is the length of the string and m is the length of the substring words = s.split(\u0026#34;, \u0026#34;) # O(n), where n is the length of the string joined = \u0026#34; - \u0026#34;.join(words) # O(n), where n is the total length of the resulting string Linked Lists # A Linked List is a linear data structure consisting of nodes, where each node contains:\nA data part that stores the actual data. A next part (or pointer) that points to the next node in the list. 【Last Update: 2024-11-14】\n## A node in a linked list can be represented as a class class ListNode: def __init__(self, data=0, next=None): self.data = data # Data of the node self.next = next # Pointer to the next node ## Inserting Nodes def insert_at_beginning(head, data): new_node = ListNode(data) # Create a new node new_node.next = head # Link the new node to the current head return new_node # New node becomes the head ## Deleting Nodes def delete_from_beginning(head): if not head: return None return head.next # The second node becomes the new head ## Searching for a Node def search(head, key): current = head while current: if current.data == key: return True # Found the data current = current.next return False # Data not found Stack # A Stack is a linear data structure that stores items in a Last-In/First-Out (LIFO) or First-In/Last-Out (FILO) manner. In stack, a new element is added at one end and an element is removed from that end only.\npush(a) – Inserts the element ‘a’ at the top of the stack – Time Complexity: O(1) pop() – Deletes the topmost element of the stack – Time Complexity: O(1) Peek - View the top element without removing it. Empty - Check if the stack is empty. 【Last Update: 2024-11-19】\nstack = [] # Push elements onto the stack stack.append(1) stack.append(2) # Pop element from the stack top = stack.pop() # Removes and returns 2 # Peek the top element top = stack[-1] if stack else None # Returns 1 # Check if the stack is empty is_empty = len(stack) == 0 Queue # Queue is a linear data structure that stores items in First In First Out (FIFO) manner. With a queue the least recently added item is removed first.\nEnqueue: Adds an item to the queue. If the queue is full, then it is said to be an Overflow condition – Time Complexity : O(1) Dequeue: Removes an item from the queue. The items are popped in the same order in which they are pushed. If the queue is empty, then it is said to be an Underflow condition – Time Complexity : O(1) Peek: View the front element without removing it. Empty: Check if the queue is empty. 【Last Update: 2024-11-19】\nfrom collections import deque # Initialize a queue queue = deque() # Enqueue elements queue.append(1) queue.append(2) # Dequeue element front = queue.popleft() # Removes and returns 1 # Peek at the front element front = queue[0] if queue else None # Check if the queue is empty is_empty = len(queue) == 0 Deque # A deque is a generalized queue that allows insertion and deletion from both ends with O(1) complexity. Internally, it is implemented as a doubly linked list or a circular buffer.\n【Last Update: 2024-11-25】\nfrom collections import deque # Initialize a deque dq = deque() # Add elements dq.append(1) # Add to the right dq.appendleft(2) # Add to the left # Remove elements dq.pop() # Remove from the right dq.popleft() # Remove from the left # Access and manipulation dq.extend([3, 4]) # Add multiple elements to the right dq.extendleft([0, -1]) # Add multiple elements to the left (reversed order) dq.rotate(1) # Rotate elements right dq.rotate(-1) # Rotate elements left dq.clear() # Clear all elements Advanced Data Structures # Overview # Trees: Explore the concepts of binary trees, binary search trees, AVL trees, and tree traversals (in-order, pre-order, post-order, level-order). Graphs: Learn about different graph representations (adjacency list, adjacency matrix), and traversal algorithms (Depth-First Search, Breadth-First Search) to solve problems like finding connected components or checking cycles. Hash Tables: Study how hash tables work, including hashing functions, handling collisions, and applications in tasks like item counting or implementing dictionaries. Heap # A heap is a complete binary tree stored as an array. It maintains the heap property: in a min-heap, the parent is less than or equal to its children. Insertions and deletions are O(log n) due to the need to maintain the heap property.\nTwo main types: Min-Heap: The root node is the smallest, and every parent node is smaller than or equal to its children. Max-Heap: The root node is the largest, and every parent node is larger than or equal to its children. Root Node Access: Min-Heap: Root is the smallest element Max-Heap: Root is the largest element. Efficient Operations: Insert and delete both take O(log n). Maintains heap properties using adjustments (upward or downward shifts). 【Last Update: 2024-11-25】\nimport heapq # Initialize a heap heap = [] # Add elements heapq.heappush(heap, 3) # Push element into the heap heapq.heappush(heap, 1) heapq.heappush(heap, 4) # Access the smallest element smallest = heap[0] # Remove elements min_element = heapq.heappop(heap) # Pop the smallest element # Heapify an existing list nums = [4, 1, 7, 3] heapq.heapify(nums) # Get n largest or smallest elements largest = heapq.nlargest(2, nums) smallest = heapq.nsmallest(2, nums) Tree # A tree is a hierarchical data structure with nodes connected by edges. The topmost node is the root, and nodes with no children are called leaves.\nBinary Tree: Each node has at most two children. Binary Search Tree (BST): A binary tree where the left child contains values less than the parent, and the right child contains values greater. Balanced Tree: A tree where the height difference between left and right subtrees of any node is minimal (e.g., AVL tree, Red-Black tree). Tree Traversals: Preorder Traversal (Root, Left, Right) Inorder Traversal (Left, Root, Right) Postorder Traversal (Left, Right, Root) ## Trees are often represented using classes. class TreeNode: def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = right ## Preorder Traversal (Root, Left, Right) def preorder_traversal(root): if root: print(root.val) preorder_traversal(root.left) preorder_traversal(root.right) ## Inorder Traversal (Left, Root, Right) def inorder_traversal(root): if root: inorder_traversal(root.left) print(root.val) inorder_traversal(root.right) ## Postorder Traversal (Left, Right, Root) def postorder_traversal(root): if root: postorder_traversal(root.left) postorder_traversal(root.right) print(root.val) ## Binary Search Tree (BST) Operations ## 1. Insert a Node def insert_into_bst(root, val): if not root: return TreeNode(val) if val \u0026lt; root.val: root.left = insert_into_bst(root.left, val) else: root.right = insert_into_bst(root.right, val) return root ## 2. Search for a Value def search_bst(root, val): if not root or root.val == val: return root if val \u0026lt; root.val: return search_bst(root.left, val) return search_bst(root.right, val) ## 3. Delete a Node def delete_node(root, key): if not root: return None if key \u0026lt; root.val: root.left = delete_node(root.left, key) elif key \u0026gt; root.val: root.right = delete_node(root.right, key) else: if not root.left: return root.right if not root.right: return root.left min_larger_node = root.right while min_larger_node.left: min_larger_node = min_larger_node.left root.val = min_larger_node.val root.right = delete_node(root.right, root.val) return root Hash Tables # In Python, the built-in dict type (short for dictionary) functions as a hash table. Hash tables are a key data structure used for efficient data retrieval and storage, providing average time complexities of O(1) for insertion, deletion, and lookup operations due to their underlying hashing mechanism.\n【Last Update: 2024-11-06】\nmy_dict = {} # Creating an empty dictionary my_dict = {\u0026#39;key1\u0026#39;: \u0026#39;value1\u0026#39;, \u0026#39;key2\u0026#39;: \u0026#39;value2\u0026#39;} # Creating a dictionary with initial values value = my_dict[\u0026#39;key1\u0026#39;] # Accessing a value by key my_dict[\u0026#39;key3\u0026#39;] = \u0026#39;value3\u0026#39; # Adding a new key-value pair my_dict[\u0026#39;key2\u0026#39;] = \u0026#39;new_value2\u0026#39; # Updating an existing key-value pair del my_dict[\u0026#39;key1\u0026#39;] # Removing an entry by key value = my_dict.pop(\u0026#39;key2\u0026#39;) # Popping an entry (removes and returns the value) exists = \u0026#39;key3\u0026#39; in my_dict # # Checking if a key is in the dictionary [True] for key in my_dict: print(key, my_dict[key]) # Iterating through keys for key, value in my_dict.items(): # Iterating through key-value pairs print(key, value) for value in my_dict.values(): # Iterating through values print(value) # defaultdict 使用方法，没见过的元素不会报错。适用于计数、分组和嵌套字典等应用。 from collections import defaultdict # 使用 int 类型的 defaultdict dd = defaultdict(int) print(dd[\u0026#39;missing_key\u0026#39;]) # 输出：0，因为 int() 的默认值是 0 print(dd) # 输出：defaultdict(\u0026lt;class \u0026#39;int\u0026#39;\u0026gt;, {\u0026#39;missing_key\u0026#39;: 0}) # 统计元素出现次数 data = \u0026#34;abracadabra\u0026#34; counter = defaultdict(int) for char in data: counter[char] += 1 print(counter) # 输出：defaultdict(\u0026lt;class \u0026#39;int\u0026#39;\u0026gt;, {\u0026#39;a\u0026#39;: 5, \u0026#39;b\u0026#39;: 2, \u0026#39;r\u0026#39;: 2, \u0026#39;c\u0026#39;: 1, \u0026#39;d\u0026#39;: 1}) # defaultdict(list)常用于将多个值归类到同一个键下。 data = [(\u0026#34;apple\u0026#34;, 1), (\u0026#34;banana\u0026#34;, 2), (\u0026#34;apple\u0026#34;, 3), (\u0026#34;banana\u0026#34;, 4)] grouped_data = defaultdict(list) for fruit, count in data: grouped_data[fruit].append(count) print(grouped_data) # 输出：defaultdict(\u0026lt;class \u0026#39;list\u0026#39;\u0026gt;, {\u0026#39;apple\u0026#39;: [1, 3], \u0026#39;banana\u0026#39;: [2, 4]}) # 可以使用dict()将defaultdict转换为普通字典。 dd = defaultdict(int) dd[\u0026#39;a\u0026#39;] += 1 print(dict(dd)) # 输出：{\u0026#39;a\u0026#39;: 1} Core Algorithms # Overview # Two Pointer: The two-pointer technique is used primarily in solving array and linked list problems. It involves using two pointers to traverse the data structure, allowing for efficient searching and processing of elements. Sorting Algorithms: Review the mechanisms and use cases for quicksort, mergesort, and heapsort. Understand the trade-offs in terms of time and space complexity. Search Algorithms: Study binary search on sorted arrays, and learn about its variations for finding the first or last position of an element. Recursion and Backtracking: Understand how to apply recursion for solving problems involving permutations, combinations, and other backtrack-required scenarios. Study the call stack mechanism and how to optimize recursion through memoization. Prefix Sum and Suffix Sum: Prefix Sum and Suffix Sum are techniques used to compute the sum of elements in a subarray quickly by precomputing cumulative sums. Two Pointer # Finding Pairs with a Given Sum: When looking for two numbers in a sorted array that add up to a specific target. Reversing a String or Array: Using two pointers to swap elements from the start and end until they meet in the middle. Merging Two Sorted Arrays: Traversing both arrays simultaneously to create a new sorted array. Removing Duplicates from a Sorted Array: Using two pointers to track unique elements. 设置 two pointers 的时候，left 一般会在最前面，但是 right 不一定在最后，可以设置在 left 后面。 【Last Update: 2024-11-07】\nPrefix Sum and Suffix Sum # Prefix Sum: For an array nums, the prefix sum at each index i is the sum of all elements from the start of the array up to i. This allows you to find the sum of any subarray [i, j] in constant time by calculating prefix[j+1] - prefix[i]. Suffix Sum: For the same array nums, the suffix sum at index i is the sum of all elements from i to the end of the array. It enables efficient queries for sums of subarrays that start from any index i to a given end by using suffix[i] - suffix[j+1]. ## Input [1, 2, 3, 4] -\u0026gt; Output [2x3x4, 1x3x4, 1x2x4, 1x2x3] = [24, 12, 8, 6] ## Predix -\u0026gt; [0, 1, 1x2, 1x2x3] = [0, 1, 2, 6] ## Suffix -\u0026gt; [2x3x4, 3x4, 4, 0] = [24, 12, 4, 0] def productExceptSelf(self, nums: List[int]) -\u0026gt; List[int]: res = [1] * len(nums) prefix, suffix = 1, 1 for i in range(len(nums)): res[i] = prefix prefix *= nums[i] for j in range(len(nums)-1,-1,-1): res[j] *= suffix suffix *= nums[j] return res 【Last Update: 2024-11-11】\nAdvanced Algorithms # Overview # Dynamic Programming: Explore the methodology of solving problems by breaking them down into smaller subproblems, storing results, and combining them to solve larger problems. Focus on understanding the concepts of overlapping subproblems and optimal substructure. Greedy Algorithms: Learn how greedy choices can lead to globally optimized solutions and their applications in problems like scheduling, graph based problems (like minimum spanning trees), and currency denomination. Graph Algorithms: Study shortest path algorithms (Dijkstra’s, Bellman-Ford) and minimum spanning tree algorithms (Prim’s, Kruskal’s). Understand their use cases and limitations. "},{"id":11,"href":"/docs/common-libraries/pandas/","title":"Pandas","section":"Common Libraries","content":" Pandas # "},{"id":12,"href":"/docs/machine-learning/supervised-learning/","title":"Supervised Learning","section":"Machine Learning","content":" Supervised Learning # "},{"id":13,"href":"/docs/deep-learning/generative-models/","title":"Generative Models","section":"Deep Learning","content":" Generative Models # "},{"id":14,"href":"/docs/common-libraries/pytorch/","title":"PyTorch","section":"Common Libraries","content":" PyTorch # "},{"id":15,"href":"/docs/machine-learning/unsupervised-learning/","title":"Unsupervised Learning","section":"Machine Learning","content":" Unsupervised Learning # "},{"id":16,"href":"/docs/machine-learning/regularization/","title":"Regularization","section":"Machine Learning","content":" Regularization # "},{"id":17,"href":"/docs/machine-learning/optimization/","title":"Optimization","section":"Machine Learning","content":" Optimization # "},{"id":18,"href":"/docs/python-basics/leetcode/practice-history/","title":"Practice History","section":"Leetcode Notes","content":" Leetcode 练习记录 # 此页面记录了我在 LeetCode 平台上完成的算法题目练习，每条记录包括完成日期、题目链接以及涉及的数据结构或算法主题。这些练习旨在巩固基础知识、提高解题技巧，并为技术面试做好充分准备。以下为部分记录：\nDate: 2024-11-06: LeetCode 1 - Two Sum【Array】【Hash Table】 Date: 2024-11-07: LeetCode 15 - 3 Sum【Array】【Two Pointers】 Date: 2024-11-08: LeetCode 121 - Best Time to Buy and Sell Stock【Array】【Dynamic Programming】 LeetCode 349 - Intersection of Two Arrays【Array】【Hash Table】 LeetCode 219 - Contains Duplicate II【Array】【Hash Table】 Date: 2024-11-10: Leetcode 454 - 4Sum II【Array】【Hash Table】 Date: 2024-11-11: Leetcode 53 - Maximum Subarray【Array】 Leetcode 238 - Product of Array Except Self【Array】【Prefix Sum】 Date: 2024-11-12: Leetcode 344 - Reverse String【String】【Two Pointers】 Leetcode 26 - Remove Duplicates from Sorted Array【Array】【Two Pointers】 Leetcode 27 - Remove Element【Array】【Two Pointers】 Date: 2024-11-13: Leetcode 3 - Longest Substring Without Repeating Characters【String】【Hash Table】 Leetcode 5 - Longest Palindromic Substring【String】【Two Pointers】 Leetcode 28 - Find the Index of the First Occurrence in a String【String】【Two Pointers】 Leetcode 49 - Group Anagrams【String】【Hash Table】 Date: 2024-11-14: Leetcode 206 - Reverse Linked List【Linked List】【Recursion】 Date: 2024-11-15: Leetcode 21 - Merge Two Sorted Lists【Linked List】【Recursion】 Leetcode 141 - Linked List Cycle【Linked List】【Hash Table】 Leetcode 203 - Remove Linked List Elements【Linked List】【Recursion】 Leetcode 83 - Remove Duplicates from Sorted List【Linked List】 Leetcode 2 - Add Two Numbers【Linked List】【Recursion】【Math】 Date: 2024-11-17: Leetcode 19 - Remove N-th Node From End of List【Linked List】【Two Pointers】 Leetcode 138 - Copy List with Random Pointer【Linked List】【Hash Table】 Leetcode 234 - Palindrome Linked List【Linked List】 Leetcode 160 - Intersection of Two Linked Lists【Linked List】【Hash Table】 Date: 2024-11-18: Leetcode 92 - Reverse Linked List II【Linked List】 Leetcode 61 - Rotate List【Linked List】【Two Pointers】 Date: 2024-11-19: Leetcode 20 - Valid Parentheses【Stack】【String】 Date: 2024-11-21: Leetcode 155 - Min Stack【Stack】 Leetcode 682 - Baseball Game【Stack】 Date: 2024-11-22: Leetcode 739 - Daily Temperatures【Stack】【Monotonic Stack】 Leetcode 232 - Implement Queue using Stacks【Queue】【Stack】 Leetcode 225 - Implement Stack using Queues【Stack】【Queue】 Leetcode 622 - Design Circular Queue【Queue】 Date: 2024-11-24: Leetcode 933 - Number of Recent Calls【Queue】 Date: 2024-11-25: Leetcode 239 - Sliding Window Maximum【Deque】【Sliding Window】 Leetcode 641 - Design Circular Deque【Deque】 Leetcode 215 - Kth Largest Element in an Array【Heap】 Leetcode 23 - Merge k Sorted Lists【Heap】 Date: 2024-11-26: Leetcode 94 - Binary Tree Inorder Traversal【Tree】【Binary Tree】 Leetcode 144 - Binary Tree Preorder Traversal【Tree】【Binary Tree】 Leetcode 145 - Binary Tree Postorder Traversal【Tree】【Binary Tree】 Leetcode 104 - Maximum Depth of Binary Tree【Tree】【Binary Tree】【Depth-First Search】 Date: 2024-11-27: Leetcode 111 - Minimum Depth of Binary Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 112 - Path Sum【Tree】【Binary Tree】【Depth-First Search】 Leetcode 101 - Symmetric Tree【Tree】【Binary Tree】【Depth-First Search】 Date: 2024-11-28: Leetcode 98 - Validate Binary Search Tree【Binary Search Tree】【Depth-First Search】 Date: 2024-12-04: Leetcode 226 - Invert Binary Tree【Binary Search Tree】【Depth-First Search】 Date: 2024-12-05: Leetcode 235 - Lowest Common Ancestor of a Binary Search Tree【Binary Search Tree】【Depth-First Search】 Leetcode 700 - Search in a Binary Search Tree【Binary Search Tree】 "}]