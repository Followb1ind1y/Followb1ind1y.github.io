[{"id":0,"href":"/posts/02_linear_regression/","title":"Linear Regression","section":"Blog","content":"Regression is a method of modelling a target value based on independent predictors. This method is mostly used for forecasting and finding out cause and effect relationship between variables. Regression techniques mostly differ based on the number of independent variables and the type of relationship between the independent and dependent variables. Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on.\nLinear Regression # Linear regression with multiple variables is also known as \u0026ldquo;multivariate linear regression\u0026rdquo;. The multivariable form of the hypothesis function accommodating these multiple features is as follows:\n\\[ \\begin{align*} \u0026\\mathrm{Hypothesis}: h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\theta_{3}x_{3} + \\cdot\\cdot\\cdot + \\theta_{n}x_{n} \\\\ \u0026\\mathrm{Cost \\ Function}: J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2 \\\\ \u0026\\mathrm{Goal}: \\min_{\\theta}J(\\theta) \\\\ \\end{align*} \\] Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:\n\\[ h_{\\theta}(x) = \\begin{bmatrix} \\theta_{0} \u0026 \\theta_{1} \u0026 \\cdot\\cdot\\cdot \u0026 \\theta_{n} \\end{bmatrix} \\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ \\cdot\\cdot\\cdot \\\\ x_{n} \\end{bmatrix} = \\theta^{T}x \\\\ \\] Gradient Descent for Linear Regression # Gradient descent is a generic optimization algorithm used in many machine learning algorithms. It iteratively tweaks the parameters of the model in order to minimize the cost function. We do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter \\(\\alpha\\) , which is called the learning rate. The gradient descent algorithm can be represented as:\n\\[ \\begin{align*} \u0026\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}}= \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)}) \\\\ \u0026\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}= \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} \\\\ \\end{align*} \\] \\[ \\begin{bmatrix} \\frac{\\partial J(\\theta)}{\\partial \\theta_{0}} \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{1}} \\\\ \\cdot\\cdot\\cdot \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{n}} \\end{bmatrix}=\\frac{1}{m}x^{T}(h_{\\theta}(x)-y) \\\\ \\] \\[ \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\cdot\\cdot\\cdot \\\\ \\theta_{n} \\end{bmatrix}=\\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\cdot\\cdot\\cdot \\\\ \\theta_{n} \\end{bmatrix}-\\alpha\\begin{bmatrix} \\frac{\\partial J(\\theta)}{\\partial \\theta_{0}} \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{1}} \\\\ \\cdot\\cdot\\cdot \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{n}} \\end{bmatrix} \\\\ \\] \\[ \\begin{align*} \u0026\\mathrm{Repect\\ until \\ convergence} \\{ \\\\ \u0026\\ \\ \\ \\ \\theta_{j}^{new} :=\\theta_{j}^{old} - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}) \\ \\ \\ \\ \\mathrm{for} \\ j = 0 \\cdot\\cdot\\cdot n \\\\ \u0026\\} \\\\ \\end{align*} \\] To demonstrate the gradient descent algorithm, we initialize the model parameters with 0. The equation becomes \\(h_{\\theta}(x) = 0\\) . Gradient descent algorithm now tries to update the value of the parameters so that we arrive at the best fit line.\nWe should adjust our parameter \\(\\alpha\\) to ensure that the gradient descent algorithm converges in a reasonable time. If \\(\\alpha\\) is too small, gradient descent can be slow. If \\(\\alpha\\) is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.\nWe can speed up gradient descent by having each of our input values in roughly the same range. This is because \\(\\theta\\) will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\nTwo techniques to help with this are feature scaling and mean normalization. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:\n\\[ x_{i} := \\frac{(x_{i}-\\mu_{i})}{s_{i}} \\\\ \\] Where \\(\\mu_{i}\\) is the average of all the values for feature (i) and \\(s_{i}\\) is the range of values (max - min), or \\(s_{i}\\) is the standard deviation. Note that dividing by the range, or dividing by the standard deviation, give different results.\nNormal Equation for Linear Regression # Gradient descent gives one way of minimizing J. In the \u0026ldquo;Normal Equation\u0026rdquo; method, we will minimize J by explicitly taking its derivatives with respect to the \\(\\theta_{j}\\) ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:\n\\[ \\theta = (X^{T}X)^{-1}X^{T}y \\\\ \\] The following is a comparison of gradient descent and the normal equation:\nGradient Descent Normal Equation Need to choose alpha No need to choose alpha Needs many iterations No need to iterate \\(O(kn^{2})\\) \\(O(n^{3})\\) , need to calculate inverse of \\(X^{T}X\\) Works well when n is large Slow if n is very large With the normal equation, computing the inversion has complexity \\(O(n^{3})\\) . So if we have a very large number of features, the normal equation will be slow. In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.\nIf \\(X^{T}X\\) is noninvertible, the common causes might be having :\nRedundant features, where two features are very closely related (i.e. they are linearly dependent) Too many features (e.g. \\(m ≤ n\\) ). In this case, delete some features or use \u0026ldquo;regularization\u0026rdquo;. Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.\nEvaluating the performance of the Linear Regression Model # We will be using Root mean squared error(RMSE) and Coefficient of Determination(R² score) to evaluate our model. RMSE is the square root of the average of the sum of the squares of residuals. RMSE is defined by:\n\\[ RMSE = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2} \\\\ \\] \\(R^{2}\\) score or the coefficient of determination explains how much the total variance of the dependent variable can be reduced by using the least square regression. It can be defined by:\n\\[ R^{2} = 1- \\frac{SS_{r}}{SS{t}} \\\\ \\] Where \\(SS_{t}\\) is the total sum of errors if we take the mean of the observed values as the predicted value and \\(SS_{r}\\) is the sum of the square of residuals.\n\\[ \\begin{align*} SS_{t} \u0026= \\sum_{i=1}^{m}(y^{(i)}-\\bar{y})^2 \\\\ SS_{r} \u0026= \\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2 \\\\ \\end{align*} \\] References # [1] Agarwal, A. (2018, November 14). Linear Regression using Python. Medium. https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2.\n[2] Ng, A. (n.d.). Machine Learning. Coursera. https://www.coursera.org/learn/machine-learning.\nBlog Home "},{"id":1,"href":"/docs/machine-learning/supervised-learning/linear-regression/","title":"Linear Regression","section":"Supervised Learning","content":" 线性回归 # 线性回归（Linear Regression） # 线性回归是一种最简单的回归模型，目标是通过一个或多个输入变量预测一个 连续的（continuous） 输出变量。其核心思想是 拟合一条直线来描述输入（input）与输出（output）之间 的关系。其数学公式为：\n\\[ \\hat{y} = XW + b \\] 其中 \\(N\\) 是总样本数量， \\(\\hat{y}\\) 是预测值 \\(\\in \\mathbb{R}^{N \\times 1}\\) ， \\(X\\) 是输入值 \\(\\in \\mathbb{R}^{N \\times D}\\) ， \\(W\\) 是 Weight \\(\\in \\mathbb{R}^{D \\times 1}\\) ， \\(b\\) 是 Bias \\(\\in \\mathbb{R}^{1}\\) 。\n损失函数（Loss Function） # 线性回归的训练目标是找到一组最优参数（权重 \\(W\\) 和偏置 \\(b\\) ），使得模型对训练数据的预测值与实际目标值之间的误差最小。具体来说，模型的目标是最小化误差函数（也称损失函数）。即使预测值 \\(\\hat{y_{i}}\\) 和实际值 \\(y_{i}\\) 的差异最小化。\n均方误差（Mean Squared Error, MSE） 是线性回归中最常用的损失函数。它计算预测值与真实值之间差异的平方和的均值：\n\\[ MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2 \\] 通过调整参数 \\(W\\) 和 \\(b\\) ，最小化 \\(MSE\\) 的目的是：\n惩罚大的预测误差（使得较大的误差对总体损失影响更显著）。 保证损失函数是连续且可导的（continuously differentiable），方便优化算法（如梯度下降）进行求解。 梯度下降法在线性回归中的应用（Gradient Descent） # 在线性回归中，梯度下降通过调整模型参数 \\(W\\) （权重）和 \\(b\\) （偏置），逐步逼近最优解。我们通过对 Loss Funcion 求导（函数的切线）来实现这一点。切线（slope）的斜率就是该点的导数，它将为我们提供前进的方向。我们沿着下降最快的方向逐步降低 Loss Function。每一步的大小由参数 \\(\\alpha\\) 决定，该参数称为学习率。梯度下降算法可以表示为：\n\\[ \\begin{align*} \u0026\\frac{\\partial L}{\\partial w}= -\\frac{2}{N}\\sum_{i=1}^{N}(y_{i}-\\hat{y_{i}})x_{i} \\\\ \u0026\\frac{\\partial L}{\\partial b}= -\\frac{2}{N}\\sum_{i=1}^{N}(y_{i}-\\hat{y_{i}}) \\\\ \u0026 w := w - \\alpha \\frac{\\partial L}{\\partial w}, \\quad b := b - \\alpha \\frac{\\partial L}{\\partial b} \\\\ \\end{align*} \\] \\[ \\begin{align*} \u0026\\mathrm{Repect\\ until \\ convergence} \\{ \\\\ \u0026\\ \\ \\ \\ w_{j}^{new} :=w_{j}^{old} - \\alpha\\frac{2}{N}\\sum_{i=1}^{N}(y_{i}-\\hat{y_{i}}) \\ \\ \\ \\ \\mathrm{for} \\ j = 0 \\cdot\\cdot\\cdot d \\\\ \u0026\\} \\\\ \\end{align*} \\] 收敛准则 损失函数的变化小于某个阈值（如 \\(\\Delta L \u003c \\epsilon\\) ）。 达到预设的最大迭代次数。 学习率的影响 # 我们应该调整参数 \\(\\alpha\\) 以确保梯度下降算法在合理的时间内收敛。如果 \\(\\alpha\\) 太小，梯度下降可能会很慢。如果 \\(\\alpha\\) 太大，梯度下降可能会超过最小值。它可能无法收敛，甚至发散（ fail to converge, or even diverge）。无法收敛或花费太多时间获得最小值意味着我们的步长是错误的。\n\\(\\alpha\\) 太大：可能导致更新过快，错过最优解，甚至发散。 \\(\\alpha\\) 太小：收敛速度慢，需要更多迭代。 性能评估（Evaluation Metrics） # 在训练完线性回归模型后，需要对模型的性能进行评估，以了解模型的拟合效果和预测能力。我们常使用均方根误差（RMSE）和确定系数（ \\(R^2 \\) 得分）来评估我们的模型。RMSE是残差平方和平均值的平方根。RMSE的定义是：\n\\[ RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2} \\] \\(R^2 \\) 得分（ \\(R^2 \\) score）表示模型解释目标变量总变异的比例：。它可以被定义为：\n\\[ R^2 = 1 - \\frac{\\sum_{i=1}^{N} (y_i - \\hat{y}i)^2}{\\sum_{i=1}^{N} (y_i - \\bar{y})^2} \\] 其中：\n\\(\\bar{y}\\) 是目标值的均值。 \\(R^2\\) 的取值范围是 [0, 1]（可以小于 0，表示模型比均值模型还差）。 解释：\n\\(R^2 = 1\\) ：模型能完全解释目标变量。 \\(R^2 = 0\\) ：模型的表现与仅使用目标值均值的基准模型相同。 \\(R^2 \u003c 0\\) ：模型表现比基准模型差。 Linear Regression 代码实现 # # \u0026lt;--- From scratch ---\u0026gt; import numpy as np from sklearn.model_selection import train_test_split # 生成数据：y = 2 + 3*X + 噪声 X = np.random.rand(100, 1) # 随机生成100个样本，只有一个特征 y = 2 + 3 * X + np.random.rand(100, 1) # 添加噪声项模拟真实数据 # 数据划分：80%训练集，20%测试集 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 均方误差函数 (Mean Squared Error) def MSE(y_1, y_2): return np.square(np.subtract(y_1, y_2)).mean() # 均方根误差函数 (Root Mean Squared Error) def RMSE(y_1, y_2): return np.sqrt(MSE(y_1, y_2)) # 线性回归的梯度下降实现 def LinearRegression_with_GD(X, y, learning_rate=0.001, threshold=0.001, max_iter=100): \u0026#34;\u0026#34;\u0026#34; X: 输入特征矩阵 y: 输出目标向量 learning_rate: 学习率，控制每次梯度更新的步长 threshold: 损失函数收敛的阈值，差异小于该值则停止迭代 max_iter: 最大迭代次数 \u0026#34;\u0026#34;\u0026#34; # 添加偏置项 (Intercept term) X_new = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1) # 在特征矩阵前添加一列1 total_sample, features = X_new.shape[0], X_new.shape[1] # 获取样本数和特征数 W = np.random.rand(features, 1) # 初始化权重为随机值 losses = [] # 用于记录每次迭代的损失值 for i in range(max_iter): y_pred = np.dot(X_new, W) # 计算预测值：h(X) = X_new * W loss = MSE(y_pred, y) # 计算当前的MSE损失 if losses and losses[-1] - loss \u0026lt; threshold: # 如果损失下降小于阈值，提前停止 losses.append(loss) break losses.append(loss) # 记录当前损失 gradient = np.dot(X_new.T, (np.subtract(y_pred, y))) # 计算梯度：∇J(W) W -= learning_rate * gradient # 使用梯度下降更新权重 print(\u0026#34;Final Training Loss:\u0026#34;, losses[-1]) # 输出最终训练损失 return W # 返回训练好的权重 # 使用训练好的权重进行预测 def LinearRegression_Predict(X, y, W): \u0026#34;\u0026#34;\u0026#34; X: 输入特征矩阵 y: 真实目标向量 W: 已训练的权重 \u0026#34;\u0026#34;\u0026#34; X_new = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1) # 添加偏置项 y_pred = np.dot(X_new, W) # 计算预测值：h(X) = X_new * W rmse = RMSE(y_pred, y) # 计算均方根误差 print(\u0026#34;RMSE:\u0026#34;, rmse) # 输出预测的RMSE return y_pred # 返回预测值 # 调用梯度下降实现线性回归 W = LinearRegression_with_GD(X_train, y_train) # 使用训练好的模型预测测试集 y_pred = LinearRegression_Predict(X_test, y_test, W) # \u0026lt;--- From scikit-learn ---\u0026gt; from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split import numpy as np # 生成数据 X = np.random.rand(100, 1) # 随机生成100个样本，每个样本只有一个特征 y = 2 + 3 * X + np.random.rand(100, 1) # 模拟线性关系，并加入噪声 # 数据集分割 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 创建线性回归模型，设置可选参数 model = LinearRegression() # 训练模型 model.fit(X_train, y_train) # 用训练集拟合模型 # 预测测试集 y_pred = model.predict(X_test) # 使用模型预测测试集 # 输出截距和权重 print(\u0026#34;Intercept (Bias):\u0026#34;, model.intercept_) # 截距项 print(\u0026#34;Coefficients (Weights):\u0026#34;, model.coef_) # 权重项 多项式回归（Polynomial Regression） # 多项式回归是一种线性回归的扩展形式，它适用于因变量与自变量之间呈现非线性关系的数据。通过在输入特征上应用多项式变换，进行了非线性扩展，将其映射到更高维的特征空间，使模型可以拟合复杂的非线性数据。多项式回归中，模型的参数（权重 \\(W\\) ）仍然是线性求解的，因此它在数学本质上是线性模型。其数学公式为: \\[ \\hat{y} = X_{poly}W + b = W_{1}x + W_{2}x^2 + \\cdots + W_{n}x^n + b \\] 特征处理： 线性回归：直接使用输入特征。 多项式回归：对输入特征进行非线性扩展。 拟合能力： 线性回归：只能拟合线性关系，容易欠拟合。 多项式回归：能够拟合非线性关系，但高次多项式可能导致过拟合。 多项式回归的步骤 # 数据准备：准备训练数据，其中包含输入特征（自变量）和目标值（因变量）。假设我们有一个简单的一维输入特征 \\(X\\) 和目标值 \\(y\\) ，目标是通过多项式回归来拟合这些数据。 特征工程：将原始特征扩展为多项式特征，使模型能够捕捉数据中的非线性关系。假设我们选择二次多项式（degree=2）。我们会将输入特征 \\(X = [1, 2, 3, 4, 5]\\) 扩展为： \\[ X_{\\text{poly}} = \\begin{bmatrix} 1 \u0026 x_1 \u0026 x_1^2 \\\\ 1 \u0026 x_2 \u0026 x_2^2 \\\\ 1 \u0026 x_3 \u0026 x_3^2 \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \\\\ 1 \u0026 x_n \u0026 x_n^2 \\end{bmatrix} \\] 模型训练：多项式回归本质上是在线性回归的基础上进行特征扩展。所以在特征扩展之后，我们依然使用线性回归的公式来训练模型： \\[ \\hat{y} = W_{0} + W_{1}x + W_{2}x^2 + \\cdots + W_{n}x^n \\] 回归模型的训练过程就是通过最小化 均方误差（MSE） 来求解模型的参数: \\[ MSE = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2 \\] 模型评估：通过训练误差和验证误差评估模型的性能。 Polynomial Regression 代码实现 # # \u0026lt;--- From scikit-learn ---\u0026gt; import numpy as np from sklearn.preprocessing import PolynomialFeatures # 导入多项式特征扩展模块 from sklearn.linear_model import LinearRegression # 导入线性回归模型 from sklearn.model_selection import train_test_split # 导入数据集分割模块 # 生成随机数据作为输入 data = np.random.normal(size=(200, 2)) # 生成一个包含200个样本，2个特征的正态分布数据集 result = 2 + data[:, 0] ** 3 + 4 * data[:, 1] # 计算目标值，包含多项式（3次方项和1次方项） X_train, X_test, y_train, y_test = train_test_split(data, result, test_size=0.3, random_state=0) # 将数据集划分为训练集和测试集，30%的数据为测试集 # 定义多项式回归函数 def Polynomial_Regression(train_input_features, train_outputs, prediction_features): # 创建多项式特征转换器，设置多项式的阶数为3（因为示例中包含3次方项） poly = PolynomialFeatures(degree=3) # 对训练数据进行拟合并转换，得到多项式特征 X_poly_train = poly.fit_transform(train_input_features) # 训练线性回归模型 model = LinearRegression() model.fit(X_poly_train, train_outputs) # 用训练数据训练模型 # 使用相同的多项式转换器对预测数据进行转换 X_poly_pred = poly.transform(prediction_features) # 对转换后的数据进行预测 predictions = model.predict(X_poly_pred) return predictions # 返回预测结果 # 调用多项式回归函数进行预测 y_pred = Polynomial_Regression(X_train, y_train, X_test) "},{"id":2,"href":"/docs/machine-learning/machine-learning-basics/","title":"Machine Learning Basics","section":"Machine Learning","content":" Machine Learning Basics # 机器学习的定义与类型 # 定义 # Machine Learning（机器学习） 是一种人工智能技术，核心目标是使计算机能够从数据中自动学习，并根据这些数据改进对特定任务的性能，而无需明确的编程指令。\n机器学习通过以下三个要素实现：\n数据（Data）：输入的原始数据或特征数据。 模型（Model）：表示学习的假设空间，决定如何从数据中学习模式。 优化目标（Object）：定义如何评估模型好坏并改进其性能（如最小化误差）。 主要类型 # 监督学习 (Supervised Learning) # 监督学习 (Supervised Learning) 从标记数据中学习。在这种情况下，输入数据及其对应的输出值被提供给算法，算法学习输入和输出值之间的映射（the mapping between the input and output values）。监督学习的目标是对新的、看不见的输入数据做出准确的预测。\n输入（Input）：特征数据 \\(X\\) 和目标变量 \\(y\\) （如标签或真实值）。\n输出（Output）：预测模型，用于对新数据进行分类或回归。\n应用场景：\n分类问题（Classification）：将输入数据划分到预定义类别中。 回归问题（Regression）：预测连续数值的目标变量。 无监督学习 (Unsupervised Learning) # 无监督学习 (Unsupervised Learning) 从未标记的数据中学习。在这种情况下，输入数据没有标记，算法自己学习在数据中寻找模式和结构。无监督学习的目标是发现数据中隐藏的模式和结构。\n输入（Input）：仅有特征数据 \\(X\\) 。 输出（Output）：数据的潜在结构或表示。 应用场景： 聚类 (Clustering)：将数据分组到不同簇中。 降维 (Dimensionality Reduction)：简化数据表示，同时保留主要信息。 半监督学习 (Semi-Supervised Learning) # 半监督学习 (Semi-Supervised Learning) 从标记和未标记数据的组合中进行学习。在这种情况下，算法学习在未标记数据中寻找模式和结构，并使用标记数据来指导学习过程。\n输入（Input）：部分标注的数据和大量未标注的数据。 输出（Output）：用于分类或回归的预测模型。 应用场景： 在标注数据有限或获取标签成本高昂的情况下（如医学影像标注）。 强化学习 (Reinforcement Learning) # 强化学习 (Reinforcement Learning) 通过与 环境（environment） 交互进行学习。在这种情况下，算法学习采取 行动（action） 来最大化环境提供的奖励信号。强化学习的目标是学习最大化长期 奖励（reward） 的 策略（policy）。\n输入（Input）：状态 \\(S\\) 、动作 \\(A\\) 、奖励 \\(R\\) 。 输出（Output）：一个策略 \\(\\pi\\) ，指引在不同状态下的最佳行动。 应用场景： 游戏 AI：如 AlphaGo 使用强化学习在围棋中击败人类选手。 机器人导航：训练机器人在环境中找到最佳路径。 泛化能力（Generalization） # 机器学习的核心挑战在于，我们必须在新的、以前未见过的输入上表现良好——而不仅仅是我们的模型所训练的输入。在以前未观察到的输入上表现良好的能力称为泛化（generalization）。在训练过程中，我们通过降低 训练误差 (Training Error) 优化模型。但模型不仅需要在训练集上表现优异，还需要降低泛化误差 (Generalization Error)，即 测试误差（Test Error）。\n数据集（Datasets）分类和误差（Errors） # 训练集（Train Set） # 定义：训练集是模型学习的主要数据来源（占大多数），包括 输入特征 和对应的 目标输出（对于监督学习）。 功能：用于训练模型（的参数），通过优化算法最小化训练误差。 训练误差（Training Error）：训练误差是模型在训练数据上的错误率。它是通过测量每个训练示例的预测输出（predicted output）与实际输出（actual output）之间的差异来计算的。由于模型是在此数据上训练的，因此预计它会在此数据上表现良好，并且训练误差通常较低。\n验证集（Validation Set） # 定义：验证集是从训练数据中分离出来的一部分，用于评估模型在未见数据上的表现。 功能： 帮助调整超参数（如学习率、正则化系数、模型结构等）。 用于选择最佳模型，例如在多次训练后选择验证误差最低的模型。 验证误差（Validation Error）：验证误差是模型在验证数据上的错误率。用于评估训练期间模型的性能，目标是找到验证误差最低的模型。\n测试集（Test Set） # 定义：测试集是完全独立于训练和验证的数据，模型在训练和验证过程中从未接触过。 功能：用于评估模型的最终泛化性能，反映模型在实际场景中的表现。 测试误差（Test Error）：测试误差是模型在测试数据上的错误率。测试数据是与训练和验证数据完全独立的数据集，用于评估模型的最终性能。测试误差是 最重要的误差指标，因为它告诉我们模型在新的、未见过的数据上的表现如何。\n欠拟合 (Underfitting) # 定义：当模型过于简单而无法捕捉数据中的底层模式时，就会发生欠拟合。该模型具有高偏差和低方差，这意味着它在训练和测试数据上的表现都很差。 表现： 模型复杂度低，无法很好地拟合训练数据。 训练误差与测试误差都较大，模型性能较差。 原因： 模型过于简单，无法学习到数据中的复杂关系或特征。 特征不足，数据无法充分表达问题。 训练时间不足，模型未完全收敛。 解决方法： 增加模型复杂度：选择更复杂的模型（如从线性模型切换到非线性模型）。 增加特征：引入更多特征或通过特征工程提取更有效的特征。 延长训练时间：确保模型充分训练直到收敛。 过拟合 (Overfitting) # 定义：当模型过于复杂，与训练数据的拟合度过高时，就会发生过度拟合，从而捕获数据中的噪声和随机波动。因此，该模型在新的、未见过的数据上表现不佳。 表现： 模型对训练数据拟合良好，训练误差很低。 测试误差较高，模型泛化能力差。 原因： 模型复杂度过高，学习到了训练数据中的噪声或无意义模式。 训练数据过少，噪声占比高。 缺乏正则化约束，模型自由度太高。 解决方法： 减少模型复杂度：降低模型自由度（如减少神经网络层数或节点数）。 增加数据量：收集更多样本，减少模型对噪声的敏感性。 正则化： \\(L_1\\) 正则化：鼓励稀疏性，减少不重要的参数。 \\(L_2\\) 正则化：限制参数的幅度，防止过大权重。 交叉验证：通过交叉验证选择模型或超参数，避免过拟合。 偏差-方差权衡（Bias-Variance Tradeoﬀ） # 偏差 (Bias) # 定义：偏差衡量模型预测值的期望值与真实值之间的偏离程度。 公式： \\[ \\text{Bias} = E[f(x)] - f^*(x) \\] \\(f(x)\\) ：模型的预测值 \\(f^*(x)\\) ：真实值或目标函数 方差 (Variance) # 定义：方差衡量模型在不同训练数据集上的预测值的变化幅度。 公式： \\[ \\text{Variance} = E[(f(x) - E[f(x)])^2] \\] 方差的平方根称为标准差，表示为 \\(SE(x)\\) 总误差分解 # 模型的总误差（Expected Error）可以分解为三部分：偏差、方差和噪声。在实际应用中，我们需要平衡 Bias 和 Variance 以此来找到最小的 Expected Error，目标是找到偏差和方差的最佳平衡点，既能保证低训练误差，又能有良好的泛化能力。\n\\[ \\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise} \\] \\(\\text{Bias}^2\\) : 表示系统误差，与模型的表达能力有关。 \\(\\text{Variance}\\) : 表示模型对训练数据的敏感程度。 \\(\\text{Irreducible Noise}\\) : 数据中固有的随机噪声，无法通过任何模型降低。 Bias-Variance Tradeoff 理解 # 偏差-方差权衡指的是模型很好地拟合训练数据的能力（低偏差） 与其推广到新数据的能力（低方差） 之间的权衡。\n随着模型复杂度的增加，偏差趋于减小，方差趋于增大。如果模型太简单，它可能具有高偏差，这意味着它无法捕捉数据中的潜在模式，训练误差和测试误差会很高。如果模型太复杂，它可能具有高方差，这意味着它对训练数据中的噪声过于敏感，并且可能无法很好地推广到新数据，从而导致过度拟合。为了在偏差和方差之间取得平衡，我们需要找到模型的最佳复杂度。\n低Bias但高Variance：复杂模型，过度拟合，表现为训练误差低但测试误差高。 高Bias但低Variance：简单模型，欠拟合，表现为训练误差和测试误差都高。 选择模型评估方法 # 交叉验证 (Cross-Validation)：通过分割数据集来更好地估计模型的偏差和方差。 学习曲线 (Learning Curve)：观察训练集误差和验证集误差随样本数量或模型复杂度变化的趋势，帮助分析模型的偏差和方差问题。 交叉验证（Cross Validation） # 交叉验证是机器学习中用于评估模型在独立数据集上性能的一种技术。交叉验证的基本思想是将可用数据分成两个或多个部分，其中一个部分用于训练模型，另一个部分用于验证模型。交叉验证用于通过提供模型对新数据的泛化程度的估计来防止过度拟合。它有效解决了仅用单一验证集或测试集可能导致的评估结果不稳定或偏差的问题。它也可以用于调整模型的超参数。\nNote：在交叉验证（Cross-Validation）中，每一次计算的是 验证误差（validation error），而不是训练误差（training error）。交叉验证的目的是估计模型在未见数据上的性能，因此重点在于验证集的误差。\nK折交叉验证 (K-Fold Cross-Validation) # 将数据集划分为 K 个不重叠的子集（folds）。每次取一个子集作为验证集，其余 K-1 个子集作为训练集。重复 K 次，每次更换验证集，最终对所有验证结果（Validation Error）取平均。 在 Cross-Validation 的框架内，调节模型的 hyperparameters，对使用不同参数的模型进行 Cross-Validation 验证。根据最终结果选择最佳模型。 在选择好最佳的 hyperparameters 和模型后，用全部的数据进行训练。 使用完全独立的 Testset 来评估模型的泛化能力。 优缺点： # 优点：可靠性高，适合数据量较大的情况。 缺点：当 K 较大时，计算成本较高。 K-Fold Cross-Validation 代码实现： # # \u0026lt;--- From scratch ---\u0026gt; import numpy as np from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error # 简单的数据集生成 X = np.array([[i] for i in range(100)]) y = np.array([i*2 for i in range(100)]) model = LinearRegression() def K_fold_CrossValidation(X, y, model, k=5): # 数据集划分，按顺序划分为K个子集 fold = len(X) // k error = [] for i in range(k): val_start_idx = i * fold val_end_idx = (i + 1) * fold # 创建训练集和验证集 X_train = np.concatenate(X[:val_start_idx], X[val_end_idx:]), y_train =np.concatenate(y[:val_start_idx], y[val_end_idx:]) X_val = X[val_start_idx:val_end_idx] y_val = y[val_start_idx:val_end_idx] # 使用模型进行训练和预测 model.fit(X_train, y_train) y_pred = model.predict(X_val) # 计算误差 curr_error = mean_squared_error(y_val, y_pred) error.append(curr_error) return np.mean(error) # 执行交叉验证 avg_error = K_fold_CrossValidation(X, y, model, k=5) print(f\u0026#34;Average cross-validation error: {avg_error}\u0026#34;) # \u0026lt;--- From scikit-learn ---\u0026gt; from sklearn.model_selection import cross_val_score from sklearn.linear_model import LinearRegression from sklearn.datasets import make_regression import numpy as np # 生成数据集 X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42) # 初始化模型 model = LinearRegression() # 执行5折交叉验证，评分标准为负均方误差（neg_mean_squared_error） scores = cross_val_score( estimator=model, # estimator 类型对象，必须实现 fit 和 predict 方法。 X=X, # 特征数据 y=y, # 标签数据 cv=5, # 指定使用5折交叉验证 scoring=\u0026#39;neg_mean_squared_error\u0026#39;, # 表示使用的评分方法（如 accuracy, neg_mean_squared_error, f1, roc_auc 等） return_train_score=False # 不返回训练集得分，只返回验证集得分 ) # 输出每次折的误差 print(f\u0026#34;Cross-validation errors for each fold: {-scores}\u0026#34;) # 输出平均误差 print(f\u0026#34;Average cross-validation error: {-scores.mean()}\u0026#34;) 留一法 (Leave-One-Out Cross-Validation, LOOCV) # 数据集中每个样本单独作为一次验证集，其余样本作为训练集。 模型训练次数等于样本数 N，最后计算所有验证集的误差平均值。 优缺点： # 优点：不浪费数据，最全面的评估方法。 缺点：计算代价极高，尤其是数据集较大时。 常见的机器学习完整流程 # 数据的准备和预处理 从数据库、API、文件等来源收集数据。 对所有的数据进行数据清洗 (e.g 处理缺失值，处理异常值，数据格式转换) 数据分割（训练集80%、验证集10%、测试集10%） 数据预处理 - 仅针对训练集（标准化/归一化，特征工程/选择/变换，Label Encoding，One-Hot Encoding） 模型训练与评估 选择模型：根据任务性质选择初始模型 设置交叉验证策略，交叉验证中的模型训练 - 使用训练集。 评估指标：不仅观察平均值（Validation Error），还需关注标准差，评估模型的稳定性。 超参数调优 网格搜索 (Grid Search)：枚举所有可能的超参数组合，使用交叉验证评估每一组参数的表现。选择评估结果最优的参数组合。 随机搜索 (Random Search)：从参数空间中随机采样一定数量的超参数组合进行评估。 测试集上的最终评估 固定最佳模型：在交叉验证确定的最佳超参数和模型结构上，重新训练模型，使用全体训练数据。 在测试集上评估：用测试集数据评估最终模型的性能，作为模型实际泛化能力的最终指标。Test Set 仅在最终测试时使用一次以防止数据泄漏（Data Leakage） "},{"id":3,"href":"/docs/deep-learning/perceptrons-and-neural-network/","title":"Perceptrons and Neural Network","section":"Deep Learning","content":" 感知机和神经网络 # 感知机和神经网络（Perceptrons and Neural Network） # 感知机（Perceptron）是神经网络（Neural Network）的雏形，基于单个或多个 神经元（Neuron） 通过线性组合和简单的激活函数（如阶跃函数）实现输入数据的分类。神经元是感知机的基本组成单元，其功能是对输入特征进行加权、偏置调整并生成输出。随着任务复杂性的增加，感知机被扩展为 多层感知机（Multilayer Perceptron, MLP），通过堆叠多个隐藏层并引入非线性激活函数，使网络能够表达复杂的映射关系，解决非线性问题，是前馈神经网络（Feedforward Neural Network）的核心架构。\n神经元（Neuron） # 神经元（Neuron）是神经网络的最基本单元，模拟生物神经元的行为。它的主要功能是 接收输入、加权处理并通过激活函数生成输出。\n\\[ y = f\\left(\\sum_{i=1}^n w_i x_i + b\\right) \\] 其中：\n\\(x_i\\) : 输入特征。 \\(w_i\\) : 权重，衡量每个输入的影响程度。 \\(b\\) : 偏置，用于调整输出的灵活性。 \\(f(\\cdot)\\) : 激活函数，增加非线性能力（如ReLU、Sigmoid）。 \\(y\\) : 输出信号。 感知机（Perceptrons） # 感知器（Perceptrons）是一种执行 二元分类（binary classification） 的神经网络，它将输入特征映射到输出决策，通常将数据分为两个类别之一，例如 0 或 1。感知器由一层输入节点（input nodes）组成，这些节点与一层输出节点（output nodes）完全连接（fully connected）。感知子可以用图像表示为：\n单个感知机可以看作是一个最简单的神经元，专注于实现线性分类任务。单个神经元是感知机的扩展形式，通过引入更灵活的激活函数和多层结构，可以应用于更复杂的问题。\n模型结构 # 感知机的基本结构包括以下组成部分：\n输入层（Input Layer）：接收输入特征向量 \\(x = [x_1, x_2, \\dots, x_n]\\) 。 权重向量（Weights）：每个输入特征 \\(x_i\\) 对应的权重 \\(w_i\\) 。 偏置（Bias, b）：平移决策边界，增强模型的灵活性。 线性组合： \\[ z = W^T X + b = \\sum_{i=1}^n w_i x_i + b \\] 激活函数（Activation Function）：通常为符号函数 \\(\\text{sign}(z)\\) ，用于将加权和映射为输出标签。 输出结果： \\[ y = \\text{sign}(z) = \\begin{cases} +1, \u0026 \\text{if } z \\geq 0 \\\\ -1, \u0026 \\text{if } z \u003c 0 \\end{cases} \\] 损失函数 # 感知机的损失函数本质上是用来惩罚误分类样本，从而引导模型学习到能够正确分类所有样本的权重参数。他的主要目标是找到一个超平面（hyperplane）： \\(w^T x + b = 0\\) 。使得数据点能够被正确分类，即：\n如果 \\(y_i = +1\\) ，那么希望 \\(w^T x_i + b \u003e 0\\) ； 如果 \\(y_i = -1\\) ，那么希望 \\(w^T x_i + b \u003c 0\\) 。 感知机的损失函数只关注那些被误分类的样本。对于误分类的样本，有： \\[ y_i (w^T x_i + b) \\leq 0 \\] 损失函数定义为所有误分类样本的负边界距离的总和： \\[ L(w, b) = -\\sum_{i \\in M} y_i (w^T x_i + b) \\] \\(M\\) 表示所有被误分类的样本的集合； \\(y_i (w^T x_i + b)\\) 表示样本 \\(x_i\\) 到决策超平面的有符号距离。 更新规则 # 严格来说，感知机本身并不使用梯度下降法进行优化，因为感知机的损失函数是分段的、非连续的，无法直接对其求导。感知机的权重更新公式是： \\[ \\begin{align*} \u0026w \\leftarrow w + \\eta \\cdot y_i \\cdot x_i \\\\ \u0026b \\leftarrow b + \\eta \\cdot y_i \\\\ \\end{align*} \\] 感知机的权重更新可以看作是一种离散化、非平滑的近似梯度下降过程:\n每次只对一个误分类样本 \\(x_i\\) 更新权重和偏置； 更新方向为该样本的贡献（ \\(-y_i x_i\\) 的负梯度方向）； 从几何角度看：如果一个样本被误分类，更新方向是沿着样本 \\(x_i\\) 的方向，并且朝着正确分类 \\(y_i\\) 的方向推进决策边界。\n感知机（Perceptrons）和逻辑回归（Logistic Regression）的区别 # 感知机（Perceptrons）和逻辑回归（Logistic Regression）在表面上确实有很多相似之处，因为它们都属于线性模型，但它们的核心区别在于损失函数和输出目标。\n激活函数区别\nPerceptrons 使用符号函数（sign function）作为激活函数。这意味着感知机的输出是基于决策边界的二元结果，不提供概率信息。 \\[ y = \\text{sign}(w^T x + b) \\] Logistic Regression 使用逻辑函数（sigmoid function）将线性组合结果映射到概率范围： \\[ P(y=1|x) = \\sigma(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}} \\] 损失函数区别\nPerceptrons 的学习目标是最小化误分类样本的数量，仅对被误分类的样本进行权重更新。 \\[ L(w, b) = -\\sum_{i \\in M} y_i (w^T x_i + b) \\] Logistic Regression 通过最大化条件概率 \\(P(y|x)\\) 的对数似然来优化参数： \\[ L(w, b) = -\\sum_{i=1}^N \\left[y_i \\log(\\sigma(w^T x_i + b)) + (1 - y_i) \\log(1 - \\sigma(w^T x_i + b)) \\right] \\] 决策边界区别\n感知机和逻辑回归都假设数据是线性可分的，因此其决策边界都是一个超平面：\nPerceptrons 直接依赖超平面将数据划分为两个类别，但没有提供关于样本距离边界的任何信息。 Logistic Regression 利用概率信息描述样本在边界两侧的信心度，决策边界定义为 \\(P(y=1|x) = 0.5\\) 。 Note： Perceptrons 通常仅适用于线性可分的数据。算法在数据线性可分时会收敛；但如果数据线性不可分，则会陷入无限循环。Logistic Regression 可处理线性不可分数据，即使数据线性不可分，也能找到最优的权重（通过拟合概率分布）。\n感知机（Perceptrons） 代码实现 # # \u0026lt;--- From scratch ---\u0026gt; import numpy as np def Perceptrons(X, y, lr, max_iter): n_samples, n_features = X.shape weights = np.zeros(n_features) bias = 0 for _ in range(max_iter): for i in range(n_samples): # 计算线性输出 linear_output = np.dot(X[i], weights) + bias # 如果预测错误，则更新权重和偏置 if y[i] * linear_output \u0026lt;= 0: weights += lr * y[i] * X[i] bias += lr * y[i] return weights, bias # 模型预测 def predict(X, weights, bias): linear_output = np.dot(X, weights) + bias return np.where(linear_output \u0026gt;= 0, 1, -1) # 数据示例 X = np.array([[1, 1], [2, 1], [1, 2], [-1, -1], [-2, -1], [-1, -2]]) y = np.array([1, 1, 1, -1, -1, -1]) # 模型训练 weights, bias = Perceptrons(X, y, lr=0.1, max_iter=10) # 输出结果 print(\u0026#34;学习到的权重:\u0026#34;, weights) print(\u0026#34;学习到的偏置:\u0026#34;, bias) # 进行预测 predictions = predict(X, weights, bias) print(\u0026#34;预测结果:\u0026#34;, predictions) 多层感知机（Multilayer Perceptron） # 多层感知机（MLP）是最常见的前馈神经网络（Feedforward Networks）之一，由多个全连接层组成。它是神经网络的基础结构，广泛用于分类、回归等任务。MLP的核心思想是通过隐藏层和非线性激活函数，提取输入数据的特征并映射到目标输出。\n输入层（Input Layer） # 输入层是多层感知机（MLP）的第一部分，用于接收外部输入数据并将其传递给网络的隐藏层。输入层的设计直接决定了模型对数据的适配能力。输入层可以视为数据和网络之间的接口：\n数据接受：接收外部特征输入，通常以向量或矩阵的形式表示。 维度映射：将原始数据的特征维度（ \\(d\\) ）映射到神经网络的内部表示维度。输入数据的特征维度需与输入层的线性变换参数兼容。 数据传递：输入层通过线性变换（如 \\(xW + b\\) ）将输入映射到第一个隐藏层的维度。它仅负责将输入数据直接传递到隐藏层。 神经元（Neuron）数量：输入层的神经元数量等于第一个隐藏层的神经元数量，即输入层通过线性变换将输入特征映射到第一个隐藏层的维度。输入数据的特征数量（ \\(d\\) ）决定了输入层每个神经元的权重数量：\n对于输入维度为 \\(d\\) 的数据，每个神经元会有 \\(d\\) 个权重加上一个偏置项。 示例: 如果输入数据具有 4 个特征（如 \\([x_1, x_2, x_3, x_4]\\) ），且第一个隐藏层包含 10 个神经元，则输入层需要： 10 个神经元（每个神经元与隐藏层的每个神经元一一对应）。 每个神经元包含 4 个权重（分别对应 4 个输入特征）和 1 个偏置项。 输入数据格式：输入数据通常为一个向量或矩阵：\n单样本输入: 向量形式，如 \\([x_1, x_2, …, x_d]\\) 。 批量输入: 矩阵形式，形状为 \\((\\text{batch size}, d)\\) ，其中 \\( \\text{batch size} \\) 为每次输入的样本数， \\(d\\) 为特征数。 代码示例: class SimpleInputLayer(nn.Module): def __init__(self, input_dim, hidden_dim): super(SimpleInputLayer, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) # 从输入层到第一个隐藏层的线性变换 self.relu = nn.ReLU() # 激活函数（ReLU） def forward(self, x): x = self.fc1(x) # 输入数据经过线性变换 x = self.relu(x) # 使用 ReLU 激活函数 return x 隐藏层 # 隐藏层是神经网络中介于输入层和输出层之间的层，它是神经网络学习和表示复杂映射关系的关键部分。隐藏层通过层叠的神经元以及非线性激活函数，能够从数据中提取特征和学习模式，是深度学习的核心。隐藏层的主要功能有：\n特征提取：隐藏层负责从输入数据中提取有用的特征，形成更高维、更抽象的表示。 非线性映射：通过非线性激活函数（如 ReLU、Sigmoid、Tanh），隐藏层能够学习复杂的非线性关系，而非仅仅是简单的线性变换。 数据处理：每个隐藏层接收上一个层的输出，经过线性变换和激活函数处理后，传递到下一层。 每个隐藏层由以下三部分组成：\n神经元： 每个神经元代表一个计算单元，接收来自上一层所有神经元的输入，加权求和后进行激活函数变换。 隐藏层的神经元数量是一个超参数，需要根据具体问题进行选择。 权重（Weights）和偏置（Biases）： 权重：连接两层之间的神经元，并决定输入的重要性。 偏置：用于调整激活函数的输出，增加模型的表达能力。 公式： \\(z = xW + b\\) ，其中 \\(x\\) 是输入， \\(W\\) 是权重矩阵， \\(b\\) 是偏置向量。 激活函数： 激活函数引入非线性，使神经网络能够学习复杂的非线性映射。\n常见激活函数： ReLU (Rectified Linear Unit)\n\\[ f(x) = \\max(0, x) \\] 计算简单：仅比较输入值是否大于 0，操作速度快。 非线性：尽管形式简单，但 ReLU 是非线性的，可帮助网络学习复杂特征。 稀疏性：当输入小于 0 时，输出为 0，相当于让部分神经元不激活，提升模型稀疏性。 问题：可能导致“神经元死亡”（当许多权重使输入始终小于 0，导致该神经元永不更新）。 ReLU 广泛应用于深层神经网络，一般情况下是默认选择，适合大多数场景。 Sigmoid\n\\[ f(x) = \\frac{1}{1 + e^{-x}} \\] 输出范围： \\([0, 1]\\) ，适合表示概率值。 单调性：对于输入增大，输出逐渐趋近于 1，但变化减缓。 梯度消失问题：当 \\(x\\) 的绝对值较大时，函数的梯度趋近于 0，导致反向传播时权重更新困难。 Tanh (双曲正切函数)\n\\[ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\] 输出范围： \\([-1, 1]\\) ，比 Sigmoid 更对称，适合隐藏层激活函数。 对称性：中心对称于原点，有助于加速收敛。 梯度消失问题：与 Sigmoid 类似，当 \\(x\\) 的绝对值较大时，梯度会趋近于 0。 关于非线性\n线性关系：\n一个函数是线性的，如果它满足叠加性和齐次性，即： \\(f(ax + by) = a \\cdot f(x) + b \\cdot f(y)\\) 例如： \\(y = 2x + 3\\) 是线性函数。它的图形是一直线。\n非线性关系：\n非线性函数不满足上述性质。例如： \\(y = x^2\\) 或 \\(y = \\sin(x)\\) 是非线性的。它的图形可能是弯曲的、不规则的。\nNote： 如果神经网络的每一层都只包含线性变换（如\n\\(y = xW + b\\) ），那么即使增加多层隐藏层，整个网络仍然是一个线性函数的组合。 隐藏层理解\n隐藏层的作用可以类比为数据的逐步“翻译”或“加工”：\n低层特征提取：第一层隐藏层处理输入数据的基本特征（如简单的边缘、颜色或频率等）。 中层特征组合：中间层将低层特征组合成更高阶的特征（如形状、局部模式或局部结构）。 高层特征整合：更深层次的隐藏层进一步整合复杂特征，用于最终分类或预测任务。 每一层将数据映射到新的特征空间中，这种映射使得神经网络能够捕获从简单到复杂的模式。隐藏层维度的设计通常遵循“逐步扩展—再收缩”的模式：\n增加维度：扩展特征空间 增加维度可以让模型在更高维的特征空间中提取更加复杂和细粒度的模式。 例如，输入层可能包含较少的原始特征（如像素、频谱或特定数值），但这些特征经过线性变换和激活后，隐藏层可以生成更多维度的“隐含特征”。 扩展特征空间类似于“打开数据的潜力”，为网络提供更丰富的信息处理能力。 减少维度：聚合有用信息 在后续隐藏层中减少维度是为了压缩特征表示，去除冗余信息，仅保留与任务相关的高质量特征。 这一过程可以防止模型过拟合，同时提高计算效率和泛化能力。 减少维度还可以实现对数据的进一步“压缩”，形成对输入数据的简洁而有力的表示。 Note： 为什么逐步增加维度（e.g. dim4 -\u0026gt; dim128 -\u0026gt; dim256 -\u0026gt; dim512）而不是直接扩展到高维（e.g. dim4 -\u0026gt; dim512）？\n直接扩展到高维，模型可能无法有效捕获特征层次，会使学习过程更加困难。需要的权重参数过多，特别是在数据量不足时效果较差。同时由于缺少逐步提取特征的过程，模型可能过于依赖输入数据的特定模式。\n输出层（Output Layer） # 输出层是神经网络的最后一部分，其核心职责是根据模型的目标任务（Cost function），生成适合应用场景的输出。不同任务对输出层的设计要求不同，例如分类、回归或生成任务等。输出层的实现通常结合特定的单元（如线性、Sigmoid、Softmax）和损失函数，以适应不同类型的数据分布和学习目标。\n线性单元（Linear Units） 适用场景: 连续型输出（如回归任务）。 数学表达式: 输出值 \\(y = xW + b\\) （无激活函数）。 解释: 线性单元生成实值输出，不引入非线性变换。 损失函数通常为均方误差（MSE）: \\(L = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\) ，其中 \\(\\hat{y}_i\\) 是预测值。 代码示例: class LinearOutputLayer(nn.Module): def __init__(self, input_dim): super(LinearOutputLayer, self).__init__() self.fc = nn.Linear(input_dim, 1) # 线性变换（输出一个数值） def forward(self, x): x = self.fc(x) # 线性变换 return x Sigmoid 单元 适用场景: 二分类任务。 数学表达式: \\(y = \\sigma(xW + b) = \\frac{1}{1 + e^{-(xW + b)}}\\) 。 解释: Sigmoid 单元将输出值压缩到区间 \\((0, 1)\\) ，解释为正样本的概率 \\(P(y=1|x)\\) 。 损失函数通常为二元交叉熵损失（Binary Cross-Entropy Loss）: \\(L = - \\frac{1}{n} \\sum_{i=1}^n [y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]\\) ，其中 \\(\\hat{y}_i\\) 是预测的概率值。 代码示例: class BinaryOutputLayer(nn.Module): def __init__(self, input_dim): super(BinaryOutputLayer, self).__init__() self.fc = nn.Linear(input_dim, 1) # 线性层，将输入映射到 1 个输出（概率） self.sigmoid = nn.Sigmoid() # Sigmoid 激活函数 def forward(self, x): x = self.fc(x) # 线性变换 x = self.sigmoid(x) # Sigmoid 激活函数 return x Softmax 单元 适用场景: 多分类任务。 数学表达式: \\(y_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}\\) ，其中 \\(z_j\\) 是第 \\(j\\) 类的得分。 解释: Softmax 单元将多个输出值转换为概率分布，保证 \\(\\sum_{j=1}^K y_j = 1\\) ，每个 \\(y_j\\) 表示属于第 \\(j\\) 类的概率。 损失函数通常为多分类交叉熵损失（Categorical Cross-Entropy Loss）: \\(L = - \\sum_{i=1}^n \\sum_{j=1}^K y_{ij} \\log(\\hat{y}_{ij})\\) ，其中 \\(y_{ij}\\) 是真实标签， \\(\\hat{y}_{ij}\\) 是预测概率。 代码示例: class MultiClassOutputLayer(nn.Module): def __init__(self, input_dim, num_classes): super(MultiClassOutputLayer, self).__init__() self.fc = nn.Linear(input_dim, num_classes) # 输出类别数个神经元 self.softmax = nn.Softmax(dim=1) # Softmax 激活函数 def forward(self, x): x = self.fc(x) # 线性变换 x = self.softmax(x) # Softmax 激活函数 return x Architecture Design # 向前传播 # 向后传播 # MLP的完整流程 # MLP 代码实现 # "},{"id":4,"href":"/posts/03_logistic_regression/","title":"Logistic Regression","section":"Blog","content":"Binary Logistic Regression is one of the most simple and commonly used Machine Learning algorithms for two-class classification. It is easy to implement and can be used as the baseline for any binary classification problem. Its basic fundamental concepts are also constructive in deep learning. Logistic regression describes and estimates the relationship between one dependent binary variable and independent variables.\n\u0026ldquo;分类\u0026quot;是应用 逻辑回归(Logistic Regression) 的目的和结果, 但中间过程依旧是\u0026quot;回归\u0026rdquo;. 通过逻辑回归模型, 我们得到的计算结果是0-1之间的连续数字, 可以把它称为\u0026quot;可能性\u0026quot;（概率）. 然后, 给这个可能性加一个阈值, 就成了分类. 例如, 可能性大于 0.5 即记为 1, 可能性小于 0.5 则记为 0.\nThe classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which \\(y\\) can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.)\n\\[ log\\frac{p}{1-p} = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{3} + \\cdot\\cdot\\cdot + \\beta_{n}x_{n} = \\beta^{T}x \\\\ \\] \\[ \\begin{align*} P(y = 1) \u0026= p = \\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x}} \\\\ P(y = 0) \u0026= 1 - p = \\frac{1}{1+e^{\\beta^{T}x}} \\end{align*} \\] We could approach the classification problem ignoring the fact that \\(y\\) is discrete-valued, and use our old linear regression algorithm to try to predict \\(y\\) given \\(x\\) . However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn’t make sense for \\(h_{\\theta}(x)\\) to take values larger than 1 or smaller than 0 when we know that \\(y \\in {0, 1}\\) . To fix this, let’s change the form for our hypotheses \\(h_{\\theta}(x)\\) to satisfy \\(0 \\leq h_{\\theta}(x) \\leq 1\\) This is accomplished by plugging \\(\\theta^{T}x\\) into the Logistic Function. Our new form uses the \u0026ldquo;Sigmoid Function,\u0026rdquo; also called the \u0026ldquo;Logistic Function\u0026rdquo;:\n\\[ f(x) = \\frac{1}{1+e^{-(x)}} \\\\ \\] Logistic Regression # First we need to define a Probability Mass Function:\n\\[ \\begin{align*} \u0026\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P(Y=1|X=x) = \\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x}} \\\\ \u0026\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P(Y=0|X=x) = 1 - \\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x}} = \\frac{1}{1+e^{\\beta^{T}x}} \\\\ \u0026\\Rightarrow \\ \\ \\ \\ P(Y \\ |X=x_{i}) = (\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}})^{y_{i}} (\\frac{1}{1+e^{\\beta^{T}x_{i}}})^{1-y_{i}} \\\\ \\end{align*} \\] Naturally, we want to maximize the right-hand-side of the above statement. We will use Maximun Likelihood Estimation(MLE) to find \\(\\beta\\) :\n\\[ \\hat{\\beta}_{MLE}= \\arg\\max_{\\beta} L(\\beta) \\\\ \\] \\[ L(\\beta) = \\prod_{i=1}^n P(Y=y_{i} |x_{i}) = \\prod_{i=1}^n (\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}})^{y_{i}} (\\frac{1}{1+e^{\\beta^{T}x_{i}}})^{1-y_{i}} \\\\ \\] \\[ \\begin{align*} l(\\beta) = log\\ L(\\beta) \u0026= \\sum_{i=1}^n y_{i}\\left[\\beta^{T}x_{i} - log(1+e^{\\beta^{T}x_{i}})] + (1-y_{i})[-log(1+e^{\\beta^{T}x_{i}})\\right] \\\\ \u0026=\\sum_{i=1}^n y_{i}\\beta^{T}x_{i}- log(1+e^{\\beta^{T}x_{i}}) \\\\ \\end{align*} \\] Newton‐Raphson Method for Binary Logistic Regression # Newton’s Method is an iterative equation solver: it is an algorithm to find the roots of a convex function. Equivalently, the method solves for root of the derivative of the convex function. The idea behind this method is ro use a quadratic approximate of the convex function and solve for its minimum at each step. For a convex function \\(f(x)\\) , the step taken in each iteration is \\(-(\\nabla^{2}f(x))^{-1}\\nabla f(x)\\) . while \\(\\lVert\\nabla f(\\beta)\\rVert \u003e \\varepsilon\\) :\n\\[ \\beta^{new} = \\beta^{old}-(\\nabla^{2}f(x))^{-1}\\nabla f(x) \\\\ \\] Where \\(\\nabla f(x)\\) is the Gradient of \\(f(x)\\) and \\(\\nabla^{2} f(x)\\) is the Hessian Matrix of \\(f(x)\\) .\n\\[ \\begin{align*} \\nabla f(x) = \\frac{\\partial l}{\\partial \\beta} \u0026= \\sum_{i=1}^n y_{i}x_{i}- (\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}})\\cdot x_{i}^{T} \\\\ \u0026= \\sum_{i=1}^n (y_{i}- \\underbrace{\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}}}_{p_{i}})\\cdot x_{i}^{T} = X(y-p) \\\\ \\end{align*} \\] \\[ \\nabla^{2}f(x) = \\frac{\\partial^{2} l}{\\partial \\beta \\partial \\beta^{T}} = \\sum_{i=1}^n - \\underbrace{\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}}}_{p_{i}}\\cdot \\underbrace{\\frac{1}{1+e^{\\beta^{T}x_{i}}}}_{1-p_{i}}x_{i} \\cdot x_{i}^{T} = -XWX^{T} \\\\ \\] Where \\(W\\) is a diagonal \\((n,n)\\) matrix with the \\(i^{th}\\) diagonal element defined as\n\\[ W = \\begin{bmatrix} p_{i}(1-p_{i}) \u0026 \u0026 \\\\ \u0026 \\ddots \u0026 \u0026 \\\\ \u0026 \u0026 \u0026 \\\\ \\end{bmatrix}_{\\ n x n} \\\\ \\] The Newton‐Raphson algorithm can now be expressed as:\n\\[ \\begin{align*} \\beta^{new} \u0026= \\beta^{old}-(\\nabla^{2}f(x))^{-1}\\nabla f(x) \\\\ \u0026= \\beta^{old}+ (XWX^{T})^{-1}X(y-p) \\\\ \u0026= \\beta^{old}+ (XWX^{T})^{-1}[XWX^{T}\\beta^{t}+ X(y-p)] \\\\ \u0026= \\beta^{old}+ (XWX^{T})^{-1}XWZ \\\\ \\end{align*} \\] Where \\(Z\\) can be expressed as: \\(Z = X^{T}\\beta^{t}+ W^{-1}(y-p) \\) . This algorithm is also known as Iteratively Reweighted Least Squares(IRLS).\n\\[ \\beta^{t+1} = \\arg\\min_{\\beta}(Z - X\\beta)^{T}W(Z-X\\beta) \\\\ \\] Other types of Logistic Regression # Multinomial Logistic Regression # Three or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan).\nOrdinal Logistic Regression # Three or more categories with ordering. Example: Movie rating from 1 to 5.\nReferences # [1] K, D. (2021, April 8). Logistic Regression in Python using Scikit-learn. Medium. https://heartbeat.fritz.ai/logistic-regression-in-python-using-scikit-learn-d34e882eebb1.\n[2] Li, S. (2019, February 27). Building A Logistic Regression in Python, Step by Step. Medium. https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8.\n[3] Christian. (2020, September 17). Plotting the decision boundary of a logistic regression model. https://scipython.com/blog/plotting-the-decision-boundary-of-a-logistic-regression-model/.\nBlog Home "},{"id":5,"href":"/docs/deep-learning/convolutional-neural-networks/","title":"Convolutional Neural Networks","section":"Deep Learning","content":" Convolutional Neural Networks # "},{"id":6,"href":"/docs/machine-learning/data-preprocessing/","title":"Data Preprocessing","section":"Machine Learning","content":" Data Preprocessing # "},{"id":7,"href":"/docs/machine-learning/supervised-learning/logistic-regression/","title":"Logistic Regression","section":"Supervised Learning","content":" 逻辑回归 # 逻辑回归（Logistic Regression） # Logistic Regression（逻辑回归）是一种用于分类问题的统计模型，本质上是一种线性模型，通过 Sigmoid 函数将线性回归的输出映射到 (0,1) 区间，用于预测概率。分类是应用 逻辑回归(Logistic Regression) 的目的和结果, 但中间过程依旧是回归. 通过逻辑回归模型, 我们得到的计算结果是 0-1 之间的连续数字, 可以把它称为\u0026quot;可能性\u0026quot;（概率）. 然后, 给这个可能性加一个阈值, 就成了分类. 例如, 可能性大于 0.5 即记为 1, 可能性小于 0.5 则记为 0。其数学公式可以表达为：\n\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} ，z = w^T x + b \\] 输出概率: \\[ \\begin{align*} \u0026P(y=1|x) = \\sigma(w^T x + b) \\\\ \u0026P(y=0|x) = 1 - \\sigma(w^T x + b) \\\\ \\end{align*} \\] 决策边界： \\(P(y=1|x) \\geq 0.5\\) 时预测为1，反之预测为0。\nSigmoid 函数 # Sigmoid 函数是一种常用的激活函数，将任意实数映射到区间 (0, 1)。\tLogistic回归中，Sigmoid的输出可以帮助解释为事件发生的概率。它的数学表达式为：\n\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\] 值域：Sigmoid 函数的输出值范围是 (0, 1) ，这使得它特别适合用于概率预测。 单调递增：Sigmoid 是单调递增函数，意味着 输入值越大，输出值越接近 1。 中心对称：以点 (0, 0.5) 为对称中心。 平滑性：Sigmoid 函数是光滑的，具有连续的一阶和二阶导数。 Note： 逻辑回归中，Sigmoid 函数的输出是 分类的概率，而不是分类的类别。\n损失函数（Loss Function） # Logistic 回归的训练目标是通过优化目标函数找到最优的模型参数，使模型能够对输入样本进行概率预测，并最大程度地准确分类数据，最小化训练数据的损失函数（Loss Function）。Logistic 回归的损失函数是基于 交叉熵损失（Cross-Entropy Loss） 定义的，它反映了模型预测值与实际值之间的不一致程度。\n对单个样本的损失函数：Logistic 回归的损失函数采用对数似然函数的负值，针对二分类任务的每个样本： \\[ \\text{Loss}(y, \\hat{y}) = -\\left[ y \\log \\hat{y} + (1 - y) \\log (1 - \\hat{y}) \\right] \\] \\(y \\in \\{0, 1\\}\\) 是实际标签。 \\(\\hat{y} = P(y=1|x)\\) 是模型预测的概率。 该损失函数的两种情况：\n当 \\(y = 1\\) ：损失为 \\(-\\log(\\hat{y})\\) ，鼓励模型将预测概率 \\(\\hat{y}\\) 接近 1。 当 \\(y = 0\\) ：损失为 \\(-\\log(1 - \\hat{y})\\) ，鼓励模型将预测概率 \\(\\hat{y}\\) 接近 0。 总体损失函数：对整个数据集的损失函数是所有样本损失的平均值： \\[ \\mathcal{L}(w, b) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log \\hat{y}_i + (1 - y_i) \\log (1 - \\hat{y}_i) \\right] \\] 这里 \\(\\hat{y}_i = \\sigma(z_i) = \\frac{1}{1 + e^{-z_i}}\\) ，其中 \\(z_i = w^T x_i + b \\) 。\n梯度下降（Gradient Descent） # Logistic Regression 使用梯度下降（Gradient Descent）优化其损失函数。在优化过程中，需要计算损失函数的梯度以更新模型参数 \\(w\\) , \\(b\\) ：\n损失函数对权重的梯度： \\[ \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\sigma(w^T x_i + b) - y_i \\right] x_i \\] 其中 \\(\\sigma(w^T x_i + b) - y_i\\) 是预测值与真实值的误差。\n损失函数对偏置的梯度： \\[ \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\sigma(w^T x_i + b) - y_i \\right] \\] 利用梯度更新参数： \\[ w := w - \\alpha \\frac{\\partial L}{\\partial w}, \\quad b := b - \\alpha \\frac{\\partial L}{\\partial b} \\] 性能评估（Evaluation Metrics） # 混淆矩阵 (Confusion Matrix) # 混淆矩阵是分类模型的基本评价工具，用于总结预测结果的分类情况。对于二分类问题，矩阵包含以下四个元素：\n预测正类 \\( (\\hat{y} = 1) \\) 预测负类 \\((\\hat{y} = 0)\\) 实际正类 \\((y = 1)\\) TP (True Positive) FN (False Negative) 实际负类 \\((y = 0)\\) FP (False Positive) TN (True Negative) TP (True Positive): 实际为正，预测也为正。 FN (False Negative): 实际为正，但预测为负。 FP (False Positive): 实际为负，但预测为正。 TN (True Negative): 实际为负，预测也为负。 准确率 (Accuracy) # \\[ \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} \\] 定义：模型预测正确的样本占总样本的比例。 优点：简单直观。 缺点：当类别不平衡时（正负样本比例悬殊），准确率可能误导。 精确率 (Precision) # \\[ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} \\] 描述模型预测正类的可靠性。 适用场景：当 FP 的代价较高时，例如垃圾邮件过滤（FP 表示误判正常邮件为垃圾邮件）。 召回率 (Recall) # \\[ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \\] 描述模型对正类样本的捕捉能力。 适用场景：当 FN 的代价较高时，例如疾病检测（FN 表示漏诊病人）。 F1-Score # \\[ \\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\] F1-Score 是 Precision 和 Recall 的调和平均，用于权衡两者之间的关系。 适用场景：当 Precision 和 Recall 同等重要时。 ROC 曲线 和 AUC (Area Under the Curve) # 横轴：假正率 ( \\(FPR = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\\) )。 纵轴：真正率 ( \\(TPR = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\) )。 ROC 曲线展示了不同阈值下模型性能的变化。 阈值（threshold） 的变化直接影响模型的 TPR（真正例率） 和 FPR（假正例率），从而决定曲线上每个点的位置：\n起点与终点： 当 阈值 = 1.0（极高阈值）：所有样本都被预测为负类， \\(\\text{TPR} = 0，\\text{FPR} = 0\\) ，即曲线起点 (0,0)。 当 阈值 = 0.0（极低阈值）：所有样本都被预测为正类， \\(\\text{TPR} = 1，\\text{FPR} = 1\\) ，即曲线终点 (1,1)。 中间变化： 随着阈值从高到低移动，曲线从 (0,0) 开始，逐渐向 (1,1) 延展。 这些点的位置和曲线的形状取决于模型在不同阈值下的 TPR 和 FPR。 关键点： 特定阈值（如 0.5 或其他业务相关的值）对应的 TPR 和 FPR 可通过 ROC 图直接观察，帮助选择最佳阈值。 AUC (Area Under the Curve) # ROC 曲线下的面积，取值范围为 [0, 1]。AUC 的意义：\nAUC = 1：完美分类器。 AUC = 0.5：随机猜测。 0.5 \u0026lt; AUC \u0026lt; 1：模型有一定的区分能力。 Logistic Regression 代码实现 # # \u0026lt;--- From scratch ---\u0026gt; import numpy as np def sigmoid(z): return 1 / (1 + np.exp(-z)) def cross_entropy_loss(y, y_pred): \u0026#34;\u0026#34;\u0026#34; y: 实际标签 (0 或 1) y_pred: 模型预测值 (范围在 0 和 1 之间) 返回: 平均交叉熵损失 \u0026#34;\u0026#34;\u0026#34; # 防止 log(0) 导致的数值错误，添加一个小的正数 epsilon epsilon = 1e-15 y_pred = np.clip(y_pred, epsilon, 1 - epsilon) # 保证 y_pred 不会等于 0 或 1 return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred)) def Logistic_Regression(X, y, lr, max_iter): \u0026#34;\u0026#34;\u0026#34; X: 特征矩阵 (n_samples x n_features) y: 标签向量 (n_samples,) lr: 学习率 max_iter: 最大迭代次数 返回: 训练好的权重和偏置 \u0026#34;\u0026#34;\u0026#34; n_samples, n_features = X.shape # 样本数量和特征数量 weights = np.zeros(n_features) # 初始化权重为 0 bias = 0 # 初始化偏置为 0 losses = [] for i in range(max_iter): # 计算预测值 y_pred = sigmoid(np.dot(X, weights) + bias) # 计算梯度 weight_grad = (1 / n_samples) * np.dot(X.T, (y_pred - y)) # 权重的梯度 bias_grad = (1 / n_samples) * np.sum(y_pred - y) # 偏置的梯度 # 计算损失并存储 loss = cross_entropy_loss(y, y_pred) losses.append(loss) # 使用梯度下降法更新权重和偏置 weights -= lr * weight_grad bias -= lr * bias_grad return weights, bias def Logistic_Regression_Predict(X, weights, bias): # 计算预测概率 pred_y = sigmoid(np.dot(X, weights) + bias) # 将概率转换为二分类标签 return [1 if i \u0026gt;= 0.5 else 0 for i in pred_y] from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split # 生成模拟分类数据集 X, y = make_classification(n_samples=1000, n_features=10, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) lr = 0.1 # 学习率 max_iter = 1000 # 最大迭代次数 # 训练模型 weights, bias = Logistic_Regression(X_train, y_train, lr, max_iter) # 使用测试集进行预测 predictions = Logistic_Regression_Predict(X_test, weights, bias) # 打印预测结果示例 print(\u0026#34;Predictions:\u0026#34;, predictions[:10]) # \u0026lt;--- From scikit-learn ---\u0026gt; from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, confusion_matrix, classification_report from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=10, random_state=42) # 数据划分 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 模型训练 model = LogisticRegression( penalty=\u0026#39;l2\u0026#39;, # 正则化类型：\u0026#39;l1\u0026#39;, \u0026#39;l2\u0026#39;, \u0026#39;elasticnet\u0026#39;, 或 \u0026#39;none\u0026#39;（默认 \u0026#39;l2\u0026#39;） C=1.0, # 正则化强度的倒数，值越小正则化越强，默认为 1.0 solver=\u0026#39;lbfgs\u0026#39;, # 优化算法：如 \u0026#39;lbfgs\u0026#39;, \u0026#39;liblinear\u0026#39;, \u0026#39;sag\u0026#39;, \u0026#39;saga\u0026#39; 等 max_iter=100, # 最大迭代次数，防止迭代过多导致训练时间过长 random_state=42 # 随机种子，保证结果可复现 ) model.fit(X_train, y_train) # 预测 y_pred = model.predict(X_test) # 评价指标 accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) report = classification_report(y_test, y_pred) print(f\u0026#34;Accuracy: {accuracy}\u0026#34;) print(f\u0026#34;Confusion Matrix:\\n{conf_matrix}\u0026#34;) print(f\u0026#34;Classification Report:\\n{report}\u0026#34;) "},{"id":8,"href":"/docs/common-libraries/numpy/","title":"NumPy","section":"Common Libraries","content":" NumPy # "},{"id":9,"href":"/docs/python-basics/python-fundamentals/","title":"Python Fundamentals","section":"Python Basics","content":" Python Fundamentals # 列表 (Lists) # 列表 (Lists) 是 有序的 (ordered)、可变的 (mutable) 值集合，这些值以逗号分隔并用方括号括起来。列表可以由许多不同类型的变量组成。\n# Creating a list x = [3, \u0026#34;hello\u0026#34;, 1.2] print (x) [3, \u0026#39;hello\u0026#39;, 1.2] 我们可以使用 append 函数将新的值添加到 列表 (Lists) 中：\n# Adding to a list x.append(7) print (x) print (len(x)) [3, \u0026#39;hello\u0026#39;, 1.2, 7] 4 或者直接替换现有的值：\n# Replacing items in a list x[1] = \u0026#34;bye\u0026#34; print (x) [3, \u0026#39;bye\u0026#39;, 1.2, 7] 并可以直接对 列表 (list) 执行操作：\n# Operations y = [2.4, \u0026#34;world\u0026#34;] z = x + y print (z) [3, \u0026#39;bye\u0026#39;, 1.2, 7, 2.4, \u0026#39;world\u0026#39;] 元组 (Tuples) # 元组 (Tuples) 是 有序 (ordered) 且 不可变 (immutable) 的集合。我们将使用元组来存储 永远不会改变 的值。\n# Creating a tuple x = (3.0, \u0026#34;hello\u0026#34;) # tuples start and end with () print (x) (3.0, \u0026#39;hello\u0026#39;) # Adding values to a tuple x = x + (5.6, 4) print (x) (3.0, \u0026#39;hello\u0026#39;, 5.6, 4) # Try to change (it won\u0026#39;t work and we get an error) x[0] = 1.2 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ----\u0026gt; 1 x[0] = 1.2 TypeError: \u0026#39;tuple\u0026#39; object does not support item assignment 集合 (Sets) # 集合(sets) 是 无序(unordered) 且 可变(mutable) 的。但是，集合中的每个项目必须是 唯一(unique) 的。\n# Sets text = \u0026#34;Learn ML with Made With ML\u0026#34; print (set(text)) print (set(text.split(\u0026#34; \u0026#34;))) {\u0026#39;e\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39; \u0026#39;, \u0026#34;r\u0026#34;, \u0026#34;w\u0026#34;, \u0026#39;d\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;t\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;L\u0026#39;, \u0026#39;n\u0026#39;, \u0026#34;w\u0026#34;} {\u0026#39;with\u0026#39;, \u0026#39;Learn\u0026#39;, \u0026#39;ML\u0026#39;, \u0026#39;Made\u0026#39;, \u0026#39;With\u0026#39;} 字典 (Dictionaries) # 字典 (Dictionaries) 是 无序 (unordered) 且 可变 (mutable) 的 键值对(key-value pair) 集合。我们可以根据 键(key) 检索 值(value)，但字典不能有两个相同的键。\n# Creating a dictionary person = {\u0026#34;name\u0026#34;: \u0026#34;Goku\u0026#34;, \u0026#34;eye_color\u0026#34;: \u0026#34;brown\u0026#34;} print (person) print (person[\u0026#34;name\u0026#34;]) print (person[\u0026#34;eye_color\u0026#34;]) {\u0026#34;name\u0026#34;: \u0026#34;Goku\u0026#34;, \u0026#34;eye_color\u0026#34;: \u0026#34;brown\u0026#34;} Goku brown # Changing the value for a key person[\u0026#34;eye_color\u0026#34;] = \u0026#34;green\u0026#34; print (person) {\u0026#34;name\u0026#34;: \u0026#34;Goku\u0026#34;, \u0026#34;eye_color\u0026#34;: \u0026#34;green\u0026#34;} # Adding new key-value pairs person[\u0026#34;age\u0026#34;] = 24 print (person) {\u0026#34;name\u0026#34;: \u0026#34;Goku\u0026#34;, \u0026#34;eye_color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;age\u0026#34;: 24} # Length of a dictionary print (len(person)) 3 索引 (Indexing) # 通过列表的 索引(indexing) 和 切片 (slicing)，我们可以检索列表中的特定值。请注意，索引可以是正数（从 0 开始）或负数（-1 及以下，其中 -1 是列表中的最后一项）。\n# Indexing x = [3, \u0026#34;hello\u0026#34;, 1.2] print (\u0026#34;x[0]: \u0026#34;, x[0]) print (\u0026#34;x[1]: \u0026#34;, x[1]) print (\u0026#34;x[-1]: \u0026#34;, x[-1]) # the last item print (\u0026#34;x[-2]: \u0026#34;, x[-2]) # the second to last item x[0]: 3 x[1]: hello x[-1]: 1.2 x[-2]: hello # Slicing print (\u0026#34;x[:]: \u0026#34;, x[:]) # all indices print (\u0026#34;x[1:]: \u0026#34;, x[1:]) # index 1 to the end of the list print (\u0026#34;x[1:2]: \u0026#34;, x[1:2]) # index 1 to index 2 (not including index 2) print (\u0026#34;x[:-1]: \u0026#34;, x[:-1]) # index 0 to last index (not including last index) x[:]: [3, \u0026#39;hello\u0026#39;, 1.2] x[1:]: [\u0026#39;hello\u0026#39;, 1.2] x[1:2]: [\u0026#39;hello\u0026#39;] x[:-1]: [3, \u0026#39;hello\u0026#39;] if 语句 (if statements) # 我们可以使用 if 语句有条件地执行某项操作。条件由单词 if、elif（代表 else if）和 else 定义。我们可以根据需要使用任意数量的 elif 语句。每个条件下方的缩进代码是条件为 True 时将执行的代码。\n# If statement x = 4 if x \u0026lt; 1: score = \u0026#34;low\u0026#34; elif x \u0026lt;= 4: # elif = else if score = \u0026#34;medium\u0026#34; else: score = \u0026#34;high\u0026#34; print (score) medium # If statement with a boolean x = True if x: print (\u0026#34;it worked\u0026#34;) it worked 循环语句 (Loop) # For loops # for 循环可以迭代值集合（列表 (list)、元组 (tuple)、字典 (dictionaries)等）。缩进的代码针对值集合中的 每个项目 执行。\n# For loop veggies = [\u0026#34;carrots\u0026#34;, \u0026#34;broccoli\u0026#34;, \u0026#34;beans\u0026#34;] for veggie in veggies: print (veggie) carrots broccoli beans 当循环遇到 break 命令时，循环将立即终止。如果列表中还有更多项目，则不会处理它们。\n# `break` from a for loop veggies = [\u0026#34;carrots\u0026#34;, \u0026#34;broccoli\u0026#34;, \u0026#34;beans\u0026#34;] for veggie in veggies: if veggie == \u0026#34;broccoli\u0026#34;: break print (veggie) carrots 当循环遇到 continue 命令时，循环将仅跳过列表中该项目的所有其他操作。如果列表中还有更多项目，循环将正常继续。\n# `continue` to the next iteration veggies = [\u0026#34;carrots\u0026#34;, \u0026#34;broccoli\u0026#34;, \u0026#34;beans\u0026#34;] for veggie in veggies: if veggie == \u0026#34;broccoli\u0026#34;: continue print (veggie) carrots beans While loops # 只要条件为 True，while 循环就可以重复执行。我们也可以在 while 循环中使用 continue 和 break 命令。\n# While loop x = 3 while x \u0026gt; 0: x -= 1 # same as x = x - 1 print (x) 2 1 0 列表推导式 (list comprehension) 和生成器表达式 (generator expression) # 快速构建列表或生成器，尤其适合在数据处理或特征工程中清洗数据或生成特定序列。\n列表推导式 (list comprehension)：返回一个完整的列表。 生成器表达式 (generator expression)：返回一个惰性生成器，节省内存。 # 过滤列表中的偶数 numbers = [1, 2, 3, 4, 5, 6] even_numbers = [x for x in numbers if x % 2 == 0] print(even_numbers) # [2, 4, 6] # 生成器表达式（惰性求值） gen = (x**2 for x in range(5)) # 不占用大量内存 for val in gen: print(val) [2, 4, 6] 0 1 4 9 16 Lambda 函数和 map, filter 的使用 # Lambda函数：定义简洁的匿名函数，适合简单逻辑。(e.g. func = lambda var1 var2 : function =\u0026gt; func(var1, var2)) map：对可迭代对象中的每个元素应用函数。 filter：筛选符合条件的元素。 from functools import reduce # Lambda函数示例 add = lambda x, y: x + y print(add(3, 5)) # 8 # map: 计算平方 squares = list(map(lambda x: x**2, [1, 2, 3])) print(squares) # [1, 4, 9] # filter: 筛选出大于2的元素 filtered = list(filter(lambda x: x \u0026gt; 2, [1, 2, 3, 4])) print(filtered) # [3, 4] 8 [1, 4, 9] [3, 4] 函数 (Function) 封装 # 在ML项目中封装代码逻辑，便于维护和复用。例如，封装数据预处理步骤或模型训练流程。\ndef preprocess_data(data): preprocessed_data = data.dropna() return preprocessed_data # \u0026lt;---------- Usage ----------\u0026gt; # cleaned_data = preprocess_data(raw_data) def f(*args, **kwargs) 是另一种定义函数的方式，用来接收可变数量的参数。它允许函数在调用时传入任意数量的位置参数和关键字参数，从而使函数更加灵活。\n*args: 可变长度位置参数，接收任意数量的未命名参数 (arguments)，作为一个元组。 **kwargs: 可变长度关键字参数，接收任意数量的命名参数 (keyword arguments)，作为一个字典。 def f(*args, **kwargs): # 从位置参数中提取第一个值，赋值给变量 x x = args[0] # 从关键字参数中获取键 \u0026#34;y\u0026#34; 的值，若不存在返回 None y = kwargs.get(\u0026#34;y\u0026#34;) print (f\u0026#34;x: {x}, y: {y}\u0026#34;) # 调用函数 f，传入一个位置参数 5 和一个关键字参数 y=2 f(5, y=2) x: 5, y: 2 类 (Class) 封装 # class DataHandler: def __init__(self, filepath): self.filepath = filepath def load_data(self): print(f\u0026#34;Loading data from {self.filepath}...\u0026#34;) return {\u0026#34;data\u0026#34;: [1, 2, 3, 4]} def preprocess_data(self, data): return [x * 2 for x in data] # \u0026lt;---------- Usage ----------\u0026gt; # new_handler = DataHandler(\u0026#39;data.csv\u0026#39;) # new_data = new_handler.load_data() # preprocess_data = new_handler.preprocess_data(new_data[\u0026#39;data\u0026#39;]) 继承 (Inheritance) # 继承用于让一个类（子类）从另一个类（父类）中获得 属性(properties) 和 方法(methods)，从而实现代码复用和扩展。\n子类继承父类的方法和属性。 使用 super() 调用父类的方法。 方法重写：子类可重写父类的方法实现。 # 定义一个基础模型类 class BaseModel: def __init__(self, name): self.name = name def train(self): print(f\u0026#34;{self.name} is training...\u0026#34;) def test(self): print(f\u0026#34;{self.name} is testing...\u0026#34;) # 子类继承基础模型类 class RegressionModel(BaseModel): def __init__(self, name, num_features): super().__init__(name) self.num_features = num_features def train_1(self): super().train() print(f\u0026#34;Training a regression model with {self.num_features} features.\u0026#34;) # 使用子类 model = RegressionModel(\u0026#34;LinearRegression\u0026#34;, 10) model.train() model.test() LinearRegression is training... LinearRegression is testing... 方法 (Methods) # 实例方法 (Instance Method)：实例方法的第一个参数是 self，用于访问实例属性和其他实例方法。它需要通过实例对象调用，依赖于具体的实例状态。当方法需要操作实例的属性（如 self.name）或依赖于实例的状态时，实例方法是最合适的选择。 类方法 (Class Method)：类方法的第一个参数是 cls，表示类本身（而不是实例）。它使用 @classmethod 装饰器修饰，可以通过类对象或实例对象调用。当方法需要操作类级别的状态（如 total_models）而不依赖于任何具体实例时，类方法可以保持逻辑的清晰和一致性。 静态方法 (Static Method)：使用 @staticmethod，与类或实例 (self 或 cls)无绑定，仅实现功能逻辑。当方法的逻辑与类相关，但完全独立于类或实例的状态时，静态方法可以避免无意义的参数（如 self 或 cls），提高代码的简洁性。 class MLModel: total_models = 0 # 类属性 def __init__(self, name): self.name = name MLModel.total_models += 1 def display(self): # 实例方法 print(f\u0026#34;Model Name: {self.name}\u0026#34;) @classmethod def get_total_models(cls): # 类方法 print(f\u0026#34;Total models created: {cls.total_models}\u0026#34;) @staticmethod def utility_function(x): # 静态方法 return x**2 # 使用 model = MLModel(\u0026#34;RandomForest\u0026#34;) model.display() MLModel.get_total_models() print(MLModel.utility_function(5)) Model Name: RandomForest Total models created: 1 25 装饰器 (Decorators) # 装饰器是一种函数，接受另一个函数或方法作为输入并返回一个修改后的函数，用于动态扩展功能。\n@decorator def function(): pass 等价于：\ndef function(): pass function = decorator(function) import time # 定义一个装饰器 def timer_decorator(func): def wrapper(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print(f\u0026#34;Function \u0026#39;{func.__name__}\u0026#39; executed in {end_time - start_time:.4f}s\u0026#34;) return result return wrapper # 装饰训练函数 class MLModel: @timer_decorator def train(self, data): print(\u0026#34;Training model...\u0026#34;) time.sleep(1) # 模拟训练时间 return \u0026#34;Training Complete\u0026#34; model = MLModel() print(model.train([])) Training model... Function \u0026#39;train\u0026#39; executed in 1.0015s Training Complete 回调函数 (Callbacks) # 回调是一个函数作为参数传递给另一个函数，并在适当时调用。它在机器学习中常用于动态调整训练流程（如早停、学习率调整）。\nclass TrainingCallback: def on_epoch_start(self, epoch): print(f\u0026#34;Epoch {epoch} started.\u0026#34;) def on_epoch_end(self, epoch, loss): print(f\u0026#34;Epoch {epoch} ended with loss: {loss}.\u0026#34;) # 使用回调 class MLTrainer: def __init__(self, callback=None): self.callback = callback def train(self, epochs): for epoch in range(epochs): if self.callback: self.callback.on_epoch_start(epoch) # 模拟训练过程 loss = 0.01 * (epochs - epoch) if self.callback: self.callback.on_epoch_end(epoch, loss) # 测试回调 callback = TrainingCallback() trainer = MLTrainer(callback) trainer.train(3) Epoch 0 started. Epoch 0 ended with loss: 0.03. Epoch 1 started. Epoch 1 ended with loss: 0.02. Epoch 2 started. Epoch 2 ended with loss: 0.01. 模块 (Module) 封装 # project/ ├── data_processing.py # Contains functions for data processing ├── model_training.py # Contains model training logic ├── evaluation.py # Contains evaluation methods # \u0026lt;---------- Usage ----------\u0026gt; # \u0026lt;---------- data_processing.py ----------\u0026gt; # def clean_data(data): # \u0026#34;\u0026#34;\u0026#34;Clean the data by dropping missing values.\u0026#34;\u0026#34;\u0026#34; # print(\u0026#34;Cleaning data...\u0026#34;) # return data.dropna() # \u0026lt;---------- main.py ----------\u0026gt; # import data_processing as dp # data = dp.load_data(\u0026#34;data.csv\u0026#34;) # cleaned_data = dp.clean_data(data) 异步编程 (Asynchronous Programming) # 在训练、数据下载、或者与API通信时异步执行，提高性能。\nasync 用于定义一个异步函数。使用 async def 定义，表示该函数会返回一个协程对象。 await 用于调用另一个异步函数，并等待其执行完成，直到结果返回。 asyncio 库提供了一个框架来实现异步编程。 import nest_asyncio import asyncio nest_asyncio.apply() # Allows nested use of asyncio.run [Only for Jupyter Notebook] # 定义一个异步函数 async def say_hello(): print(\u0026#34;Hello\u0026#34;) await asyncio.sleep(1) # 模拟一个耗时的异步操作 print(\u0026#34;World\u0026#34;) # 运行异步任务 asyncio.run(say_hello()) Hello World import asyncio # 定义异步任务 async def fetch_data(): print(\u0026#34;Fetching data...\u0026#34;) await asyncio.sleep(2) # 模拟耗时的异步操作 return \u0026#34;Data fetched\u0026#34; async def process_data(): print(\u0026#34;Processing data...\u0026#34;) await asyncio.sleep(1) # 模拟耗时的异步操作 return \u0026#34;Data processed\u0026#34; # 主程序 async def main(): data_task = asyncio.create_task(fetch_data()) # 启动任务1 process_task = asyncio.create_task(process_data()) # 启动任务2 result1 = await data_task # 等待任务1完成 result2 = await process_task # 等待任务2完成 print(result1) print(result2) # 启动事件循环 asyncio.run(main()) Fetching data... Processing data... Data fetched Data processed "},{"id":10,"href":"/posts/04_lda_and_qda_for_classification/","title":"LDA and QDA for Classification","section":"Blog","content":"Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data.\nLinear Discriminant Analysis for Classification # LDA assumes that all classes are linearly separable and according to this multiple linear discrimination function representing several hyperplanes in the feature space are created to distinguish the classes. If there are two classes then the LDA draws one hyperplane and projects the data onto this hyperplane in such a way as to maximize the separation of the two categories. This hyperplane is created according to the two criteria considered simultaneously:\nMaximizing the distance between the means of two classes; Minimizing the variation between each category. Suppose that \\(Y \\in \\{1, ..., K\\}\\) is assigned a prior \\(\\hat{\\pi}_{k}\\) such that \\(\\sum_{i=1}^k \\hat{\\pi}_{k} = 1\\) . According to Bayes’ rule, the posterior probability is\n\\[ P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{\\sum_{i=1}^{K}f_{i}(x)\\pi_{i}} \\\\ \\] where \\(f_{k}(x)\\) is the density of \\(X\\) conditioned on \\(k\\) . The Bayes Classifier can be expessed as:\n\\[ h^{*}(x) = \\arg\\max_{k}\\{P(Y=k|X=x)\\} = \\arg\\max_{k}\\delta_{k}(x) \\\\ \\] For we assume that the random variable \\(X\\) is a vector \\(X=(X_1,X_2,...,X_k)\\) which is drawn from a multivariate Gaussian with class-specific mean vector and a common covariance matrix \\(\\Sigma \\ (i.e. \\Sigma_{k} = \\Sigma, \\forall k)\\) . In other words the covariance matrix is common to all K classes: \\(Cov(X)=\\Sigma\\) of shape \\(d \\times d\\) .\nSince \\(x\\) follows a multivariate Gaussian distribution, the probability \\(P(X=x|Y=k)\\) is given by: ( \\(\\mu_k\\) is the mean of inputs for category \\(k\\) )\n\\[ f_k(x)=\\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k)) \\\\ \\] Then we can find the posterior distribution as:\n\\[ P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{P(X=x)} = C \\cdot f_{k}(x)\\pi_{k} \\\\ \\] Since \\(P(X=x)\\) does not depend on \\(k\\) so we write it as a constant. We will now proceed to expand and simplify the algebra, putting all constant terms into \\(C,C^{'},C{''}\\) etc..\n\\[ \\begin{align*} p_{k}(x) = \u0026P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{P(X=x)} = C \\cdot f_{k}(x)\\pi_{k} \\\\ \u0026=C \\cdot \\pi_{k} \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k)) \\\\ \u0026=C^{'} \\cdot \\pi_{k} exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k)) \\\\ \\end{align*} \\] Take the log of both sides:\n\\[ \\begin{align*} logp_{k}(x) \u0026=log(C^{'} \\cdot \\pi_{k} exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k))) \\\\ \u0026= logC^{'} + log\\pi_k -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k) \\\\ \u0026= logC^{'} + log\\pi_k -\\frac{1}{2}[(x^{T}\\Sigma^{-1}x+\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}]+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ \u0026= C^{''} + log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ \\end{align*} \\] And so the objective function, sometimes called the linear discriminant function or linear score function is:\n\\[ \\delta_{k} = log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ \\] Which means that given an input \\(x\\) we predict the class with the highest value of \\(\\delta_{k}(x)\\) .\nTo find the Dicision Boundary, we will set \\(P(Y=k|X=x) = P(Y=l|X=x)\\) which is \\(\\delta_{k}(x) = \\delta_{l}(x)\\) :\n\\[ log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} = log\\pi_l -\\frac{1}{2}\\mu_{l}^{T}\\Sigma^{-1}\\mu_{l}+x^{T}\\Sigma^{-1}\\mu_{l} \\\\ log\\frac{\\pi_{k}}{\\pi_{l}} -\\underbrace{\\frac{1}{2}(\\mu_{k}-\\mu_{l})^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l})}_{\\mathrm{Constant}}+\\underbrace{x^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l})}_{\\mathrm{Linear \\ in} \\ x} = 0 \\\\ \\Rightarrow a^{T}x + b = 0 \\\\ \\] Which is a linear function in \\(x\\) - this explains why the decision boundaries are linear - hence the name Linear Discriminant Analysis.\nQuadratic Discrimination Analysis for Classification # LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector, but a covariance matrix that is common to all \\(K\\) classes. Quadratic discriminant analysis provides an alternative approach by assuming that each class has its own covariance matrix \\(\\Sigma_{k}\\) .\nTo derive the quadratic score function, we return to the previous derivation, but now \\(\\Sigma_{k}\\) is a function of \\(k\\) , so we cannot push it into the constant anymore.\n\\[ p_{k}(x) = \\pi_{k}\\frac{1}{(2\\pi)^{d/2}|\\Sigma_{k}|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k)) \\\\ \\] \\[ \\begin{align*} logp_{k}(x) \u0026= C +log\\pi_{k} - \\frac{1}{2}log|\\Sigma_{k}|-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \\\\ \u0026= C +log\\pi_{k} - \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}x^{T}\\Sigma_{k}^{-1}x +x^{T}\\Sigma_{k}^{-1}\\mu_{k} -\\frac{1}{2}\\mu_{k}^{T}\\Sigma_{k}^{-1}\\mu_{k} \\\\ \\end{align*} \\] Which is a quadratic function of \\(x\\) . Under this less restrictive assumption, the classifier assigns an observation \\(X=x\\) to the class for which the quadratic score function is the largest:\n\\[ \\delta_{k}(x) = log\\pi_k- \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \\\\ \\] To find the Quadratic Dicision Boundary, we will set \\(P(Y=k|X=x) = P(Y=l|X=x)\\) which is \\(\\delta_{k}(x) = \\delta_{l}(x)\\) :\n\\[ log\\pi_k- \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) = log\\pi_l- \\frac{1}{2}log|\\Sigma_{l}| -\\frac{1}{2}(x-\\mu_l)^{T}\\Sigma_{l}^{-1}(x-\\mu_l) \\\\ \\frac{1}{2}x^{T}\\underbrace{(\\Sigma_{l}-\\Sigma_{k})}_{A}x+\\underbrace{(\\mu_{k}^{T}\\Sigma_{k}^{-1}-\\mu_{l}^{T}\\Sigma_{l}^{-1})}_{b^{T}}x +\\underbrace{\\frac{1}{2}(\\mu_{k}-\\mu_{l})^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l}) + log(\\frac{\\pi_{l}}{\\pi_{k}}) + log(\\frac{|\\Sigma_{k}|^{1/2}}{|\\Sigma_{l}|^{1/2}})}_{c} = 0 \\\\ \\Rightarrow x^{T}Ax + b^{T}x + c = 0 \\\\ \\] Case 1 : When \\(\\Sigma_{k} = I\\) # We first concider the case that \\(\\Sigma_{k} = I, \\forall k\\) . This is the case where each distribution is spherical, around the mean point. Then we can have:\n\\[ \\delta_{k}(x) = - \\frac{1}{2}log|I| + log\\pi_k -\\frac{1}{2}(x-\\mu_k)^{T}I(x-\\mu_k) \\\\ \\] Where \\(log(|I|) = log(1) = 0\\) and \\((x-\\mu_k)^{T}I(x-\\mu_k) = (x-\\mu_k)^{T}(x-\\mu_k)\\) is the Squared Euclidean Distance between two points \\(x\\) and \\(\\mu_{k}\\) .\nThus under this condition (i.e. \\(\\Sigma = I\\) ) , a new point can be classified by its distance from the center of a class, adjusted by some prior. Further, for two-class problem with equal prior, the discriminating function would be the perpendicular bisector of the 2-class’s means.\nCase 2 : When \\(\\Sigma_{k} \\neq I\\) # Since \\(\\Sigma_{k}\\) is a symmetric matrix \\(\\Sigma_{k} = \\Sigma_{k}^{T}\\) , by using the Singular Value Decomposition (SVD) of \\(\\Sigma_{k}\\) , we can get:\n\\[ \\Sigma_{k} = U_{k}S_{k}U_{k}^{T} \\\\ \\Sigma_{k}^{-1} = (U_{k}S_{k}U_{k}^{T})^{-1} = U_{k}S_{k}^{-1}U_{k}^{T}\\\\ \\] Then,\n\\[ \\begin{align*} (x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \u0026= (x-\\mu_k)^{T}U_{k}S_{k}^{-1}U_{k}^{T}(x-\\mu_k) \\\\ \u0026= (U_{k}^{T}x-U_{k}^{T}\\mu_k)^{T}S_{k}^{-1}(U_{k}^{T}x-U_{k}^{T}\\mu_k) \\\\ \u0026= (U_{k}^{T}x-U_{k}^{T}\\mu_k)^{T}S_{k}^{-\\frac{1}{2}}S_{k}^{-\\frac{1}{2}}(U_{k}^{T}x-U_{k}^{T}\\mu_k) \\\\ \u0026= (S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}I(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ \u0026= (S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ \\end{align*} \\] Which is also known as the Mahalanobis distance.\nThink of \\(S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\) as a linear transformation that takes points in class \\(k\\) and distributes them spherically around a point, like in Case 1. Thus when we are given a new point, we can apply the modified \\(\\delta_{k}\\) values to calculate \\(h^{*}(x)\\) . After applying the singular value decomposition, \\(\\Sigma_{k}^{-1}\\) is considered to be an identity matrix such that:\n\\[ \\delta_{k}(x) = - \\frac{1}{2}log|I| + log\\pi_k -\\frac{1}{2}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ \\] Where \\(log(|I|) = log(1) = 0\\) The difference between Case 1 and Case 2 (i.e. the difference between using the Euclidean and Mahalanobis distance) can be seen in the illustration below:\nLDA and QDA in practice # In practice we don’t know the parameters of Gaussian and will need to estimate them using our training data.\n\\[ \\hat{\\pi}_{k} = \\hat{P}(y=k) = \\frac{n_{k}}{n} \\\\ \\] where \\(n_{k}\\) is the number of class \\(k\\) observations.\n\\[ \\hat{\\mu}_{k} = \\frac{1}{n_{k}}\\sum_{i:y_{i}=k}x_{i} \\\\ \\] \\[ \\hat{\\Sigma}_{k} = \\frac{1}{n_{k}-k}\\sum_{i:y_{i}=k}(x_{i}-\\hat{\\mu}_{k})(x_{i}-\\hat{\\mu}_{k})^{T} \\\\ \\] If we wish to use LDA we must calculate a common covariance, so we average all the covariances, e.g.\n\\[ \\Sigma = \\frac{\\sum_{r=1}^k(n_{r}\\Sigma_{r})}{\\sum_{r=1}^k n_{r}} \\\\ \\] Where:\n\\(n_{r}\\) is the number of data points in class \\(r\\) .\n\\(\\Sigma_{r}\\) is the covariance of class \\(r\\) and \\(n\\) is the total number of data points.\n\\(k\\) is the number of classes.\nReference # [1] Sicotte, X. B. (2018, June 22). Xavier Bourret Sicotte. Linear and Quadratic Discriminant Analysis - Data Blog. https://xavierbourretsicotte.github.io/LDA_QDA.html.\n"},{"id":11,"href":"/docs/deep-learning/computer-vision/","title":"Computer Vision","section":"Deep Learning","content":" Computer Vision # "},{"id":12,"href":"/docs/python-basics/leetcode/","title":"Leetcode Notes","section":"Python Basics","content":" Leetcode Interview Preparation Notes # Basic Data Structures # Arrays # In Python, arrays are typically represented using lists. While Python doesn\u0026rsquo;t have a native array type as seen in other languages like Java or C++, lists are versatile and can be used similarly to arrays.\n【Last Update: 2024-08-14】\narr = [] # O(1) arr = [1, 2, 3] # O(n), where n is the number of elements first_element = arr[0] # O(1) arr[1] = 10 # O(1) arr.append(6) # O(1) on average for appending arr.insert(2, 15) # O(n), where n is the number of elements after the insertion index arr.remove(15) # O(n), where n is the number of elements in the list [remove the first 15 in the array] del arr[2] # O(n), where n is the number of elements after the deleted index last_element = arr.pop() # O(1) arr.sort() # 原地排序 sorted_arr = sorted(arr) # 返回排序后的数组 arr[::-1] # arr 倒序 ## Counter() 的常用语法和使用情况 from collections import Counter arr = [1, 2, 2, 3, 3, 3] counts = Counter(arr) # 结果：Counter({3: 3, 2: 2, 1: 1}) ## 找到出现次数最多的元素 most_common_element = counts.most_common(1)[0] # 结果：(3, 3) ## 判断出现的元素是否相同 arr1 = [1, 2, 3] arr2 = [3, 2, 1] is_anagram = Counter(arr1) == Counter(arr2) # 结果：True ## set() 的常用语法和使用情况 arr = [1, 2, 2, 3, 4, 4] ## 快速查找 seen = set(arr) if 3 in seen: print(\u0026#34;3 is in array\u0026#34;) ## 去重 unique_elements = list(set(arr)) # 结果：[1, 2, 3, 4] ## 两个数组的交集 arr1 = [1, 2, 2, 3] arr2 = [2, 3, 4] intersection = list(set(arr1) \u0026amp; set(arr2)) # 结果：[2, 3] Strings # Strings in Python are immutable sequences of characters. You can perform various operations on strings using built-in methods and operators.\n【Last Update: 2024-08-14】\ns = \u0026#34;Hello, World!\u0026#34; # O(n), where n is the length of the string first_char = s[0] # O(1) substring = s[7:12] # O(k), where k is the length of the substring combined = s + \u0026#34; Python\u0026#34; # O(n + m), where n and m are the lengths of the two strings repeated = s * 2 # O(n * k), where k is the number of repetitions upper_s = s.upper() # O(n), where n is the length of the string lower_s = s.lower() # O(n), where n is the length of the string starts_with_hello = s.startswith(\u0026#34;Hello\u0026#34;) # O(n), where n is the length of the prefix contains_world = \u0026#34;World\u0026#34; in s # O(n * m), where n is the length of the string and m is the length of the substring replaced_s = s.replace(\u0026#34;World\u0026#34;, \u0026#34;Python\u0026#34;) # O(n * m), where n is the length of the string and m is the length of the substring words = s.split(\u0026#34;, \u0026#34;) # O(n), where n is the length of the string joined = \u0026#34; - \u0026#34;.join(words) # O(n), where n is the total length of the resulting string Linked Lists # A Linked List is a linear data structure consisting of nodes, where each node contains:\nA data part that stores the actual data. A next part (or pointer) that points to the next node in the list. 【Last Update: 2024-11-14】\n## A node in a linked list can be represented as a class class ListNode: def __init__(self, data=0, next=None): self.data = data # Data of the node self.next = next # Pointer to the next node ## Inserting Nodes def insert_at_beginning(head, data): new_node = ListNode(data) # Create a new node new_node.next = head # Link the new node to the current head return new_node # New node becomes the head ## Deleting Nodes def delete_from_beginning(head): if not head: return None return head.next # The second node becomes the new head ## Searching for a Node def search(head, key): current = head while current: if current.data == key: return True # Found the data current = current.next return False # Data not found Stack # A Stack is a linear data structure that stores items in a Last-In/First-Out (LIFO) or First-In/Last-Out (FILO) manner. In stack, a new element is added at one end and an element is removed from that end only.\npush(a) – Inserts the element ‘a’ at the top of the stack – Time Complexity: O(1) pop() – Deletes the topmost element of the stack – Time Complexity: O(1) Peek - View the top element without removing it. Empty - Check if the stack is empty. stack = [] # Push elements onto the stack stack.append(1) stack.append(2) # Pop element from the stack top = stack.pop() # Removes and returns 2 # Peek the top element top = stack[-1] if stack else None # Returns 1 # Check if the stack is empty is_empty = len(stack) == 0 Monotonic Stack (单调堆栈) # A monotonic stack is a specialized data structure used to solve problems involving arrays or sequences, particularly where you need to efficiently find next greater/smaller elements or previous greater/smaller elements. The stack is maintained in either increasing or decreasing order based on the problem requirements.\n使用堆栈存储数组的 indices 或 value。 通过在处理新元素时 popping 违反顺序的元素来保持单调性。 通常迭代数组一次（从左到右或从右到左）以实现所需的结果。 【Last Update: 2024-12-10】\nQueue # Queue is a linear data structure that stores items in First In First Out (FIFO) manner. With a queue the least recently added item is removed first.\nEnqueue: Adds an item to the queue. If the queue is full, then it is said to be an Overflow condition – Time Complexity : O(1) Dequeue: Removes an item from the queue. The items are popped in the same order in which they are pushed. If the queue is empty, then it is said to be an Underflow condition – Time Complexity : O(1) Peek: View the front element without removing it. Empty: Check if the queue is empty. 【Last Update: 2024-11-19】\nfrom collections import deque # Initialize a queue queue = deque() # Enqueue elements queue.append(1) queue.append(2) # Dequeue element front = queue.popleft() # Removes and returns 1 # Peek at the front element front = queue[0] if queue else None # Check if the queue is empty is_empty = len(queue) == 0 Deque # A deque is a generalized queue that allows insertion and deletion from both ends with O(1) complexity. Internally, it is implemented as a doubly linked list or a circular buffer.\n【Last Update: 2024-11-25】\nfrom collections import deque # Initialize a deque dq = deque() # Add elements dq.append(1) # Add to the right dq.appendleft(2) # Add to the left # Remove elements dq.pop() # Remove from the right dq.popleft() # Remove from the left # Access and manipulation dq.extend([3, 4]) # Add multiple elements to the right dq.extendleft([0, -1]) # Add multiple elements to the left (reversed order) dq.rotate(1) # Rotate elements right dq.rotate(-1) # Rotate elements left dq.clear() # Clear all elements Advanced Data Structures # Heap # A heap is a complete binary tree stored as an array. It maintains the heap property: in a min-heap, the parent is less than or equal to its children. Insertions and deletions are O(log n) due to the need to maintain the heap property.\nTwo main types: Min-Heap: The root node is the smallest, and every parent node is smaller than or equal to its children. Max-Heap: The root node is the largest, and every parent node is larger than or equal to its children. Root Node Access: Min-Heap: Root is the smallest element Max-Heap: Root is the largest element. Efficient Operations: Insert and delete both take O(log n). Maintains heap properties using adjustments (upward or downward shifts). 【Last Update: 2024-11-25】\nimport heapq # Initialize a heap heap = [] # Add elements heapq.heappush(heap, 3) # Push element into the heap heapq.heappush(heap, 1) heapq.heappush(heap, 4) # Access the smallest element smallest = heap[0] # Remove elements min_element = heapq.heappop(heap) # Pop the smallest element # Heapify an existing list nums = [4, 1, 7, 3] heapq.heapify(nums) # Get n largest or smallest elements largest = heapq.nlargest(2, nums) smallest = heapq.nsmallest(2, nums) Hash Tables # In Python, the built-in dict type (short for dictionary) functions as a hash table. Hash tables are a key data structure used for efficient data retrieval and storage, providing average time complexities of O(1) for insertion, deletion, and lookup operations due to their underlying hashing mechanism.\n【Last Update: 2024-11-06】\nmy_dict = {} # Creating an empty dictionary my_dict = {\u0026#39;key1\u0026#39;: \u0026#39;value1\u0026#39;, \u0026#39;key2\u0026#39;: \u0026#39;value2\u0026#39;} # Creating a dictionary with initial values value = my_dict[\u0026#39;key1\u0026#39;] # Accessing a value by key my_dict[\u0026#39;key3\u0026#39;] = \u0026#39;value3\u0026#39; # Adding a new key-value pair my_dict[\u0026#39;key2\u0026#39;] = \u0026#39;new_value2\u0026#39; # Updating an existing key-value pair del my_dict[\u0026#39;key1\u0026#39;] # Removing an entry by key value = my_dict.pop(\u0026#39;key2\u0026#39;) # Popping an entry (removes and returns the value) exists = \u0026#39;key3\u0026#39; in my_dict # # Checking if a key is in the dictionary [True] for key in my_dict: print(key, my_dict[key]) # Iterating through keys for key, value in my_dict.items(): # Iterating through key-value pairs print(key, value) for value in my_dict.values(): # Iterating through values print(value) # defaultdict 使用方法，没见过的元素不会报错。适用于计数、分组和嵌套字典等应用。 from collections import defaultdict # 使用 int 类型的 defaultdict dd = defaultdict(int) print(dd[\u0026#39;missing_key\u0026#39;]) # 输出：0，因为 int() 的默认值是 0 print(dd) # 输出：defaultdict(\u0026lt;class \u0026#39;int\u0026#39;\u0026gt;, {\u0026#39;missing_key\u0026#39;: 0}) # 统计元素出现次数 data = \u0026#34;abracadabra\u0026#34; counter = defaultdict(int) for char in data: counter[char] += 1 print(counter) # 输出：defaultdict(\u0026lt;class \u0026#39;int\u0026#39;\u0026gt;, {\u0026#39;a\u0026#39;: 5, \u0026#39;b\u0026#39;: 2, \u0026#39;r\u0026#39;: 2, \u0026#39;c\u0026#39;: 1, \u0026#39;d\u0026#39;: 1}) # defaultdict(list)常用于将多个值归类到同一个键下。 data = [(\u0026#34;apple\u0026#34;, 1), (\u0026#34;banana\u0026#34;, 2), (\u0026#34;apple\u0026#34;, 3), (\u0026#34;banana\u0026#34;, 4)] grouped_data = defaultdict(list) for fruit, count in data: grouped_data[fruit].append(count) print(grouped_data) # 输出：defaultdict(\u0026lt;class \u0026#39;list\u0026#39;\u0026gt;, {\u0026#39;apple\u0026#39;: [1, 3], \u0026#39;banana\u0026#39;: [2, 4]}) # 可以使用dict()将defaultdict转换为普通字典。 dd = defaultdict(int) dd[\u0026#39;a\u0026#39;] += 1 print(dict(dd)) # 输出：{\u0026#39;a\u0026#39;: 1} Tree # A tree is a hierarchical data structure with nodes connected by edges. The topmost node is the root, and nodes with no children are called leaves.\nBinary Tree: Each node has at most two children. Binary Search Tree (BST): A binary tree where the left child contains values less than the parent, and the right child contains values greater. Balanced Tree: A tree where the height difference between left and right subtrees of any node is minimal (e.g., AVL tree, Red-Black tree). Tree Traversals: Preorder Traversal (Root, Left, Right) Inorder Traversal (Left, Root, Right) Postorder Traversal (Left, Right, Root) ## Trees are often represented using classes. class TreeNode: def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = right ## Preorder Traversal (Root, Left, Right) def preorder_traversal(root): if root: print(root.val) preorder_traversal(root.left) preorder_traversal(root.right) ## Inorder Traversal (Left, Root, Right) def inorder_traversal(root): if root: inorder_traversal(root.left) print(root.val) inorder_traversal(root.right) ## Postorder Traversal (Left, Right, Root) def postorder_traversal(root): if root: postorder_traversal(root.left) postorder_traversal(root.right) print(root.val) ## Binary Search Tree (BST) Operations ## 1. Insert a Node def insert_into_bst(root, val): if not root: return TreeNode(val) if val \u0026lt; root.val: root.left = insert_into_bst(root.left, val) else: root.right = insert_into_bst(root.right, val) return root ## 2. Search for a Value def search_bst(root, val): if not root or root.val == val: return root if val \u0026lt; root.val: return search_bst(root.left, val) return search_bst(root.right, val) ## 3. Delete a Node def delete_node(root, key): if not root: return None if key \u0026lt; root.val: root.left = delete_node(root.left, key) elif key \u0026gt; root.val: root.right = delete_node(root.right, key) else: if not root.left: return root.right if not root.right: return root.left min_larger_node = root.right while min_larger_node.left: min_larger_node = min_larger_node.left root.val = min_larger_node.val root.right = delete_node(root.right, root.val) return root Core Algorithms # Overview # Two Pointer: The two-pointer technique is used primarily in solving array and linked list problems. It involves using two pointers to traverse the data structure, allowing for efficient searching and processing of elements. Sorting Algorithms: Review the mechanisms and use cases for quicksort, mergesort, and heapsort. Understand the trade-offs in terms of time and space complexity. Search Algorithms: Study binary search on sorted arrays, and learn about its variations for finding the first or last position of an element. Recursion and Backtracking: Understand how to apply recursion for solving problems involving permutations, combinations, and other backtrack-required scenarios. Study the call stack mechanism and how to optimize recursion through memoization. Prefix Sum and Suffix Sum: Prefix Sum and Suffix Sum are techniques used to compute the sum of elements in a subarray quickly by precomputing cumulative sums. Two Pointer # Finding Pairs with a Given Sum: When looking for two numbers in a sorted array that add up to a specific target. Reversing a String or Array: Using two pointers to swap elements from the start and end until they meet in the middle. Merging Two Sorted Arrays: Traversing both arrays simultaneously to create a new sorted array. Removing Duplicates from a Sorted Array: Using two pointers to track unique elements. 设置 two pointers 的时候，left 一般会在最前面，但是 right 不一定在最后，可以设置在 left 后面。 【Last Update: 2024-11-07】\nPrefix Sum and Suffix Sum # Prefix Sum: For an array nums, the prefix sum at each index i is the sum of all elements from the start of the array up to i. This allows you to find the sum of any subarray [i, j] in constant time by calculating prefix[j+1] - prefix[i]. Suffix Sum: For the same array nums, the suffix sum at index i is the sum of all elements from i to the end of the array. It enables efficient queries for sums of subarrays that start from any index i to a given end by using suffix[i] - suffix[j+1]. 【Last Update: 2024-11-11】\n## Input [1, 2, 3, 4] -\u0026gt; Output [2x3x4, 1x3x4, 1x2x4, 1x2x3] = [24, 12, 8, 6] ## Predix -\u0026gt; [0, 1, 1x2, 1x2x3] = [0, 1, 2, 6] ## Suffix -\u0026gt; [2x3x4, 3x4, 4, 0] = [24, 12, 4, 0] def productExceptSelf(self, nums: List[int]) -\u0026gt; List[int]: res = [1] * len(nums) prefix, suffix = 1, 1 for i in range(len(nums)): res[i] = prefix prefix *= nums[i] for j in range(len(nums)-1,-1,-1): res[j] *= suffix suffix *= nums[j] return res Recursion # 递归（Recursion）中一个函数会直接或间接调用自身来解决问题。它通常用于将问题分解为子问题的形式。\n问题结构: 问题可以分解为更小的子问题，且这些子问题具有相同的结构。 终止条件: 每个递归必须有基准情况（Base Case）以防止无限递归。 无“回溯”操作: 递归函数只关注每一层的子问题，不涉及撤销操作。 用途: 适用于分治问题（如二分搜索、归并排序）或递归定义的问题（如树的遍历）。 【Last Update: 2024-12-11】\ndef fibonacci(n): if n \u0026lt;= 1: return n # 基准情况 return fibonacci(n-1) + fibonacci(n-2) # 递归关系 Backtracking # 回溯（Backtracking）是一种试探性的搜索算法，它通过递归探索所有可能的解，并在发现某条路径不满足条件时，撤销（回溯）当前的选择并尝试其他路径。\n探索并撤销: 在探索某条路径时，如果发现不符合条件，会回退到上一步尝试其他可能。 约束条件: 通过加入剪枝（pruning）优化搜索，避免无意义的计算。 用途: 适用于需要生成所有解并验证解的正确性的问题。 【Last Update: 2024-12-11】\ndef permutations(nums): result = [] def backtrack(path, remaining): if not remaining: # 终止条件：所有元素已被使用 result.append(path) return for i in range(len(remaining)): backtrack(path + [remaining[i]], remaining[:i] + remaining[i+1:]) backtrack([], nums) return result Depth-First Search (DFS) # DFS 是一种递归（Recursive）或使用栈（Stack）实现的算法。它会优先深入访问一个分支，直到该分支无法继续，再回溯（Backtrack）到上一层继续访问未探索的分支。更适合用于解决路径、连通性和循环检测等问题。\n应用: 检测循环（Cycle Detection）：通过递归栈检测图中是否存在循环。 连通分量（Connected Components）：在无向图中找到所有连通分量。 路径搜索（Path Search）：在迷宫（Maze）中找到从起点到终点的一条路径。 【Last Update: 2024-12-10】\ndef dfs(graph, start, visited=None): if visited is None: visited = set() visited.add(start) for neighbor in graph[start]: if neighbor not in visited: dfs(graph, neighbor, visited) return visited graph = {0: [1, 2], 1: [0, 3], 2: [0, 4], 3: [1], 4: [2]} print(dfs(graph, 0)) # Output: {0, 1, 2, 3, 4} Breadth-First Search (BFS) # BFS 是一种逐层遍历（Level-Order Traversal）的算法，优先访问距离起点较近的节点。它使用队列（Queue）实现，以确保节点按照发现的顺序访问。更适合解决最短路径（Shortest Path）等问题。\n应用: 最短路径（Shortest Path）：在无权图（Unweighted Graph）中计算两点之间的最短路径。 层级关系（Level Order Traversal）：例如二叉树的层序遍历。 连通性检查（Connectivity Check）：检查从某节点是否可以到达所有其他节点。 【Last Update: 2024-12-10】\nfrom collections import deque def bfs_tree(root): if not root: return queue = deque([root]) while queue: node = queue.popleft() print(node.val, end=\u0026#34; \u0026#34;) # 访问当前节点 if node.left: queue.append(node.left) if node.right: queue.append(node.right) bfs_tree(root) # 输出: 1 2 3 4 5 Advanced Algorithms # Overview # Dynamic Programming: Explore the methodology of solving problems by breaking them down into smaller subproblems, storing results, and combining them to solve larger problems. Focus on understanding the concepts of overlapping subproblems and optimal substructure. Greedy Algorithms: Learn how greedy choices can lead to globally optimized solutions and their applications in problems like scheduling, graph based problems (like minimum spanning trees), and currency denomination. Graph Algorithms: Study shortest path algorithms (Dijkstra’s, Bellman-Ford) and minimum spanning tree algorithms (Prim’s, Kruskal’s). Understand their use cases and limitations. "},{"id":13,"href":"/docs/common-libraries/pandas/","title":"Pandas","section":"Common Libraries","content":" Pandas # "},{"id":14,"href":"/docs/machine-learning/supervised-learning/","title":"Supervised Learning","section":"Machine Learning","content":" 监督学习 # 监督学习方法（Supervised Learning） # 监督学习是机器学习中的一类算法，在这种方法中，模型通过已标注的数据进行训练。目标是让模型学会从输入特征（ \\(X\\) ）到输出标签（ \\(y\\) ）的映射关系。在监督学习中，训练数据由输入数据和对应的正确输出（标签）组成。这种方法通常用于分类和回归任务。\n本文件汇总了各种监督学习方法的概述。每种方法都在单独的页面中进行详细描述，涵盖算法的基本原理、应用场景以及理论基础。\n目录 # 线性回归 (Linear Regression) 逻辑回归 (Logistic Regression) K-近邻算法 (K-Nearest Neighbors) 支持向量机 (Support Vector Machines) 决策树 (Decision Trees) 随机森林 (Random Forests) 梯度提升机 (Gradient Boosting Machines) 朴素贝叶斯 (Naive Bayes) 学习目标 # 理解每种监督学习算法的基本概念、数学公式和实现原理 掌握算法在不同数据场景下的应用方法 理解各算法的优缺点、适用范围及如何选择合适的算法 使用说明 # 每种方法都在单独的页面中进行总结和详细描述。 目录部分链接到每个算法的详细页面。 为了便于理解，文件中包含了示例和代码片段。 "},{"id":15,"href":"/docs/deep-learning/generative-models/","title":"Generative Models","section":"Deep Learning","content":" Generative Models # "},{"id":16,"href":"/docs/common-libraries/pytorch/","title":"PyTorch","section":"Common Libraries","content":" PyTorch # "},{"id":17,"href":"/docs/machine-learning/unsupervised-learning/","title":"Unsupervised Learning","section":"Machine Learning","content":" Unsupervised Learning # "},{"id":18,"href":"/docs/machine-learning/regularization/","title":"Regularization","section":"Machine Learning","content":" Regularization # "},{"id":19,"href":"/docs/machine-learning/optimization/","title":"Optimization","section":"Machine Learning","content":" Optimization # "},{"id":20,"href":"/docs/python-basics/leetcode/practice-history/","title":"Practice History","section":"Leetcode Notes","content":" Leetcode 练习记录 # 此页面记录了我在 LeetCode 平台上完成的算法题目练习，每条记录包括完成日期、题目链接以及涉及的数据结构或算法主题。这些练习旨在巩固基础知识、提高解题技巧，并为技术面试做好充分准备。以下为部分记录：\n按日期排序 # 2024年12月 Date: 2024-12-13: Leetcode 70 - Climbing Stairs【Dynamic Programming】 Leetcode 509 - Fibonacci Number【Dynamic Programming】 Leetcode 17 - Letter Combinations of a Phone Number【Backtracking】 Date: 2024-12-12: Leetcode 22 - Generate Parentheses【Depth-First Search】【Backtracking】 Leetcode 39 - Combination Sum【Backtracking】 Date: 2024-12-11: Leetcode 200 - Number of Islands【Depth-First Search】 Leetcode 695 - Max Area of Island【Depth-First Search】 Date: 2024-12-10: Leetcode 100 - Same Tree【Tree】【Depth-First Search】 Date: 2024-12-05: Leetcode 235 - Lowest Common Ancestor of a Binary Search Tree【Binary Search Tree】【Depth-First Search】 Leetcode 700 - Search in a Binary Search Tree【Binary Search Tree】 Date: 2024-12-04: Leetcode 226 - Invert Binary Tree【Binary Search Tree】【Depth-First Search】 2024年11月 Date: 2024-11-28: Leetcode 98 - Validate Binary Search Tree【Binary Search Tree】【Depth-First Search】 Date: 2024-11-27: Leetcode 111 - Minimum Depth of Binary Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 112 - Path Sum【Tree】【Binary Tree】【Depth-First Search】 Leetcode 101 - Symmetric Tree【Tree】【Binary Tree】【Depth-First Search】 Date: 2024-11-26: Leetcode 94 - Binary Tree Inorder Traversal【Tree】【Binary Tree】 Leetcode 144 - Binary Tree Preorder Traversal【Tree】【Binary Tree】 Leetcode 145 - Binary Tree Postorder Traversal【Tree】【Binary Tree】 Leetcode 104 - Maximum Depth of Binary Tree【Tree】【Binary Tree】【Depth-First Search】 Date: 2024-11-25: Leetcode 239 - Sliding Window Maximum【Deque】【Sliding Window】 Leetcode 641 - Design Circular Deque【Deque】 Leetcode 215 - Kth Largest Element in an Array【Heap】 Leetcode 23 - Merge k Sorted Lists【Heap】 Date: 2024-11-24: Leetcode 933 - Number of Recent Calls【Queue】 Date: 2024-11-22: Leetcode 739 - Daily Temperatures【Stack】【Monotonic Stack】 Leetcode 232 - Implement Queue using Stacks【Queue】【Stack】 Leetcode 225 - Implement Stack using Queues【Stack】【Queue】 Leetcode 622 - Design Circular Queue【Queue】 Date: 2024-11-21: Leetcode 155 - Min Stack【Stack】 Leetcode 682 - Baseball Game【Stack】 Date: 2024-11-19: Leetcode 20 - Valid Parentheses【Stack】【String】 Date: 2024-11-18: Leetcode 92 - Reverse Linked List II【Linked List】 Leetcode 61 - Rotate List【Linked List】【Two Pointers】 Date: 2024-11-17: Leetcode 19 - Remove N-th Node From End of List【Linked List】【Two Pointers】 Leetcode 138 - Copy List with Random Pointer【Linked List】【Hash Table】 Leetcode 234 - Palindrome Linked List【Linked List】 Leetcode 160 - Intersection of Two Linked Lists【Linked List】【Hash Table】 Date: 2024-11-15: Leetcode 21 - Merge Two Sorted Lists【Linked List】【Recursion】 Leetcode 141 - Linked List Cycle【Linked List】【Hash Table】 Leetcode 203 - Remove Linked List Elements【Linked List】【Recursion】 Leetcode 83 - Remove Duplicates from Sorted List【Linked List】 Leetcode 2 - Add Two Numbers【Linked List】【Recursion】【Math】 Date: 2024-11-14: Leetcode 206 - Reverse Linked List【Linked List】【Recursion】 Date: 2024-11-13: Leetcode 3 - Longest Substring Without Repeating Characters【String】【Hash Table】 Leetcode 5 - Longest Palindromic Substring【String】【Two Pointers】 Leetcode 28 - Find the Index of the First Occurrence in a String【String】【Two Pointers】 Leetcode 49 - Group Anagrams【String】【Hash Table】 Date: 2024-11-12: Leetcode 344 - Reverse String【String】【Two Pointers】 Leetcode 26 - Remove Duplicates from Sorted Array【Array】【Two Pointers】 Leetcode 27 - Remove Element【Array】【Two Pointers】 Date: 2024-11-11: Leetcode 53 - Maximum Subarray【Array】 Leetcode 238 - Product of Array Except Self【Array】【Prefix Sum】 Date: 2024-11-10: Leetcode 454 - 4Sum II【Array】【Hash Table】 Date: 2024-11-08: Leetcode 121 - Best Time to Buy and Sell Stock【Array】【Dynamic Programming】 Leetcode 349 - Intersection of Two Arrays【Array】【Hash Table】 Leetcode 219 - Contains Duplicate II【Array】【Hash Table】 Date: 2024-11-07: LeetCode 15 - 3 Sum【Array】【Two Pointers】 Date: 2024-11-06: LeetCode 1 - Two Sum【Array】【Hash Table】 按题目编号排序 # 题目编号 Leetcode 1 - Two Sum【Array】【Hash Table】 Leetcode 2 - Add Two Numbers【Linked List】【Recursion】【Math】 Leetcode 3 - Longest Substring Without Repeating Characters【String】【Hash Table】 Leetcode 5 - Longest Palindromic Substring【String】【Two Pointers】 Leetcode 15 - 3 Sum【Array】【Two Pointers】 Leetcode 17 - Letter Combinations of a Phone Number【Backtracking】 Leetcode 19 - Remove N-th Node From End of List【Linked List】【Two Pointers】 Leetcode 20 - Valid Parentheses【Stack】【String】 Leetcode 21 - Merge Two Sorted Lists【Linked List】【Recursion】 Leetcode 22 - Generate Parentheses【Depth-First Search】【Backtracking】 Leetcode 23 - Merge k Sorted Lists【Heap】 Leetcode 26 - Remove Duplicates from Sorted Array【Array】【Two Pointers】 Leetcode 27 - Remove Element【Array】【Two Pointers】 Leetcode 28 - Find the Index of the First Occurrence in a String【String】【Two Pointers】 Leetcode 39 - Combination Sum【Backtracking】 Leetcode 49 - Group Anagrams【String】【Hash Table】 Leetcode 53 - Maximum Subarray【Array】 Leetcode 61 - Rotate List【Linked List】【Two Pointers】 Leetcode 70 - Climbing Stairs【Dynamic Programming】 Leetcode 83 - Remove Duplicates from Sorted List【Linked List】 Leetcode 92 - Reverse Linked List II【Linked List】 Leetcode 94 - Binary Tree Inorder Traversal【Tree】【Binary Tree】 Leetcode 98 - Validate Binary Search Tree【Binary Search Tree】【Depth-First Search】 Leetcode 100 - Same Tree【Tree】【Depth-First Search】 Leetcode 101 - Symmetric Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 104 - Maximum Depth of Binary Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 111 - Minimum Depth of Binary Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 112 - Path Sum【Tree】【Binary Tree】【Depth-First Search】 Leetcode 121 - Best Time to Buy and Sell Stock【Array】【Dynamic Programming】 Leetcode 138 - Copy List with Random Pointer【Linked List】【Hash Table】 Leetcode 141 - Linked List Cycle【Linked List】【Hash Table】 Leetcode 144 - Binary Tree Preorder Traversal【Tree】【Binary Tree】 Leetcode 145 - Binary Tree Postorder Traversal【Tree】【Binary Tree】 Leetcode 155 - Min Stack【Stack】 Leetcode 160 - Intersection of Two Linked Lists【Linked List】【Hash Table】 Leetcode 200 - Number of Islands【Depth-First Search】 Leetcode 203 - Remove Linked List Elements【Linked List】【Recursion】 Leetcode 206 - Reverse Linked List【Linked List】【Recursion】 Leetcode 215 - Kth Largest Element in an Array【Heap】 Leetcode 219 - Contains Duplicate II【Array】【Hash Table】 Leetcode 225 - Implement Stack using Queues【Stack】【Queue】 Leetcode 226 - Invert Binary Tree【Binary Search Tree】【Depth-First Search】 Leetcode 232 - Implement Queue using Stacks【Queue】【Stack】 Leetcode 234 - Palindrome Linked List【Linked List】 Leetcode 235 - Lowest Common Ancestor of a Binary Search Tree【Binary Search Tree】 Leetcode 238 - Product of Array Except Self【Array】【Prefix Sum】 Leetcode 239 - Sliding Window Maximum【Deque】【Sliding Window】 Leetcode 249 - Number of Recent Calls【Queue】 Leetcode 344 - Reverse String【String】【Two Pointers】 Leetcode 349 - Intersection of Two Arrays【Array】【Hash Table】 Leetcode 454 - 4Sum II【Array】【Hash Table】 Leetcode 509 - Fibonacci Number【Dynamic Programming】 Leetcode 622 - Design Circular Queue【Queue】 Leetcode 641 - Design Circular Deque【Deque】 Leetcode 682 - Baseball Game【Stack】 Leetcode 695 - Max Area of Island【Depth-First Search】 Leetcode 700 - Search in a Binary Search Tree【Binary Search Tree】 Leetcode 739 - Daily Temperatures【Stack】【Monotonic Stack】 Leetcode 933 - Number of Recent Calls【Queue】 按题目类型排序 # 题目类型 Array # Leetcode 1 - Two Sum【Array】【Hash Table】 Leetcode 15 - 3 Sum【Array】【Two Pointers】 Leetcode 53 - Maximum Subarray【Array】 Leetcode 121 - Best Time to Buy and Sell Stock【Array】【Dynamic Programming】 Leetcode 238 - Product of Array Except Self【Array】【Prefix Sum】 Leetcode 219 - Contains Duplicate II【Array】【Hash Table】 Leetcode 349 - Intersection of Two Arrays【Array】【Hash Table】 Leetcode 454 - 4Sum II【Array】【Hash Table】 Linked List # Leetcode 2 - Add Two Numbers【Linked List】【Recursion】【Math】 Leetcode 19 - Remove N-th Node From End of List【Linked List】【Two Pointers】 Leetcode 21 - Merge Two Sorted Lists【Linked List】【Recursion】 Leetcode 26 - Remove Duplicates from Sorted Array【Array】【Two Pointers】 Leetcode 61 - Rotate List【Linked List】【Two Pointers】 Leetcode 83 - Remove Duplicates from Sorted List【Linked List】 Leetcode 92 - Reverse Linked List II【Linked List】 Leetcode 138 - Copy List with Random Pointer【Linked List】【Hash Table】 Leetcode 141 - Linked List Cycle【Linked List】【Hash Table】 Leetcode 160 - Intersection of Two Linked Lists【Linked List】【Hash Table】 Leetcode 203 - Remove Linked List Elements【Linked List】【Recursion】 Leetcode 206 - Reverse Linked List【Linked List】【Recursion】 Leetcode 234 - Palindrome Linked List【Linked List】 String # Leetcode 3 - Longest Substring Without Repeating Characters【String】【Hash Table】 Leetcode 5 - Longest Palindromic Substring【String】【Two Pointers】 Leetcode 28 - Find the Index of the First Occurrence in a String【String】【Two Pointers】 Leetcode 49 - Group Anagrams【String】【Hash Table】 Leetcode 344 - Reverse String【String】【Two Pointers】 Stack # Leetcode 20 - Valid Parentheses【Stack】【String】 Leetcode 155 - Min Stack【Stack】 Leetcode 225 - Implement Stack using Queues【Stack】【Queue】 Leetcode 682 - Baseball Game【Stack】 Leetcode 739 - Daily Temperatures【Stack】【Monotonic Stack】 Queue # Leetcode 232 - Implement Queue using Stacks【Queue】【Stack】 Leetcode 249 - Number of Recent Calls【Queue】 Leetcode 622 - Design Circular Queue【Queue】 Leetcode 641 - Design Circular Deque【Deque】 Heap # Leetcode 23 - Merge k Sorted Lists【Heap】 Leetcode 215 - Kth Largest Element in an Array【Heap】 Tree # Leetcode 94 - Binary Tree Inorder Traversal【Tree】【Binary Tree】 Leetcode 98 - Validate Binary Search Tree【Binary Search Tree】【Depth-First Search】 Leetcode 100 - Same Tree【Tree】【Depth-First Search】 Leetcode 101 - Symmetric Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 104 - Maximum Depth of Binary Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 111 - Minimum Depth of Binary Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 112 - Path Sum【Tree】【Binary Tree】【Depth-First Search】 Leetcode 144 - Binary Tree Preorder Traversal【Tree】【Binary Tree】 Leetcode 145 - Binary Tree Postorder Traversal【Tree】【Binary Tree】 Leetcode 226 - Invert Binary Tree【Binary Search Tree】【Depth-First Search】 Leetcode 235 - Lowest Common Ancestor of a Binary Search Tree【Binary Search Tree】【Depth-First Search】 Leetcode 700 - Search in a Binary Search Tree【Binary Search Tree】 Binary Search Tree # Leetcode 98 - Validate Binary Search Tree【Binary Search Tree】【Depth-First Search】 Leetcode 235 - Lowest Common Ancestor of a Binary Search Tree【Binary Search Tree】【Depth-First Search】 Leetcode 700 - Search in a Binary Search Tree【Binary Search Tree】 Deque # Leetcode 239 - Sliding Window Maximum【Deque】【Sliding Window】 Leetcode 641 - Design Circular Deque【Deque】 Sliding Window # Leetcode 239 - Sliding Window Maximum【Deque】【Sliding Window】 Monotonic Stack # Leetcode 739 - Daily Temperatures【Stack】【Monotonic Stack】 Prefix Sum # Leetcode 238 - Product of Array Except Self【Array】【Prefix Sum】 Dynamic Programming # Leetcode 121 - Best Time to Buy and Sell Stock【Array】【Dynamic Programming】 Recursion # Leetcode 2 - Add Two Numbers【Linked List】【Recursion】【Math】 Leetcode 21 - Merge Two Sorted Lists【Linked List】【Recursion】 Leetcode 22 - Generate Parentheses【Depth-First Search】【Backtracking】 Leetcode 203 - Remove Linked List Elements【Linked List】【Recursion】 Leetcode 206 - Reverse Linked List【Linked List】【Recursion】 Hash Table # Leetcode 1 - Two Sum【Array】【Hash Table】 Leetcode 3 - Longest Substring Without Repeating Characters【String】【Hash Table】 Leetcode 49 - Group Anagrams【String】【Hash Table】 Leetcode 138 - Copy List with Random Pointer【Linked List】【Hash Table】 Leetcode 141 - Linked List Cycle【Linked List】【Hash Table】 Leetcode 160 - Intersection of Two Linked Lists【Linked List】【Hash Table】 Leetcode 349 - Intersection of Two Arrays【Array】【Hash Table】 "}]