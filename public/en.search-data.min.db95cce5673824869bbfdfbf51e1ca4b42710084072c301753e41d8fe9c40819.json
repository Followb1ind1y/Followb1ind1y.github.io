[{"id":0,"href":"/posts/02_linear_regression/","title":"Linear Regression","section":"Blog","content":"Regression is a method of modelling a target value based on independent predictors. This method is mostly used for forecasting and finding out cause and effect relationship between variables. Regression techniques mostly differ based on the number of independent variables and the type of relationship between the independent and dependent variables. Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on.\nLinear Regression # Linear regression with multiple variables is also known as \u0026ldquo;multivariate linear regression\u0026rdquo;. The multivariable form of the hypothesis function accommodating these multiple features is as follows:\n\\[ \\begin{align*} \u0026\\mathrm{Hypothesis}: h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\theta_{3}x_{3} + \\cdot\\cdot\\cdot + \\theta_{n}x_{n} \\\\ \u0026\\mathrm{Cost \\ Function}: J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2 \\\\ \u0026\\mathrm{Goal}: \\min_{\\theta}J(\\theta) \\\\ \\end{align*} \\] Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:\n\\[ h_{\\theta}(x) = \\begin{bmatrix} \\theta_{0} \u0026 \\theta_{1} \u0026 \\cdot\\cdot\\cdot \u0026 \\theta_{n} \\end{bmatrix} \\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ \\cdot\\cdot\\cdot \\\\ x_{n} \\end{bmatrix} = \\theta^{T}x \\\\ \\] Gradient Descent for Linear Regression # Gradient descent is a generic optimization algorithm used in many machine learning algorithms. It iteratively tweaks the parameters of the model in order to minimize the cost function. We do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter \\(\\alpha\\) , which is called the learning rate. The gradient descent algorithm can be represented as:\n\\[ \\begin{align*} \u0026\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}}= \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)}) \\\\ \u0026\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}= \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} \\\\ \\end{align*} \\] \\[ \\begin{bmatrix} \\frac{\\partial J(\\theta)}{\\partial \\theta_{0}} \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{1}} \\\\ \\cdot\\cdot\\cdot \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{n}} \\end{bmatrix}=\\frac{1}{m}x^{T}(h_{\\theta}(x)-y) \\\\ \\] \\[ \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\cdot\\cdot\\cdot \\\\ \\theta_{n} \\end{bmatrix}=\\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\cdot\\cdot\\cdot \\\\ \\theta_{n} \\end{bmatrix}-\\alpha\\begin{bmatrix} \\frac{\\partial J(\\theta)}{\\partial \\theta_{0}} \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{1}} \\\\ \\cdot\\cdot\\cdot \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{n}} \\end{bmatrix} \\\\ \\] \\[ \\begin{align*} \u0026\\mathrm{Repect\\ until \\ convergence} \\{ \\\\ \u0026\\ \\ \\ \\ \\theta_{j}^{new} :=\\theta_{j}^{old} - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}) \\ \\ \\ \\ \\mathrm{for} \\ j = 0 \\cdot\\cdot\\cdot n \\\\ \u0026\\} \\\\ \\end{align*} \\] To demonstrate the gradient descent algorithm, we initialize the model parameters with 0. The equation becomes \\(h_{\\theta}(x) = 0\\) . Gradient descent algorithm now tries to update the value of the parameters so that we arrive at the best fit line.\nWe should adjust our parameter \\(\\alpha\\) to ensure that the gradient descent algorithm converges in a reasonable time. If \\(\\alpha\\) is too small, gradient descent can be slow. If \\(\\alpha\\) is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.\nWe can speed up gradient descent by having each of our input values in roughly the same range. This is because \\(\\theta\\) will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\nTwo techniques to help with this are feature scaling and mean normalization. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:\n\\[ x_{i} := \\frac{(x_{i}-\\mu_{i})}{s_{i}} \\\\ \\] Where \\(\\mu_{i}\\) is the average of all the values for feature (i) and \\(s_{i}\\) is the range of values (max - min), or \\(s_{i}\\) is the standard deviation. Note that dividing by the range, or dividing by the standard deviation, give different results.\nNormal Equation for Linear Regression # Gradient descent gives one way of minimizing J. In the \u0026ldquo;Normal Equation\u0026rdquo; method, we will minimize J by explicitly taking its derivatives with respect to the \\(\\theta_{j}\\) ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:\n\\[ \\theta = (X^{T}X)^{-1}X^{T}y \\\\ \\] The following is a comparison of gradient descent and the normal equation:\nGradient Descent Normal Equation Need to choose alpha No need to choose alpha Needs many iterations No need to iterate \\(O(kn^{2})\\) \\(O(n^{3})\\) , need to calculate inverse of \\(X^{T}X\\) Works well when n is large Slow if n is very large With the normal equation, computing the inversion has complexity \\(O(n^{3})\\) . So if we have a very large number of features, the normal equation will be slow. In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.\nIf \\(X^{T}X\\) is noninvertible, the common causes might be having :\nRedundant features, where two features are very closely related (i.e. they are linearly dependent) Too many features (e.g. \\(m ≤ n\\) ). In this case, delete some features or use \u0026ldquo;regularization\u0026rdquo;. Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.\nEvaluating the performance of the Linear Regression Model # We will be using Root mean squared error(RMSE) and Coefficient of Determination(R² score) to evaluate our model. RMSE is the square root of the average of the sum of the squares of residuals. RMSE is defined by:\n\\[ RMSE = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2} \\\\ \\] \\(R^{2}\\) score or the coefficient of determination explains how much the total variance of the dependent variable can be reduced by using the least square regression. It can be defined by:\n\\[ R^{2} = 1- \\frac{SS_{r}}{SS{t}} \\\\ \\] Where \\(SS_{t}\\) is the total sum of errors if we take the mean of the observed values as the predicted value and \\(SS_{r}\\) is the sum of the square of residuals.\n\\[ \\begin{align*} SS_{t} \u0026= \\sum_{i=1}^{m}(y^{(i)}-\\bar{y})^2 \\\\ SS_{r} \u0026= \\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2 \\\\ \\end{align*} \\] References # [1] Agarwal, A. (2018, November 14). Linear Regression using Python. Medium. https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2.\n[2] Ng, A. (n.d.). Machine Learning. Coursera. https://www.coursera.org/learn/machine-learning.\nBlog Home "},{"id":1,"href":"/docs/machine-learning/supervised-learning/linear-regression/","title":"Linear Regression","section":"Supervised Learning","content":" 线性回归 # 线性回归（Linear Regression） # 线性回归是一种最简单的回归模型，目标是通过一个或多个输入变量预测一个 连续的（continuous） 输出变量。其核心思想是 拟合一条直线来描述输入（input）与输出（output）之间 的关系。其数学公式为：\n\\[ \\hat{y} = XW + b \\] 其中 \\(N\\) 是总样本数量， \\(\\hat{y}\\) 是预测值 \\(\\in \\mathbb{R}^{N \\times 1}\\) ， \\(X\\) 是输入值 \\(\\in \\mathbb{R}^{N \\times D}\\) ， \\(W\\) 是 Weight \\(\\in \\mathbb{R}^{D \\times 1}\\) ， \\(b\\) 是 Bias \\(\\in \\mathbb{R}^{1}\\) 。\n损失函数（Loss Function） # 线性回归的训练目标是找到一组最优参数（权重 \\(W\\) 和偏置 \\(b\\) ），使得模型对训练数据的预测值与实际目标值之间的误差最小。具体来说，模型的目标是最小化误差函数（也称损失函数）。即使预测值 \\(\\hat{y_{i}}\\) 和实际值 \\(y_{i}\\) 的差异最小化。\n均方误差（Mean Squared Error, MSE） 是线性回归中最常用的损失函数。它计算预测值与真实值之间差异的平方和的均值：\n\\[ MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2 \\] 通过调整参数 \\(W\\) 和 \\(b\\) ，最小化 \\(MSE\\) 的目的是：\n惩罚大的预测误差（使得较大的误差对总体损失影响更显著）。 保证损失函数是连续且可导的（continuously differentiable），方便优化算法（如梯度下降）进行求解。 梯度下降法在线性回归中的应用（Gradient Descent） # 在线性回归中，梯度下降通过调整模型参数 \\(W\\) （权重）和 \\(b\\) （偏置），逐步逼近最优解。我们通过对 Loss Funcion 求导（函数的切线）来实现这一点。切线（slope）的斜率就是该点的导数，它将为我们提供前进的方向。我们沿着下降最快的方向逐步降低 Loss Function。每一步的大小由参数 \\(\\alpha\\) 决定，该参数称为学习率。梯度下降算法可以表示为：\n\\[ \\begin{align*} \u0026\\frac{\\partial L}{\\partial w}= -\\frac{2}{N}\\sum_{i=1}^{N}(y_{i}-\\hat{y_{i}})x_{i} \\\\ \u0026\\frac{\\partial L}{\\partial b}= -\\frac{2}{N}\\sum_{i=1}^{N}(y_{i}-\\hat{y_{i}}) \\\\ \u0026 w := w - \\alpha \\frac{\\partial L}{\\partial w}, \\quad b := b - \\alpha \\frac{\\partial L}{\\partial b} \\\\ \\end{align*} \\] \\[ \\begin{align*} \u0026\\mathrm{Repect\\ until \\ convergence} \\{ \\\\ \u0026\\ \\ \\ \\ w_{j}^{new} :=w_{j}^{old} - \\alpha\\frac{2}{N}\\sum_{i=1}^{N}(y_{i}-\\hat{y_{i}}) \\ \\ \\ \\ \\mathrm{for} \\ j = 0 \\cdot\\cdot\\cdot d \\\\ \u0026\\} \\\\ \\end{align*} \\] 收敛准则 损失函数的变化小于某个阈值（如 \\(\\Delta L \u003c \\epsilon\\) ）。 达到预设的最大迭代次数。 学习率的影响 # 我们应该调整参数 \\(\\alpha\\) 以确保梯度下降算法在合理的时间内收敛。如果 \\(\\alpha\\) 太小，梯度下降可能会很慢。如果 \\(\\alpha\\) 太大，梯度下降可能会超过最小值。它可能无法收敛，甚至发散（ fail to converge, or even diverge）。无法收敛或花费太多时间获得最小值意味着我们的步长是错误的。\n\\(\\alpha\\) 太大：可能导致更新过快，错过最优解，甚至发散。 \\(\\alpha\\) 太小：收敛速度慢，需要更多迭代。 性能评估（Evaluation Metrics） # 在训练完线性回归模型后，需要对模型的性能进行评估，以了解模型的拟合效果和预测能力。我们常使用均方根误差（RMSE）和确定系数（ \\(R^2 \\) 得分）来评估我们的模型。RMSE是残差平方和平均值的平方根。RMSE的定义是：\n\\[ RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2} \\] \\(R^2 \\) 得分（ \\(R^2 \\) score）表示模型解释目标变量总变异的比例：。它可以被定义为：\n\\[ R^2 = 1 - \\frac{\\sum_{i=1}^{N} (y_i - \\hat{y}i)^2}{\\sum_{i=1}^{N} (y_i - \\bar{y})^2} \\] 其中：\n\\(\\bar{y}\\) 是目标值的均值。 \\(R^2\\) 的取值范围是 [0, 1]（可以小于 0，表示模型比均值模型还差）。 解释：\n\\(R^2 = 1\\) ：模型能完全解释目标变量。 \\(R^2 = 0\\) ：模型的表现与仅使用目标值均值的基准模型相同。 \\(R^2 \u003c 0\\) ：模型表现比基准模型差。 Linear Regression 代码实现 # # \u0026lt;--- From scratch ---\u0026gt; import numpy as np from sklearn.model_selection import train_test_split # 生成数据：y = 2 + 3*X + 噪声 X = np.random.rand(100, 1) # 随机生成100个样本，只有一个特征 y = 2 + 3 * X + np.random.rand(100, 1) # 添加噪声项模拟真实数据 # 数据划分：80%训练集，20%测试集 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 均方误差函数 (Mean Squared Error) def MSE(y_1, y_2): return np.square(np.subtract(y_1, y_2)).mean() # 均方根误差函数 (Root Mean Squared Error) def RMSE(y_1, y_2): return np.sqrt(MSE(y_1, y_2)) # 线性回归的梯度下降实现 def LinearRegression_with_GD(X, y, learning_rate=0.001, threshold=0.001, max_iter=100): \u0026#34;\u0026#34;\u0026#34; X: 输入特征矩阵 y: 输出目标向量 learning_rate: 学习率，控制每次梯度更新的步长 threshold: 损失函数收敛的阈值，差异小于该值则停止迭代 max_iter: 最大迭代次数 \u0026#34;\u0026#34;\u0026#34; # 添加偏置项 (Intercept term) X_new = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1) # 在特征矩阵前添加一列1 total_sample, features = X_new.shape[0], X_new.shape[1] # 获取样本数和特征数 W = np.random.rand(features, 1) # 初始化权重为随机值 losses = [] # 用于记录每次迭代的损失值 for i in range(max_iter): y_pred = np.dot(X_new, W) # 计算预测值：h(X) = X_new * W loss = MSE(y_pred, y) # 计算当前的MSE损失 if losses and losses[-1] - loss \u0026lt; threshold: # 如果损失下降小于阈值，提前停止 losses.append(loss) break losses.append(loss) # 记录当前损失 gradient = np.dot(X_new.T, (np.subtract(y_pred, y))) # 计算梯度：∇J(W) W -= learning_rate * gradient # 使用梯度下降更新权重 print(\u0026#34;Final Training Loss:\u0026#34;, losses[-1]) # 输出最终训练损失 return W # 返回训练好的权重 # 使用训练好的权重进行预测 def LinearRegression_Predict(X, y, W): \u0026#34;\u0026#34;\u0026#34; X: 输入特征矩阵 y: 真实目标向量 W: 已训练的权重 \u0026#34;\u0026#34;\u0026#34; X_new = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1) # 添加偏置项 y_pred = np.dot(X_new, W) # 计算预测值：h(X) = X_new * W rmse = RMSE(y_pred, y) # 计算均方根误差 print(\u0026#34;RMSE:\u0026#34;, rmse) # 输出预测的RMSE return y_pred # 返回预测值 # 调用梯度下降实现线性回归 W = LinearRegression_with_GD(X_train, y_train) # 使用训练好的模型预测测试集 y_pred = LinearRegression_Predict(X_test, y_test, W) # \u0026lt;--- From scikit-learn ---\u0026gt; from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split import numpy as np # 生成数据 X = np.random.rand(100, 1) # 随机生成100个样本，每个样本只有一个特征 y = 2 + 3 * X + np.random.rand(100, 1) # 模拟线性关系，并加入噪声 # 数据集分割 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 创建线性回归模型，设置可选参数 model = LinearRegression() # 训练模型 model.fit(X_train, y_train) # 用训练集拟合模型 # 预测测试集 y_pred = model.predict(X_test) # 使用模型预测测试集 # 输出截距和权重 print(\u0026#34;Intercept (Bias):\u0026#34;, model.intercept_) # 截距项 print(\u0026#34;Coefficients (Weights):\u0026#34;, model.coef_) # 权重项 多项式回归（Polynomial Regression） # 多项式回归是一种线性回归的扩展形式，它适用于因变量与自变量之间呈现非线性关系的数据。通过在输入特征上应用多项式变换，进行了非线性扩展，将其映射到更高维的特征空间，使模型可以拟合复杂的非线性数据。多项式回归中，模型的参数（权重 \\(W\\) ）仍然是线性求解的，因此它在数学本质上是线性模型。其数学公式为: \\[ \\hat{y} = X_{poly}W + b = W_{1}x + W_{2}x^2 + \\cdots + W_{n}x^n + b \\] 特征处理： 线性回归：直接使用输入特征。 多项式回归：对输入特征进行非线性扩展。 拟合能力： 线性回归：只能拟合线性关系，容易欠拟合。 多项式回归：能够拟合非线性关系，但高次多项式可能导致过拟合。 多项式回归的步骤 # 数据准备：准备训练数据，其中包含输入特征（自变量）和目标值（因变量）。假设我们有一个简单的一维输入特征 \\(X\\) 和目标值 \\(y\\) ，目标是通过多项式回归来拟合这些数据。 特征工程：将原始特征扩展为多项式特征，使模型能够捕捉数据中的非线性关系。假设我们选择二次多项式（degree=2）。我们会将输入特征 \\(X = [1, 2, 3, 4, 5]\\) 扩展为： \\[ X_{\\text{poly}} = \\begin{bmatrix} 1 \u0026 x_1 \u0026 x_1^2 \\\\ 1 \u0026 x_2 \u0026 x_2^2 \\\\ 1 \u0026 x_3 \u0026 x_3^2 \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \\\\ 1 \u0026 x_n \u0026 x_n^2 \\end{bmatrix} \\] 模型训练：多项式回归本质上是在线性回归的基础上进行特征扩展。所以在特征扩展之后，我们依然使用线性回归的公式来训练模型： \\[ \\hat{y} = W_{0} + W_{1}x + W_{2}x^2 + \\cdots + W_{n}x^n \\] 回归模型的训练过程就是通过最小化 均方误差（MSE） 来求解模型的参数: \\[ MSE = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2 \\] 模型评估：通过训练误差和验证误差评估模型的性能。 Polynomial Regression 代码实现 # # \u0026lt;--- From scikit-learn ---\u0026gt; import numpy as np from sklearn.preprocessing import PolynomialFeatures # 导入多项式特征扩展模块 from sklearn.linear_model import LinearRegression # 导入线性回归模型 from sklearn.model_selection import train_test_split # 导入数据集分割模块 # 生成随机数据作为输入 data = np.random.normal(size=(200, 2)) # 生成一个包含200个样本，2个特征的正态分布数据集 result = 2 + data[:, 0] ** 3 + 4 * data[:, 1] # 计算目标值，包含多项式（3次方项和1次方项） X_train, X_test, y_train, y_test = train_test_split(data, result, test_size=0.3, random_state=0) # 将数据集划分为训练集和测试集，30%的数据为测试集 # 定义多项式回归函数 def Polynomial_Regression(train_input_features, train_outputs, prediction_features): # 创建多项式特征转换器，设置多项式的阶数为3（因为示例中包含3次方项） poly = PolynomialFeatures(degree=3) # 对训练数据进行拟合并转换，得到多项式特征 X_poly_train = poly.fit_transform(train_input_features) # 训练线性回归模型 model = LinearRegression() model.fit(X_poly_train, train_outputs) # 用训练数据训练模型 # 使用相同的多项式转换器对预测数据进行转换 X_poly_pred = poly.transform(prediction_features) # 对转换后的数据进行预测 predictions = model.predict(X_poly_pred) return predictions # 返回预测结果 # 调用多项式回归函数进行预测 y_pred = Polynomial_Regression(X_train, y_train, X_test) "},{"id":2,"href":"/docs/machine-learning/machine-learning-basics/","title":"Machine Learning Basics","section":"Machine Learning","content":" Machine Learning Basics # 机器学习的定义与类型 # 定义 # Machine Learning（机器学习） 是一种人工智能技术，核心目标是使计算机能够从数据中自动学习，并根据这些数据改进对特定任务的性能，而无需明确的编程指令。\n机器学习通过以下三个要素实现：\n数据（Data）：输入的原始数据或特征数据。 模型（Model）：表示学习的假设空间，决定如何从数据中学习模式。 优化目标（Object）：定义如何评估模型好坏并改进其性能（如最小化误差）。 主要类型 # 监督学习 (Supervised Learning) # 监督学习 (Supervised Learning) 从标记数据中学习。在这种情况下，输入数据及其对应的输出值被提供给算法，算法学习输入和输出值之间的映射（the mapping between the input and output values）。监督学习的目标是对新的、看不见的输入数据做出准确的预测。\n输入（Input）：特征数据 \\(X\\) 和目标变量 \\(y\\) （如标签或真实值）。\n输出（Output）：预测模型，用于对新数据进行分类或回归。\n应用场景：\n分类问题（Classification）：将输入数据划分到预定义类别中。 回归问题（Regression）：预测连续数值的目标变量。 无监督学习 (Unsupervised Learning) # 无监督学习 (Unsupervised Learning) 从未标记的数据中学习。在这种情况下，输入数据没有标记，算法自己学习在数据中寻找模式和结构。无监督学习的目标是发现数据中隐藏的模式和结构。\n输入（Input）：仅有特征数据 \\(X\\) 。 输出（Output）：数据的潜在结构或表示。 应用场景： 聚类 (Clustering)：将数据分组到不同簇中。 降维 (Dimensionality Reduction)：简化数据表示，同时保留主要信息。 半监督学习 (Semi-Supervised Learning) # 半监督学习 (Semi-Supervised Learning) 从标记和未标记数据的组合中进行学习。在这种情况下，算法学习在未标记数据中寻找模式和结构，并使用标记数据来指导学习过程。\n输入（Input）：部分标注的数据和大量未标注的数据。 输出（Output）：用于分类或回归的预测模型。 应用场景： 在标注数据有限或获取标签成本高昂的情况下（如医学影像标注）。 强化学习 (Reinforcement Learning) # 强化学习 (Reinforcement Learning) 通过与 环境（environment） 交互进行学习。在这种情况下，算法学习采取 行动（action） 来最大化环境提供的奖励信号。强化学习的目标是学习最大化长期 奖励（reward） 的 策略（policy）。\n输入（Input）：状态 \\(S\\) 、动作 \\(A\\) 、奖励 \\(R\\) 。 输出（Output）：一个策略 \\(\\pi\\) ，指引在不同状态下的最佳行动。 应用场景： 游戏 AI：如 AlphaGo 使用强化学习在围棋中击败人类选手。 机器人导航：训练机器人在环境中找到最佳路径。 显式（Explicit） 和 隐式（Implicit） # 在机器学习中，显式（Explicit） 和 隐式（Implicit） 通常用于描述模型、方法或表示的不同特性。它们反映了信息是直接表达出来还是通过间接方式体现。\n显式（Explicit） 是指信息或结构是 直接表示 的，通常可以被明确地解释或观察到。\n直接性：显式方法通常有清晰的数学公式或逻辑规则。 可解释性：显式模型的内部机制容易被理解。 可见性：输入到输出之间的关系是明确可见的。 示例：线性回归，逻辑回归，规则模型（决策树）。 隐式（Implicit） 是指信息或结构是通过 间接方式表示 的，不直接显现。\n间接性：隐式方法通常不提供明确的公式，而是通过训练或优化过程间接捕获关系。 不可解释性：隐式模型的内部机制较难理解，通常被认为是“黑箱”。 抽象性：信息的表示可能分布在多个特征或参数中，而不是单一表达。 示例：神经网络，SVM，隐变量模型（Latent Variable Models）。 泛化能力（Generalization） # 机器学习的核心挑战在于，我们必须在新的、以前未见过的输入上表现良好——而不仅仅是我们的模型所训练的输入。在以前未观察到的输入上表现良好的能力称为泛化（generalization）。在训练过程中，我们通过降低 训练误差 (Training Error) 优化模型。但模型不仅需要在训练集上表现优异，还需要降低泛化误差 (Generalization Error)，即 测试误差（Test Error）。\n数据集（Datasets）分类和误差（Errors） # 训练集（Train Set） # 定义：训练集是模型学习的主要数据来源（占大多数），包括 输入特征 和对应的 目标输出（对于监督学习）。 功能：用于训练模型（的参数），通过优化算法最小化训练误差。 训练误差（Training Error）：训练误差是模型在训练数据上的错误率。它是通过测量每个训练示例的预测输出（predicted output）与实际输出（actual output）之间的差异来计算的。由于模型是在此数据上训练的，因此预计它会在此数据上表现良好，并且训练误差通常较低。\n验证集（Validation Set） # 定义：验证集是从训练数据中分离出来的一部分，用于评估模型在未见数据上的表现。 功能： 帮助调整超参数（如学习率、正则化系数、模型结构等）。 用于选择最佳模型，例如在多次训练后选择验证误差最低的模型。 验证误差（Validation Error）：验证误差是模型在验证数据上的错误率。用于评估训练期间模型的性能，目标是找到验证误差最低的模型。\n测试集（Test Set） # 定义：测试集是完全独立于训练和验证的数据，模型在训练和验证过程中从未接触过。 功能：用于评估模型的最终泛化性能，反映模型在实际场景中的表现。 测试误差（Test Error）：测试误差是模型在测试数据上的错误率。测试数据是与训练和验证数据完全独立的数据集，用于评估模型的最终性能。测试误差是 最重要的误差指标，因为它告诉我们模型在新的、未见过的数据上的表现如何。\n欠拟合 (Underfitting) # 定义：当模型过于简单而无法捕捉数据中的底层模式时，就会发生欠拟合。该模型具有高偏差和低方差，这意味着它在训练和测试数据上的表现都很差。 表现： 模型复杂度低，无法很好地拟合训练数据。 训练误差与测试误差都较大，模型性能较差。 原因： 模型过于简单，无法学习到数据中的复杂关系或特征。 特征不足，数据无法充分表达问题。 训练时间不足，模型未完全收敛。 解决方法： 增加模型复杂度：选择更复杂的模型（如从线性模型切换到非线性模型）。 增加特征：引入更多特征或通过特征工程提取更有效的特征。 延长训练时间：确保模型充分训练直到收敛。 过拟合 (Overfitting) # 定义：当模型过于复杂，与训练数据的拟合度过高时，就会发生过度拟合，从而捕获数据中的噪声和随机波动。因此，该模型在新的、未见过的数据上表现不佳。 表现： 模型对训练数据拟合良好，训练误差很低。 测试误差较高，模型泛化能力差。 原因： 模型复杂度过高，学习到了训练数据中的噪声或无意义模式。 训练数据过少，噪声占比高。 缺乏正则化约束，模型自由度太高。 解决方法： 减少模型复杂度：降低模型自由度（如减少神经网络层数或节点数）。 增加数据量：收集更多样本，减少模型对噪声的敏感性。 正则化： \\(L_1\\) 正则化：鼓励稀疏性，减少不重要的参数。 \\(L_2\\) 正则化：限制参数的幅度，防止过大权重。 交叉验证：通过交叉验证选择模型或超参数，避免过拟合。 偏差-方差权衡（Bias-Variance Tradeoﬀ） # 偏差 (Bias) # 定义：偏差衡量模型预测值的期望值与真实值之间的偏离程度。 公式： \\[ \\text{Bias} = E[f(x)] - f^*(x) \\] \\(f(x)\\) ：模型的预测值 \\(f^*(x)\\) ：真实值或目标函数 方差 (Variance) # 定义：方差衡量模型在不同训练数据集上的预测值的变化幅度。 公式： \\[ \\text{Variance} = E[(f(x) - E[f(x)])^2] \\] 方差的平方根称为标准差，表示为 \\(SE(x)\\) 总误差分解 # 模型的总误差（Expected Error）可以分解为三部分：偏差、方差和噪声。在实际应用中，我们需要平衡 Bias 和 Variance 以此来找到最小的 Expected Error，目标是找到偏差和方差的最佳平衡点，既能保证低训练误差，又能有良好的泛化能力。\n\\[ \\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise} \\] \\(\\text{Bias}^2\\) : 表示系统误差，与模型的表达能力有关。 \\(\\text{Variance}\\) : 表示模型对训练数据的敏感程度。 \\(\\text{Irreducible Noise}\\) : 数据中固有的随机噪声，无法通过任何模型降低。 Bias-Variance Tradeoff 理解 # 偏差-方差权衡指的是模型很好地拟合训练数据的能力（低偏差） 与其推广到新数据的能力（低方差） 之间的权衡。\n随着模型复杂度的增加，偏差趋于减小，方差趋于增大。如果模型太简单，它可能具有高偏差，这意味着它无法捕捉数据中的潜在模式，训练误差和测试误差会很高。如果模型太复杂，它可能具有高方差，这意味着它对训练数据中的噪声过于敏感，并且可能无法很好地推广到新数据，从而导致过度拟合。为了在偏差和方差之间取得平衡，我们需要找到模型的最佳复杂度。\n低Bias但高Variance：复杂模型，过度拟合，表现为训练误差低但测试误差高。 高Bias但低Variance：简单模型，欠拟合，表现为训练误差和测试误差都高。 选择模型评估方法 # 交叉验证 (Cross-Validation)：通过分割数据集来更好地估计模型的偏差和方差。 学习曲线 (Learning Curve)：观察训练集误差和验证集误差随样本数量或模型复杂度变化的趋势，帮助分析模型的偏差和方差问题。 交叉验证（Cross Validation） # 交叉验证是机器学习中用于评估模型在独立数据集上性能的一种技术。交叉验证的基本思想是将可用数据分成两个或多个部分，其中一个部分用于训练模型，另一个部分用于验证模型。交叉验证用于通过提供模型对新数据的泛化程度的估计来防止过度拟合。它有效解决了仅用单一验证集或测试集可能导致的评估结果不稳定或偏差的问题。它也可以用于调整模型的超参数。\nNote：在交叉验证（Cross-Validation）中，每一次计算的是 验证误差（validation error），而不是训练误差（training error）。交叉验证的目的是估计模型在未见数据上的性能，因此重点在于验证集的误差。\nK折交叉验证 (K-Fold Cross-Validation) # 将数据集划分为 K 个不重叠的子集（folds）。每次取一个子集作为验证集，其余 K-1 个子集作为训练集。重复 K 次，每次更换验证集，最终对所有验证结果（Validation Error）取平均。 在 Cross-Validation 的框架内，调节模型的 hyperparameters，对使用不同参数的模型进行 Cross-Validation 验证。根据最终结果选择最佳模型。 在选择好最佳的 hyperparameters 和模型后，用全部的数据进行训练。 使用完全独立的 Testset 来评估模型的泛化能力。 优缺点： # 优点：可靠性高，适合数据量较大的情况。 缺点：当 K 较大时，计算成本较高。 K-Fold Cross-Validation 代码实现： # # \u0026lt;--- From scratch ---\u0026gt; import numpy as np from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error # 简单的数据集生成 X = np.array([[i] for i in range(100)]) y = np.array([i*2 for i in range(100)]) model = LinearRegression() def K_fold_CrossValidation(X, y, model, k=5): # 数据集划分，按顺序划分为K个子集 fold = len(X) // k error = [] for i in range(k): val_start_idx = i * fold val_end_idx = (i + 1) * fold # 创建训练集和验证集 X_train = np.concatenate(X[:val_start_idx], X[val_end_idx:]), y_train =np.concatenate(y[:val_start_idx], y[val_end_idx:]) X_val = X[val_start_idx:val_end_idx] y_val = y[val_start_idx:val_end_idx] # 使用模型进行训练和预测 model.fit(X_train, y_train) y_pred = model.predict(X_val) # 计算误差 curr_error = mean_squared_error(y_val, y_pred) error.append(curr_error) return np.mean(error) # 执行交叉验证 avg_error = K_fold_CrossValidation(X, y, model, k=5) print(f\u0026#34;Average cross-validation error: {avg_error}\u0026#34;) # \u0026lt;--- From scikit-learn ---\u0026gt; from sklearn.model_selection import cross_val_score from sklearn.linear_model import LinearRegression from sklearn.datasets import make_regression import numpy as np # 生成数据集 X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42) # 初始化模型 model = LinearRegression() # 执行5折交叉验证，评分标准为负均方误差（neg_mean_squared_error） scores = cross_val_score( estimator=model, # estimator 类型对象，必须实现 fit 和 predict 方法。 X=X, # 特征数据 y=y, # 标签数据 cv=5, # 指定使用5折交叉验证 scoring=\u0026#39;neg_mean_squared_error\u0026#39;, # 表示使用的评分方法（如 accuracy, neg_mean_squared_error, f1, roc_auc 等） return_train_score=False # 不返回训练集得分，只返回验证集得分 ) # 输出每次折的误差 print(f\u0026#34;Cross-validation errors for each fold: {-scores}\u0026#34;) # 输出平均误差 print(f\u0026#34;Average cross-validation error: {-scores.mean()}\u0026#34;) 留一法 (Leave-One-Out Cross-Validation, LOOCV) # 数据集中每个样本单独作为一次验证集，其余样本作为训练集。 模型训练次数等于样本数 N，最后计算所有验证集的误差平均值。 优缺点： # 优点：不浪费数据，最全面的评估方法。 缺点：计算代价极高，尤其是数据集较大时。 常见的机器学习完整流程 # 数据的准备和预处理 从数据库、API、文件等来源收集数据。 对所有的数据进行数据清洗 (e.g 处理缺失值，处理异常值，数据格式转换) 数据分割（训练集80%、验证集10%、测试集10%） 数据预处理 - 仅针对训练集（标准化/归一化，特征工程/选择/变换，Label Encoding，One-Hot Encoding） 模型训练与评估 选择模型：根据任务性质选择初始模型 设置交叉验证策略，交叉验证中的模型训练 - 使用训练集。 评估指标：不仅观察平均值（Validation Error），还需关注标准差，评估模型的稳定性。 超参数调优 网格搜索 (Grid Search)：枚举所有可能的超参数组合，使用交叉验证评估每一组参数的表现。选择评估结果最优的参数组合。 随机搜索 (Random Search)：从参数空间中随机采样一定数量的超参数组合进行评估。 测试集上的最终评估 固定最佳模型：在交叉验证确定的最佳超参数和模型结构上，重新训练模型，使用全体训练数据。 在测试集上评估：用测试集数据评估最终模型的性能，作为模型实际泛化能力的最终指标。Test Set 仅在最终测试时使用一次以防止数据泄漏（Data Leakage） "},{"id":3,"href":"/docs/deep-learning/perceptrons-and-neural-network/","title":"Perceptrons and Neural Network","section":"Deep Learning","content":" 感知机和神经网络（Perceptrons and Neural Network） # 感知机（Perceptron）是神经网络（Neural Network）的雏形，基于单个或多个 神经元（Neuron） 通过线性组合和简单的激活函数（如阶跃函数）实现输入数据的分类。神经元是感知机的基本组成单元，其功能是对输入特征进行加权、偏置调整并生成输出。随着任务复杂性的增加，感知机被扩展为 多层感知机（Multilayer Perceptron, MLP），通过堆叠多个隐藏层并引入非线性激活函数，使网络能够表达复杂的映射关系，解决非线性问题，是前馈神经网络（Feedforward Neural Network）的核心架构。\n神经元（Neuron） # 神经元（Neuron）是神经网络的最基本单元，模拟生物神经元的行为。它的主要功能是 接收输入、加权处理并通过激活函数生成输出。\n\\[ y = f\\left(\\sum_{i=1}^n w_i x_i + b\\right) \\] 其中：\n\\(x_i\\) : 输入特征。 \\(w_i\\) : 权重，衡量每个输入的影响程度。 \\(b\\) : 偏置，用于调整输出的灵活性。 \\(f(\\cdot)\\) : 激活函数，增加非线性能力（如ReLU、Sigmoid）。 \\(y\\) : 输出信号。 感知机（Perceptrons） # 感知器（Perceptrons）是一种执行 二元分类（binary classification） 的神经网络，它将输入特征映射到输出决策，通常将数据分为两个类别之一，例如 0 或 1。感知器由一层输入节点（input nodes）组成，这些节点与一层输出节点（output nodes）完全连接（fully connected）。感知子可以用图像表示为：\n单个感知机可以看作是一个最简单的神经元，专注于实现线性分类任务。单个神经元是感知机的扩展形式，通过引入更灵活的激活函数和多层结构，可以应用于更复杂的问题。\n模型结构 # 感知机的基本结构包括以下组成部分：\n输入层（Input Layer）：接收输入特征向量 \\(x = [x_1, x_2, \\dots, x_n]\\) 。 权重向量（Weights）：每个输入特征 \\(x_i\\) 对应的权重 \\(w_i\\) 。 偏置（Bias, b）：平移决策边界，增强模型的灵活性。 线性组合： \\[ z = W^T X + b = \\sum_{i=1}^n w_i x_i + b \\] 激活函数（Activation Function）：通常为符号函数 \\(\\text{sign}(z)\\) ，用于将加权和映射为输出标签。 输出结果： \\[ y = \\text{sign}(z) = \\begin{cases} +1, \u0026 \\text{if } z \\geq 0 \\\\ -1, \u0026 \\text{if } z \u003c 0 \\end{cases} \\] 损失函数 # 感知机的损失函数本质上是用来惩罚误分类样本，从而引导模型学习到能够正确分类所有样本的权重参数。他的主要目标是找到一个超平面（hyperplane）： \\(w^T x + b = 0\\) 。使得数据点能够被正确分类，即：\n如果 \\(y_i = +1\\) ，那么希望 \\(w^T x_i + b \u003e 0\\) ； 如果 \\(y_i = -1\\) ，那么希望 \\(w^T x_i + b \u003c 0\\) 。 感知机的损失函数只关注那些被误分类的样本。对于误分类的样本，有： \\[ y_i (w^T x_i + b) \\leq 0 \\] 损失函数定义为所有误分类样本的负边界距离的总和： \\[ L(w, b) = -\\sum_{i \\in M} y_i (w^T x_i + b) \\] \\(M\\) 表示所有被误分类的样本的集合； \\(y_i (w^T x_i + b)\\) 表示样本 \\(x_i\\) 到决策超平面的有符号距离。 更新规则 # 严格来说，感知机本身并不使用梯度下降法进行优化，因为感知机的损失函数是分段的、非连续的，无法直接对其求导。感知机的权重更新公式是： \\[ \\begin{align*} \u0026w \\leftarrow w + \\eta \\cdot y_i \\cdot x_i \\\\ \u0026b \\leftarrow b + \\eta \\cdot y_i \\\\ \\end{align*} \\] 感知机的权重更新可以看作是一种离散化、非平滑的近似梯度下降过程:\n每次只对一个误分类样本 \\(x_i\\) 更新权重和偏置； 更新方向为该样本的贡献（ \\(-y_i x_i\\) 的负梯度方向）； 从几何角度看：如果一个样本被误分类，更新方向是沿着样本 \\(x_i\\) 的方向，并且朝着正确分类 \\(y_i\\) 的方向推进决策边界。\n感知机（Perceptrons）和逻辑回归（Logistic Regression）的区别 # 感知机（Perceptrons）和逻辑回归（Logistic Regression）在表面上确实有很多相似之处，因为它们都属于线性模型，但它们的核心区别在于损失函数和输出目标。\n激活函数区别\nPerceptrons 使用符号函数（sign function）作为激活函数。这意味着感知机的输出是基于决策边界的二元结果，不提供概率信息。 \\[ y = \\text{sign}(w^T x + b) \\] Logistic Regression 使用逻辑函数（sigmoid function）将线性组合结果映射到概率范围： \\[ P(y=1|x) = \\sigma(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}} \\] 损失函数区别\nPerceptrons 的学习目标是最小化误分类样本的数量，仅对被误分类的样本进行权重更新。 \\[ L(w, b) = -\\sum_{i \\in M} y_i (w^T x_i + b) \\] Logistic Regression 通过最大化条件概率 \\(P(y|x)\\) 的对数似然来优化参数： \\[ L(w, b) = -\\sum_{i=1}^N \\left[y_i \\log(\\sigma(w^T x_i + b)) + (1 - y_i) \\log(1 - \\sigma(w^T x_i + b)) \\right] \\] 决策边界区别\n感知机和逻辑回归都假设数据是线性可分的，因此其决策边界都是一个超平面：\nPerceptrons 直接依赖超平面将数据划分为两个类别，但没有提供关于样本距离边界的任何信息。 Logistic Regression 利用概率信息描述样本在边界两侧的信心度，决策边界定义为 \\(P(y=1|x) = 0.5\\) 。 Note： Perceptrons 通常仅适用于线性可分的数据。算法在数据线性可分时会收敛；但如果数据线性不可分，则会陷入无限循环。Logistic Regression 可处理线性不可分数据，即使数据线性不可分，也能找到最优的权重（通过拟合概率分布）。\n感知机代码实现 # # \u0026lt;--- From scratch ---\u0026gt; import numpy as np def Perceptrons(X, y, lr, max_iter): n_samples, n_features = X.shape weights = np.zeros(n_features) bias = 0 for _ in range(max_iter): for i in range(n_samples): # 计算线性输出 linear_output = np.dot(X[i], weights) + bias # 如果预测错误，则更新权重和偏置 if y[i] * linear_output \u0026lt;= 0: weights += lr * y[i] * X[i] bias += lr * y[i] return weights, bias # 模型预测 def predict(X, weights, bias): linear_output = np.dot(X, weights) + bias return np.where(linear_output \u0026gt;= 0, 1, -1) # 数据示例 X = np.array([[1, 1], [2, 1], [1, 2], [-1, -1], [-2, -1], [-1, -2]]) y = np.array([1, 1, 1, -1, -1, -1]) # 模型训练 weights, bias = Perceptrons(X, y, lr=0.1, max_iter=10) # 输出结果 print(\u0026#34;学习到的权重:\u0026#34;, weights) print(\u0026#34;学习到的偏置:\u0026#34;, bias) # 进行预测 predictions = predict(X, weights, bias) print(\u0026#34;预测结果:\u0026#34;, predictions) 多层感知机（Multilayer Perceptron，MLP） # 多层感知机（MLP）是最常见的前馈神经网络（Feedforward Networks）之一，由多个全连接层组成。它是神经网络的基础结构，广泛用于分类、回归等任务。MLP的核心思想是通过隐藏层和非线性激活函数，提取输入数据的特征并映射到目标输出。\n输入层（Input Layer） # 输入层是多层感知机（MLP）的第一部分，用于接收外部输入数据并将其传递给网络的隐藏层。输入层的设计直接决定了模型对数据的适配能力。输入层可以视为数据和网络之间的接口：\n数据接受：接收外部特征输入，通常以向量或矩阵的形式表示。 维度映射：将原始数据的特征维度（ \\(d\\) ）映射到神经网络的内部表示维度。输入数据的特征维度需与输入层的线性变换参数兼容。 数据传递：输入层通过线性变换（如 \\(xW + b\\) ）将输入映射到第一个隐藏层的维度。它仅负责将输入数据直接传递到隐藏层。 神经元（Neuron）数量：输入层的神经元数量等于第一个隐藏层的神经元数量，即输入层通过线性变换将输入特征映射到第一个隐藏层的维度。输入数据的特征数量（ \\(d\\) ）决定了输入层每个神经元的权重数量：\n对于输入维度为 \\(d\\) 的数据，每个神经元会有 \\(d\\) 个权重加上一个偏置项。 示例: 如果输入数据具有 4 个特征（如 \\([x_1, x_2, x_3, x_4]\\) ），且第一个隐藏层包含 10 个神经元，则输入层需要： 10 个神经元（每个神经元与隐藏层的每个神经元一一对应）。 每个神经元包含 4 个权重（分别对应 4 个输入特征）和 1 个偏置项。 输入数据格式：输入数据通常为一个向量或矩阵：\n单样本输入: 向量形式，如 \\([x_1, x_2, …, x_d]\\) 。 批量输入: 矩阵形式，形状为 \\((\\text{batch size}, d)\\) ，其中 \\( \\text{batch size} \\) 为每次输入的样本数， \\(d\\) 为特征数。 代码示例: class SimpleInputLayer(nn.Module): def __init__(self, input_dim, hidden_dim): super(SimpleInputLayer, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) # 从输入层到第一个隐藏层的线性变换 self.relu = nn.ReLU() # 激活函数（ReLU） def forward(self, x): x = self.fc1(x) # 输入数据经过线性变换 x = self.relu(x) # 使用 ReLU 激活函数 return x 隐藏层（Hidden Layer） # 隐藏层是神经网络中介于输入层和输出层之间的层，它是神经网络学习和表示复杂映射关系的关键部分。隐藏层通过层叠的神经元以及非线性激活函数，能够从数据中提取特征和学习模式，是深度学习的核心。隐藏层的主要功能有：\n特征提取：隐藏层负责从输入数据中提取有用的特征，形成更高维、更抽象的表示。 非线性映射：通过非线性激活函数（如 ReLU、Sigmoid、Tanh），隐藏层能够学习复杂的非线性关系，而非仅仅是简单的线性变换。 数据处理：每个隐藏层接收上一个层的输出，经过线性变换和激活函数处理后，传递到下一层。 每个隐藏层由以下三部分组成：\n神经元： 每个神经元代表一个计算单元，接收来自上一层所有神经元的输入，加权求和后进行激活函数变换。 隐藏层的神经元数量是一个超参数，需要根据具体问题进行选择。 权重（Weights）和偏置（Biases）： 权重：连接两层之间的神经元，并决定输入的重要性。 偏置：用于调整激活函数的输出，增加模型的表达能力。 公式： \\(z = xW + b\\) ，其中 \\(x\\) 是输入， \\(W\\) 是权重矩阵， \\(b\\) 是偏置向量。 激活函数： 激活函数引入非线性，使神经网络能够学习复杂的非线性映射。\n常见激活函数：\nReLU (Rectified Linear Unit)\n\\[ f(x) = \\max(0, x) \\] 计算简单：仅比较输入值是否大于 0，操作速度快。 非线性：尽管形式简单，但 ReLU 是非线性的，可帮助网络学习复杂特征。 稀疏性：当输入小于 0 时，输出为 0，相当于让部分神经元不激活，提升模型稀疏性。 问题：可能导致“神经元死亡”（当许多权重使输入始终小于 0，导致该神经元永不更新）。 ReLU 广泛应用于深层神经网络，一般情况下是默认选择，适合大多数场景。 Sigmoid\n\\[ f(x) = \\frac{1}{1 + e^{-x}} \\] 输出范围： \\([0, 1]\\) ，适合表示概率值。 单调性：对于输入增大，输出逐渐趋近于 1，但变化减缓。 梯度消失问题：当 \\(x\\) 的绝对值较大时，函数的梯度趋近于 0，导致反向传播时权重更新困难。 Tanh (双曲正切函数)\n\\[ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\] 输出范围： \\([-1, 1]\\) ，比 Sigmoid 更对称，适合隐藏层激活函数。 对称性：中心对称于原点，有助于加速收敛。 梯度消失问题：与 Sigmoid 类似，当 \\(x\\) 的绝对值较大时，梯度会趋近于 0。 关于非线性\n线性关系：一个函数是线性的，如果它满足叠加性和齐次性，即： \\(f(ax + by) = a \\cdot f(x) + b \\cdot f(y)\\) 例如： \\(y = 2x + 3\\) 是线性函数。它的图形是一直线。 非线性关系：非线性函数不满足上述性质。例如： \\(y = x^2\\) 或 \\(y = \\sin(x)\\) 是非线性的。它的图形可能是弯曲的、不规则的。 Note： 如果神经网络的每一层都只包含线性变换，那么即使增加多层隐藏层，整个网络仍然是一个线性函数的组合。非线性让网络能够学习复杂的映射关系，如分类非线性可分的数据、逼近复杂函数等。\n激活函数的形状的确会影响它的性能，但并非关键因素。重要的是它能够引入 非线性特性，打破线性关系的局限性。\n激活函数通常是 element-wise（逐元素）操作，逐元素操作不改变张量的形状，只改变其值，适合深度学习网络中逐层特征变换的需要。他的作用是提取特征和表示，而不是直接产生概率分布。\nNote： 神经网络的目标 不是直接寻找一个显式的数学公式（如通用曲线），而是构建一种复杂的非线性映射，将输入特征与输出目标连接起来。这种映射是通过网络参数的学习过程（如权重和偏置的调整）隐式表达的。虽然网络可以实现“圆形”或“复杂形状”的决策边界，但这不是通过显式地拟合一个圆的方程，而是通过层层线性变换与非线性激活的组合自动得出的结果。\n隐藏层理解\n隐藏层的作用可以类比为数据的逐步“翻译”或“加工”：\n低层特征提取：第一层隐藏层处理输入数据的基本特征（如简单的边缘、颜色或频率等）。 中层特征组合：中间层将低层特征组合成更高阶的特征（如形状、局部模式或局部结构）。 高层特征整合：更深层次的隐藏层进一步整合复杂特征，用于最终分类或预测任务。 每一层将数据映射到新的特征空间中，这种映射使得神经网络能够捕获从简单到复杂的模式。隐藏层维度的设计通常遵循“逐步扩展—再收缩”的模式：\n增加维度：扩展特征空间 增加维度可以让模型在更高维的特征空间中提取更加复杂和细粒度的模式。 例如，输入层可能包含较少的原始特征（如像素、频谱或特定数值），但这些特征经过线性变换和激活后，隐藏层可以生成更多维度的“隐含特征”。 扩展特征空间类似于“打开数据的潜力”，为网络提供更丰富的信息处理能力。 减少维度：聚合有用信息 在后续隐藏层中减少维度是为了压缩特征表示，去除冗余信息，仅保留与任务相关的高质量特征。 这一过程可以防止模型过拟合，同时提高计算效率和泛化能力。 减少维度还可以实现对数据的进一步“压缩”，形成对输入数据的简洁而有力的表示。 Note： 为什么逐步增加维度（e.g. dim4 -\u0026gt; dim128 -\u0026gt; dim256 -\u0026gt; dim512）而不是直接扩展到高维（e.g. dim4 -\u0026gt; dim512）？\n直接扩展到高维，模型可能无法有效捕获特征层次，会使学习过程更加困难。需要的权重参数过多，特别是在数据量不足时效果较差。同时由于缺少逐步提取特征的过程，模型可能过于依赖输入数据的特定模式。\n代码示例： import torch import torch.nn as nn class SimpleHiddenLayer(nn.Module): def __init__(self, input_dim, hidden_dims): super(SimpleHiddenLayer, self).__init__() layers = [] for i in range(len(hidden_dims)): in_dim = input_dim if i == 0 else hidden_dims[i - 1] out_dim = hidden_dims[i] layers.append(nn.Linear(in_dim, out_dim)) # 线性变换 layers.append(nn.ReLU()) # ReLU 激活函数 self.hidden_layers = nn.Sequential(*layers) # 将所有隐藏层组合为一个模块 def forward(self, x): return self.hidden_layers(x) # 输入依次通过所有隐藏层 输出层（Output Layer） # 输出层是神经网络的最后一部分，其核心职责是根据模型的目标任务（Cost function），生成适合应用场景的输出。不同任务对输出层的设计要求不同，例如分类、回归或生成任务等。输出层的实现通常结合特定的单元（如线性、Sigmoid、Softmax）和损失函数，以适应不同类型的数据分布和学习目标。\n线性单元（Linear Units） 适用场景: 连续型输出（如回归任务）。 数学表达式: 输出值 \\(y = xW + b\\) （无激活函数）。 解释: 线性单元生成实值输出，不引入非线性变换。 损失函数通常为均方误差（MSE）: \\(L = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\) ，其中 \\(\\hat{y}_i\\) 是预测值。 代码示例: class LinearOutputLayer(nn.Module): def __init__(self, input_dim): super(LinearOutputLayer, self).__init__() self.fc = nn.Linear(input_dim, 1) # 线性变换（输出一个数值） def forward(self, x): x = self.fc(x) # 线性变换 return x Sigmoid 单元 适用场景: 二分类任务。 数学表达式: \\(y = \\sigma(xW + b) = \\frac{1}{1 + e^{-(xW + b)}}\\) 。 解释: Sigmoid 单元将输出值压缩到区间 \\((0, 1)\\) ，解释为正样本的概率 \\(P(y=1|x)\\) 。 损失函数通常为二元交叉熵损失（Binary Cross-Entropy Loss）: \\(L = - \\frac{1}{n} \\sum_{i=1}^n [y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]\\) ，其中 \\(\\hat{y}_i\\) 是预测的概率值。 代码示例: class BinaryOutputLayer(nn.Module): def __init__(self, input_dim): super(BinaryOutputLayer, self).__init__() self.fc = nn.Linear(input_dim, 1) # 线性层，将输入映射到 1 个输出（概率） self.sigmoid = nn.Sigmoid() # Sigmoid 激活函数 def forward(self, x): x = self.fc(x) # 线性变换 x = self.sigmoid(x) # Sigmoid 激活函数 return x Softmax 单元 适用场景: 多分类任务。 数学表达式: \\(y_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}\\) ，其中 \\(z_j\\) 是第 \\(j\\) 类的得分。 解释: Softmax 单元将多个输出值转换为概率分布，保证 \\(\\sum_{j=1}^K y_j = 1\\) ，每个 \\(y_j\\) 表示属于第 \\(j\\) 类的概率。 损失函数通常为多分类交叉熵损失（Categorical Cross-Entropy Loss）: \\(L = - \\sum_{i=1}^n \\sum_{j=1}^K y_{ij} \\log(\\hat{y}_{ij})\\) ，其中 \\(y_{ij}\\) 是真实标签， \\(\\hat{y}_{ij}\\) 是预测概率。 代码示例: class MultiClassOutputLayer(nn.Module): def __init__(self, input_dim, num_classes): super(MultiClassOutputLayer, self).__init__() self.fc = nn.Linear(input_dim, num_classes) # 输出类别数个神经元 self.softmax = nn.Softmax(dim=1) # Softmax 激活函数 def forward(self, x): x = self.fc(x) # 线性变换 x = self.softmax(x) # Softmax 激活函数 return x 神经网络运行流程和原理 # 向前传播 (Forward Propagation) # 向前传播 (Forward Propagation) 通过网络层层传递数据，逐步对输入进行线性变换和非线性映射，提取特征并输出预测结果。\n输入层到隐藏层： 假设输入层的输入为 \\( x \\) ，第一个隐藏层的权重矩阵为 \\( W^{(1)} \\) ，偏置为 \\( b^{(1)} \\) 。计算该层的线性变换：\n线性变换： \\( z^{(1)} = W^{(1)} x + b^{(1)} \\) 其中， \\( z^{(1)} \\) 是第一个隐藏层的线性变换输出。 激活函数： 经过激活函数 \\( \\varphi(\\cdot) \\) 后得到隐藏层的输出： \\( h^{(1)} = \\varphi(z^{(1)}) = \\varphi(W^{(1)} x + b^{(1)}) \\) 其中， \\( \\varphi(\\cdot) \\) 可以是常见的激活函数，如 ReLU 或 Sigmoid，具体的形式取决于任务需求。 隐藏层到输出层： 假设输出层的权重矩阵为 \\( W^{(2)} \\) ，偏置为 \\( b^{(2)} \\) 。该层的输出经过线性变换并得到最终的输出。\n线性变换： \\( z^{(2)} = W^{(2)} h^{(1)} + b^{(2)} \\) 其中， \\( h^{(1)} \\) 是上一层的输出， \\( z^{(2)} \\) 是当前层的线性变换输出。 激活函数： 直接得到预测值（例如在回归问题中，输出层可能不使用激活函数）。 如果是分类问题： \\( \\hat{y} = \\varphi(z^{(2)}) = \\varphi(W^{(2)} h^{(1)} + b^{(2)}) \\) 如果是回归问题（没有激活函数）： \\( \\hat{y} = W^{(2)} h^{(1)} + b^{(2)} \\) 其中， \\( \\hat{y} \\) 是最终输出。 损失函数的计算： 最终，我们计算输出与真实标签之间的损失，通常使用损失函数 \\( L = l(\\hat{y} , y) \\) ，其中 \\( \\hat{y} \\) 是网络的预测输出， \\( y \\) 是实际标签。\n完整流程： \\[ z^{(1)} = W^{(1)} x + b^{(1)} → h^{(1)} = \\varphi(z^{(1)}) → z^{(2)} = W^{(2)}h^{(1)} + b^{(2)}→ \\hat{y} = \\varphi(z^{(2)}) → L = l(\\hat{y}, y) \\] 向后传播 (Backward Propagation) # 向后传播是计算梯度并调整模型参数的过程，用于优化神经网络。通过链式法则，逐层计算损失函数对每个参数的偏导数。我们在向后传播中关注的重点在于计算权重（weight）和偏置（bias）的梯度。\n链式法则的数学公式: 设有复合函数： \\( y = f(g(h(x))) \\) 展开形式： \\( \\frac{dy}{dx} = \\frac{dy}{dg} \\cdot \\frac{dg}{dh} \\cdot \\frac{dh}{dx} \\) 损失函数对输出的梯度： 首先，从损失函数出发，计算损失函数对输出层的梯度。\n损失函数： \\( L = l(\\hat{y}, y) \\) 其中， \\( \\hat{y} \\) 是模型的输出， \\( y \\) 是真实标签。 损失函数关于输出层的梯度是： \\[ \\frac{\\partial L}{\\partial \\hat{y}} = \\frac{\\partial l(\\hat{y}, y)}{\\partial \\hat{y}} \\] 输出层到隐藏层的梯度： 接下来，我们需要计算损失函数对第二层权重矩阵 \\( W^{(2)} \\) 和偏置项 \\( b^{(2)} \\) 的梯度 (i.e. \\( \\frac{\\partial L}{\\partial W^{(2)}}\\) , \\( \\frac{\\partial L}{\\partial b^{(2)}}\\) )。\n我们有 \\(\\hat{y} = \\varphi(z^{(2)})\\) ，由此可得输出层的梯度对第二层的加权输入 \\( z^{(2)} \\) 的偏导数： \\[ \\frac{\\partial L}{\\partial z^{(2)}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z^{(2)}} \\] 我们有 \\(z^{(2)} = W^{(2)}h^{(1)} + b^{(2)}\\) ，然后计算损失函数对第二层权重矩阵 \\( W^{(2)} \\) 和偏置项 \\( b^{(2)}\\) 的梯度： \\[ \\frac{\\partial L}{\\partial W^{(2)}} = \\frac{\\partial L}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(2)}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(2)}} \\] 以及 \\( \\frac{\\partial L}{\\partial b^{(2)}} = \\frac{\\partial L}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial b^{(2)}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial b^{(2)}} \\) 隐藏层到输入层的梯度： 接着，我们需要计算损失函数对第一层的权重矩阵 \\( W^{(1)} \\) 和偏置项 \\( b^{(1)} \\) 的梯度。\n从 \\(z^{(2)} = W^{(2)}h^{(1)} + b^{(2)}\\) 和 \\(h^{(1)} = \\varphi(z^{(1)})\\) 我们可以得到： \\[ \\frac{\\partial L}{\\partial z^{(1)}} = \\frac{\\partial L}{\\partial h^{(1)}} \\cdot \\frac{\\partial h^{(1)}}{\\partial z^{(1)}} = \\frac{\\partial L}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial h^{(1)}} \\cdot \\frac{\\partial h^{(1)}}{\\partial z^{(1)}} \\] 然后从 \\(z^{(1)} = W^{(1)} x + b^{(1)}\\) ，计算损失函数对第一层权重矩阵 \\( W^{(1)} \\) 和偏置项 \\( b^{(1)}\\) 的梯度： \\[ \\frac{\\partial L}{\\partial W^{(1)}} = \\frac{\\partial L}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial W^{(1)}}=\\frac{\\partial L}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial h^{(1)}} \\cdot \\frac{\\partial h^{(1)}}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial W^{(1)}} \\] 以及 \\( \\frac{\\partial L}{\\partial b^{(1)}} = \\frac{\\partial L}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial b^{(1)}}=\\frac{\\partial L}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial h^{(1)}} \\cdot \\frac{\\partial h^{(1)}}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial b^{(1)}} \\) 参数更新： 在向后传播过程中，我们计算出了所有需要的梯度，接下来进行参数更新。在每次迭代中，通过梯度下降更新每个权重和偏置项：\n更新权重： \\( W^{(1)} = W^{(1)} - \\eta \\frac{\\partial L}{\\partial W^{(1)}} \\) ， \\( W^{(2)} = W^{(2)} - \\eta \\frac{\\partial L}{\\partial W^{(2)}} \\) 更新偏置： \\( b^{(1)} = b^{(1)} - \\eta \\frac{\\partial L}{\\partial b^{(1)}} \\) ， \\( b^{(2)} = b^{(2)} - \\eta \\frac{\\partial L}{\\partial b^{(2)}} \\) Note：激活值本身并不会直接被用于反向传播，而是激活值的导数和前一层的梯度共同作用于权重的更新。在反向传播中，计算每一层的梯度时需要用到激活函数的导数，而不是激活值本身。\n简单神经网络代码实现 # # \u0026lt;--- From scratch ---\u0026gt; import numpy as np def ReLu(x): return np.maximum(0, x) def ReLU_derivative(z): return (z \u0026gt; 0).astype(float) def Softmax(x): exp_x = np.exp(x - np.max(x, axis=1, keepdims=True)) # 防止溢出 return exp_x / np.sum(exp_x, axis=1, keepdims=True) # 损失函数和其导数 def Cross_entropy_loss(y_hat, y): m = y.shape[0] return -np.sum(y * np.log(y_hat + 1e-15)) / m def Cross_entropy_loss_derivative(y_hat, y): return y_hat - y def MLP(X, y, max_iter, learning_rate): input_dim = 3 hidden_dim = 5 output_dim = 1 W1 = np.random.randn(hidden_dim, input_dim) * 0.01 b1 = np.zeros((hidden_dim, 1)) W2 = np.random.randn(output_dim, hidden_dim) * 0.01 b2 = np.zeros((output_dim, 1)) def Forward_Propagation(X, W1, b1, W2, b2): z1 = np.dot(W1, X.T) + b1 h1 = ReLu(z1) z2 = np.dot(W2, h1.T) + b2 y_hat = Softmax(z2.T) return z1, h1, z2, y_hat def Backward_Propagation(X, y, z1, h1, z2, y_hat, W1, W2): m = X.shape[0] dz2 = cross_entropy_loss_derivative(y_hat, y) dW2 = np.dot(dz2.T, h1.T) / m db2 = np.sum(dz2.T, axis=1, keepdims=True) / m dh1 = np.dot(W2.T, dz2.T) dz1 = dh1 * relu_derivative(z1) dW1 = np.dot(dz1, X) / m db1 = np.sum(dz1, axis=1, keepdims=True) / m return dW1, db1, dW2, db2 for i in range(max_iter): z1, h1, z2, y_hat = Forward_Propagation(X, W1, b1, W2, b2) loss = Cross_entropy_loss(y_hat, y) dW1, db1, dW2, db2 = Backward_Propagation(X, y, z1, h1, z2, y_hat, W1, W2) W1 -= learning_rate * dW1 b1 -= learning_rate * db1 W2 -= learning_rate * dW2 b2 -= learning_rate * db2 return W1, b1, W2, b2 # \u0026lt;--- From pytorch ---\u0026gt; import torch import torch.nn as nn import torch.optim as optim class MLP(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super().__init__() self.layer1 = nn.Linear(input_dim, hidden_dim) self.layer2 = nn.Linear(hidden_dim, output_dim) self.relu = nn.ReLU() def forward(self, x): x = self.relu(self.layer1(x)) x = self.layer2(x) return x torch.manual_seed(42) X = torch.randn(100, 4) # 输入特征 y = torch.randint(0, 3, (100,)) # 3 类标签 # 模型初始化 input_dim = X.shape[1] hidden_dim = 10 output_dim = 3 max_iter = 1000 model = MLP(input_dim, hidden_dim, output_dim) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.01) for epoch in range(max_iter): y_hat = model(X) loss = criterion(y_hat, y) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 100 == 0: print(f\u0026#34;Epoch {epoch}, Loss: {loss.item():.4f}\u0026#34;) "},{"id":4,"href":"/posts/03_logistic_regression/","title":"Logistic Regression","section":"Blog","content":"Binary Logistic Regression is one of the most simple and commonly used Machine Learning algorithms for two-class classification. It is easy to implement and can be used as the baseline for any binary classification problem. Its basic fundamental concepts are also constructive in deep learning. Logistic regression describes and estimates the relationship between one dependent binary variable and independent variables.\n\u0026ldquo;分类\u0026quot;是应用 逻辑回归(Logistic Regression) 的目的和结果, 但中间过程依旧是\u0026quot;回归\u0026rdquo;. 通过逻辑回归模型, 我们得到的计算结果是0-1之间的连续数字, 可以把它称为\u0026quot;可能性\u0026quot;（概率）. 然后, 给这个可能性加一个阈值, 就成了分类. 例如, 可能性大于 0.5 即记为 1, 可能性小于 0.5 则记为 0.\nThe classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which \\(y\\) can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.)\n\\[ log\\frac{p}{1-p} = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{3} + \\cdot\\cdot\\cdot + \\beta_{n}x_{n} = \\beta^{T}x \\\\ \\] \\[ \\begin{align*} P(y = 1) \u0026= p = \\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x}} \\\\ P(y = 0) \u0026= 1 - p = \\frac{1}{1+e^{\\beta^{T}x}} \\end{align*} \\] We could approach the classification problem ignoring the fact that \\(y\\) is discrete-valued, and use our old linear regression algorithm to try to predict \\(y\\) given \\(x\\) . However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn’t make sense for \\(h_{\\theta}(x)\\) to take values larger than 1 or smaller than 0 when we know that \\(y \\in {0, 1}\\) . To fix this, let’s change the form for our hypotheses \\(h_{\\theta}(x)\\) to satisfy \\(0 \\leq h_{\\theta}(x) \\leq 1\\) This is accomplished by plugging \\(\\theta^{T}x\\) into the Logistic Function. Our new form uses the \u0026ldquo;Sigmoid Function,\u0026rdquo; also called the \u0026ldquo;Logistic Function\u0026rdquo;:\n\\[ f(x) = \\frac{1}{1+e^{-(x)}} \\\\ \\] Logistic Regression # First we need to define a Probability Mass Function:\n\\[ \\begin{align*} \u0026\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P(Y=1|X=x) = \\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x}} \\\\ \u0026\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P(Y=0|X=x) = 1 - \\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x}} = \\frac{1}{1+e^{\\beta^{T}x}} \\\\ \u0026\\Rightarrow \\ \\ \\ \\ P(Y \\ |X=x_{i}) = (\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}})^{y_{i}} (\\frac{1}{1+e^{\\beta^{T}x_{i}}})^{1-y_{i}} \\\\ \\end{align*} \\] Naturally, we want to maximize the right-hand-side of the above statement. We will use Maximun Likelihood Estimation(MLE) to find \\(\\beta\\) :\n\\[ \\hat{\\beta}_{MLE}= \\arg\\max_{\\beta} L(\\beta) \\\\ \\] \\[ L(\\beta) = \\prod_{i=1}^n P(Y=y_{i} |x_{i}) = \\prod_{i=1}^n (\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}})^{y_{i}} (\\frac{1}{1+e^{\\beta^{T}x_{i}}})^{1-y_{i}} \\\\ \\] \\[ \\begin{align*} l(\\beta) = log\\ L(\\beta) \u0026= \\sum_{i=1}^n y_{i}\\left[\\beta^{T}x_{i} - log(1+e^{\\beta^{T}x_{i}})] + (1-y_{i})[-log(1+e^{\\beta^{T}x_{i}})\\right] \\\\ \u0026=\\sum_{i=1}^n y_{i}\\beta^{T}x_{i}- log(1+e^{\\beta^{T}x_{i}}) \\\\ \\end{align*} \\] Newton‐Raphson Method for Binary Logistic Regression # Newton’s Method is an iterative equation solver: it is an algorithm to find the roots of a convex function. Equivalently, the method solves for root of the derivative of the convex function. The idea behind this method is ro use a quadratic approximate of the convex function and solve for its minimum at each step. For a convex function \\(f(x)\\) , the step taken in each iteration is \\(-(\\nabla^{2}f(x))^{-1}\\nabla f(x)\\) . while \\(\\lVert\\nabla f(\\beta)\\rVert \u003e \\varepsilon\\) :\n\\[ \\beta^{new} = \\beta^{old}-(\\nabla^{2}f(x))^{-1}\\nabla f(x) \\\\ \\] Where \\(\\nabla f(x)\\) is the Gradient of \\(f(x)\\) and \\(\\nabla^{2} f(x)\\) is the Hessian Matrix of \\(f(x)\\) .\n\\[ \\begin{align*} \\nabla f(x) = \\frac{\\partial l}{\\partial \\beta} \u0026= \\sum_{i=1}^n y_{i}x_{i}- (\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}})\\cdot x_{i}^{T} \\\\ \u0026= \\sum_{i=1}^n (y_{i}- \\underbrace{\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}}}_{p_{i}})\\cdot x_{i}^{T} = X(y-p) \\\\ \\end{align*} \\] \\[ \\nabla^{2}f(x) = \\frac{\\partial^{2} l}{\\partial \\beta \\partial \\beta^{T}} = \\sum_{i=1}^n - \\underbrace{\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}}}_{p_{i}}\\cdot \\underbrace{\\frac{1}{1+e^{\\beta^{T}x_{i}}}}_{1-p_{i}}x_{i} \\cdot x_{i}^{T} = -XWX^{T} \\\\ \\] Where \\(W\\) is a diagonal \\((n,n)\\) matrix with the \\(i^{th}\\) diagonal element defined as\n\\[ W = \\begin{bmatrix} p_{i}(1-p_{i}) \u0026 \u0026 \\\\ \u0026 \\ddots \u0026 \u0026 \\\\ \u0026 \u0026 \u0026 \\\\ \\end{bmatrix}_{\\ n x n} \\\\ \\] The Newton‐Raphson algorithm can now be expressed as:\n\\[ \\begin{align*} \\beta^{new} \u0026= \\beta^{old}-(\\nabla^{2}f(x))^{-1}\\nabla f(x) \\\\ \u0026= \\beta^{old}+ (XWX^{T})^{-1}X(y-p) \\\\ \u0026= \\beta^{old}+ (XWX^{T})^{-1}[XWX^{T}\\beta^{t}+ X(y-p)] \\\\ \u0026= \\beta^{old}+ (XWX^{T})^{-1}XWZ \\\\ \\end{align*} \\] Where \\(Z\\) can be expressed as: \\(Z = X^{T}\\beta^{t}+ W^{-1}(y-p) \\) . This algorithm is also known as Iteratively Reweighted Least Squares(IRLS).\n\\[ \\beta^{t+1} = \\arg\\min_{\\beta}(Z - X\\beta)^{T}W(Z-X\\beta) \\\\ \\] Other types of Logistic Regression # Multinomial Logistic Regression # Three or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan).\nOrdinal Logistic Regression # Three or more categories with ordering. Example: Movie rating from 1 to 5.\nReferences # [1] K, D. (2021, April 8). Logistic Regression in Python using Scikit-learn. Medium. https://heartbeat.fritz.ai/logistic-regression-in-python-using-scikit-learn-d34e882eebb1.\n[2] Li, S. (2019, February 27). Building A Logistic Regression in Python, Step by Step. Medium. https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8.\n[3] Christian. (2020, September 17). Plotting the decision boundary of a logistic regression model. https://scipython.com/blog/plotting-the-decision-boundary-of-a-logistic-regression-model/.\nBlog Home "},{"id":5,"href":"/docs/deep-learning/convolutional-neural-networks/","title":"Convolutional Neural Networks","section":"Deep Learning","content":" 卷积神经网络（Convolutional Neural Networks） # 卷积神经网络（CNN）的必要性在于它能够高效地处理具有高维结构的数据，特别是图像数据。在处理如图像这样的高维感知数据时，传统的多层感知机（MLP）存在局限性，因为它没有考虑到数据中的空间结构，导致在图像分类等任务中，参数量和计算开销巨大，训练起来非常不实用。举例来说，在猫狗图片分类任务中，使用百万像素的图像作为输入时，全连接层将产生数量庞大的参数，这不仅需要大量的计算资源，还可能导致过拟合。因此，卷积神经网络通过局部连接和权重共享的方式有效减少了参数数量，利用图像本身的空间结构进行特征提取，使得网络能够在较少的参数下仍能有效学习图像中的模式和特征。这种方式不仅极大地减少了计算复杂度，还能在图像处理任务中取得显著的性能提升。\n从全连接层到卷积（From Fully Connected Layers to Convolutions） # 想象一下，假设我们想从一张图片中找到某个物体。 合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。卷积神经网络正是将 空间不变性（spatial invariance） 的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。我们将上述想法总结一下，从而帮助我们设计适合于计算机视觉的神经网络架构。\n平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。 局部性（locality）：神经网络的前面几层应该 只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。 "},{"id":6,"href":"/docs/machine-learning/data-preprocessing/","title":"Data Preprocessing","section":"Machine Learning","content":" Data Preprocessing # "},{"id":7,"href":"/docs/machine-learning/supervised-learning/logistic-regression/","title":"Logistic Regression","section":"Supervised Learning","content":" 逻辑回归 # 逻辑回归（Logistic Regression） # Logistic Regression（逻辑回归）是一种用于分类问题的统计模型，本质上是一种线性模型，通过 Sigmoid 函数将线性回归的输出映射到 (0,1) 区间，用于预测概率。分类是应用 逻辑回归(Logistic Regression) 的目的和结果, 但中间过程依旧是回归. 通过逻辑回归模型, 我们得到的计算结果是 0-1 之间的连续数字, 可以把它称为\u0026quot;可能性\u0026quot;（概率）. 然后, 给这个可能性加一个阈值, 就成了分类. 例如, 可能性大于 0.5 即记为 1, 可能性小于 0.5 则记为 0。其数学公式可以表达为：\n\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} ，z = w^T x + b \\] 输出概率: \\[ \\begin{align*} \u0026P(y=1|x) = \\sigma(w^T x + b) \\\\ \u0026P(y=0|x) = 1 - \\sigma(w^T x + b) \\\\ \\end{align*} \\] 决策边界： \\(P(y=1|x) \\geq 0.5\\) 时预测为1，反之预测为0。\nSigmoid 函数 # Sigmoid 函数是一种常用的激活函数，将任意实数映射到区间 (0, 1)。\tLogistic回归中，Sigmoid的输出可以帮助解释为事件发生的概率。它的数学表达式为：\n\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\] 值域：Sigmoid 函数的输出值范围是 (0, 1) ，这使得它特别适合用于概率预测。 单调递增：Sigmoid 是单调递增函数，意味着 输入值越大，输出值越接近 1。 中心对称：以点 (0, 0.5) 为对称中心。 平滑性：Sigmoid 函数是光滑的，具有连续的一阶和二阶导数。 Note： 逻辑回归中，Sigmoid 函数的输出是 分类的概率，而不是分类的类别。\n损失函数（Loss Function） # Logistic 回归的训练目标是通过优化目标函数找到最优的模型参数，使模型能够对输入样本进行概率预测，并最大程度地准确分类数据，最小化训练数据的损失函数（Loss Function）。Logistic 回归的损失函数是基于 交叉熵损失（Cross-Entropy Loss） 定义的，它反映了模型预测值与实际值之间的不一致程度。\n对单个样本的损失函数：Logistic 回归的损失函数采用对数似然函数的负值，针对二分类任务的每个样本： \\[ \\text{Loss}(y, \\hat{y}) = -\\left[ y \\log \\hat{y} + (1 - y) \\log (1 - \\hat{y}) \\right] \\] \\(y \\in \\{0, 1\\}\\) 是实际标签。 \\(\\hat{y} = P(y=1|x)\\) 是模型预测的概率。 该损失函数的两种情况：\n当 \\(y = 1\\) ：损失为 \\(-\\log(\\hat{y})\\) ，鼓励模型将预测概率 \\(\\hat{y}\\) 接近 1。 当 \\(y = 0\\) ：损失为 \\(-\\log(1 - \\hat{y})\\) ，鼓励模型将预测概率 \\(\\hat{y}\\) 接近 0。 总体损失函数：对整个数据集的损失函数是所有样本损失的平均值： \\[ \\mathcal{L}(w, b) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log \\hat{y}_i + (1 - y_i) \\log (1 - \\hat{y}_i) \\right] \\] 这里 \\(\\hat{y}_i = \\sigma(z_i) = \\frac{1}{1 + e^{-z_i}}\\) ，其中 \\(z_i = w^T x_i + b \\) 。\n梯度下降（Gradient Descent） # Logistic Regression 使用梯度下降（Gradient Descent）优化其损失函数。在优化过程中，需要计算损失函数的梯度以更新模型参数 \\(w\\) , \\(b\\) ：\n损失函数对权重的梯度： \\[ \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\sigma(w^T x_i + b) - y_i \\right] x_i \\] 其中 \\(\\sigma(w^T x_i + b) - y_i\\) 是预测值与真实值的误差。\n损失函数对偏置的梯度： \\[ \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\sigma(w^T x_i + b) - y_i \\right] \\] 利用梯度更新参数： \\[ w := w - \\alpha \\frac{\\partial L}{\\partial w}, \\quad b := b - \\alpha \\frac{\\partial L}{\\partial b} \\] 性能评估（Evaluation Metrics） # 混淆矩阵 (Confusion Matrix) # 混淆矩阵是分类模型的基本评价工具，用于总结预测结果的分类情况。对于二分类问题，矩阵包含以下四个元素：\n预测正类 \\( (\\hat{y} = 1) \\) 预测负类 \\((\\hat{y} = 0)\\) 实际正类 \\((y = 1)\\) TP (True Positive) FN (False Negative) 实际负类 \\((y = 0)\\) FP (False Positive) TN (True Negative) TP (True Positive): 实际为正，预测也为正。 FN (False Negative): 实际为正，但预测为负。 FP (False Positive): 实际为负，但预测为正。 TN (True Negative): 实际为负，预测也为负。 准确率 (Accuracy) # \\[ \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} \\] 定义：模型预测正确的样本占总样本的比例。 优点：简单直观。 缺点：当类别不平衡时（正负样本比例悬殊），准确率可能误导。 精确率 (Precision) # \\[ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} \\] 描述模型预测正类的可靠性。 适用场景：当 FP 的代价较高时，例如垃圾邮件过滤（FP 表示误判正常邮件为垃圾邮件）。 召回率 (Recall) # \\[ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \\] 描述模型对正类样本的捕捉能力。 适用场景：当 FN 的代价较高时，例如疾病检测（FN 表示漏诊病人）。 F1-Score # \\[ \\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\] F1-Score 是 Precision 和 Recall 的调和平均，用于权衡两者之间的关系。 适用场景：当 Precision 和 Recall 同等重要时。 ROC 曲线 和 AUC (Area Under the Curve) # 横轴：假正率 ( \\(FPR = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\\) )。 纵轴：真正率 ( \\(TPR = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\) )。 ROC 曲线展示了不同阈值下模型性能的变化。 阈值（threshold） 的变化直接影响模型的 TPR（真正例率） 和 FPR（假正例率），从而决定曲线上每个点的位置：\n起点与终点： 当 阈值 = 1.0（极高阈值）：所有样本都被预测为负类， \\(\\text{TPR} = 0，\\text{FPR} = 0\\) ，即曲线起点 (0,0)。 当 阈值 = 0.0（极低阈值）：所有样本都被预测为正类， \\(\\text{TPR} = 1，\\text{FPR} = 1\\) ，即曲线终点 (1,1)。 中间变化： 随着阈值从高到低移动，曲线从 (0,0) 开始，逐渐向 (1,1) 延展。 这些点的位置和曲线的形状取决于模型在不同阈值下的 TPR 和 FPR。 关键点： 特定阈值（如 0.5 或其他业务相关的值）对应的 TPR 和 FPR 可通过 ROC 图直接观察，帮助选择最佳阈值。 AUC (Area Under the Curve) # ROC 曲线下的面积，取值范围为 [0, 1]。AUC 的意义：\nAUC = 1：完美分类器。 AUC = 0.5：随机猜测。 0.5 \u0026lt; AUC \u0026lt; 1：模型有一定的区分能力。 Logistic Regression 代码实现 # # \u0026lt;--- From scratch ---\u0026gt; import numpy as np def sigmoid(z): return 1 / (1 + np.exp(-z)) def cross_entropy_loss(y, y_pred): \u0026#34;\u0026#34;\u0026#34; y: 实际标签 (0 或 1) y_pred: 模型预测值 (范围在 0 和 1 之间) 返回: 平均交叉熵损失 \u0026#34;\u0026#34;\u0026#34; # 防止 log(0) 导致的数值错误，添加一个小的正数 epsilon epsilon = 1e-15 y_pred = np.clip(y_pred, epsilon, 1 - epsilon) # 保证 y_pred 不会等于 0 或 1 return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred)) def Logistic_Regression(X, y, lr, max_iter): \u0026#34;\u0026#34;\u0026#34; X: 特征矩阵 (n_samples x n_features) y: 标签向量 (n_samples,) lr: 学习率 max_iter: 最大迭代次数 返回: 训练好的权重和偏置 \u0026#34;\u0026#34;\u0026#34; n_samples, n_features = X.shape # 样本数量和特征数量 weights = np.zeros(n_features) # 初始化权重为 0 bias = 0 # 初始化偏置为 0 losses = [] for i in range(max_iter): # 计算预测值 y_pred = sigmoid(np.dot(X, weights) + bias) # 计算梯度 weight_grad = (1 / n_samples) * np.dot(X.T, (y_pred - y)) # 权重的梯度 bias_grad = (1 / n_samples) * np.sum(y_pred - y) # 偏置的梯度 # 计算损失并存储 loss = cross_entropy_loss(y, y_pred) losses.append(loss) # 使用梯度下降法更新权重和偏置 weights -= lr * weight_grad bias -= lr * bias_grad return weights, bias def Logistic_Regression_Predict(X, weights, bias): # 计算预测概率 pred_y = sigmoid(np.dot(X, weights) + bias) # 将概率转换为二分类标签 return [1 if i \u0026gt;= 0.5 else 0 for i in pred_y] from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split # 生成模拟分类数据集 X, y = make_classification(n_samples=1000, n_features=10, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) lr = 0.1 # 学习率 max_iter = 1000 # 最大迭代次数 # 训练模型 weights, bias = Logistic_Regression(X_train, y_train, lr, max_iter) # 使用测试集进行预测 predictions = Logistic_Regression_Predict(X_test, weights, bias) # 打印预测结果示例 print(\u0026#34;Predictions:\u0026#34;, predictions[:10]) # \u0026lt;--- From scikit-learn ---\u0026gt; from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, confusion_matrix, classification_report from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=10, random_state=42) # 数据划分 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 模型训练 model = LogisticRegression( penalty=\u0026#39;l2\u0026#39;, # 正则化类型：\u0026#39;l1\u0026#39;, \u0026#39;l2\u0026#39;, \u0026#39;elasticnet\u0026#39;, 或 \u0026#39;none\u0026#39;（默认 \u0026#39;l2\u0026#39;） C=1.0, # 正则化强度的倒数，值越小正则化越强，默认为 1.0 solver=\u0026#39;lbfgs\u0026#39;, # 优化算法：如 \u0026#39;lbfgs\u0026#39;, \u0026#39;liblinear\u0026#39;, \u0026#39;sag\u0026#39;, \u0026#39;saga\u0026#39; 等 max_iter=100, # 最大迭代次数，防止迭代过多导致训练时间过长 random_state=42 # 随机种子，保证结果可复现 ) model.fit(X_train, y_train) # 预测 y_pred = model.predict(X_test) # 评价指标 accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) report = classification_report(y_test, y_pred) print(f\u0026#34;Accuracy: {accuracy}\u0026#34;) print(f\u0026#34;Confusion Matrix:\\n{conf_matrix}\u0026#34;) print(f\u0026#34;Classification Report:\\n{report}\u0026#34;) "},{"id":8,"href":"/docs/common-libraries/numpy/","title":"NumPy","section":"Common Libraries","content":" NumPy # "},{"id":9,"href":"/docs/python-basics/python-fundamentals/","title":"Python Fundamentals","section":"Python Basics","content":" Python Fundamentals # 列表 (Lists) # 列表 (Lists) 是 有序的 (ordered)、可变的 (mutable) 值集合，这些值以逗号分隔并用方括号括起来。列表可以由许多不同类型的变量组成。\n# Creating a list x = [3, \u0026#34;hello\u0026#34;, 1.2] print (x) [3, \u0026#39;hello\u0026#39;, 1.2] 我们可以使用 append 函数将新的值添加到 列表 (Lists) 中：\n# Adding to a list x.append(7) print (x) print (len(x)) [3, \u0026#39;hello\u0026#39;, 1.2, 7] 4 或者直接替换现有的值：\n# Replacing items in a list x[1] = \u0026#34;bye\u0026#34; print (x) [3, \u0026#39;bye\u0026#39;, 1.2, 7] 并可以直接对 列表 (list) 执行操作：\n# Operations y = [2.4, \u0026#34;world\u0026#34;] z = x + y print (z) [3, \u0026#39;bye\u0026#39;, 1.2, 7, 2.4, \u0026#39;world\u0026#39;] 元组 (Tuples) # 元组 (Tuples) 是 有序 (ordered) 且 不可变 (immutable) 的集合。我们将使用元组来存储 永远不会改变 的值。\n# Creating a tuple x = (3.0, \u0026#34;hello\u0026#34;) # tuples start and end with () print (x) (3.0, \u0026#39;hello\u0026#39;) # Adding values to a tuple x = x + (5.6, 4) print (x) (3.0, \u0026#39;hello\u0026#39;, 5.6, 4) # Try to change (it won\u0026#39;t work and we get an error) x[0] = 1.2 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ----\u0026gt; 1 x[0] = 1.2 TypeError: \u0026#39;tuple\u0026#39; object does not support item assignment 集合 (Sets) # 集合(sets) 是 无序(unordered) 且 可变(mutable) 的。但是，集合中的每个项目必须是 唯一(unique) 的。\n# Sets text = \u0026#34;Learn ML with Made With ML\u0026#34; print (set(text)) print (set(text.split(\u0026#34; \u0026#34;))) {\u0026#39;e\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39; \u0026#39;, \u0026#34;r\u0026#34;, \u0026#34;w\u0026#34;, \u0026#39;d\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;t\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;L\u0026#39;, \u0026#39;n\u0026#39;, \u0026#34;w\u0026#34;} {\u0026#39;with\u0026#39;, \u0026#39;Learn\u0026#39;, \u0026#39;ML\u0026#39;, \u0026#39;Made\u0026#39;, \u0026#39;With\u0026#39;} 字典 (Dictionaries) # 字典 (Dictionaries) 是 无序 (unordered) 且 可变 (mutable) 的 键值对(key-value pair) 集合。我们可以根据 键(key) 检索 值(value)，但字典不能有两个相同的键。\n# Creating a dictionary person = {\u0026#34;name\u0026#34;: \u0026#34;Goku\u0026#34;, \u0026#34;eye_color\u0026#34;: \u0026#34;brown\u0026#34;} print (person) print (person[\u0026#34;name\u0026#34;]) print (person[\u0026#34;eye_color\u0026#34;]) {\u0026#34;name\u0026#34;: \u0026#34;Goku\u0026#34;, \u0026#34;eye_color\u0026#34;: \u0026#34;brown\u0026#34;} Goku brown # Changing the value for a key person[\u0026#34;eye_color\u0026#34;] = \u0026#34;green\u0026#34; print (person) {\u0026#34;name\u0026#34;: \u0026#34;Goku\u0026#34;, \u0026#34;eye_color\u0026#34;: \u0026#34;green\u0026#34;} # Adding new key-value pairs person[\u0026#34;age\u0026#34;] = 24 print (person) {\u0026#34;name\u0026#34;: \u0026#34;Goku\u0026#34;, \u0026#34;eye_color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;age\u0026#34;: 24} # Length of a dictionary print (len(person)) 3 索引 (Indexing) # 通过列表的 索引(indexing) 和 切片 (slicing)，我们可以检索列表中的特定值。请注意，索引可以是正数（从 0 开始）或负数（-1 及以下，其中 -1 是列表中的最后一项）。\n# Indexing x = [3, \u0026#34;hello\u0026#34;, 1.2] print (\u0026#34;x[0]: \u0026#34;, x[0]) print (\u0026#34;x[1]: \u0026#34;, x[1]) print (\u0026#34;x[-1]: \u0026#34;, x[-1]) # the last item print (\u0026#34;x[-2]: \u0026#34;, x[-2]) # the second to last item x[0]: 3 x[1]: hello x[-1]: 1.2 x[-2]: hello # Slicing print (\u0026#34;x[:]: \u0026#34;, x[:]) # all indices print (\u0026#34;x[1:]: \u0026#34;, x[1:]) # index 1 to the end of the list print (\u0026#34;x[1:2]: \u0026#34;, x[1:2]) # index 1 to index 2 (not including index 2) print (\u0026#34;x[:-1]: \u0026#34;, x[:-1]) # index 0 to last index (not including last index) x[:]: [3, \u0026#39;hello\u0026#39;, 1.2] x[1:]: [\u0026#39;hello\u0026#39;, 1.2] x[1:2]: [\u0026#39;hello\u0026#39;] x[:-1]: [3, \u0026#39;hello\u0026#39;] if 语句 (if statements) # 我们可以使用 if 语句有条件地执行某项操作。条件由单词 if、elif（代表 else if）和 else 定义。我们可以根据需要使用任意数量的 elif 语句。每个条件下方的缩进代码是条件为 True 时将执行的代码。\n# If statement x = 4 if x \u0026lt; 1: score = \u0026#34;low\u0026#34; elif x \u0026lt;= 4: # elif = else if score = \u0026#34;medium\u0026#34; else: score = \u0026#34;high\u0026#34; print (score) medium # If statement with a boolean x = True if x: print (\u0026#34;it worked\u0026#34;) it worked 循环语句 (Loop) # For loops # for 循环可以迭代值集合（列表 (list)、元组 (tuple)、字典 (dictionaries)等）。缩进的代码针对值集合中的 每个项目 执行。\n# For loop veggies = [\u0026#34;carrots\u0026#34;, \u0026#34;broccoli\u0026#34;, \u0026#34;beans\u0026#34;] for veggie in veggies: print (veggie) carrots broccoli beans 当循环遇到 break 命令时，循环将立即终止。如果列表中还有更多项目，则不会处理它们。\n# `break` from a for loop veggies = [\u0026#34;carrots\u0026#34;, \u0026#34;broccoli\u0026#34;, \u0026#34;beans\u0026#34;] for veggie in veggies: if veggie == \u0026#34;broccoli\u0026#34;: break print (veggie) carrots 当循环遇到 continue 命令时，循环将仅跳过列表中该项目的所有其他操作。如果列表中还有更多项目，循环将正常继续。\n# `continue` to the next iteration veggies = [\u0026#34;carrots\u0026#34;, \u0026#34;broccoli\u0026#34;, \u0026#34;beans\u0026#34;] for veggie in veggies: if veggie == \u0026#34;broccoli\u0026#34;: continue print (veggie) carrots beans While loops # 只要条件为 True，while 循环就可以重复执行。我们也可以在 while 循环中使用 continue 和 break 命令。\n# While loop x = 3 while x \u0026gt; 0: x -= 1 # same as x = x - 1 print (x) 2 1 0 列表推导式 (list comprehension) 和生成器表达式 (generator expression) # 快速构建列表或生成器，尤其适合在数据处理或特征工程中清洗数据或生成特定序列。\n列表推导式 (list comprehension)：返回一个完整的列表。 生成器表达式 (generator expression)：返回一个惰性生成器，节省内存。 # 过滤列表中的偶数 numbers = [1, 2, 3, 4, 5, 6] even_numbers = [x for x in numbers if x % 2 == 0] print(even_numbers) # [2, 4, 6] # 生成器表达式（惰性求值） gen = (x**2 for x in range(5)) # 不占用大量内存 for val in gen: print(val) [2, 4, 6] 0 1 4 9 16 Lambda 函数和 map, filter 的使用 # Lambda函数：定义简洁的匿名函数，适合简单逻辑。(e.g. func = lambda var1 var2 : function =\u0026gt; func(var1, var2)) map：对可迭代对象中的每个元素应用函数。 filter：筛选符合条件的元素。 from functools import reduce # Lambda函数示例 add = lambda x, y: x + y print(add(3, 5)) # 8 # map: 计算平方 squares = list(map(lambda x: x**2, [1, 2, 3])) print(squares) # [1, 4, 9] # filter: 筛选出大于2的元素 filtered = list(filter(lambda x: x \u0026gt; 2, [1, 2, 3, 4])) print(filtered) # [3, 4] 8 [1, 4, 9] [3, 4] 函数 (Function) 封装 # 在ML项目中封装代码逻辑，便于维护和复用。例如，封装数据预处理步骤或模型训练流程。\ndef preprocess_data(data): preprocessed_data = data.dropna() return preprocessed_data # \u0026lt;---------- Usage ----------\u0026gt; # cleaned_data = preprocess_data(raw_data) def f(*args, **kwargs) 是另一种定义函数的方式，用来接收可变数量的参数。它允许函数在调用时传入任意数量的位置参数和关键字参数，从而使函数更加灵活。\n*args: 可变长度位置参数，接收任意数量的未命名参数 (arguments)，作为一个元组。 **kwargs: 可变长度关键字参数，接收任意数量的命名参数 (keyword arguments)，作为一个字典。 def f(*args, **kwargs): # 从位置参数中提取第一个值，赋值给变量 x x = args[0] # 从关键字参数中获取键 \u0026#34;y\u0026#34; 的值，若不存在返回 None y = kwargs.get(\u0026#34;y\u0026#34;) print (f\u0026#34;x: {x}, y: {y}\u0026#34;) # 调用函数 f，传入一个位置参数 5 和一个关键字参数 y=2 f(5, y=2) x: 5, y: 2 类 (Class) 封装 # class DataHandler: def __init__(self, filepath): self.filepath = filepath def load_data(self): print(f\u0026#34;Loading data from {self.filepath}...\u0026#34;) return {\u0026#34;data\u0026#34;: [1, 2, 3, 4]} def preprocess_data(self, data): return [x * 2 for x in data] # \u0026lt;---------- Usage ----------\u0026gt; # new_handler = DataHandler(\u0026#39;data.csv\u0026#39;) # new_data = new_handler.load_data() # preprocess_data = new_handler.preprocess_data(new_data[\u0026#39;data\u0026#39;]) 继承 (Inheritance) # 继承用于让一个类（子类）从另一个类（父类）中获得 属性(properties) 和 方法(methods)，从而实现代码复用和扩展。\n子类继承父类的方法和属性。 使用 super() 调用父类的方法。 方法重写：子类可重写父类的方法实现。 # 定义一个基础模型类 class BaseModel: def __init__(self, name): self.name = name def train(self): print(f\u0026#34;{self.name} is training...\u0026#34;) def test(self): print(f\u0026#34;{self.name} is testing...\u0026#34;) # 子类继承基础模型类 class RegressionModel(BaseModel): def __init__(self, name, num_features): super().__init__(name) self.num_features = num_features def train_1(self): super().train() print(f\u0026#34;Training a regression model with {self.num_features} features.\u0026#34;) # 使用子类 model = RegressionModel(\u0026#34;LinearRegression\u0026#34;, 10) model.train() model.test() LinearRegression is training... LinearRegression is testing... 方法 (Methods) # 实例方法 (Instance Method)：实例方法的第一个参数是 self，用于访问实例属性和其他实例方法。它需要通过实例对象调用，依赖于具体的实例状态。当方法需要操作实例的属性（如 self.name）或依赖于实例的状态时，实例方法是最合适的选择。 类方法 (Class Method)：类方法的第一个参数是 cls，表示类本身（而不是实例）。它使用 @classmethod 装饰器修饰，可以通过类对象或实例对象调用。当方法需要操作类级别的状态（如 total_models）而不依赖于任何具体实例时，类方法可以保持逻辑的清晰和一致性。 静态方法 (Static Method)：使用 @staticmethod，与类或实例 (self 或 cls)无绑定，仅实现功能逻辑。当方法的逻辑与类相关，但完全独立于类或实例的状态时，静态方法可以避免无意义的参数（如 self 或 cls），提高代码的简洁性。 class MLModel: total_models = 0 # 类属性 def __init__(self, name): self.name = name MLModel.total_models += 1 def display(self): # 实例方法 print(f\u0026#34;Model Name: {self.name}\u0026#34;) @classmethod def get_total_models(cls): # 类方法 print(f\u0026#34;Total models created: {cls.total_models}\u0026#34;) @staticmethod def utility_function(x): # 静态方法 return x**2 # 使用 model = MLModel(\u0026#34;RandomForest\u0026#34;) model.display() MLModel.get_total_models() print(MLModel.utility_function(5)) Model Name: RandomForest Total models created: 1 25 装饰器 (Decorators) # 装饰器是一种函数，接受另一个函数或方法作为输入并返回一个修改后的函数，用于动态扩展功能。\n@decorator def function(): pass 等价于：\ndef function(): pass function = decorator(function) import time # 定义一个装饰器 def timer_decorator(func): def wrapper(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print(f\u0026#34;Function \u0026#39;{func.__name__}\u0026#39; executed in {end_time - start_time:.4f}s\u0026#34;) return result return wrapper # 装饰训练函数 class MLModel: @timer_decorator def train(self, data): print(\u0026#34;Training model...\u0026#34;) time.sleep(1) # 模拟训练时间 return \u0026#34;Training Complete\u0026#34; model = MLModel() print(model.train([])) Training model... Function \u0026#39;train\u0026#39; executed in 1.0015s Training Complete 回调函数 (Callbacks) # 回调是一个函数作为参数传递给另一个函数，并在适当时调用。它在机器学习中常用于动态调整训练流程（如早停、学习率调整）。\nclass TrainingCallback: def on_epoch_start(self, epoch): print(f\u0026#34;Epoch {epoch} started.\u0026#34;) def on_epoch_end(self, epoch, loss): print(f\u0026#34;Epoch {epoch} ended with loss: {loss}.\u0026#34;) # 使用回调 class MLTrainer: def __init__(self, callback=None): self.callback = callback def train(self, epochs): for epoch in range(epochs): if self.callback: self.callback.on_epoch_start(epoch) # 模拟训练过程 loss = 0.01 * (epochs - epoch) if self.callback: self.callback.on_epoch_end(epoch, loss) # 测试回调 callback = TrainingCallback() trainer = MLTrainer(callback) trainer.train(3) Epoch 0 started. Epoch 0 ended with loss: 0.03. Epoch 1 started. Epoch 1 ended with loss: 0.02. Epoch 2 started. Epoch 2 ended with loss: 0.01. 模块 (Module) 封装 # project/ ├── data_processing.py # Contains functions for data processing ├── model_training.py # Contains model training logic ├── evaluation.py # Contains evaluation methods # \u0026lt;---------- Usage ----------\u0026gt; # \u0026lt;---------- data_processing.py ----------\u0026gt; # def clean_data(data): # \u0026#34;\u0026#34;\u0026#34;Clean the data by dropping missing values.\u0026#34;\u0026#34;\u0026#34; # print(\u0026#34;Cleaning data...\u0026#34;) # return data.dropna() # \u0026lt;---------- main.py ----------\u0026gt; # import data_processing as dp # data = dp.load_data(\u0026#34;data.csv\u0026#34;) # cleaned_data = dp.clean_data(data) 异步编程 (Asynchronous Programming) # 在训练、数据下载、或者与API通信时异步执行，提高性能。\nasync 用于定义一个异步函数。使用 async def 定义，表示该函数会返回一个协程对象。 await 用于调用另一个异步函数，并等待其执行完成，直到结果返回。 asyncio 库提供了一个框架来实现异步编程。 import nest_asyncio import asyncio nest_asyncio.apply() # Allows nested use of asyncio.run [Only for Jupyter Notebook] # 定义一个异步函数 async def say_hello(): print(\u0026#34;Hello\u0026#34;) await asyncio.sleep(1) # 模拟一个耗时的异步操作 print(\u0026#34;World\u0026#34;) # 运行异步任务 asyncio.run(say_hello()) Hello World import asyncio # 定义异步任务 async def fetch_data(): print(\u0026#34;Fetching data...\u0026#34;) await asyncio.sleep(2) # 模拟耗时的异步操作 return \u0026#34;Data fetched\u0026#34; async def process_data(): print(\u0026#34;Processing data...\u0026#34;) await asyncio.sleep(1) # 模拟耗时的异步操作 return \u0026#34;Data processed\u0026#34; # 主程序 async def main(): data_task = asyncio.create_task(fetch_data()) # 启动任务1 process_task = asyncio.create_task(process_data()) # 启动任务2 result1 = await data_task # 等待任务1完成 result2 = await process_task # 等待任务2完成 print(result1) print(result2) # 启动事件循环 asyncio.run(main()) Fetching data... Processing data... Data fetched Data processed "},{"id":10,"href":"/posts/04_lda_and_qda_for_classification/","title":"LDA and QDA for Classification","section":"Blog","content":"Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data.\nLinear Discriminant Analysis for Classification # LDA assumes that all classes are linearly separable and according to this multiple linear discrimination function representing several hyperplanes in the feature space are created to distinguish the classes. If there are two classes then the LDA draws one hyperplane and projects the data onto this hyperplane in such a way as to maximize the separation of the two categories. This hyperplane is created according to the two criteria considered simultaneously:\nMaximizing the distance between the means of two classes; Minimizing the variation between each category. Suppose that \\(Y \\in \\{1, ..., K\\}\\) is assigned a prior \\(\\hat{\\pi}_{k}\\) such that \\(\\sum_{i=1}^k \\hat{\\pi}_{k} = 1\\) . According to Bayes’ rule, the posterior probability is\n\\[ P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{\\sum_{i=1}^{K}f_{i}(x)\\pi_{i}} \\\\ \\] where \\(f_{k}(x)\\) is the density of \\(X\\) conditioned on \\(k\\) . The Bayes Classifier can be expessed as:\n\\[ h^{*}(x) = \\arg\\max_{k}\\{P(Y=k|X=x)\\} = \\arg\\max_{k}\\delta_{k}(x) \\\\ \\] For we assume that the random variable \\(X\\) is a vector \\(X=(X_1,X_2,...,X_k)\\) which is drawn from a multivariate Gaussian with class-specific mean vector and a common covariance matrix \\(\\Sigma \\ (i.e. \\Sigma_{k} = \\Sigma, \\forall k)\\) . In other words the covariance matrix is common to all K classes: \\(Cov(X)=\\Sigma\\) of shape \\(d \\times d\\) .\nSince \\(x\\) follows a multivariate Gaussian distribution, the probability \\(P(X=x|Y=k)\\) is given by: ( \\(\\mu_k\\) is the mean of inputs for category \\(k\\) )\n\\[ f_k(x)=\\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k)) \\\\ \\] Then we can find the posterior distribution as:\n\\[ P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{P(X=x)} = C \\cdot f_{k}(x)\\pi_{k} \\\\ \\] Since \\(P(X=x)\\) does not depend on \\(k\\) so we write it as a constant. We will now proceed to expand and simplify the algebra, putting all constant terms into \\(C,C^{'},C{''}\\) etc..\n\\[ \\begin{align*} p_{k}(x) = \u0026P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{P(X=x)} = C \\cdot f_{k}(x)\\pi_{k} \\\\ \u0026=C \\cdot \\pi_{k} \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k)) \\\\ \u0026=C^{'} \\cdot \\pi_{k} exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k)) \\\\ \\end{align*} \\] Take the log of both sides:\n\\[ \\begin{align*} logp_{k}(x) \u0026=log(C^{'} \\cdot \\pi_{k} exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k))) \\\\ \u0026= logC^{'} + log\\pi_k -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k) \\\\ \u0026= logC^{'} + log\\pi_k -\\frac{1}{2}[(x^{T}\\Sigma^{-1}x+\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}]+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ \u0026= C^{''} + log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ \\end{align*} \\] And so the objective function, sometimes called the linear discriminant function or linear score function is:\n\\[ \\delta_{k} = log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ \\] Which means that given an input \\(x\\) we predict the class with the highest value of \\(\\delta_{k}(x)\\) .\nTo find the Dicision Boundary, we will set \\(P(Y=k|X=x) = P(Y=l|X=x)\\) which is \\(\\delta_{k}(x) = \\delta_{l}(x)\\) :\n\\[ log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} = log\\pi_l -\\frac{1}{2}\\mu_{l}^{T}\\Sigma^{-1}\\mu_{l}+x^{T}\\Sigma^{-1}\\mu_{l} \\\\ log\\frac{\\pi_{k}}{\\pi_{l}} -\\underbrace{\\frac{1}{2}(\\mu_{k}-\\mu_{l})^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l})}_{\\mathrm{Constant}}+\\underbrace{x^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l})}_{\\mathrm{Linear \\ in} \\ x} = 0 \\\\ \\Rightarrow a^{T}x + b = 0 \\\\ \\] Which is a linear function in \\(x\\) - this explains why the decision boundaries are linear - hence the name Linear Discriminant Analysis.\nQuadratic Discrimination Analysis for Classification # LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector, but a covariance matrix that is common to all \\(K\\) classes. Quadratic discriminant analysis provides an alternative approach by assuming that each class has its own covariance matrix \\(\\Sigma_{k}\\) .\nTo derive the quadratic score function, we return to the previous derivation, but now \\(\\Sigma_{k}\\) is a function of \\(k\\) , so we cannot push it into the constant anymore.\n\\[ p_{k}(x) = \\pi_{k}\\frac{1}{(2\\pi)^{d/2}|\\Sigma_{k}|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k)) \\\\ \\] \\[ \\begin{align*} logp_{k}(x) \u0026= C +log\\pi_{k} - \\frac{1}{2}log|\\Sigma_{k}|-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \\\\ \u0026= C +log\\pi_{k} - \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}x^{T}\\Sigma_{k}^{-1}x +x^{T}\\Sigma_{k}^{-1}\\mu_{k} -\\frac{1}{2}\\mu_{k}^{T}\\Sigma_{k}^{-1}\\mu_{k} \\\\ \\end{align*} \\] Which is a quadratic function of \\(x\\) . Under this less restrictive assumption, the classifier assigns an observation \\(X=x\\) to the class for which the quadratic score function is the largest:\n\\[ \\delta_{k}(x) = log\\pi_k- \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \\\\ \\] To find the Quadratic Dicision Boundary, we will set \\(P(Y=k|X=x) = P(Y=l|X=x)\\) which is \\(\\delta_{k}(x) = \\delta_{l}(x)\\) :\n\\[ log\\pi_k- \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) = log\\pi_l- \\frac{1}{2}log|\\Sigma_{l}| -\\frac{1}{2}(x-\\mu_l)^{T}\\Sigma_{l}^{-1}(x-\\mu_l) \\\\ \\frac{1}{2}x^{T}\\underbrace{(\\Sigma_{l}-\\Sigma_{k})}_{A}x+\\underbrace{(\\mu_{k}^{T}\\Sigma_{k}^{-1}-\\mu_{l}^{T}\\Sigma_{l}^{-1})}_{b^{T}}x +\\underbrace{\\frac{1}{2}(\\mu_{k}-\\mu_{l})^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l}) + log(\\frac{\\pi_{l}}{\\pi_{k}}) + log(\\frac{|\\Sigma_{k}|^{1/2}}{|\\Sigma_{l}|^{1/2}})}_{c} = 0 \\\\ \\Rightarrow x^{T}Ax + b^{T}x + c = 0 \\\\ \\] Case 1 : When \\(\\Sigma_{k} = I\\) # We first concider the case that \\(\\Sigma_{k} = I, \\forall k\\) . This is the case where each distribution is spherical, around the mean point. Then we can have:\n\\[ \\delta_{k}(x) = - \\frac{1}{2}log|I| + log\\pi_k -\\frac{1}{2}(x-\\mu_k)^{T}I(x-\\mu_k) \\\\ \\] Where \\(log(|I|) = log(1) = 0\\) and \\((x-\\mu_k)^{T}I(x-\\mu_k) = (x-\\mu_k)^{T}(x-\\mu_k)\\) is the Squared Euclidean Distance between two points \\(x\\) and \\(\\mu_{k}\\) .\nThus under this condition (i.e. \\(\\Sigma = I\\) ) , a new point can be classified by its distance from the center of a class, adjusted by some prior. Further, for two-class problem with equal prior, the discriminating function would be the perpendicular bisector of the 2-class’s means.\nCase 2 : When \\(\\Sigma_{k} \\neq I\\) # Since \\(\\Sigma_{k}\\) is a symmetric matrix \\(\\Sigma_{k} = \\Sigma_{k}^{T}\\) , by using the Singular Value Decomposition (SVD) of \\(\\Sigma_{k}\\) , we can get:\n\\[ \\Sigma_{k} = U_{k}S_{k}U_{k}^{T} \\\\ \\Sigma_{k}^{-1} = (U_{k}S_{k}U_{k}^{T})^{-1} = U_{k}S_{k}^{-1}U_{k}^{T}\\\\ \\] Then,\n\\[ \\begin{align*} (x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \u0026= (x-\\mu_k)^{T}U_{k}S_{k}^{-1}U_{k}^{T}(x-\\mu_k) \\\\ \u0026= (U_{k}^{T}x-U_{k}^{T}\\mu_k)^{T}S_{k}^{-1}(U_{k}^{T}x-U_{k}^{T}\\mu_k) \\\\ \u0026= (U_{k}^{T}x-U_{k}^{T}\\mu_k)^{T}S_{k}^{-\\frac{1}{2}}S_{k}^{-\\frac{1}{2}}(U_{k}^{T}x-U_{k}^{T}\\mu_k) \\\\ \u0026= (S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}I(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ \u0026= (S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ \\end{align*} \\] Which is also known as the Mahalanobis distance.\nThink of \\(S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\) as a linear transformation that takes points in class \\(k\\) and distributes them spherically around a point, like in Case 1. Thus when we are given a new point, we can apply the modified \\(\\delta_{k}\\) values to calculate \\(h^{*}(x)\\) . After applying the singular value decomposition, \\(\\Sigma_{k}^{-1}\\) is considered to be an identity matrix such that:\n\\[ \\delta_{k}(x) = - \\frac{1}{2}log|I| + log\\pi_k -\\frac{1}{2}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ \\] Where \\(log(|I|) = log(1) = 0\\) The difference between Case 1 and Case 2 (i.e. the difference between using the Euclidean and Mahalanobis distance) can be seen in the illustration below:\nLDA and QDA in practice # In practice we don’t know the parameters of Gaussian and will need to estimate them using our training data.\n\\[ \\hat{\\pi}_{k} = \\hat{P}(y=k) = \\frac{n_{k}}{n} \\\\ \\] where \\(n_{k}\\) is the number of class \\(k\\) observations.\n\\[ \\hat{\\mu}_{k} = \\frac{1}{n_{k}}\\sum_{i:y_{i}=k}x_{i} \\\\ \\] \\[ \\hat{\\Sigma}_{k} = \\frac{1}{n_{k}-k}\\sum_{i:y_{i}=k}(x_{i}-\\hat{\\mu}_{k})(x_{i}-\\hat{\\mu}_{k})^{T} \\\\ \\] If we wish to use LDA we must calculate a common covariance, so we average all the covariances, e.g.\n\\[ \\Sigma = \\frac{\\sum_{r=1}^k(n_{r}\\Sigma_{r})}{\\sum_{r=1}^k n_{r}} \\\\ \\] Where:\n\\(n_{r}\\) is the number of data points in class \\(r\\) .\n\\(\\Sigma_{r}\\) is the covariance of class \\(r\\) and \\(n\\) is the total number of data points.\n\\(k\\) is the number of classes.\nReference # [1] Sicotte, X. B. (2018, June 22). Xavier Bourret Sicotte. Linear and Quadratic Discriminant Analysis - Data Blog. https://xavierbourretsicotte.github.io/LDA_QDA.html.\n"},{"id":11,"href":"/docs/deep-learning/computer-vision/","title":"Computer Vision","section":"Deep Learning","content":" Computer Vision # "},{"id":12,"href":"/docs/python-basics/leetcode/","title":"Leetcode Notes","section":"Python Basics","content":" Leetcode Interview Preparation Notes # Basic Data Structures # Arrays # In Python, arrays are typically represented using lists. While Python doesn\u0026rsquo;t have a native array type as seen in other languages like Java or C++, lists are versatile and can be used similarly to arrays.\n【Last Update: 2024-08-14】\narr = [] # O(1) arr = [1, 2, 3] # O(n), where n is the number of elements first_element = arr[0] # O(1) arr[1] = 10 # O(1) arr.append(6) # O(1) on average for appending arr.insert(2, 15) # O(n), where n is the number of elements after the insertion index arr.remove(15) # O(n), where n is the number of elements in the list [remove the first 15 in the array] del arr[2] # O(n), where n is the number of elements after the deleted index last_element = arr.pop() # O(1) arr.sort() # 原地排序 sorted_arr = sorted(arr) # 返回排序后的数组 arr[::-1] # arr 倒序 ## Counter() 的常用语法和使用情况 from collections import Counter arr = [1, 2, 2, 3, 3, 3] counts = Counter(arr) # 结果：Counter({3: 3, 2: 2, 1: 1}) ## 找到出现次数最多的元素 most_common_element = counts.most_common(1)[0] # 结果：(3, 3) ## 判断出现的元素是否相同 arr1 = [1, 2, 3] arr2 = [3, 2, 1] is_anagram = Counter(arr1) == Counter(arr2) # 结果：True ## set() 的常用语法和使用情况 arr = [1, 2, 2, 3, 4, 4] ## 快速查找 seen = set(arr) if 3 in seen: print(\u0026#34;3 is in array\u0026#34;) ## 去重 unique_elements = list(set(arr)) # 结果：[1, 2, 3, 4] ## 两个数组的交集 arr1 = [1, 2, 2, 3] arr2 = [2, 3, 4] intersection = list(set(arr1) \u0026amp; set(arr2)) # 结果：[2, 3] Strings # Strings in Python are immutable sequences of characters. You can perform various operations on strings using built-in methods and operators.\n【Last Update: 2024-08-14】\ns = \u0026#34;Hello, World!\u0026#34; # O(n), where n is the length of the string first_char = s[0] # O(1) substring = s[7:12] # O(k), where k is the length of the substring combined = s + \u0026#34; Python\u0026#34; # O(n + m), where n and m are the lengths of the two strings repeated = s * 2 # O(n * k), where k is the number of repetitions upper_s = s.upper() # O(n), where n is the length of the string lower_s = s.lower() # O(n), where n is the length of the string starts_with_hello = s.startswith(\u0026#34;Hello\u0026#34;) # O(n), where n is the length of the prefix contains_world = \u0026#34;World\u0026#34; in s # O(n * m), where n is the length of the string and m is the length of the substring replaced_s = s.replace(\u0026#34;World\u0026#34;, \u0026#34;Python\u0026#34;) # O(n * m), where n is the length of the string and m is the length of the substring words = s.split(\u0026#34;, \u0026#34;) # O(n), where n is the length of the string joined = \u0026#34; - \u0026#34;.join(words) # O(n), where n is the total length of the resulting string string = \u0026#34;Hello, World!\u0026#34; reversed_string = string[::-1] # 使用切片语法将字符串的顺序反过来 -\u0026gt; !dlroW ,olleH Linked Lists # A Linked List is a linear data structure consisting of nodes, where each node contains:\nA data part that stores the actual data. A next part (or pointer) that points to the next node in the list. 【Last Update: 2024-11-14】\n## A node in a linked list can be represented as a class class ListNode: def __init__(self, data=0, next=None): self.data = data # Data of the node self.next = next # Pointer to the next node ## Inserting Nodes def insert_at_beginning(head, data): new_node = ListNode(data) # Create a new node new_node.next = head # Link the new node to the current head return new_node # New node becomes the head ## Deleting Nodes def delete_from_beginning(head): if not head: return None return head.next # The second node becomes the new head ## Searching for a Node def search(head, key): current = head while current: if current.data == key: return True # Found the data current = current.next return False # Data not found Stack # A Stack is a linear data structure that stores items in a Last-In/First-Out (LIFO) or First-In/Last-Out (FILO) manner. In stack, a new element is added at one end and an element is removed from that end only.\npush(a) – Inserts the element ‘a’ at the top of the stack – Time Complexity: O(1) pop() – Deletes the topmost element of the stack – Time Complexity: O(1) Peek - View the top element without removing it. Empty - Check if the stack is empty. stack = [] # Push elements onto the stack stack.append(1) stack.append(2) # Pop element from the stack top = stack.pop() # Removes and returns 2 # Peek the top element top = stack[-1] if stack else None # Returns 1 # Check if the stack is empty is_empty = len(stack) == 0 Monotonic Stack (单调堆栈) # A monotonic stack is a specialized data structure used to solve problems involving arrays or sequences, particularly where you need to efficiently find next greater/smaller elements or previous greater/smaller elements. The stack is maintained in either increasing or decreasing order based on the problem requirements.\n使用堆栈存储数组的 indices 或 value。 通过在处理新元素时 popping 违反顺序的元素来保持单调性。 通常迭代数组一次（从左到右或从右到左）以实现所需的结果。 【Last Update: 2024-12-10】\nQueue # Queue is a linear data structure that stores items in First In First Out (FIFO) manner. With a queue the least recently added item is removed first.\nEnqueue: Adds an item to the queue. If the queue is full, then it is said to be an Overflow condition – Time Complexity : O(1) Dequeue: Removes an item from the queue. The items are popped in the same order in which they are pushed. If the queue is empty, then it is said to be an Underflow condition – Time Complexity : O(1) Peek: View the front element without removing it. Empty: Check if the queue is empty. 【Last Update: 2024-11-19】\nfrom collections import deque # Initialize a queue queue = deque() # Enqueue elements queue.append(1) queue.append(2) # Dequeue element front = queue.popleft() # Removes and returns 1 # Peek at the front element front = queue[0] if queue else None # Check if the queue is empty is_empty = len(queue) == 0 Deque # A deque is a generalized queue that allows insertion and deletion from both ends with O(1) complexity. Internally, it is implemented as a doubly linked list or a circular buffer.\n【Last Update: 2024-11-25】\nfrom collections import deque # Initialize a deque dq = deque() # Add elements dq.append(1) # Add to the right dq.appendleft(2) # Add to the left # Remove elements dq.pop() # Remove from the right dq.popleft() # Remove from the left # Access and manipulation dq.extend([3, 4]) # Add multiple elements to the right dq.extendleft([0, -1]) # Add multiple elements to the left (reversed order) dq.rotate(1) # Rotate elements right dq.rotate(-1) # Rotate elements left dq.clear() # Clear all elements Advanced Data Structures # Heap # A heap is a complete binary tree stored as an array. It maintains the heap property: in a min-heap, the parent is less than or equal to its children. Insertions and deletions are O(log n) due to the need to maintain the heap property.\nTwo main types: Min-Heap: The root node is the smallest, and every parent node is smaller than or equal to its children. Max-Heap: The root node is the largest, and every parent node is larger than or equal to its children. Root Node Access: Min-Heap: Root is the smallest element Max-Heap: Root is the largest element. Efficient Operations: Insert and delete both take O(log n). Maintains heap properties using adjustments (upward or downward shifts). 【Last Update: 2024-11-25】\nimport heapq # Initialize a heap heap = [] # Add elements heapq.heappush(heap, 3) # Push element into the heap heapq.heappush(heap, 1) heapq.heappush(heap, 4) # Access the smallest element smallest = heap[0] # Remove elements min_element = heapq.heappop(heap) # Pop the smallest element # Heapify an existing list nums = [4, 1, 7, 3] heapq.heapify(nums) # Get n largest or smallest elements largest = heapq.nlargest(2, nums) smallest = heapq.nsmallest(2, nums) Hash Tables # In Python, the built-in dict type (short for dictionary) functions as a hash table. Hash tables are a key data structure used for efficient data retrieval and storage, providing average time complexities of O(1) for insertion, deletion, and lookup operations due to their underlying hashing mechanism.\n【Last Update: 2024-11-06】\nmy_dict = {} # Creating an empty dictionary my_dict = {\u0026#39;key1\u0026#39;: \u0026#39;value1\u0026#39;, \u0026#39;key2\u0026#39;: \u0026#39;value2\u0026#39;} # Creating a dictionary with initial values value = my_dict[\u0026#39;key1\u0026#39;] # Accessing a value by key my_dict[\u0026#39;key3\u0026#39;] = \u0026#39;value3\u0026#39; # Adding a new key-value pair my_dict[\u0026#39;key2\u0026#39;] = \u0026#39;new_value2\u0026#39; # Updating an existing key-value pair del my_dict[\u0026#39;key1\u0026#39;] # Removing an entry by key value = my_dict.pop(\u0026#39;key2\u0026#39;) # Popping an entry (removes and returns the value) exists = \u0026#39;key3\u0026#39; in my_dict # # Checking if a key is in the dictionary [True] for key in my_dict: print(key, my_dict[key]) # Iterating through keys for key, value in my_dict.items(): # Iterating through key-value pairs print(key, value) for value in my_dict.values(): # Iterating through values print(value) # defaultdict 使用方法，没见过的元素不会报错。适用于计数、分组和嵌套字典等应用。 from collections import defaultdict # 使用 int 类型的 defaultdict dd = defaultdict(int) print(dd[\u0026#39;missing_key\u0026#39;]) # 输出：0，因为 int() 的默认值是 0 print(dd) # 输出：defaultdict(\u0026lt;class \u0026#39;int\u0026#39;\u0026gt;, {\u0026#39;missing_key\u0026#39;: 0}) # 统计元素出现次数 data = \u0026#34;abracadabra\u0026#34; counter = defaultdict(int) for char in data: counter[char] += 1 print(counter) # 输出：defaultdict(\u0026lt;class \u0026#39;int\u0026#39;\u0026gt;, {\u0026#39;a\u0026#39;: 5, \u0026#39;b\u0026#39;: 2, \u0026#39;r\u0026#39;: 2, \u0026#39;c\u0026#39;: 1, \u0026#39;d\u0026#39;: 1}) # defaultdict(list)常用于将多个值归类到同一个键下。 data = [(\u0026#34;apple\u0026#34;, 1), (\u0026#34;banana\u0026#34;, 2), (\u0026#34;apple\u0026#34;, 3), (\u0026#34;banana\u0026#34;, 4)] grouped_data = defaultdict(list) for fruit, count in data: grouped_data[fruit].append(count) print(grouped_data) # 输出：defaultdict(\u0026lt;class \u0026#39;list\u0026#39;\u0026gt;, {\u0026#39;apple\u0026#39;: [1, 3], \u0026#39;banana\u0026#39;: [2, 4]}) # 可以使用dict()将defaultdict转换为普通字典。 dd = defaultdict(int) dd[\u0026#39;a\u0026#39;] += 1 print(dict(dd)) # 输出：{\u0026#39;a\u0026#39;: 1} Tree # A tree is a hierarchical data structure with nodes connected by edges. The topmost node is the root, and nodes with no children are called leaves.\nBinary Tree: Each node has at most two children. Binary Search Tree (BST): A binary tree where the left child contains values less than the parent, and the right child contains values greater. Balanced Tree: A tree where the height difference between left and right subtrees of any node is minimal (e.g., AVL tree, Red-Black tree). Tree Traversals: Preorder Traversal (Root, Left, Right) Inorder Traversal (Left, Root, Right) Postorder Traversal (Left, Right, Root) ## Trees are often represented using classes. class TreeNode: def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = right ## Preorder Traversal (Root, Left, Right) def preorder_traversal(root): if root: print(root.val) preorder_traversal(root.left) preorder_traversal(root.right) ## Inorder Traversal (Left, Root, Right) def inorder_traversal(root): if root: inorder_traversal(root.left) print(root.val) inorder_traversal(root.right) ## Postorder Traversal (Left, Right, Root) def postorder_traversal(root): if root: postorder_traversal(root.left) postorder_traversal(root.right) print(root.val) ## Binary Search Tree (BST) Operations ## 1. Insert a Node def insert_into_bst(root, val): if not root: return TreeNode(val) if val \u0026lt; root.val: root.left = insert_into_bst(root.left, val) else: root.right = insert_into_bst(root.right, val) return root ## 2. Search for a Value def search_bst(root, val): if not root or root.val == val: return root if val \u0026lt; root.val: return search_bst(root.left, val) return search_bst(root.right, val) ## 3. Delete a Node def delete_node(root, key): if not root: return None if key \u0026lt; root.val: root.left = delete_node(root.left, key) elif key \u0026gt; root.val: root.right = delete_node(root.right, key) else: if not root.left: return root.right if not root.right: return root.left min_larger_node = root.right while min_larger_node.left: min_larger_node = min_larger_node.left root.val = min_larger_node.val root.right = delete_node(root.right, root.val) return root Core Algorithms # Overview # Two Pointer: The two-pointer technique is used primarily in solving array and linked list problems. It involves using two pointers to traverse the data structure, allowing for efficient searching and processing of elements. Sorting Algorithms: Review the mechanisms and use cases for quicksort, mergesort, and heapsort. Understand the trade-offs in terms of time and space complexity. Search Algorithms: Study binary search on sorted arrays, and learn about its variations for finding the first or last position of an element. Recursion and Backtracking: Understand how to apply recursion for solving problems involving permutations, combinations, and other backtrack-required scenarios. Study the call stack mechanism and how to optimize recursion through memoization. Prefix Sum and Suffix Sum: Prefix Sum and Suffix Sum are techniques used to compute the sum of elements in a subarray quickly by precomputing cumulative sums. Two Pointer # Finding Pairs with a Given Sum: When looking for two numbers in a sorted array that add up to a specific target. Reversing a String or Array: Using two pointers to swap elements from the start and end until they meet in the middle. Merging Two Sorted Arrays: Traversing both arrays simultaneously to create a new sorted array. Removing Duplicates from a Sorted Array: Using two pointers to track unique elements. 设置 two pointers 的时候，left 一般会在最前面，但是 right 不一定在最后，可以设置在 left 后面。 【Last Update: 2024-11-07】\nPrefix Sum and Suffix Sum # Prefix Sum: For an array nums, the prefix sum at each index i is the sum of all elements from the start of the array up to i. This allows you to find the sum of any subarray [i, j] in constant time by calculating prefix[j+1] - prefix[i]. Suffix Sum: For the same array nums, the suffix sum at index i is the sum of all elements from i to the end of the array. It enables efficient queries for sums of subarrays that start from any index i to a given end by using suffix[i] - suffix[j+1]. 【Last Update: 2024-11-11】\n## Input [1, 2, 3, 4] -\u0026gt; Output [2x3x4, 1x3x4, 1x2x4, 1x2x3] = [24, 12, 8, 6] ## Predix -\u0026gt; [0, 1, 1x2, 1x2x3] = [0, 1, 2, 6] ## Suffix -\u0026gt; [2x3x4, 3x4, 4, 0] = [24, 12, 4, 0] def productExceptSelf(self, nums: List[int]) -\u0026gt; List[int]: res = [1] * len(nums) prefix, suffix = 1, 1 for i in range(len(nums)): res[i] = prefix prefix *= nums[i] for j in range(len(nums)-1,-1,-1): res[j] *= suffix suffix *= nums[j] return res Binary Search # 二分查找（Binary Search）是一种高效的查找算法，主要用于在有序数组或其他有序结构中快速定位目标值。其核心思想是每次将搜索范围缩小一半，直到找到目标值或搜索范围为空。\n初始化左右指针 left 和 right，分别指向数组的起始和结束位置。 计算中间点索引 mid = left + (right - left) // 2，避免直接 (left + right) // 2 的溢出风险。 比较中间点值 arr[mid] 与目标值 target： 如果 arr[mid] == target，找到目标，返回 mid。 如果 arr[mid] \u0026lt; target，目标值在右侧，调整 left = mid + 1。 如果 arr[mid] \u0026gt; target，目标值在左侧，调整 right = mid - 1。 循环继续，直到 left \u0026gt; right。 【Last Update: 2024-12-18】\nIterative 版本 def binary_search(arr, target): left, right = 0, len(arr) - 1 while left \u0026lt;= right: mid = left + (right - left) // 2 if arr[mid] == target: return mid # 找到目标 elif arr[mid] \u0026lt; target: left = mid + 1 # 目标值在右侧 else: right = mid - 1 # 目标值在左侧 return -1 # 未找到目标 Recursive 版本 def binary_search_recursive(arr, target, left, right): if left \u0026gt; right: return -1 mid = left + (right - left) // 2 if arr[mid] == target: return mid elif arr[mid] \u0026lt; target: return binary_search_recursive(arr, target, mid + 1, right) else: return binary_search_recursive(arr, target, left, mid - 1) Recursion # 递归（Recursion）中一个函数会直接或间接调用自身来解决问题。它通常用于将问题分解为子问题的形式。\n问题结构: 问题可以分解为更小的子问题，且这些子问题具有相同的结构。 终止条件: 每个递归必须有基准情况（Base Case）以防止无限递归。 无“回溯”操作: 递归函数只关注每一层的子问题，不涉及撤销操作。 用途: 适用于分治问题（如二分搜索、归并排序）或递归定义的问题（如树的遍历）。 【Last Update: 2024-12-11】\ndef fibonacci(n): if n \u0026lt;= 1: return n # 基准情况 return fibonacci(n-1) + fibonacci(n-2) # 递归关系 Backtracking # 回溯（Backtracking）是一种试探性的搜索算法，它通过递归探索所有可能的解，并在发现某条路径不满足条件时，撤销（回溯）当前的选择并尝试其他路径。\n探索并撤销: 在探索某条路径时，如果发现不符合条件，会回退到上一步尝试其他可能。 约束条件: 通过加入剪枝（pruning）优化搜索，避免无意义的计算。 用途: 适用于需要生成所有解并验证解的正确性的问题。 【Last Update: 2024-12-11】\ndef permutations(nums): result = [] def backtrack(path, remaining): if not remaining: # 终止条件：所有元素已被使用 result.append(path) return for i in range(len(remaining)): backtrack(path + [remaining[i]], remaining[:i] + remaining[i+1:]) backtrack([], nums) return result Depth-First Search (DFS) # DFS 是一种递归（Recursive）或使用栈（Stack）实现的算法。它会优先深入访问一个分支，直到该分支无法继续，再回溯（Backtrack）到上一层继续访问未探索的分支。更适合用于解决路径、连通性和循环检测等问题。\n应用: 检测循环（Cycle Detection）：通过递归栈检测图中是否存在循环。 连通分量（Connected Components）：在无向图中找到所有连通分量。 路径搜索（Path Search）：在迷宫（Maze）中找到从起点到终点的一条路径。 【Last Update: 2024-12-10】\ndef dfs(graph, start, visited=None): if visited is None: visited = set() visited.add(start) for neighbor in graph[start]: if neighbor not in visited: dfs(graph, neighbor, visited) return visited graph = {0: [1, 2], 1: [0, 3], 2: [0, 4], 3: [1], 4: [2]} print(dfs(graph, 0)) # Output: {0, 1, 2, 3, 4} Breadth-First Search (BFS) # BFS 是一种逐层遍历（Level-Order Traversal）的算法，优先访问距离起点较近的节点。它使用队列（Queue）实现，以确保节点按照发现的顺序访问。更适合解决最短路径（Shortest Path）等问题。\n应用: 最短路径（Shortest Path）：在无权图（Unweighted Graph）中计算两点之间的最短路径。 层级关系（Level Order Traversal）：例如二叉树的层序遍历。 连通性检查（Connectivity Check）：检查从某节点是否可以到达所有其他节点。 【Last Update: 2024-12-10】\nfrom collections import deque def bfs_tree(root): if not root: return queue = deque([root]) while queue: node = queue.popleft() print(node.val, end=\u0026#34; \u0026#34;) # 访问当前节点 if node.left: queue.append(node.left) if node.right: queue.append(node.right) bfs_tree(root) # 输出: 1 2 3 4 5 Advanced Algorithms # Overview # Dynamic Programming: Explore the methodology of solving problems by breaking them down into smaller subproblems, storing results, and combining them to solve larger problems. Focus on understanding the concepts of overlapping subproblems and optimal substructure. Greedy Algorithms: Learn how greedy choices can lead to globally optimized solutions and their applications in problems like scheduling, graph based problems (like minimum spanning trees), and currency denomination. Graph Algorithms: Study shortest path algorithms (Dijkstra’s, Bellman-Ford) and minimum spanning tree algorithms (Prim’s, Kruskal’s). Understand their use cases and limitations. "},{"id":13,"href":"/docs/common-libraries/pandas/","title":"Pandas","section":"Common Libraries","content":" Pandas # "},{"id":14,"href":"/docs/machine-learning/supervised-learning/","title":"Supervised Learning","section":"Machine Learning","content":" 监督学习 # 监督学习方法（Supervised Learning） # 监督学习是机器学习中的一类算法，在这种方法中，模型通过已标注的数据进行训练。目标是让模型学会从输入特征（ \\(X\\) ）到输出标签（ \\(y\\) ）的映射关系。在监督学习中，训练数据由输入数据和对应的正确输出（标签）组成。这种方法通常用于分类和回归任务。\n本文件汇总了各种监督学习方法的概述。每种方法都在单独的页面中进行详细描述，涵盖算法的基本原理、应用场景以及理论基础。\n目录 # 线性回归 (Linear Regression) 逻辑回归 (Logistic Regression) K-近邻算法 (K-Nearest Neighbors) 支持向量机 (Support Vector Machines) 决策树 (Decision Trees) 随机森林 (Random Forests) 梯度提升机 (Gradient Boosting Machines) 朴素贝叶斯 (Naive Bayes) 学习目标 # 理解每种监督学习算法的基本概念、数学公式和实现原理 掌握算法在不同数据场景下的应用方法 理解各算法的优缺点、适用范围及如何选择合适的算法 使用说明 # 每种方法都在单独的页面中进行总结和详细描述。 目录部分链接到每个算法的详细页面。 为了便于理解，文件中包含了示例和代码片段。 "},{"id":15,"href":"/docs/deep-learning/generative-models/","title":"Generative Models","section":"Deep Learning","content":" Generative Models # "},{"id":16,"href":"/docs/common-libraries/pytorch/","title":"PyTorch","section":"Common Libraries","content":" PyTorch # "},{"id":17,"href":"/docs/machine-learning/unsupervised-learning/","title":"Unsupervised Learning","section":"Machine Learning","content":" Unsupervised Learning # "},{"id":18,"href":"/docs/machine-learning/regularization/","title":"Regularization","section":"Machine Learning","content":" 正则化（Regularization） # Regularization 是一种用于防止机器学习模型过拟合（overfitting）的技术。通过在损失函数中添加正则化项（regularization term）等技术，限制模型的复杂度，从而提高模型的泛化能力（generalization ability）。\nNote： 正则化通常只在模型的训练（training）阶段生效，不会直接影响 验证（validation）阶段和 推理（inference）阶段。\n参数范数惩罚（Parameter Norm Penalties） # 许多正则化方法都是通过向目标函数 \\(J\\) 添加参数范数惩罚 \\(\\Omega(\\theta)\\) 来限制模型（例如神经网络、线性回归或逻辑回归）的容量。我们将正则化的目标函数表示为 \\(\\tilde{J}\\) ：\n\\[ \\tilde{J}(\\theta;X,y) = J(\\theta;X,y) + \\lambda\\Omega(\\theta) \\\\ \\] 其中 \\(\\lambda \\in [0,\\infty)\\) 是一个超参数，它加权范数惩罚项 \\(\\Omega\\) 相对于标准目标函数 \\(J\\) 的相对贡献。将 \\(\\lambda\\) 设置为 0 会导致无正则化，使模型更关注数据拟合。较大的 \\(\\lambda\\) 值对应更多的正则化，从而限制模型复杂度。\n当我们的训练算法最小化正则化目标函数 \\(\\tilde{J}\\) 时，它将同时降低训练数据上的原始目标 \\(J\\) 和参数 \\(\\theta\\) （或参数的某些子集）大小的某些度量。参数范数 \\(\\Omega\\) 的不同选择可能导致不同的解决方案被优先考虑。\n直观理解参数范数正则化的作用 # 抑制模型复杂度，防止过拟合：\n参数范数正则化通过惩罚参数，限制了参数 \\(\\theta\\) 的大小，促使模型学习更小的权重。 大权重通常意味着模型对输入数据的小变化非常敏感，容易过拟合。 小权重会使模型输出更加平滑，对噪声不敏感，泛化能力更强。 这使得模型在高噪声或复杂数据下不会对数据点过度拟合。 大权重的危害：如果某个权重 \\(\\theta_i\\) 特别大，模型输出就会对输入 \\(x_i\\) 的微小变化极为敏感，训练数据的噪声会被放大，导致模型记住了训练集中的噪声（过拟合）。\n正则化后：通过惩罚项，正则化会迫使所有权重尽量小，让模型输出更平滑，对噪声更鲁棒。\nL2 正则化（L2 Regularization） # L2 正则化（Ridge 正则化）通过在损失函数中添加权重平方和的惩罚项，限制模型参数的大小，降低模型复杂度。公式如下： \\[ J(\\theta) = Loss(\\theta) + \\lambda\\lVert \\theta \\rVert_{2}^{2} = Loss(\\theta) + \\frac{\\lambda}{2} \\sum_{i=1}^n \\theta_i^2 \\] \\(\\lambda\\) ：正则化强度，权衡损失函数与正则化项的重要性。 \\(\\sum_{i=1}^n \\theta_i^2\\) ：模型权重的 L2 范数（欧几里得范数）。 正则化项前的 \\(\\frac{1}{2}\\) 是为了后续求导方便。 优化参数 \\(\\theta\\) 时，我们对目标函数 \\(J(\\theta) \\) 关于 \\(\\theta\\) 求偏导：\n\\[ \\frac{\\partial J(\\theta)}{\\partial \\theta} = \\nabla J(\\theta) + \\lambda \\theta \\] 之后将求导结果用于梯度下降法的参数更新规则：\n\\[ \\theta := \\theta - \\alpha \\left( \\nabla J(\\theta) + \\lambda \\theta \\right) \\] L2 正则化的优点： 平滑性更强: L2 正则化引入了一个约束，使得参数向量 \\(\\theta\\) 不会无限增大，最终可以提高模型在测试集上的表现。 普通模型的参数 \\(\\theta\\) 没有任何约束，模型可能会将一些特征的权重学得非常大，导致输出对输入变化非常敏感。 L2 正则化的模型参数 \\(\\theta\\) 被约束在一个较小的范围内，所有权重都会趋于较小的值，模型的输出曲线更加平滑。 解决多重共线性问题 降低模型的方差： 当输入特征高度相关时（多重共线性），普通的最小二乘法会导致模型参数的方差非常大。 参数范数正则化通过惩罚参数的大小，有效降低了模型的方差，方差大，意味着模型过拟合。虽然会略微增加模型的偏差（对训练集拟合得稍差），但整体的泛化误差（训练集与测试集之间的误差）会降低。 L2 正则化的缺点: 无法实现特征选择: L2 正则化会将权重缩小到接近 0，但不完全为 0，因此模型仍然保留所有特征。 L2 正则化的适用场景: 如果需要提升模型的 鲁棒性 和 泛化能力，并且数据是 低维非稀疏数据，优先选择 L2 正则化。 L2 正则化代码实现 # \u0026lt;--- From scratch ---\u0026gt; import numpy as np # 损失函数：均方误差（MSE） + L2 正则化 def compute_loss(X, y, theta, bias, lambda_reg): n_samples = X.shape[0] predictions = np.dot(X, theta) + bias mse_loss = (1 / (2 * n_samples)) * np.sum((predictions - y) ** 2) l2_penalty = (lambda_reg / 2) * np.sum(theta ** 2) # 正则化项 return mse_loss + l2_penalty # 梯度更新 def compute_gradients(X, y, theta, bias, lambda_reg): n_samples = X.shape[0] predictions = np.dot(X, theta) + bias error = predictions - y # 梯度计算 包含 L2 惩罚 d_theta = (1 / n_samples) * np.dot(X.T, error) + lambda_reg * theta d_bias = (1 / n_samples) * np.sum(error) return d_theta, d_bias L1 正则化（L1 Regularization） # L1 正则化（Lasso 正则化）通过在损失函数中添加权重绝对值和的惩罚项，限制模型参数的大小，同时可以实现特征选择的作用。公式如下： \\[ J(\\theta) = Loss(\\theta) + \\lambda\\lVert \\theta \\rVert_{1} = Loss(\\theta) + \\lambda \\sum_{i=1}^n |\\theta_i| \\] \\(\\lambda\\) ：正则化强度，权衡损失函数与正则化项的重要性。 \\(\\sum_{i=1}^n |\\theta_i|\\) ：模型权重的 L1 范数（曼哈顿范数）。 优化参数 \\(\\theta\\) 时，我们对目标函数 \\(J(\\theta) \\) 关于 \\(\\theta\\) 求偏导：\n\\[ \\frac{\\partial J(\\theta)}{\\partial \\theta_i} = \\nabla J(\\theta_i) + \\lambda \\cdot \\text{sign}(\\theta_i) \\] 其中 \\(\\text{sign}(\\theta_i)\\) 是符号函数，定义为： \\[ \\text{sign}(\\theta_i) = \\begin{cases} 1, \u0026 \\text{if } \\theta_i \u003e 0 \\\\ -1, \u0026 \\text{if } \\theta_i \u003c 0 \\\\ 0, \u0026 \\text{if } \\theta_i = 0 \\end{cases} \\] 参数更新规则（梯度下降法）为： \\[ \\theta := \\theta - \\alpha \\left( \\nabla J(\\theta) + \\lambda \\cdot \\text{sign}(\\theta) \\right) \\] L1 正则化的优点： 特征选择功能：L1 正则化会将部分不重要的特征权重缩小到 0，从而直接移除这些特征。对于高维稀疏数据（如文本分类、基因数据分析）效果显著。 模型可解释性: 由于部分权重为 0，模型变得更加简洁，方便解读哪些特征对预测结果最重要。 L1 正则化的缺点: 参数稀疏性可能损失重要信息: 对于相关性较高的特征，L1 可能随机选择部分特征置零，而丢失其他有价值的信息。 优化复杂性: L1 正则化的目标函数由于绝对值的非连续性，可能在优化时收敛较慢。 L1 正则化的适用场景: 高维稀疏数据: 如文本分类或图像处理中的稀疏特征。 需要特征选择: 适合在数据预处理阶段对重要特征进行筛选。 L1 正则化代码实现 # \u0026lt;--- From scratch ---\u0026gt; import numpy as np # 损失函数：均方误差（MSE） + L1 正则化 def compute_loss(X, y, theta, bias, lambda_reg): n_samples = X.shape[0] predictions = np.dot(X, theta) + bias mse_loss = (1 / (2 * n_samples)) * np.sum((predictions - y) ** 2) l1_penalty = lambda_reg * np.sum(np.abs(theta)) # 正则化项（L1 范数） return mse_loss + l1_penalty # 梯度更新 def compute_gradients(X, y, theta, bias, lambda_reg): n_samples = X.shape[0] predictions = np.dot(X, theta) + bias error = predictions - y # 梯度计算 包含 L1 惩罚 d_theta = (1 / n_samples) * np.dot(X.T, error) + lambda_reg * np.sign(theta) d_bias = (1 / n_samples) * np.sum(error) return d_theta, d_bias 数据增强（Dataset Augmentation） # 数据增强是一种通过对原始数据进行变换或扩展，生成额外的训练样本的方法，旨在提升模型的鲁棒性和泛化能力。它在训练数据不足、分布偏差或过拟合等问题上尤为有效，特别是在深度学习任务（如计算机视觉和自然语言处理）中，数据增强常被用作关键的预处理技术。数据增强的核心目的有：\n增加数据多样性：在保持原始数据标签不变的前提下，生成不同的样本，模拟现实中数据的潜在变化。 减少过拟合风险：通过增加训练样本，降低模型对原始训练数据的记忆，提高模型的泛化性能。 提升模型鲁棒性：让模型更好地适应数据的扰动和不确定性，例如噪声或旋转等变化。 图像数据增强 # 图像数据增强方法通常直接作用于图像像素，以生成不同变体。常见技术包括：\n几何变换：旋转（Rotation），翻转（Flip），缩放（Scaling），平移（Translation），剪裁（Cropping）。 颜色变换：亮度调整（Brightness Adjustment），对比度调整（Contrast Adjustment），色调调整（Hue Adjustment），饱和度调整（Saturation Adjustment）。 添加噪声：高斯噪声（Gaussian Noise），盐噪声与椒噪声（Salt-and-Pepper Noise）。 文本数据增强 # 文本数据的增强方法相较于图像更加复杂，因为文本的语法和语义需要保持一致。常用技术包括：\n同义词替换（Synonym Replacement）：替换文本中部分单词为同义词，例如将 “happy” 替换为 “joyful”。 随机插入（Random Insertion）：向句子中随机插入同义词或相关词汇。 随机删除（Random Deletion）：随机删除句子中的某些单词。 回译（Back Translation）：将句子翻译成另一种语言，再翻译回原语言，生成语义一致但表述不同的句子。 句法变换（Syntactic Transformations）：改变句子的语法结构，例如主动语态转被动语态。 时间序列数据增强 # 针对时间序列数据（如音频信号、传感器数据、股票数据等），常用方法包括：\n随机噪声（Random Noise）：在时间序列信号中加入少量随机噪声。 时间偏移（Time Shifting）：将时间序列信号整体向前或向后移动。 插值与采样（Interpolation and Sampling）：对原始序列进行上下采样或插值。 窗口切片（Window Slicing）：随机选择时间序列的一部分作为新样本。 噪声注入（Noise Injection） # Noise Injection 是一种在机器学习模型训练过程中有意加入噪声的技术，旨在增强模型的鲁棒性、泛化能力以及避免过拟合。噪声注入的核心思想是通过扰动输入数据、模型参数、或隐藏层的激活值，使模型更能适应训练数据中的噪声和不确定性，从而提高其对真实数据的适应能力。\n常见的噪声注入方法 # 输入数据中的噪声注入：在训练过程中，直接对输入数据进行扰动。例如： 对数值型输入数据添加随机噪声： \\[ x' = x + \\mathcal{N}(0, \\sigma^2) \\] 其中， \\(\\mathcal{N}(0, \\sigma^2)\\) 是均值为 0，方差为 \\(\\sigma^2\\) 的高斯噪声。 对图像数据应用随机变换，例如： 随机裁剪、旋转、翻转、亮度调节等。 使用像素级别的随机扰动（例如，加盐噪声或椒盐噪声）。 作用： 使模型更能适应输入数据的多样性，降低过拟合风险。 提升对输入数据中的小扰动（如测量误差）的鲁棒性。 参数中的噪声注入：对模型的参数（例如权重）添加随机噪声： 对参数添加噪声：在每次梯度更新后，对模型的权重或偏置值添加噪声： \\[ W' = W + \\mathcal{N}(0, \\sigma^2) \\] 作用： 防止模型权重过度依赖某些特定的参数值。 增强模型对权重初始化的鲁棒性。 Note： 参数中的噪声注入 直接作用在模型的参数上，通常是权重更新过程中加入的随机扰动，改变了参数的优化路径，使得训练过程更加稳健。\n而隐藏层中的噪声注入，噪声被直接加入到隐藏层的激活值（神经元的输出）中，会使得每一次前向传播的输出不同，但不改变权重值本身，从而增强模型对输入数据的变化或中间表示不确定性的鲁棒性。\n隐藏层中的噪声注入：在神经网络的隐藏层中对激活值进行随机扰动： Dropout 是一种常用的噪声注入方法，在每次前向传播时，随机将隐藏层的某些神经元置为 0（断开连接），以防止神经网络对特定神经元的过度依赖。详见 Dropout 添加噪声：在隐藏层激活值中加入高斯噪声： \\[ h{\\prime} = h + \\mathcal{N}(0, \\sigma^2) \\] 作用： 加入随机噪声可以模拟训练中的不确定性，使模型更加稳健。 标签中的噪声注入: 有时也会对标签进行扰动（Label Smoothing）： 将离散标签的 “硬边界” 分布转化为 “软边界” 分布，例如，将类别标签从 [0, 1] 转变为 [0.1, 0.9]。 作用： 防止模型对训练数据中的标签过于自信（即减少过拟合）。 提升模型对噪声标签的鲁棒性。 正则化在神经网络中的应用 # Weight Decay # Weight Decay 是一种用于正则化机器学习模型的技术，其核心思想是通过向损失函数中添加正则化项来约束模型权重的大小，从而防止过拟合。它的本质是 L2 正则化 的一种实现方式。Weight Decay 的目标是减少模型权重过大的情况，这样可以使模型更加简单、泛化能力更强。通过在优化过程中对权重进行衰减（decay），限制其无限增大。引入 Weight Decay 后，损失函数为： \\[ L_{\\text{total}}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(f(x_i; \\theta), y_i) + \\lambda \\|\\theta\\|_2^2 \\] 其中：\n\\(\\mathcal{L}(\\cdot)\\) ：是预测值和真实值之间的损失（例如均方误差或交叉熵）。 \\(\\|\\theta\\|_{2}^2 = \\sum_{j=1}^m \\theta_j^2\\) ：表示所有权重参数的平方和（即 L2 范数的平方）。 \\(\\lambda\\) ：正则化系数（Weight Decay 参数），控制权重的惩罚力度。 在优化过程中，Weight Decay 实现了以下更新规则： \\[ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\partial L}{\\partial \\theta_t} - \\eta \\lambda \\theta_t \\] Note： Weight Decay 是 L2 正则化的具体实现，但在优化器实现时，它直接作用于梯度更新规则，而不需要显式地将正则化项加到损失函数中。两者的效果基本等价，但 Weight Decay 的实现方式更简洁，适合深度学习优化器的需求。\nWeight Decay 代码实现 # \u0026lt;--- From Pytorch ---\u0026gt; import torch import torch.nn as nn import torch.optim as optim # 模型定义 model = nn.Linear(10, 1) # 损失函数 criterion = nn.MSELoss() # 优化器 optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01) Early Stopping # Early Stopping 是一种防止模型过拟合的正则化方法。通过在验证集性能不再提升时提前终止训练，可以有效减少模型的过拟合并提高泛化能力。它的工作原理是：\n分割数据集：将数据集划分为训练集、验证集 和测试集。 训练监控：在训练过程中，同时监控训练损失和验证损失。通常，随着训练的进行： 训练损失持续降低。 验证损失先降低，随后可能开始上升（表示过拟合）。 停止条件： 当验证损失连续多个 epoch 不再降低（或验证性能不再提升）时，提前终止训练。 模型停止在验证性能最佳的那一刻，而不是继续训练到指定的最大 epoch 数。 关键步骤：\n监控指标：通常监控验证损失（如均方误差、交叉熵损失等）。 耐心（Patience）机制：如果验证损失在设定的耐心步数（patience steps）内没有改善，停止训练。 保存最佳模型：在每次验证性能提升时保存当前模型的参数（checkpoint）。训练结束时，恢复到性能最佳时的模型。 Weight Decay 代码实现 # \u0026lt;--- From Pytorch ---\u0026gt; import torch class Early_Stopping: def __init__(self, patience=5, delta=0.0, path=\u0026#39;checkpoint.pt\u0026#39;, verbose=False): \u0026#34;\u0026#34;\u0026#34; 参数： - patience: 允许验证损失不降低的连续 epoch 数 - delta: 最小的验证损失降低幅度，避免浮动引起的误判 - path: 保存最佳模型的文件路径 - verbose: 是否输出日志信息 \u0026#34;\u0026#34;\u0026#34; self.patience = patience self.delta = delta self.path = path self.verbose = verbose self.counter = 0 self.best_loss = float(\u0026#39;inf\u0026#39;) self.early_stop = False def __call__(self, val_loss, model): if val_loss \u0026lt; self.best_loss - self.delta: self.best_loss = val_loss self.counter = 0 self.save_checkpoint(val_loss, model) else: self.count += 1 if self.counter \u0026gt;= self.patience: self.early_stop = True def save_checkpoint(self, val_loss, model): \u0026#34;\u0026#34;\u0026#34;保存当前最佳模型\u0026#34;\u0026#34;\u0026#34; if self.verbose: print(f\u0026#34;Validation loss improved to {val_loss:.4f}. Saving model...\u0026#34;) torch.save(model.state_dict(), self.path) model = SimpleModel(input_dim=10) criterion = nn.MSELoss() optimizer = optim.Adam(model.parameters(), lr=0.01) # 实例化 Early Stopping early_stopping = EarlyStopping(patience=5, verbose=True, path=\u0026#39;best_model.pt\u0026#39;) # 训练循环 num_epochs = 100 for epoch in range(num_epochs): # 训练阶段 model.train() for batch_X, batch_y in train_loader: optimizer.zero_grad() outputs = model(batch_X) loss = criterion(outputs, batch_y) loss.backward() optimizer.step() # 验证阶段 model.eval() val_loss = 0.0 with torch.no_grad(): for batch_X, batch_y in val_loader: outputs = model(batch_X) loss = criterion(outputs, batch_y) val_loss += loss.item() val_loss /= len(val_loader) # 调用 Early Stopping early_stopping(val_loss, model) # 判断是否停止训练 if early_stopping.early_stop: print(\u0026#34;Early stopping triggered. Restoring the best model...\u0026#34;) model.load_state_dict(torch.load(\u0026#39;best_model.pt\u0026#39;)) break Dropout # Dropout 是一种广泛用于深度学习的正则化技术，旨在通过减少神经网络的过拟合来提高模型的泛化能力。它的核心思想是在训练过程中随机地“丢弃”一些神经元，使得网络在每一次前向传播和反向传播中都使用不同的结构。这种随机性强迫网络在不同的子网络上训练，从而增强其鲁棒性。\n训练阶段： 对于每一层的每个神经元，按照一个固定的概率 \\(p\\) （通常在 0.5 左右）随机将其“丢弃”。 被丢弃的神经元不会参与前向传播和反向传播，其输出被设置为 0。 未被丢弃的神经元的输出会被放大为 \\(\\frac{1}{1-p}\\) 倍，以保持激活值的期望不变。 测试阶段： Dropout 不会应用在测试阶段，因为网络需要全量的神经元进行预测。 为了与训练阶段保持一致，神经元的输出按比例缩放，即保持其原始激活值。 为什么 Dropout 有效？\n防止过拟合： Dropout 强迫网络不会过分依赖某些特定的神经元或路径，而是学会利用多个不同的路径。 通过这种方式，网络在面对噪声或新数据时更具鲁棒性。 类似于模型集成： 训练时，Dropout 在每次训练的前向传播中随机丢弃一些神经元，等价于在每次迭代中训练一个从完整网络中抽取的“子网络”。不同的“子网络”共享参数并被优化，最终可以看作是训练了大量的子模型。 每次迭代训练的是网络的一个随机子集，相当于训练了一些弱模型。 测试时，因为 Dropout 在训练时让模型学到了很多不同的权重组合和特征表示，所以即使在测试时每次计算时都用到了完整的神经元结构，最终的预测也会综合了所有子网络可能学习到的信息。通过对所有神经元的输出进行缩放（保留激活值的期望不变），等价于对所有训练过的子网络的预测进行平均。 训练时，丢弃掉的神经元会导致模型在每次训练时只用到部分神经元，因此每个神经元的输出会被缩小，它的输出期望值等于 \\(1 - p\\) 。 在测试时，Dropout 不再丢弃任何神经元，所有的神经元都参与计算。为了保持 训练时和测试时的输出期望一致，我们需要在测试时对每个神经元的输出进行缩放。缩放因子是 ( \\(1 - p\\) ) ，因为训练时每个神经元输出的期望是 \\((1 - p) \\cdot x\\) ，我们通过缩放使得测试时的输出期望保持一致。 Dropout 代码实现\n在 PyTorch 中，神经网络模型有两种模式：训练模式（training）和推理模式（evaluation）。可以通过 model.train() 和 model.eval() 来切换模型的模式。在训练时，Dropout 会启用，而在推理时，Dropout 会禁用并自动调整输出。\n# \u0026lt;--- From Pytorch ---\u0026gt; import torch import torch.nn as nn # 定义一个简单的神经网络，包含一个 Dropout 层 class SimpleNN(nn.Module): def __init__(self, p=0.5): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) # 输入层到隐藏层 self.fc2 = nn.Linear(50, 1) # 隐藏层到输出层 self.dropout = nn.Dropout(p) # Dropout 层，丢弃概率为 p def forward(self, x): x = self.fc1(x) x = torch.relu(x) x = self.dropout(x) # 在激活之后应用 Dropout x = self.fc2(x) return x # 创建一个简单的神经网络实例 model = SimpleNN(p=0.5) # 训练模式 model.train() input_data = torch.randn(32, 10) # 假设输入有32个样本，每个样本有10个特征 output_train = model(input_data) # 训练时，Dropout 会被应用 # 推理模式 model.eval() # 切换到推理模式 output_eval = model(input_data) # 推理时，Dropout 会被禁用 Batch Normalization # Batch Normalization (BN) 是一种深度学习中常用的正则化和加速训练的方法，最早由 Sergey Ioffe 和 Christian Szegedy 在 2015 年提出。其主要目的是通过减少输入特征的分布变化（Internal Covariate Shift）来稳定训练过程，并加快神经网络的收敛速度。\nBatch Normalization 在每一层网络的激活值（或输入特征）上进行归一化。通过对小批量数据（Batch）的统计信息进行归一化，将激活值标准化为具有零均值和单位方差的分布，同时保留一个可学习的缩放参数和偏移参数来恢复模型的表达能力。它的公式可以表达为：\n计算 Batch 均值和方差：对当前小批量的输入 \\(\\mathbf{x} = [x_1, x_2, \\ldots, x_m]\\) 计算均值和方差： \\[ \\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i, \\quad \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2 \\] 其中 \\(m\\) 是 batch 的大小。\n归一化：使用均值和方差对输入进行标准化： \\[ \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\] 其中 \\(\\epsilon\\) 是一个很小的值，用于避免分母为零。\n缩放和平移：为了保留模型的表达能力，引入两个可学习参数：缩放参数 \\(\\gamma\\) 和偏移参数 \\(\\beta\\) 。归一化后，输出为： \\[ y_i = \\gamma \\hat{x}_i + \\beta \\] 这里， \\(\\gamma\\) 和 \\(\\beta\\) 通过反向传播学习（ \\(\\frac{\\partial L}{\\partial \\gamma}\\) , \\(\\frac{\\partial L}{\\partial \\beta}\\) ），可以调整标准化后的分布，使其适应当前任务。优化器（如 SGD、Adam）会根据学习率和梯度更新 \\(\\gamma\\) 和 \\(\\beta\\) ：\n虽然归一化操作能稳定训练，但直接使用这种标准化后的数据可能会限制模型的表达能力。例如，某些任务可能需要激活值在特定范围，而不是被严格限制在零均值和单位方差。缩放和平移允许 BN 后的数据分布与任务需求一致，而不仅仅局限于零均值单位方差。\n它的作用有：\n加速训练：归一化使得梯度传播更加平滑，减少了梯度爆炸或消失问题，使得训练可以使用更高的学习率，加快收敛速度。 正则化效果：小批量数据的均值和方差会产生一定的随机性，这种扰动类似于 Dropout 的正则化效果，有助于减少过拟合。 不同小批量会导致略微不同的归一化结果，产生了噪声。这种随机性迫使模型不能过分依赖特定特征或特定路径，对训练数据的微小变化（如噪声或扰动）不敏感，从而降低了过拟合风险。 更深的网络更稳定：BN 的引入让深度模型（如 ResNet 等）更容易训练，避免梯度消失和参数更新不稳定的问题。 由于每一层的输入在每个小批量上被归一化，模型在训练时变得更加稳定。 Note： 通常在线性变换之后、激活函数之前使用 Batch Normalization。\nBatch Normalization 在优化（Optimization）问题上的具体细节详见 优化章节的 Batch Normalization。\nBatch Normalization 代码实现 # \u0026lt;--- From Pytorch ---\u0026gt; import torch import torch.nn as nn # 定义模型 class ModelWithBN(nn.Module): def __init__(self): super(ModelWithBN, self).__init__() self.fc1 = nn.Linear(10, 50) self.bn1 = nn.BatchNorm1d(num_features=50) # Batch Normalization self.fc2 = nn.Linear(50, 20) self.bn2 = nn.BatchNorm1d(num_features=20) # Batch Normalization self.relu = nn.ReLU() def forward(self, x): x = self.relu(self.bn1(self.fc1(x))) x = self.relu(self.bn2(self.fc2(x))) return x Note: \\(\\gamma\\) 和 \\(\\beta\\) 在 BatchNorm 中自动管理。\n集成学习算法（Ensemble Methods） # 详见 集成学习算法。\n对抗训练（Adversarial Training） # Adversarial Training（对抗训练）是一种增强模型鲁棒性、提升其对抗样本（adversarial examples）抵抗力的训练方法。在深度学习中，对抗样本是经过精心设计，旨在迷惑模型的输入数据。对抗训练通过将这些对抗样本添加到训练集中，使得模型在学习过程中能够处理这些干扰并增强其泛化能力。\n对抗样本的概念 # 对抗样本是指那些通过对原始输入数据添加微小但精心设计的扰动（通常是通过优化算法生成），使得模型产生错误预测的数据点。尽管这些扰动对于人眼来说几乎不可察觉，但却能够显著改变模型的输出。\n举个例子：假设我们训练一个图片分类模型，模型可能把一张猫的图片正确分类为“猫”。但通过对图片添加微小的噪声，模型可能将其错误分类为“狗”。这个微小的噪声就是对抗样本。\nNote： Adversarial Training 中的噪声是 针对模型的弱点设计的对抗噪声。这些噪声是通过优化过程生成的，目的是让模型在对抗样本中犯错。因此，噪声的设计是 精心的、具体的，每个样本的扰动都是有目的地用来欺骗模型的。\nNoise Injection 中的噪声通常是 随机的、不具目标性，并不是有意识地去迷惑模型。它通常是简单的随机噪声，直接加到输入数据、隐藏层或权重中。其目的是通过扰动来防止模型过拟合，提高模型的泛化能力。\n对抗训练的过程 # 在对抗训练中，我们通过以下步骤来训练模型，使其对抗样本具有鲁棒性：\n生成对抗样本：首先，生成对抗样本。对抗样本是通过对输入数据施加一定的扰动使得模型产生错误预测。常见的生成对抗样本的方法包括：\nFast Gradient Sign Method (FGSM)：使用模型的梯度信息来生成对抗样本。通过计算损失函数对输入数据的梯度，并按梯度方向添加一个小的扰动。在每一次正向传播（Forward Pass）和反向传播（Backward Pass）后，计算损失函数相对于输入样本 \\(x\\) 的梯度，即 \\(\\nabla_x \\mathcal{L}(\\theta, x, y)\\) ，根据计算得到的输入梯度，通过扰动样本 \\(x\\) 来生成对抗样本 \\(x_{\\text{adv}}\\) ： \\[ x_{\\text{adv}} = x + \\epsilon \\cdot \\text{sign}(\\nabla_x \\mathcal{L}(\\theta, x, y)) \\] 其中， \\(\\epsilon\\) 是扰动的大小，通常是一个小的常数，控制扰动的幅度。 结合对抗样本与正常样本进行训练：使用生成的对抗样本 \\(x_{\\text{adv}}\\) 进行正向传播，并计算损失函数。然后使用对抗样本来计算损失函数相对于模型参数的梯度，并进行参数更新。Adversarial Training 的最终损失函数通常是 原始损失 和 对抗损失 的加权和。具体的损失函数形式可以写成： \\[ \\mathcal{L}_{\\text{total}} = \\mathbb{E}_{x,y} \\left[ \\mathcal{L}(\\theta, x, y) \\right] + \\lambda \\cdot \\mathbb{E}_{x,y} \\left[ \\mathcal{L}(\\theta, x + \\delta, y) \\right] \\] 其中：\n\\(\\mathcal{L}(\\theta, x, y)\\) 是标准的原始损失，通常是交叉熵损失（或其他损失函数），用于普通训练样本。 \\(\\mathcal{L}(\\theta, x + \\delta, y)\\) 是在对抗样本 \\(x + \\delta\\) 上计算的损失。 \\(\\lambda\\) 是一个超参数，用于平衡标准损失和对抗损失的贡献。 优化训练过程：通过常规的优化方法（如梯度下降），模型会学习如何在对抗样本中保持较高的准确性。这通常会导致模型对潜在的攻击具有更强的抵抗能力。\n"},{"id":19,"href":"/docs/machine-learning/optimization/","title":"Optimization","section":"Machine Learning","content":" 优化（Optimization） # 机器学习和深度学习中的 优化问题（Optimization） 指的是在给定的模型结构和数据集下，通过调整模型的参数，使目标函数（Objective func-tion, or criterion）达到最小化 或最大化的过程。目标函数通常衡量模型预测值与真实值之间的偏差，例如均方误差或交叉熵。在优化过程中，参数更新依赖于目标函数对参数的梯度信息，通过迭代计算逐步逼近最优解。\n凸优化（Convex Optimization） 是指目标函数为凸函数的优化问题，凸函数满足以下性质： 任意两点之间的连线上的函数值不会超过这两点的函数值。 数学形式：对于任意 \\(x_1, x_2 \\in \\mathbb{R}^n\\) 和 \\(\\theta \\in [0, 1]\\) ，有 \\[ f(\\theta x_1 + (1-\\theta)x_2) \\leq \\theta f(x_1) + (1-\\theta)f(x_2) \\] 目标函数特点: 单一的全局最优解（Global Minimum）。 常见目标函数形式：二次函数（如 \\(f(x) = x^2\\) ）、对数函数、指数函数等。 使用简单的梯度下降或更高效的二阶方法（如牛顿法）即可快速收敛。 非凸优化（Non-Convex Optimization） 非凸优化是指目标函数为非凸函数的优化问题，非凸函数可能存在多个局部最优解（Local Minimum），不满足凸函数的性质。 目标函数特点: 通常为复杂的多峰形状，可能存在多个局部最优解、鞍点甚至平坦区域。 深度学习中的损失函数（如交叉熵、均方误差）大多属于此类。 解决策略: 启发式算法: 使用随机梯度下降（SGD）及其变种（如 Adam、RMSProp）通过随机性帮助跳出局部最优解。 正则化技巧: 添加 L1/L2 正则项以平滑损失函数，减少极值点的数量。 预训练与迁移学习: 通过初始化参数靠近全局最优区域来提高收敛效率。 基于梯度的优化（Gradient-Based Optimization） # 在函数 \\(y = f(x)\\) 中（其中 \\(x\\) 和 \\(y\\) 都是实数），导数 \\(f'(x)\\) （或者表示为 \\(\\frac{\\partial y}{\\partial x}\\) ） 表示函数在点 \\(x\\) 处的斜率（Slope）。它描述了输入 \\(x\\) 的一个微小变化如何引起输出 \\(y\\) 的相应变化，用以下公式近似表示： \\[ f(x + \\epsilon) \\approx f(x) + \\epsilon f'(x) \\] 导数在函数优化中非常有用，因为它指示了如何调整 \\(x\\) 以使 \\(y\\) 取得小幅改善。例如，当我们沿着导数的反方向移动时，函数值会减小，即： \\[ f(x - \\epsilon \\cdot \\text{sign}(f'(x))) \u003c f(x) \\] 这种基于导数的优化技术被称为梯度下降法（Gradient Descent）。\n关键概念 # 临界点与极值： 当 \\(f'(x) = 0\\) 时，称为临界点（Critical points,或者 stationary points）。 局部极小值（Local minimum）：周围点中 \\(f(x)\\) 最小。 局部极大值（local maximum）：周围点中 \\(f(x)\\) 最大。 鞍点（Saddle points）：既非局部极小值也非局部极大值的临界点，其中正交方向的斜率（导数）全部为零（临界点），但不是函数的局部极值。在多维空间中，不必具有 0 的特征值才能得到鞍点：只需具有正和负的特征值即可。 全局极小值（Global minimum）：函数 \\(f(x)\\) 在整个定义域内的最小值。 平坦区域（Plateaus）：平坦区域是指目标函数的梯度几乎为零的区域，网络在这些区域中移动缓慢，几乎没有任何有效的方向引导优化过程。 悬崖结构区域（Cliffs）：悬崖结构区域指的是目标函数中梯度变化非常剧烈的区域，即梯度几乎呈现出极为陡峭的下降趋势。在这种区域内，损失函数对于某些参数的变化非常敏感，导致小的参数更新可能引起损失值的剧烈波动。 多维输入优化：当函数有多个输入（ \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) ），优化需要使用梯度（Gradient）的概念。梯度是包含所有偏导数的向量，定义为： \\[ \\nabla_x f(x) = \\left[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n}\\right] \\] 在多维空间中，临界点是所有梯度分量为零的点，即 \\(\\nabla_x f(x) = 0\\) 。 学习率的选择：学习率 \\(\\epsilon\\) 决定了每一步更新的幅度，常见选择方式包括： 固定的小常数。 使用线搜索（Line Search），在多种步长中选择目标函数值最小的步长。 梯度下降的收敛：当梯度的所有梯度的分量接近零时，梯度下降算法收敛。实际上，由于优化问题的复杂性，尤其是在深度学习中，我们通常寻找“足够低”的函数值，而非严格的全局极小值。 全批量梯度下降（Batch Gradient Descent） # Batch Gradient Descent 是一种优化算法，每次使用整个数据集来计算目标函数的梯度，并基于梯度更新模型参数。这种方法适用于目标函数是所有样本损失的平均值或总和的情况。每次迭代计算所有训练样本的损失函数梯度。公式为： \\[ \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) \\] 其中， \\(\\nabla J(\\theta)\\) 是基于整个数据集的梯度。\n工作流程一般为：\n初始化模型参数 \\(\\theta\\) 为随机值或设定初值。 重复以下步骤，直到达到停止条件（如梯度足够小或迭代次数用尽）： 计算当前所有样本的目标函数值和梯度 \\(\\nabla J(\\theta)\\) 使用梯度更新模型参数： \\[ \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) \\] 输出最终优化的参数。 优点： 精确性高：每次更新都基于完整的数据集，提供了目标函数梯度的精确估计，使得更新过程稳定可靠。 容易收敛到局部或全局最优：因为梯度估计噪声较小，参数更新方向更明确。在凸优化问题中，Batch Gradient Descent 的收敛轨迹通常表现为朝向最优解的一条平滑路径，梯度的更新方向明确，不会因为噪声而偏离轨道。但是由于梯度计算使用了整个数据集，优化轨迹通常稳定地沿着梯度方向下降，容易陷入一个局部最优点或停留在鞍点上。 缺点： 计算资源消耗大：每次迭代需要对整个数据集计算梯度，在数据量大时计算成本高，不适合分布式或在线训练。 存储限制：对于大规模数据集，可能需要更多的内存或存储资源来一次性加载数据。 收敛速度慢：尤其在每次迭代中，如果数据集中样本的梯度信息存在冗余，则更新过程可能很低效。 全批量梯度下降（Batch Gradient Descent）代码实现 import numpy as np # 定义目标函数及其梯度 def loss_function(w, data): \u0026#34;\u0026#34;\u0026#34;目标函数: f(w) = 0.5 * (w - 3)^2\u0026#34;\u0026#34;\u0026#34; return 0.5 * np.sum((data - w) ** 2) def gradient(w, data): \u0026#34;\u0026#34;\u0026#34;目标函数的梯度: f\u0026#39;(w) = w - 3\u0026#34;\u0026#34;\u0026#34; return np.sum(data - w) # Batch Gradient Descent 优化过程 for epoch in range(epochs): # 计算梯度 grad = gradient(w, data) # 更新参数 w -= learning_rate * grad / len(data) # 除以数据集大小，以计算平均梯度 # 记录损失值 loss = loss_function(w, data) 随机梯度下降（Stochastic Gradient Descent） # SGD是一种优化算法，用于通过梯度下降更新模型参数，以最小化损失函数。它的核心思想是在每次迭代中，随机选择一个样本计算梯度，而不是使用整个数据集。这种方式极大地降低了每次更新的计算成本。更新公式为： \\[ \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta; x_i, y_i) \\] 优点： 高效性：单次梯度计算只涉及一个样本，计算速度快，对内存需求低。 跳出局部最优解：随机梯度的噪声可以避免陷入平滑函数中的局部最优点（local minima），尤其适合非凸优化问题。 在线学习能力：在模型需要不断更新时（如实时场景），SGD可以随着数据流实时调整参数。 Note： 随机梯度下降在每次迭代中仅使用一个样本计算梯度，因此梯度估计会带有噪声。这种噪声主要表现为梯度方向的不确定性，使得优化过程中的参数更新具有一定的随机性和波动性。随机噪声使得优化路径不完全按照损失函数表面的梯度方向前进，而是以一种“抖动”的方式探索参数空间。当优化路径接近某个局部最优点时，全批量梯度可能因所有样本的梯度方向一致而停留在该点；而随机梯度的波动可能使路径偏离局部最优，继续搜索全局最优解。\n在深度学习中的目标函数通常具有非凸性质，随机梯度的噪声可以帮助模型训练找到性能更优的解，从而避免陷入次优状态。此外研究表明，在高维空间中，随机梯度的波动尤其有助于突破鞍点，因为鞍点在高维空间中比局部最优点更常见。\n缺点： 梯度估计噪声大：由于每次迭代仅基于单个样本，梯度方向可能偏离真正的最优方向，导致优化过程不稳定。 收敛速度慢：需要更多迭代次数才能达到较优解，与Batch Gradient Descent相比，收敛速度可能较慢。 对学习率敏感：不适当的学习率可能导致震荡或过早停止收敛，往往可能需要更小的学习率（ \\(\\eta \\) ）或动态调整以避免振荡。因此学习率需要仔细调节或动态调整。 随机梯度下降（Stochastic Gradient Descent）代码实现 import numpy as np # 定义目标函数及其梯度 def loss_function(w): \u0026#34;\u0026#34;\u0026#34;目标函数: f(w) = 0.5 * (w - 3)^2\u0026#34;\u0026#34;\u0026#34; return 0.5 * (w - 3) ** 2 def gradient(w): \u0026#34;\u0026#34;\u0026#34;目标函数的梯度: f\u0026#39;(w) = w - 3\u0026#34;\u0026#34;\u0026#34; return w - 3 # SGD 优化过程 for epoch in range(epochs): # 随机选取数据点 idx = np.random.choice(len(data), batch_size=1, replace=False) x_sample = data[idx] # 计算梯度 grad = gradient(w) # 更新参数 w -= learning_rate * grad # 记录损失值 loss = loss_function(w) 小批量随机梯度下降（Minibatch Stochastic Gradient Descent） # Minibatch SGD 是一种在每次迭代中使用一小批数据（称为 Minibatch）计算梯度并更新参数的优化方法。它结合了 Batch Gradient Descent 和 Stochastic Gradient Descent 的优点，能够在计算效率和收敛稳定性之间找到平衡。\n其核心工作流程一般为： 数据划分: 将训练数据集分成若干小批量（Minibatches），每个批次包含 \\(m\\) 个样本。 Minibatch 大小 \\(m \\) 一般为 16, 32, 64, 128，根据硬件资源和模型大小调节。 梯度计算与更新：对于每个 Minibatch ( \\(X_{minibatch}, Y_{minibatch} \\) )，计算目标函数在该批次上的梯度并更新参数： \\[ \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta; X_{minibatch}, Y_{minibatch}) \\] 较大的 Minibatch 通常需要较大的学习率。可结合学习率衰减策略（如 Step Decay、Exponential Decay、Warm Restarts）来平衡收敛速度与准确性。 迭代更新: 对每个 Minibatch 重复上述步骤，直到遍历整个数据集（称为一个 epoch）。根据收敛情况执行多个 epoch。 Note：为了确保梯度估计的无偏性（unbiasedness），Minibatch的样本必须独立随机抽样。如果数据集的自然排列存在相关性（如医疗数据按患者排序），在选择Minibatch前需要对数据集进行随机打乱（shuffle）。所以我们需要在每个 epoch 开始时随机打乱数据，避免梯度计算因样本顺序产生偏差。\nMinibatch大小的选择： 梯度估计的准确性：批量越大，梯度估计越精确，但收益递减（标准误差与样本数量的平方根成反比）。 计算资源限制：较小的批量可能导致多核硬件或GPU利用率不足；较大的批量需要更多内存。 硬件优化：GPU通常在批量大小为2的幂（如32, 64, 128）时性能最佳。 正则化效果：较小批量可以引入噪声，具有正则化作用，但需要调整较小的学习率。 Note：Minibatch（小批量）在计算上具有优势（相比于大批量）的原因是，小批量数据（如 32 或 64 个样本）可以被完全加载到 GPU 中进行高效的并行计算。GPU 的计算效率在处理适量数据时达到峰值。与此同时，在大批量中需要对更多样本进行梯度计算和聚合，梯度计算过程更复杂，占用更多时间。例如，矩阵计算的开销随数据规模呈非线性增长。\n训练速度比较：\nSGD 的单次更新虽然快，但更新频率极高，导致整体时间长。 Minibatch SGD 在更新频率和计算量之间取得了平衡，往往在一个 epoch 的整体速度最快，是实际应用中的首选。 Batch Gradient Descent 由于每次更新都需要遍历整个数据集，在大规模数据上效率低。 小批量随机梯度下降（Minibatch Stochastic Gradient Descent）代码实现\nimport numpy as np # 定义目标函数及其梯度 def loss_function(w): \u0026#34;\u0026#34;\u0026#34;目标函数: f(w) = 0.5 * (w - 3)^2\u0026#34;\u0026#34;\u0026#34; return 0.5 * (w - 3) ** 2 def gradient(w): \u0026#34;\u0026#34;\u0026#34;目标函数的梯度: f\u0026#39;(w) = w - 3\u0026#34;\u0026#34;\u0026#34; return w - 3 # Mini-batch SGD 优化过程 for epoch in range(epochs): # 每个 epoch 中分成多个 mini-batch np.random.shuffle(data) # 打乱数据集 for i in range(0, len(data), batch_size): # 从数据集中取出一个 mini-batch x_sample = data[i:i+batch_size] # 计算梯度 grad = gradient(w) # 这里假设梯度是目标函数的梯度，针对每个样本计算 # 更新参数 w -= learning_rate * grad 优化深度神经网络的挑战（Challenges in Neural Network Optimization） # 优化深度神经网络面临诸多挑战，因为网络的目标函数通常是非凸函数，即包含多个局部极值点、鞍点和平坦区域等复杂结构。此外，即便是凸优化问题，也会因为高维度和数据特性而复杂化。\n局部极小值（Local Minima） # 深度神经网络的目标函数（例如分类或回归问题中的损失函数）通常是非凸的。非凸函数意味着它可能有多个局部极小值、鞍点、以及平坦区域。深度网络的复杂结构使得这些局部极小值的位置和数量变得更加复杂和难以预测。虽然局部极小值可能在低维问题中更常见，但在高维空间中，局部极小值的影响通常被更复杂的平坦区域或鞍点替代。\n局部极小值的影响： 训练停滞：局部极小值会导致优化算法的停滞，尤其是梯度下降算法。当优化器在一个局部极小值附近时，梯度变得非常小或几乎为零，参数更新几乎无法继续，导致训练过程无法继续进行。 降低训练效率：即使局部极小值的影响并不完全阻止训练过程，停滞在局部极小值附近也会显著增加训练时间。优化器可能需要很长时间才能逃脱这些局部极小值，增加了收敛的时间。 全局最优解的错失：如果优化器陷入局部极小值，则无法继续向全局最优解前进。尤其在复杂的神经网络中，局部极小值可能位于一个比较低的损失值附近，因此模型在训练过程中可能会错过更优的解。 优化过程中诊断局部极小值问题： 通过实时监控训练和验证损失曲线，可以快速定位优化是否正常进行。如果损失曲线在训练过程中停滞不前，长时间保持在一个较高的值，我们可以推测优化遇到了局部最小值的问题。 平坦区域和鞍点（Plateaus and Saddle Points） # 深度神经网络包含大量的参数，尤其是深度模型中，每一层的参数都会在目标函数的定义中增加维度。随着维度的增加，非凸优化问题的复杂性大大增加。鞍点和梯度为零的平坦区域在高维空间中更为常见，且它们在这些维度上的“上升”和“下降”趋势很容易相互作用，导致目标函数在这些点附近的行为变得难以预测。鞍点的存在表明，在优化过程中，目标函数的某些方向上可能是上升的，而其他方向上可能是下降的。这种局部结构使得优化过程充满不确定性。\n平坦区域和鞍点的影响： 梯度更新变慢：在平坦区域，梯度接近零，意味着优化算法的更新非常缓慢。无论是在平坦区域还是鞍点附近，梯度下降算法（如 SGD）都可能因为梯度过小而在这些区域停滞不前，导致收敛速度变慢，甚至完全停滞。 优化过程的非稳定性：鞍点区域的存在导致梯度下降算法可能会 “卡在”某些不理想的位置，无法有效地向全局最优解逼近。这些点的复杂几何性质使得简单的梯度下降方法无法高效逃离它们，因此可能长时间停留在鞍点区域或平坦区域，无法有效继续优化。 参数更新难度增加：在优化过程中，深度神经网络的参数更新依赖于梯度信息。如果网络被困在平坦区域或鞍点，它将无法获得有效的梯度信息，从而使得参数的更新难以进行。 Note：常见应对局部极小值，平坦区域和鞍点的解决方法：\n使用随机初始化：随机初始化可以帮助模型从不同的起点开始优化过程，从而增加逃离局部极小值的可能性。如果某次训练陷入局部极小值，其他随机初始化可能会找到更优的解。 引入动量（Momentum）：动量方法通过累积之前的梯度方向，可以加速沿低曲率方向的收敛，并帮助优化器跨越较浅的局部极小值。 调整学习率：学习率过小可能导致优化器在鞍点附近徘徊；适当增大学习率有助于跳出鞍点。 自适应优化算法：Adam 和 RMSProp 等算法通过对梯度变化的跟踪，能够更快地从鞍点中脱离。 梯度爆炸（Exploding Gradients） # 梯度爆炸（Exploding Gradients） 是指在反向传播过程中，梯度值在某些层中异常增大，导致权重更新时出现非常大的步长，从而导致训练过程中参数的值急剧增大，甚至溢出。\n梯度爆炸的原因： 链式法则：在深度神经网络中，反向传播过程通过链式法则计算梯度。当网络深度较大时，梯度的计算会依赖于多个层的梯度乘积。如果某些层的梯度值较大，它们会在反向传播过程中不断放大，导致最终梯度值急剧增大，这就造成了梯度爆炸。 不合理的权重初始化：如果模型的权重初始化不当（如权重值太大），会使得每一层的激活值非常大，导致反向传播时梯度的放大效应。 不适当的激活函数：某些激活函数，如 ReLU，在特定情况下可能间接导致梯度爆炸。尤其是当网络的输入值过大时，ReLU 激活函数会输出较大的值，这样在反向传播时，梯度可能会累积放大。如果网络较深且没有采取有效的梯度抑制措施，这种累积放大会导致梯度爆炸。 Note：激活值本身并不会直接被用于反向传播，而是激活值的导数和前一层的梯度共同作用于权重的更新。在反向传播中，计算每一层的梯度时需要用到激活函数的导数，而不是激活值本身。激活值是当前层权重和偏置作用下的输出，作为下一层的输入，间接影响了梯度计算。如果激活值较大，可能导致下一层的输入过大，从而使激活函数进入饱和区，导致梯度消失；或者在某些情况下，激活函数导数恒定（如 ReLU 的正区间），结合较大的权重累积，会导致梯度放大的问题。\n梯度爆炸的影响： 梯度爆炸会导致模型参数的值变得异常大，进而导致数值溢出或数值不稳定。具体表现为：训练过程中，损失函数的值出现波动甚至爆炸，模型的参数值可能变得非常大，难以更新，甚至会导致训练停止。 更严重的是，梯度爆炸可能完全破坏训练过程，导致模型无法收敛，并可能使得计算资源的浪费加剧。 优化过程中诊断梯度爆炸问题： 如果损失在训练中突然暴增到无穷大（NaN 或 Inf），我们可以推测优化遇到了梯度爆炸的问题。 如果权重值快速变大，远超合理范围，或者发现激活值过大（尤其是 ReLU 输出远超预期范围）时，也可以推断出现了梯度爆炸。 Note：常见应对梯度爆炸的解决方法：\n自适应优化算法：自适应优化算法（如 RMSProp、Adam）能够缓解梯度爆炸问题，这些优化器会动态调整学习率，使梯度变化较为平稳。 正则化方法：在损失函数中加入权重惩罚项，限制参数的大小。限制梯度值的放大，防止权重过大。 权重初始化：恰当的权重初始化能够有效防止梯度过大：如 Xavier 初始化（适用于 sigmoid 或 tanh 激活函数），He 初始化（适用于 ReLU 激活函数）。 归一化技术：批量归一化（Batch Normalization）在每一层将激活值归一化，使其均值为 0，方差为 1。在更新前，将梯度归一化为固定尺度。缓解梯度爆炸，同时加速收敛。 梯度消失（Long-Term Dependencies and Gradient Vanishing） # 梯度消失问题发生在反向传播（backpropagation）过程中，当网络中的梯度值在传播时逐渐变小，最终接近零。由于梯度变得非常小，权重更新的步伐变得非常缓慢，导致训练变得非常困难甚至无法收敛。\n梯度消失的原因：\n链式法则：反向传播算法依赖链式法则来计算梯度，即通过每一层的梯度传递，最终得到损失函数对每个参数的梯度。然而，在深层神经网络或者长序列的网络中，梯度的传递通过多个层或时间步进行叠加。如果每一层或每一步的梯度值都小于1（通常是通过激活函数计算的），那么多次乘积会导致梯度指数级下降，最终变得非常小。 激活函数的性质：常见的激活函数（如sigmoid、tanh）会在其饱和区间（即输入非常大或非常小的时候）将梯度压缩到接近零。（e.g. 当输入值非常大或非常小时，sigmoid函数的梯度接近于零，这就导致了反向传播过程中梯度的快速衰减。） 深层网络和长时间序列：在深层神经网络（特别是RNN或LSTM）中，层数过多或时间步过长时，梯度必须沿着多个路径传播。这使得梯度在每一步都逐渐缩小，导致梯度几乎无法传递到网络的最早层，尤其是在处理长时间依赖时尤为严重。 梯度消失的影响：\n学习效率低下：当梯度消失时，网络的参数几乎不更新，尤其是接近输入层的参数。这样，网络在训练过程中无法有效地学习到有用的特征，特别是长时间依赖的特征（例如，RNN中用于记忆先前输入的长期依赖关系）。 训练停滞或失败：对于长序列数据，梯度消失会导致模型无法捕捉到远程依赖关系。训练可能会停滞，模型的性能无法提高，导致训练过程失败或收敛到不理想的结果。 优化过程中诊断梯度消失问题：\n如果损失缓慢下降甚至趋于平坦，优化进度显著减慢，我们可以推测优化遇到了梯度消失的问题。 如果激活值过小或趋于饱和（如 Sigmoid 输出接近 0 或 1），或者权重更新幅度很小，长期接近初始值，也可以推断出现了梯度消失。 Note：常见应对梯度消失的解决方法：\n使用合适的激活函数 权重初始化：恰当的权重初始化能够有效防止梯度过大：如 Xavier 初始化（适用于 sigmoid 或 tanh 激活函数），He 初始化（适用于 ReLU 激活函数）。 归一化技术：批量归一化（Batch Normalization）在每一层将激活值归一化，使其均值为 0，方差为 1。在更新前，将梯度归一化为固定尺度。缓解梯度爆炸，同时加速收敛。 不精确的梯度（Inexact Gradients） # 在训练深度神经网络时，不精确的梯度指的是在某些情况下，计算出的梯度并不是目标函数的准确梯度，而是经过近似或估算的。这种不精确的梯度通常出现在基于小批量（mini-batch）梯度下降方法的训练过程中，也可能出现在其他优化方法中，如随机梯度下降（SGD）。这些不精确的梯度可能会影响优化的稳定性和收敛速度。\n不精确的梯度的原因：\n小批量梯度计算：因为每个小批量的数据量有限，因此计算出的梯度只是目标函数在该小批量数据上的估计值，而不是在整个训练数据集上的真实梯度。通常情况下，计算出的梯度存在一些随机噪声。这意味着每次梯度更新时，参数并未沿着真正的目标函数梯度方向进行更新，而是被噪声扰动，导致偏离最优方向。这种偏差特别是在训练初期更为显著，因为网络的参数尚未经过充分训练，梯度计算更容易受到数据和噪声的影响。 不精确的梯度的影响：\n收敛速度减慢：因为不精确的梯度会引入噪声和偏差，优化过程会变得不稳定，导致梯度下降算法的收敛速度减慢。具体来说，不精确的梯度可能导致模型参数朝错误的方向更新，甚至陷入局部最优解，而不能快速接近全局最优解。 过度依赖随机性：不精确的梯度也使得优化算法过度依赖随机性。虽然这种随机性在一定程度上能够帮助优化过程跳出局部最小值，但过度的随机性可能导致算法无法有效地找到全局最优解，并且在不稳定的情况下表现得更加差劲。 动量（Momentum） # Momentum（动量）是一种基于梯度下降法（Stochastic Gradient Descent）的优化算法改进，它通过引入动量的概念，在参数更新中融入历史梯度的信息，从而加速优化过程，尤其是在复杂的非凸损失表面中表现出色。Momentum 通常作为 SGD 的扩展版本使用，可以与 Mini-Batch SGD 结合。\nMomentum 模仿物理学中的动量概念：\n在物理中，动量是物体的质量与速度的乘积。物体的运动状态会受到惯性影响，越大的动量意味着越难改变方向。 在优化中，Momentum 会记录梯度下降的更新方向，并累积这些更新，以使得参数在一致的方向上更新得更快，而在噪声较大的方向上减少振荡。 Momentum 基本算法的更新规则如下： \\[ v_t = \\gamma v_{t-1} + \\eta \\nabla J(\\theta_{t-1}) \\] \\[ \\theta_t = \\theta_{t-1} - v_t \\] 动量项 \\(\\gamma v_{t-1}\\) ： 表示上一次更新的方向，具有惯性，当前更新会参考之前的方向。 \\(\\gamma\\) ：动量系数，通常取值在 [0.8, 0.99] 之间。 梯度项 \\(\\eta \\nabla J(\\theta_{t-1})\\) ： 当前的梯度信息，驱动参数向最小值方向移动。 参数更新 \\(\\theta_t\\) ： 综合了惯性和当前梯度，更新参数。 Note：SGD 每次仅使用一个样本或小批量样本来估计梯度，随机梯度的变化使得优化路径在复杂非凸损失表面上具有探索能力，有一定概率跳出局部最优解或鞍点。但由于梯度估计不稳定，优化路径可能会呈现高频振荡，尤其是在陡峭方向或平坦区域，导致收敛速度较慢。\nMomentum 可以被看作对梯度信息的一种“平滑操作”，减少不必要的小幅度方向变化，优化路径更趋于稳定。但Momentum 并没有完全消除随机性。因为梯度更新仍基于 Minibatch SGD 或 SGD，随机性依然存在，只是短期的高频随机波动被抑制了。\n对比标准SGD的更新公式： \\[ \\theta_t = \\theta_{t-1} - \\eta \\nabla J(\\theta_{t-1}) \\] 和 Momentum的更新公式： \\[ \\theta_t = \\theta_{t-1} - \\gamma v_{t-1} - \\eta \\nabla J(\\theta_{t-1}) \\] Momentum 保留了 SGD 的随机特性，同时通过动量的累积减少了噪声对更新路径的干扰。\nMomentum 的主要改进与目标问题 # Momentum 主要针对以下的问题进行优化：\n问题一：收敛速度慢。在陡峭方向（梯度较大）和较平坦方向（梯度较小）上，SGD 的更新幅度相差较大。在较平坦方向上，梯度小导致更新缓慢，而在陡峭方向上，反复振荡消耗了时间。 Momentum 如何解决： 动量累积历史梯度：Momentum 会在参数更新时累积之前的梯度信息，形成一个惯性（动量），从而增强优化的方向性。 加速沿低梯度方向的更新：对于较平坦的方向，由于动量项的累积作用，更新步长会逐渐加快。 平滑陡峭方向的振荡：在陡峭方向上，梯度的更新方向会相互抵消（正负方向交替），动量能够有效减少振荡幅度，使得更新更平稳。 问题二：局部振荡问题。在目标函数中接近局部最优点或鞍点时，SGD 可能因为随机梯度引入的噪声而反复振荡，无法稳定收敛。振荡现象在非凸优化问题中尤为严重，常导致收敛效率低。 Momentum 如何解决： 动量的平滑作用：Momentum 会对当前的梯度值和之前的梯度值进行加权平均，从而减少梯度噪声的影响。 更新方向的稳定性：动量累积了一段时间内梯度的总体方向，使得参数更新不易受到局部梯度噪声的干扰，避免振荡。 问题三：避免陷入局部最优点。在复杂的高维非凸函数中，优化算法可能会陷入局部最优点，难以找到全局最优解。SGD 由于其更新完全依赖当前梯度，缺乏全局视角，更容易陷入局部最优。 Momentum 如何解决： 动量的历史信息积累：Momentum 通过累积多次梯度更新信息，使得优化过程具有更强的惯性，能够 “冲出”某些局部最优点或平坦区域。 优化路径的惯性推动：当优化路径接近局部最优时，动量机制仍会保持一定的前进趋势，避免算法在局部区域停滞。 Momentum 代码实现 import numpy as np # 定义目标函数及其梯度 def loss_function(w): \u0026#34;\u0026#34;\u0026#34;目标函数: f(w) = 0.5 * (w - 3)^2\u0026#34;\u0026#34;\u0026#34; return 0.5 * (w - 3) ** 2 def gradient(w): \u0026#34;\u0026#34;\u0026#34;目标函数的梯度: f\u0026#39;(w) = w - 3\u0026#34;\u0026#34;\u0026#34; return w - 3 # 初始化参数 w = np.random.randn() # 初始化权重，假设是一个标量 learning_rate = 0.1 # 学习率 epochs = 100 # 训练的总轮次 momentum = 0.9 # 动量系数 v = 0 # 初始化动量项 # SGD with Momentum优化过程 for epoch in range(epochs): # 计算梯度 grad = gradient(w) # 更新动量（标准动量公式） v = momentum * v + learning_rate * grad # v是历史动量和当前梯度的加权和 # 更新参数 w -= v # 更新参数，减去动量 Nesterov Momentum # Nesterov Momentum（Nesterov加速梯度）是对传统Momentum方法的一种改进。它被设计用来加速优化过程并提高收敛速度。其关键思想是，在更新参数之前，预先计算梯度，并且利用当前的 “预估”位置 来指导下一步的更新。与传统Momentum方法相比，Nesterov Momentum通常可以获得更好的收敛性能。因为它能够提前“看到”更远的方向，从而更精确地更新参数。它更能利用 动量的“提前预见”来加速收敛。具体来说，Nesterov Momentum方法的步骤如下：\n计算预估位置：在更新之前，首先通过动量项预测出一个“预估”位置。即： \\[ \\theta_{\\text{temp}} = \\theta_t - \\gamma v_{t-1} \\] 其中， \\(\\theta\\) 是当前的参数， \\(\\gamma\\) 是动量系数， \\(v\\) 是上一轮的动量。 计算梯度：使用这个预估位置 \\(\\theta_{\\text{temp}}\\) 来计算梯度： \\[ \\nabla J(\\theta_t - \\gamma v_{t-1}) \\] 更新动量和参数：然后基于计算得到的梯度，更新动量和参数： \\[ v_t = \\gamma v_{t-1} + \\eta \\nabla J(\\theta_t - \\mu v_{t-1}) \\] \\[ \\theta_{t+1} = \\theta_t - v_t \\] Nesterov Momentum的优势在于，它能够在更新之前“预测”到未来的趋势，避免了传统Momentum方法中的滞后效应。传统Momentum通过当前的梯度来更新动量，这可能导致参数更新的方向滞后于实际的优化目标。\n提前获得信息：由于在更新参数时使用的是当前动量的预估位置，这使得梯度下降方向更加“前瞻性”，有时比直接使用当前梯度更有效。具体来说，Nesterov Momentum在梯度计算时，不是简单地使用当前参数 \\(\\theta_t\\) ，而是使用预估的参数 \\(\\theta_t - \\mu v_{t-1}\\) ，即“预测”的位置。\n减少滞后：传统Momentum容易受到过去梯度的影响，尤其是在损失函数具有不规则曲线时。例如，当优化过程进入谷底（参数值接近最优解时），新的梯度非常小（接近0），但是动量项会继续沿着之前的方向更新，这就是所谓的“滞后”现象。由于动量项是对历史梯度的加权平均，它不会立刻根据当前的梯度信息来调整方向，而是会继续沿着之前的方向更新参数，直到动量被新的梯度信息所替代。\nMomentum的本质是“惯性”，它通过加权之前的梯度来推测更新的方向。在梯度发生变化的区域，Momentum仍然会继续沿着先前的方向进行更新。这会导致在梯度发生急剧变化时，动量更新的方向滞后于实际的需求。Nesterov Momentum通过提前估计未来的梯度变化，减少了这种滞后，通常会加速收敛。\nNote：在Momentum中，动量是直接根据当前点的梯度来更新的，而没有提前预测。\n在Nesterov Momentum中，动量的更新是通过先计算出一个预估位置（基于上一步的动量），然后在这个预估位置计算梯度，再利用这个梯度来更新动量（提前往前走一步）。这样一来，Nesterov Momentum就“提前”计算了梯度，从而可以更准确地引导下一步的更新。因此，Nesterov Momentum并不是完全“准确”的梯度，它是基于当前动量的预测来计算的一个梯度，这个梯度的计算位置是偏向于未来的（即梯度的计算基于预估位置）。\nNesterov Momentum 代码实现 import numpy as np # 定义目标函数及其梯度 def loss_function(w): \u0026#34;\u0026#34;\u0026#34;目标函数: f(w) = 0.5 * (w - 3)^2\u0026#34;\u0026#34;\u0026#34; return 0.5 * (w - 3) ** 2 def gradient(w): \u0026#34;\u0026#34;\u0026#34;目标函数的梯度: f\u0026#39;(w) = w - 3\u0026#34;\u0026#34;\u0026#34; return w - 3 # 超参数设置 learning_rate = 0.1 momentum = 0.9 epochs = 100 w = np.random.randn() # 初始参数 v = 0 # 初始化动量 # Nesterov Momentum优化过程 for epoch in range(epochs): # 在更新前，先利用当前动量预测参数的位置 w_nesterov = w - momentum * v # Nesterov的预估位置 # 计算梯度 grad = gradient(w_nesterov) # 更新动量（Nesterov公式） v = momentum * v + learning_rate * grad # v是历史动量和当前梯度的加权和 # 更新参数 w -= v # 更新参数，减去动量 参数初始化策略（Parameter Initialization Strategies） # 在深度学习中，参数初始化是训练神经网络的重要一步。良好的初始化可以加速收敛，避免梯度消失或梯度爆炸的问题，同时提高最终模型的性能。而不当的初始化则可能导致网络训练缓慢、不稳定，甚至完全无法收敛。\n避免梯度消失与梯度爆炸：当网络层数较深时，若初始化不当，前向传播中的激活值或反向传播中的梯度可能会指数级地缩小或增大，导致： 梯度消失：参数更新几乎停止，无法学习深层特征。 梯度爆炸：梯度值过大，导致参数更新不稳定。 提高训练速度：良好的初始化使得激活值和梯度的分布在整个网络中保持适中，从而提高收敛速度。 避免对称性问题：若所有权重初始值相同，反向传播时每个神经元将计算出相同的梯度，导致网络无法打破对称性，降低模型的表达能力。 初始化需要确保不同的隐藏单元计算不同的函数。 如果多个隐藏单元具有相同的输入和初始化参数，它们将始终更新为相同的值，丧失多样性。 随机初始化通常用于打破对称性，避免输入模式或梯度模式丢失。 Note：对称性问题指的是网络中不同神经元的参数更新路径完全相同，从而导致它们学习的特征也完全相同，失去了网络的表达能力。这种现象被称为对称性破坏不足。如果多个神经元初始化时的权重完全相同（例如都为 0 或相同的常数），并且它们接收到完全相同的输入数据，那么：\n它们的前向传播计算结果相同。 它们的梯度在反向传播时也完全相同。 经过多轮更新后，这些神经元仍然保持相同的权重值，无法学到不同的特征。 通过给每个神经元分配不同的初始权重值（如随机初始化），可以破坏对称性，使得每个神经元学到的特征变得多样化。这是因为：\n前向传播阶段： 相同的输入 \\(x\\) ，在与不同的权重 \\(w_1, w_2, \\dots\\) 相乘相加后，会生成不同的中间值。 这些不同的中间值通过激活函数（如 ReLU、sigmoid）映射后，输入到下一层神经元时已经变得完全不同。 反向传播阶段： 不同权重初始化会导致每个神经元接收到的梯度不同。 即便它们的输入和输出在一开始类似，梯度更新的路径会逐渐让它们的参数走向不同的方向。 常用初始化方法 # 随机初始化（Random Initialization） # 通常从高斯分布或均匀分布中随机采样权重。 均匀分布：如 \\(W \\sim U(-a, a)\\) 正态分布：如 \\(W \\sim N(0, \\sigma^2)\\) 分布类型对结果影响较小，但分布的 尺度（scale） 对优化效果和泛化能力有较大影响。 权重过大：前向传播时，可能导致值爆炸。反向传播时，可能导致梯度爆炸或混沌行为（如RNN）。激活函数饱和，梯度完全丢失。 权重过小：信号在前向传播中消失，导致网络无法学习。 随机初始化（Random Initialization）代码实现 import numpy as np # 随机初始化函数 def random_initialize(input_size, output_size): # 权重初始化：均匀分布 W = np.random.randn(output_size, input_size) * 0.01 # 乘以一个小常数（通常是0.01）来避免较大的权重值 b = np.zeros((output_size, 1)) # 偏置初始化为0 return W, b Xavier 初始化（Xavier Initialization） # 对权重 \\(W\\) 的每个元素，从以下分布中采样：\n均匀分布： \\[ W_{i,j} \\sim U\\left(-\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\right) \\] 正态分布： \\[ W_{i,j} \\sim N\\left(0, \\frac{2}{n_{\\text{in}} + n_{\\text{out}}}\\right) \\] 其中， \\(n_{\\text{in}}\\) 是输入单元数， \\(n_{\\text{out}}\\) 是输出单元数。 Xavier 初始化适用于激活函数是 sigmoid 或 tanh 的网络，或者网络层较浅或中等深度的场景。他的优点在于：\n平衡输入和输出信号的方差，避免激活值和梯度值的极端变化。 避免信号在网络中消失或爆炸。 缺点：不适用于ReLU类激活函数。 Note：Xavier 初始化适用于激活函数是 sigmoid 或 tanh 的网络的原因是这些激活函数的导数容易趋于零，尤其是在输入值落入激活函数的饱和区（Sigmoid 的两侧平坦区域）。如果权重初始化过大，输入会快速进入饱和区，导致梯度消失。如果权重初始化过小，输出信号会逐层衰减，最终导致梯度消失。\nXavier 的初始化方法将权重分布限定在一个较小的范围内，使输入值主要分布在 Sigmoid 和 Tanh 的线性区，避免梯度消失。在较深的网络中，信号可能仍会因为层数的累积效应导致衰减或放大。\nXavier 初始化代码实现 import numpy as np def xavier_initialization(input_size, output_size): # Xavier初始化公式：使用均匀分布来初始化权重 # 计算权重初始化的范围 limit = np.sqrt(6 / (input_size + output_size)) # 均匀分布初始化权重 weights = np.random.uniform(-limit, limit, (input_size, output_size)) return weights # 示例：假设输入层有3个神经元，输出层有2个神经元 input_size = 3 output_size = 2 weights = xavier_initialization(input_size, output_size) He 初始化（He Initialization） # 具体公式如下：\n均匀分布： \\[ W_{i,j} \\sim U\\left(-\\sqrt{\\frac{6}{n_{\\text{in}}}}, \\sqrt{\\frac{6}{n_{\\text{in}}}}\\right) \\] 正态分布： \\[ W_{i,j} \\sim N\\left(0, \\frac{2}{n_{\\text{in}}}\\right) \\] 其中， \\(n_{\\text{in}}\\) 是输入单元数。 He 初始化适用于激活函数是ReLU及其变种（如Leaky ReLU、Parametric ReLU），或者深层网络的场景。他的优点在于：\n专为ReLU类激活函数设计，能够更好地传递正向和反向信号。 缺点：对非ReLU激活函数可能效果较差。 Note：He 初始化适用于激活函数是ReLU及其变种的原因是对于 ReLU，当输入为负时，输出恒为 0；当输入为正时，输出为原值。由于一部分神经元输出会被截断为 0，导致有效的参与计算的神经元数量减少（称为“稀疏激活”现象）。如果初始化权重过小，信号会迅速减弱，导致梯度消失；而如果权重过大，信号会迅速放大，导致梯度爆炸。\nHe 初始化通过设定较大的方差，补偿了 ReLU 截断负值导致的信号损失。这样可以让激活值的分布更均衡，避免信号快速衰减或放大。He 初始化根据输入层大小调整权重的方差，使每层的输出方差保持相对稳定，即使网络层数增加，信号也不会显著衰减或爆炸。\nHe 初始化代码实现 import numpy as np def he_initialization(input_size, output_size): # 权重初始化 weights = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size) # 偏置初始化为 0 biases = np.zeros((output_size, 1)) return weights, biases # 示例：初始化一个有 3 个输入节点和 2 个输出节点的层 input_size = 3 output_size = 2 weights, biases = he_initialization(input_size, output_size) 现代优化中的初始化改进 # 学习率与初始化的协同： 初始化与学习率的选择需要协同设计。 比如，较大的初始化可能需要较小的学习率，反之亦然。 Batch Normalization 的引入： 批量归一化（Batch Normalization）可以在一定程度上缓解初始化不当带来的问题，使得更宽泛的初始化策略能够被有效利用。 自适应优化算法： 优化算法如 Adam 和 RMSProp 可以通过动态调整学习率，减少对初始化的敏感性。 批量标准化（Batch Normalization） # 具体细节详见 正则化章节的 Batch Normalization。\nNote：Batch Normalization 在 正则化（Regularization） 中的作用体现在 防止过拟合，提高模型泛化能力。通过小批量统计引入噪声，减轻对特定神经元的依赖，间接限制权重的自由度，使模型更加简洁和泛化性更强。\nBatch Normalization 在 优化（Optimization） 中的作用体现在 加速收敛并提高稳定性。归一化减少 Internal Covariate Shift，并限制激活值范围，缓解梯度消失和爆炸问题，从而允许更大的学习率。\nBatch Normalization 在两个方面的作用相辅相成，但本质上是优化导向的技术，正则化效果是其附带的一个益处。\nBatch Normalization 在优化中的作用及原因 # 减少 Internal Covariate Shift：在训练过程中，每层的输入分布可能会随着前层参数更新而改变，这被称为 Internal Covariate Shift。这种分布变化会导致：训练变得困难，梯度传播不稳定，学习率难以设置。Batch Normalization 通过强制每一层的输入分布保持稳定（即归一化到零均值和单位方差），有效减小了 Internal Covariate Shift，使模型更容易选择较大的学习率。\n缓解梯度消失和梯度爆炸问题：深度网络中的梯度可能因为激活函数（如 sigmoid 或 tanh）的饱和区域而迅速缩小或增长，导致：梯度消失（权重更新速度慢）或者梯度爆炸（权重更新不稳定）。\n通过将输入归一化，BN 限制了激活值的范围，避免了梯度过大或过小。归一化后的输入值分布更接近零均值和单位方差，减少了激活函数饱和的概率。梯度在反向传播中更稳定，使得优化过程更加平滑。\n提高优化效率：BN 允许使用更大的学习率，从而加速训练。BN 的归一化过程对激活值和梯度值进行了平滑化处理，即使学习率较大，梯度更新仍然稳定。这种特性让优化过程中的收敛速度显著提高。\n改善权重初始值的鲁棒性：传统的神经网络对权重初始化非常敏感，糟糕的初始化会导致：训练时间变长，模型性能变差。BN 减轻了对权重初始化的依赖，因为归一化后的输入分布消除了初始化权重引起的输入偏移。在每一层中，输入经过归一化后分布固定，减少了初始化权重对训练初期表现的影响。\n具有自适应学习率的算法（Algorithms with Adaptive Learning Rates） # 学习率（Learning Rate） 在神经网络训练中是最重要的超参数之一，它控制着参数更新的步幅。选择合适的学习率对于训练过程至关重要，但它往往是 最难设置的超参数之一。原因主要有以下几点：\n不同方向的灵敏度：在参数空间的某些方向，代价函数对参数的敏感度较高（即梯度较大），而在其他方向上则不敏感（即梯度较小）。这种非均匀的敏感度使得在所有方向上使用统一的学习率变得困难。即使在同一方向上，代价函数的变化幅度也可能不同，这导致在训练过程中很难确定合适的步长。 局部极小值或鞍点：神经网络的损失函数通常具有多个局部极小值或鞍点，学习率过大可能导致模型错过最优解，而学习率过小则可能导致训练过慢，甚至陷入局部极小值。因此，选择合适的学习率是一个平衡问题，过大可能导致发散，过小可能导致收敛过慢。 需要在训练过程中动态调整：固定的学习率在整个训练过程中可能并不适用，因为随着训练的进行，参数的更新越来越小，需要逐渐减小学习率才能更精细地搜索最优解。因此，动态调整学习率成为优化问题中的重要部分。 为了应对学习率调节的难题，研究者们提出了许多自适应学习率的方法，这些方法通过根据梯度的变化来自动调整每个参数的学习率。这些方法的核心思想是，如果某个方向上的梯度信息发生变化，那么相应的学习率也应该做出相应调整，从而更高效地进行参数更新。\nAdaGrad # AdaGrad的核心理念是通过调整每个参数的学习率来加速收敛，尤其是对于那些参数更新频繁的方向，给它们一个较小的学习率，而对于那些更新较少的方向，给它们一个较大的学习率。这是通过累积每个参数的梯度平方来实现的。每个参数的学习率会随着它的历史梯度大小的变化而逐步调整，从而使得学习率能够适应参数的梯度信息。AdaGrad算法的步骤可以总结为：\n初始化：初始化每个参数的学习率 \\(\\eta_0\\) （通常为一个小的常数），并初始化梯度累积项 \\(G_i = 0\\) ，其中 \\(G_i\\) 是该参数的梯度平方和的累积。 计算梯度：在每次迭代中，根据当前的损失函数计算每个参数 \\(\\theta_i\\) 的梯度 \\(g_i(t) = \\nabla_{\\theta} L(\\theta_t)\\) 。 累积梯度的平方：对每个参数 \\(\\theta_i\\) ，累积梯度的平方，得到 \\(G_t\\) ，表示每个参数历史梯度的平方和。 \\[ G_i(t) = G_i(t-1) + g_i(t)^2 \\] 更新参数：然后，根据每个参数的累积梯度平方来调整学习率，并更新参数。AdaGrad的更新规则为： \\[ \\theta_i(t) = \\theta_i(t-1) - \\frac{\\eta}{\\sqrt{G_i(t)} + \\epsilon} \\cdot g_i(t) \\] 其中： \\(\\eta\\) 是全局学习率（一个小的常数）。 \\(G_t\\) 是梯度的平方和。 \\(\\epsilon\\) 是为了避免除以0而加上的一个小常数，通常设置为 \\(10^{-8}\\) 。 AdaGrad的特点与优势：\n自适应调整学习率：AdaGrad 的主要特点是自适应调整每个参数的学习率。它根据每个参数的梯度历史调整学习率，对于频繁更新的参数，学习率会变得较小，而对于较少更新的参数，学习率则较大。 适应稀疏数据：AdaGrad 特别适用于处理稀疏数据（如文本数据或大规模稀疏矩阵）。因为稀疏特征的梯度较少，AdaGrad 会给予这些特征更大的学习率，从而有效地加快收敛。 无需手动调整学习率：由于 AdaGrad 会根据历史梯度自动调整学习率，它减少了手动调节学习率的需求。 AdaGrad的缺点：\n学习率下降过快：AdaGrad 的最大缺点是其学习率会随着训练的进行不断减小，尤其是在训练的早期，梯度的累积效应可能导致学习率迅速下降到非常小的值，进而影响后续的训练。这意味着在训练过程中，模型可能会在某些参数上收敛得过快，无法再进一步优化。\n不适合长时间训练：由于学习率的衰减，AdaGrad 在长时间训练过程中可能会导致模型在后期停止更新参数，从而影响最终的收敛效果。\nAdaGrad代码实现\nimport numpy as np # 定义目标函数及其梯度 def loss_function(w): \u0026#34;\u0026#34;\u0026#34;目标函数: f(w) = 0.5 * (w - 3)^2\u0026#34;\u0026#34;\u0026#34; return 0.5 * (w - 3) ** 2 def gradient(w): \u0026#34;\u0026#34;\u0026#34;目标函数的梯度: f\u0026#39;(w) = w - 3\u0026#34;\u0026#34;\u0026#34; return w - 3 # AdaGrad 优化过程 def adagrad_optimizer(learning_rate, epochs, initial_w, epsilon=1e-8): w = initial_w # 初始化参数 G = 0 # 初始化梯度的平方和 for epoch in range(epochs): grad = gradient(w) # 计算梯度 # 更新梯度的平方和 G += grad ** 2 # 计算每个参数的学习率 adjusted_lr = learning_rate / (np.sqrt(G) + epsilon) # 更新参数 w -= adjusted_lr * grad # 计算并记录损失值 loss = loss_function(w) return w RMSProp # RMSProp 的主要思想是通过维护每个参数梯度的平方的指数衰减平均值来调整每个参数的学习率。与 AdaGrad 相比，RMSProp 引入了一个指数加权平均（Exponential Moving Average, EMA） 来控制梯度平方的累积，从而防止学习率在训练过程中过快衰减。RMSProp算法的步骤可以总结为：\n初始化：与其他优化算法类似，初始化参数 \\(\\theta_i\\) ，并设定初始学习率 \\(\\eta\\) 和衰减因子 \\(\\gamma\\) 。此外，初始化一个梯度平方的累积值 \\(G_i = 0\\) ，用于存储每个参数的梯度平方的加权平均。 计算梯度：在每次迭代中，计算每个参数 \\(\\theta_i\\) 对应的梯度 \\(g_i(t) = \\nabla_{\\theta} L(\\theta_t)\\) （即损失函数相对于该参数的偏导数）。 更新梯度平方的加权平均：使用指数加权平均来更新梯度平方的值： \\[ G_i(t) = \\gamma G_i(t-1) + (1 - \\gamma) g_i(t)^2 \\] 其中， \\(\\gamma\\) 是衰减因子（通常设置为接近 1，如 0.9），它控制了历史梯度信息对当前梯度平方值的影响。较小的 \\(\\gamma\\) 会让历史梯度信息对当前更新影响较小，而较大的 \\(\\gamma\\) 会保留更多历史信息。 更新参数：使用更新后的梯度平方的加权平均值来计算参数更新： \\[ \\theta_i(t) = \\theta_i(t-1) - \\frac{\\eta}{\\sqrt{G_i(t)} + \\epsilon} \\cdot g_i(t) \\] 其中， \\(\\eta\\) 是全局学习率， \\(G_i(t)\\) 是当前的梯度平方加权平均值， \\(\\epsilon\\) 是为了避免除零错误而加上的一个小常数（通常设置为 \\(10^{-8}\\) ）。 RMSProp的特点与优势：\n更稳定的学习率调整：相比 AdaGrad，RMSProp 在训练过程中保持了较为稳定的学习率，不会因梯度过大或过小导致训练过程不稳定。 适用于递归神经网络和强化学习：由于 RMSProp 在处理梯度变化较大的情况时非常有效，因此它广泛应用于递归神经网络（RNNs）和强化学习等任务。 RMSProp的缺点：\n依赖衰减因子选择：RMSProp 的性能依赖于衰减因子 \\(\\gamma\\) 的选择。虽然常见的 \\(\\gamma = 0.9\\) 或 0.99 在很多任务中表现良好，但对于不同的任务，合适的衰减因子可能有所不同。\nRMSProp代码实现\nimport numpy as np # 定义目标函数及其梯度 def loss_function(w): \u0026#34;\u0026#34;\u0026#34;目标函数: f(w) = 0.5 * (w - 3)^2\u0026#34;\u0026#34;\u0026#34; return 0.5 * (w - 3) ** 2 def gradient(w): \u0026#34;\u0026#34;\u0026#34;目标函数的梯度: f\u0026#39;(w) = w - 3\u0026#34;\u0026#34;\u0026#34; return w - 3 # RMSProp 优化过程 def rmsprop_optimizer(learning_rate, epochs, initial_w, beta=0.9, epsilon=1e-8): w = initial_w # 初始化参数 v = 0 # 初始化梯度平方的移动平均 for epoch in range(epochs): grad = gradient(w) # 计算梯度 # 更新梯度平方的移动平均 v = beta * v + (1 - beta) * grad ** 2 # 计算每个参数的学习率 adjusted_lr = learning_rate / (np.sqrt(v) + epsilon) # 更新参数 w -= adjusted_lr * grad # 计算并记录损失值 loss = loss_function(w) return w Adam # Adam 通过计算梯度的一阶矩（梯度的均值，表示动量）和二阶矩（梯度的平方的均值，表示梯度的方差）来动态调整每个参数的学习率。具体来说，Adam 通过下面两个过程来更新参数：\n一阶矩估计（动量）：这部分计算的是梯度的指数加权平均。它帮助优化器记住过去梯度的趋势，减少振荡和加速收敛。 二阶矩估计（自适应学习率）：这部分计算的是梯度平方的指数加权平均。它根据梯度的方差调整每个参数的学习率，使得模型能够对不同的梯度大小做出不同的响应。 Adam 的更新过程通过结合这两个矩估计（即梯度的一阶矩和二阶矩）来计算每个参数的自适应学习率，并更新参数。Adam 的更新过程可以分为以下几个步骤：\n初始化参数： 参数 \\(\\theta\\) （模型的可学习参数） 学习率 \\(\\eta\\) 衰减因子 \\(\\beta_1\\) 和 \\(\\beta_2\\) ，分别用于计算一阶矩和二阶矩的指数加权平均（通常设置为 \\(\\beta_1 = 0.9\\) ， \\(\\beta_2 = 0.999\\) ） 偏置修正常数 \\(\\epsilon\\) （通常为 \\(10^{-8}\\) ） 计算梯度：计算损失函数 \\(L(\\theta)\\) 相对于参数 \\(\\theta\\) 的梯度 \\(g_t\\) ，即 \\(g_t = \\nabla_{\\theta} L(\\theta_t)\\) 更新一阶矩和二阶矩估计： \\[ m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t \\] \\[ v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 \\] 其中， \\(m_t\\) 是梯度的动量， \\(v_t\\) 是梯度的平方的加权平均。 偏置修正：由于 \\(m_t\\) 和 \\(v_t\\) 在训练的初期会有偏置，因此需要对它们进行修正： \\[ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} \\] \\[ \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\] 这两个修正是为了消除训练初期 \\(m_t\\) 和 \\(v_t\\) 的偏置。 更新参数： \\[ \\theta_t = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t \\] 最终，通过带有自适应学习率的梯度更新每个参数。 Adam的特点与优势：\n自适应学习率：Adam 自动调整每个参数的学习率，避免了手动调整学习率的需求，尤其是在复杂的深度学习任务中。不同的参数可能具有不同的梯度尺度和变化，因此使用自适应学习率有助于提升训练效率。 动量优化：通过引入动量，Adam 能够加速梯度下降过程，减少振荡，从而更快地收敛。 计算效率高：尽管 Adam 使用了两个矩的估计（动量和方差），但它只需要存储两个额外的向量（ \\(m_t\\) 和 \\(v_t\\) ），因此计算量相对较小，且能够适用于大规模数据集。 Adam的缺点：\n超参数设置问题：尽管 Adam 在许多任务中表现得很出色，但它的超参数（如 \\(\\beta_1\\) ， \\(\\beta_2\\) 和 \\(\\epsilon\\) ）对最终结果仍有较大影响。尽管默认设置通常有效，但对于特定问题，可能仍需要进行调优。\n偏置修正的延迟：尽管偏置修正能消除初期的偏差，但在非常长的训练过程中，仍可能出现一些偏差，影响训练的稳定性。\n可能导致过拟合：由于 Adam 在训练过程中可以快速适应数据集特性，它可能会导致过拟合，尤其是在数据较少或模型过于复杂的情况下。因此，在某些任务中，需要结合正则化技术（如 dropout）来避免过拟合。\nAdam代码实现\nimport numpy as np # 定义目标函数及其梯度 def loss_function(w): \u0026#34;\u0026#34;\u0026#34;目标函数: f(w) = 0.5 * (w - 3)^2\u0026#34;\u0026#34;\u0026#34; return 0.5 * (w - 3) ** 2 def gradient(w): \u0026#34;\u0026#34;\u0026#34;目标函数的梯度: f\u0026#39;(w) = w - 3\u0026#34;\u0026#34;\u0026#34; return w - 3 # Adam 优化过程 def adam_optimizer(learning_rate, epochs, initial_w, beta1=0.9, beta2=0.999, epsilon=1e-8): w = initial_w # 初始化参数 m = 0 # 一阶矩的初始化 v = 0 # 二阶矩的初始化 t = 0 # 时间步 for epoch in range(epochs): t += 1 grad = gradient(w) # 计算梯度 # 更新一阶矩（动量） m = beta1 * m + (1 - beta1) * grad # 更新二阶矩（梯度平方的移动平均） v = beta2 * v + (1 - beta2) * grad ** 2 # 偏差修正（为了抵消初始化时 m 和 v 的偏差） m_hat = m / (1 - beta1 ** t) v_hat = v / (1 - beta2 ** t) # 计算每个参数的更新量 w -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon) # 计算并记录损失值 loss = loss_function(w) return w 学习率调度器（Learning Rate Scheduling） # Learning Rate Scheduling 是指在训练过程中逐步调整学习率的策略。其核心思想是在训练过程中随着时间的推移，逐步减小学习率，使得模型能够更细致地调整参数，从而获得更好的收敛性能。Learning Rate Scheduling 提供了以下优势：\n避免梯度爆炸或梯度消失：通过在训练的不同阶段动态调整学习率，避免在训练的初期学习率过大导致梯度爆炸，或者在后期学习率过小导致收敛过慢。 提高收敛速度：随着训练的进行，减小学习率可以让模型在更精细的层面进行参数调整，提高最终收敛的精度。 避免过拟合：逐步减小学习率有助于模型在后期避免过拟合，尤其是当模型已经接近全局最优解时。 Note：尽管自适应学习率和学习率调度都涉及到调整学习率，但它们之间有本质的区别：\n自适应学习率（如 AdaGrad、Adam）是基于每个参数的历史梯度信息来调整学习率。这是一个动态调整过程，能够针对每个参数的训练需求进行个性化调整。它在优化过程中有自动调整的优势，但它通常没有显式的学习率衰减策略。 学习率调度（如 Step Decay）是针对全局学习率的调整，它遵循某种预设的规则，并通常在整个训练过程中逐步减小学习率。它更关注整体训练过程的学习率变化，而非单个参数的调节。 常见的 Learning Rate Scheduling 策略 # 固定学习率（Constant Learning Rate）： 这是最简单的学习率策略。在整个训练过程中，学习率保持不变，通常适用于模型较为简单的任务。虽然这种策略的优点是实现简单，但往往不能充分利用训练过程中的信息。\n逐步衰减（Step Decay）： 逐步衰减是一种经典的学习率调度策略，其基本思想是按照一定的间隔将学习率降低。通常在每训练几个epoch后，将学习率按比例衰减。 \\[ \\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor \\frac{t}{\\text{step\\_size}} \\rfloor} \\] 其中：\n\\(\\eta_t\\) 是当前的学习率， \\(\\eta_0\\) 是初始学习率， \\(\\gamma\\) 是衰减因子（通常 \\(0.1 \\leq \\gamma \\leq 0.5\\) ）， \\(\\text{step\\_size}\\) 是衰减的步长（通常以 epoch 为单位）。 指数衰减（Exponential Decay）： 在指数衰减中，学习率在每次更新时按照指数方式衰减，而不是按照固定的步长进行调整。这种策略在训练的后期可以逐步减小学习率，使得模型更加精细地调整参数。 \\[ \\eta_t = \\eta_0 \\cdot e^{-\\lambda t} \\] 其中：\n\\(\\eta_t\\) 是当前的学习率， \\(\\eta_0\\) 是初始学习率， \\(\\lambda\\) 是衰减率，控制衰减的速度， \\(t\\) 是当前 epoch 数。 Note：在实际的深度学习训练中，自适应学习率（如 Adam、RMSProp）和学习率调度（Learning Rate Scheduling）可以结合使用，也可以单独使用。\n自适应学习率算法（如 Adam）自动调整每个参数的学习率，可以减少手动调参的工作量，非常适合快速验证模型设计或算法选择。自适应学习率通常表现出较快的初期收敛速度，但在训练后期可能难以找到最优解。因此，当更关注模型能快速达到“足够好”的性能时，仅使用自适应学习率可能已经足够。\n但在对模型最终性能要求较高，或在大规模数据集上训练深层网络时，自适应学习率可以快速收敛，而学习率调度可以避免在后期震荡。这时结合自适应学习率和学习率调度更为常见。例如，使用 Adam 优化器加上 学习率逐步降低（如 Step Decay）。这样既能快速优化，又能在训练后期稳定收敛。\n学习率调度器代码实现 import torch import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import StepLR # 模型、损失函数和优化器 model = SimpleNN() criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.1) # StepLR 学习率调度器：每 10 个 epoch 将学习率减少为原来的 0.1 scheduler = StepLR(optimizer, step_size=10, gamma=0.1) # scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9) # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50) # 训练过程 epochs = 50 for epoch in range(epochs): model.train() optimizer.zero_grad() # 前向传播 outputs = model(data) # 计算损失 loss = criterion(outputs, targets) # 反向传播和优化 loss.backward() optimizer.step() # 每个 epoch 后更新学习率 scheduler.step() 优化方法的综合运用 # 在深度学习训练中，优化流程通常分为三个阶段，每个阶段结合多种方法以逐步提升模型性能。\n初期探索阶段 初期探索阶段的目标是快速验证模型设计是否合理，通常使用 Adam 优化器，其默认设置适合大多数任务，能快速收敛。此时使用 He 初始化（针对 ReLU 激活函数）或 Xavier 初始化（针对 Sigmoid 或 Tanh），并引入 Batch Normalization 来稳定激活值分布。为了防止过拟合，可以选择适量的 Dropout。同时，将输入数据标准化为均值 0、方差 1，选择小批量大小（通常 32 ~ 128）以平衡训练稳定性与效率。\n中期优化阶段 中期优化阶段旨在进一步提升模型性能并解决可能出现的优化挑战。在这个阶段，可以继续使用 Adam 优化器，或者切换到 SGD with Momentum，以获得更稳定的优化过程。同时，结合 学习率调度器（如 ReduceLROnPlateau），动态调整学习率，避免训练停滞。对于梯度爆炸问题，启用 梯度裁剪 限制梯度范围；对稀疏特征或大型模型，则增加 Dropout 比例和 L2 正则化力度，进一步防止过拟合。\n后期精调阶段 后期精调阶段的重点是通过微调参数实现最终的性能优化。此时，通常切换到 SGD with Momentum 并降低学习率到较小值（如 \\(1e^{-5}\\) ），结合固定步长下降（Step Decay）或动态调度策略（如 ReduceLROnPlateau）控制学习率变化。同时，可以逐渐减少或停用 Dropout，以提高模型的表达能力，仅保留少量的 L2 正则化。在验证集上增加评估频率，通过 早停（Early Stopping） 策略监控性能并防止过拟合。\n"},{"id":20,"href":"/docs/python-basics/leetcode/practice-history/","title":"Practice History","section":"Leetcode Notes","content":" Leetcode 练习记录 # 此页面记录了我在 LeetCode 平台上完成的算法题目练习，每条记录包括完成日期、题目链接以及涉及的数据结构或算法主题。这些练习旨在巩固基础知识、提高解题技巧，并为技术面试做好充分准备。以下为部分记录：\n按日期排序 # 2024年12月 Date: 2024-12-20: Leetcode 33 - Search in Rotated Sorted Array【Binary Search】 Date: 2024-12-19: Leetcode 35 - Search Insert Position【Binary Search】 Leetcode 69 - Sqrt(x)【Binary Search】 Leetcode 374 - Guess Number Higher or Lower【Binary Search】 Date: 2024-12-18: Leetcode 704 - Binary Search【Binary Search】 Date: 2024-12-17: Leetcode 131 - Palindrome Partitioning【Backtracking】 Date: 2024-12-16: Leetcode 46 - Permutations【Backtracking】 Leetcode 47 - Permutations II【Backtracking】 Leetcode 77 - Combinations【Backtracking】 Leetcode 78 - Subsets【Backtracking】 Leetcode 90 - Subsets II【Backtracking】 Date: 2024-12-13: Leetcode 70 - Climbing Stairs【Dynamic Programming】 Leetcode 509 - Fibonacci Number【Dynamic Programming】 Leetcode 17 - Letter Combinations of a Phone Number【Backtracking】 Date: 2024-12-12: Leetcode 22 - Generate Parentheses【Depth-First Search】【Backtracking】 Leetcode 39 - Combination Sum【Backtracking】 Date: 2024-12-11: Leetcode 200 - Number of Islands【Depth-First Search】 Leetcode 695 - Max Area of Island【Depth-First Search】 Date: 2024-12-10: Leetcode 100 - Same Tree【Tree】【Depth-First Search】 Date: 2024-12-05: Leetcode 235 - Lowest Common Ancestor of a Binary Search Tree【Binary Search Tree】【Depth-First Search】 Leetcode 700 - Search in a Binary Search Tree【Binary Search Tree】 Date: 2024-12-04: Leetcode 226 - Invert Binary Tree【Binary Search Tree】【Depth-First Search】 2024年11月 Date: 2024-11-28: Leetcode 98 - Validate Binary Search Tree【Binary Search Tree】【Depth-First Search】 Date: 2024-11-27: Leetcode 111 - Minimum Depth of Binary Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 112 - Path Sum【Tree】【Binary Tree】【Depth-First Search】 Leetcode 101 - Symmetric Tree【Tree】【Binary Tree】【Depth-First Search】 Date: 2024-11-26: Leetcode 94 - Binary Tree Inorder Traversal【Tree】【Binary Tree】 Leetcode 144 - Binary Tree Preorder Traversal【Tree】【Binary Tree】 Leetcode 145 - Binary Tree Postorder Traversal【Tree】【Binary Tree】 Leetcode 104 - Maximum Depth of Binary Tree【Tree】【Binary Tree】【Depth-First Search】 Date: 2024-11-25: Leetcode 239 - Sliding Window Maximum【Deque】【Sliding Window】 Leetcode 641 - Design Circular Deque【Deque】 Leetcode 215 - Kth Largest Element in an Array【Heap】 Leetcode 23 - Merge k Sorted Lists【Heap】 Date: 2024-11-24: Leetcode 933 - Number of Recent Calls【Queue】 Date: 2024-11-22: Leetcode 739 - Daily Temperatures【Stack】【Monotonic Stack】 Leetcode 232 - Implement Queue using Stacks【Queue】【Stack】 Leetcode 225 - Implement Stack using Queues【Stack】【Queue】 Leetcode 622 - Design Circular Queue【Queue】 Date: 2024-11-21: Leetcode 155 - Min Stack【Stack】 Leetcode 682 - Baseball Game【Stack】 Date: 2024-11-19: Leetcode 20 - Valid Parentheses【Stack】【String】 Date: 2024-11-18: Leetcode 92 - Reverse Linked List II【Linked List】 Leetcode 61 - Rotate List【Linked List】【Two Pointers】 Date: 2024-11-17: Leetcode 19 - Remove N-th Node From End of List【Linked List】【Two Pointers】 Leetcode 138 - Copy List with Random Pointer【Linked List】【Hash Table】 Leetcode 234 - Palindrome Linked List【Linked List】 Leetcode 160 - Intersection of Two Linked Lists【Linked List】【Hash Table】 Date: 2024-11-15: Leetcode 21 - Merge Two Sorted Lists【Linked List】【Recursion】 Leetcode 141 - Linked List Cycle【Linked List】【Hash Table】 Leetcode 203 - Remove Linked List Elements【Linked List】【Recursion】 Leetcode 83 - Remove Duplicates from Sorted List【Linked List】 Leetcode 2 - Add Two Numbers【Linked List】【Recursion】【Math】 Date: 2024-11-14: Leetcode 206 - Reverse Linked List【Linked List】【Recursion】 Date: 2024-11-13: Leetcode 3 - Longest Substring Without Repeating Characters【String】【Hash Table】 Leetcode 5 - Longest Palindromic Substring【String】【Two Pointers】 Leetcode 28 - Find the Index of the First Occurrence in a String【String】【Two Pointers】 Leetcode 49 - Group Anagrams【String】【Hash Table】 Date: 2024-11-12: Leetcode 344 - Reverse String【String】【Two Pointers】 Leetcode 26 - Remove Duplicates from Sorted Array【Array】【Two Pointers】 Leetcode 27 - Remove Element【Array】【Two Pointers】 Date: 2024-11-11: Leetcode 53 - Maximum Subarray【Array】 Leetcode 238 - Product of Array Except Self【Array】【Prefix Sum】 Date: 2024-11-10: Leetcode 454 - 4Sum II【Array】【Hash Table】 Date: 2024-11-08: Leetcode 121 - Best Time to Buy and Sell Stock【Array】【Dynamic Programming】 Leetcode 349 - Intersection of Two Arrays【Array】【Hash Table】 Leetcode 219 - Contains Duplicate II【Array】【Hash Table】 Date: 2024-11-07: LeetCode 15 - 3 Sum【Array】【Two Pointers】 Date: 2024-11-06: LeetCode 1 - Two Sum【Array】【Hash Table】 按题目编号排序 # 题目编号 Leetcode 1 - Two Sum【Array】【Hash Table】 Leetcode 2 - Add Two Numbers【Linked List】【Recursion】【Math】 Leetcode 3 - Longest Substring Without Repeating Characters【String】【Hash Table】 Leetcode 5 - Longest Palindromic Substring【String】【Two Pointers】 Leetcode 15 - 3 Sum【Array】【Two Pointers】 Leetcode 17 - Letter Combinations of a Phone Number【Backtracking】 Leetcode 19 - Remove N-th Node From End of List【Linked List】【Two Pointers】 Leetcode 20 - Valid Parentheses【Stack】【String】 Leetcode 21 - Merge Two Sorted Lists【Linked List】【Recursion】 Leetcode 22 - Generate Parentheses【Depth-First Search】【Backtracking】 Leetcode 23 - Merge k Sorted Lists【Heap】 Leetcode 26 - Remove Duplicates from Sorted Array【Array】【Two Pointers】 Leetcode 27 - Remove Element【Array】【Two Pointers】 Leetcode 28 - Find the Index of the First Occurrence in a String【String】【Two Pointers】 Leetcode 35 - Search Insert Position【Binary Search】 Leetcode 39 - Combination Sum【Backtracking】 Leetcode 46 - Permutations【Backtracking】 Leetcode 47 - Permutations II【Backtracking】 Leetcode 49 - Group Anagrams【String】【Hash Table】 Leetcode 53 - Maximum Subarray【Array】 Leetcode 61 - Rotate List【Linked List】【Two Pointers】 Leetcode 69 - Sqrt(x)【Binary Search】 Leetcode 70 - Climbing Stairs【Dynamic Programming】 Leetcode 77 - Combinations【Backtracking】 Leetcode 78 - Subsets【Backtracking】 Leetcode 83 - Remove Duplicates from Sorted List【Linked List】 Leetcode 90 - Subsets II【Backtracking】 Leetcode 92 - Reverse Linked List II【Linked List】 Leetcode 94 - Binary Tree Inorder Traversal【Tree】【Binary Tree】 Leetcode 98 - Validate Binary Search Tree【Binary Search Tree】【Depth-First Search】 Leetcode 100 - Same Tree【Tree】【Depth-First Search】 Leetcode 101 - Symmetric Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 104 - Maximum Depth of Binary Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 111 - Minimum Depth of Binary Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 112 - Path Sum【Tree】【Binary Tree】【Depth-First Search】 Leetcode 121 - Best Time to Buy and Sell Stock【Array】【Dynamic Programming】 Leetcode 131 - Palindrome Partitioning【Backtracking】 Leetcode 138 - Copy List with Random Pointer【Linked List】【Hash Table】 Leetcode 141 - Linked List Cycle【Linked List】【Hash Table】 Leetcode 144 - Binary Tree Preorder Traversal【Tree】【Binary Tree】 Leetcode 145 - Binary Tree Postorder Traversal【Tree】【Binary Tree】 Leetcode 155 - Min Stack【Stack】 Leetcode 160 - Intersection of Two Linked Lists【Linked List】【Hash Table】 Leetcode 200 - Number of Islands【Depth-First Search】 Leetcode 203 - Remove Linked List Elements【Linked List】【Recursion】 Leetcode 206 - Reverse Linked List【Linked List】【Recursion】 Leetcode 215 - Kth Largest Element in an Array【Heap】 Leetcode 219 - Contains Duplicate II【Array】【Hash Table】 Leetcode 225 - Implement Stack using Queues【Stack】【Queue】 Leetcode 226 - Invert Binary Tree【Binary Search Tree】【Depth-First Search】 Leetcode 232 - Implement Queue using Stacks【Queue】【Stack】 Leetcode 234 - Palindrome Linked List【Linked List】 Leetcode 235 - Lowest Common Ancestor of a Binary Search Tree【Binary Search Tree】 Leetcode 238 - Product of Array Except Self【Array】【Prefix Sum】 Leetcode 239 - Sliding Window Maximum【Deque】【Sliding Window】 Leetcode 249 - Number of Recent Calls【Queue】 Leetcode 344 - Reverse String【String】【Two Pointers】 Leetcode 349 - Intersection of Two Arrays【Array】【Hash Table】 Leetcode 374 - Guess Number Higher or Lower【Binary Search】 Leetcode 454 - 4Sum II【Array】【Hash Table】 Leetcode 509 - Fibonacci Number【Dynamic Programming】 Leetcode 622 - Design Circular Queue【Queue】 Leetcode 641 - Design Circular Deque【Deque】 Leetcode 682 - Baseball Game【Stack】 Leetcode 695 - Max Area of Island【Depth-First Search】 Leetcode 700 - Search in a Binary Search Tree【Binary Search Tree】 Leetcode 704 - Binary Search【Binary Search】 Leetcode 739 - Daily Temperatures【Stack】【Monotonic Stack】 Leetcode 933 - Number of Recent Calls【Queue】 按题目类型排序 # 题目类型 Array # Leetcode 1 - Two Sum【Array】【Hash Table】 Leetcode 15 - 3 Sum【Array】【Two Pointers】 Leetcode 53 - Maximum Subarray【Array】 Leetcode 121 - Best Time to Buy and Sell Stock【Array】【Dynamic Programming】 Leetcode 238 - Product of Array Except Self【Array】【Prefix Sum】 Leetcode 219 - Contains Duplicate II【Array】【Hash Table】 Leetcode 349 - Intersection of Two Arrays【Array】【Hash Table】 Leetcode 454 - 4Sum II【Array】【Hash Table】 Linked List # Leetcode 2 - Add Two Numbers【Linked List】【Recursion】【Math】 Leetcode 19 - Remove N-th Node From End of List【Linked List】【Two Pointers】 Leetcode 21 - Merge Two Sorted Lists【Linked List】【Recursion】 Leetcode 26 - Remove Duplicates from Sorted Array【Array】【Two Pointers】 Leetcode 61 - Rotate List【Linked List】【Two Pointers】 Leetcode 83 - Remove Duplicates from Sorted List【Linked List】 Leetcode 92 - Reverse Linked List II【Linked List】 Leetcode 138 - Copy List with Random Pointer【Linked List】【Hash Table】 Leetcode 141 - Linked List Cycle【Linked List】【Hash Table】 Leetcode 160 - Intersection of Two Linked Lists【Linked List】【Hash Table】 Leetcode 203 - Remove Linked List Elements【Linked List】【Recursion】 Leetcode 206 - Reverse Linked List【Linked List】【Recursion】 Leetcode 234 - Palindrome Linked List【Linked List】 String # Leetcode 3 - Longest Substring Without Repeating Characters【String】【Hash Table】 Leetcode 5 - Longest Palindromic Substring【String】【Two Pointers】 Leetcode 28 - Find the Index of the First Occurrence in a String【String】【Two Pointers】 Leetcode 49 - Group Anagrams【String】【Hash Table】 Leetcode 344 - Reverse String【String】【Two Pointers】 Stack # Leetcode 20 - Valid Parentheses【Stack】【String】 Leetcode 155 - Min Stack【Stack】 Leetcode 225 - Implement Stack using Queues【Stack】【Queue】 Leetcode 682 - Baseball Game【Stack】 Leetcode 739 - Daily Temperatures【Stack】【Monotonic Stack】 Queue # Leetcode 232 - Implement Queue using Stacks【Queue】【Stack】 Leetcode 249 - Number of Recent Calls【Queue】 Leetcode 622 - Design Circular Queue【Queue】 Leetcode 641 - Design Circular Deque【Deque】 Heap # Leetcode 23 - Merge k Sorted Lists【Heap】 Leetcode 215 - Kth Largest Element in an Array【Heap】 Tree # Leetcode 94 - Binary Tree Inorder Traversal【Tree】【Binary Tree】 Leetcode 98 - Validate Binary Search Tree【Binary Search Tree】【Depth-First Search】 Leetcode 100 - Same Tree【Tree】【Depth-First Search】 Leetcode 101 - Symmetric Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 104 - Maximum Depth of Binary Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 111 - Minimum Depth of Binary Tree【Tree】【Binary Tree】【Depth-First Search】 Leetcode 112 - Path Sum【Tree】【Binary Tree】【Depth-First Search】 Leetcode 144 - Binary Tree Preorder Traversal【Tree】【Binary Tree】 Leetcode 145 - Binary Tree Postorder Traversal【Tree】【Binary Tree】 Leetcode 226 - Invert Binary Tree【Binary Search Tree】【Depth-First Search】 Leetcode 235 - Lowest Common Ancestor of a Binary Search Tree【Binary Search Tree】【Depth-First Search】 Leetcode 700 - Search in a Binary Search Tree【Binary Search Tree】 Binary Search Tree # Leetcode 98 - Validate Binary Search Tree【Binary Search Tree】【Depth-First Search】 Leetcode 235 - Lowest Common Ancestor of a Binary Search Tree【Binary Search Tree】【Depth-First Search】 Leetcode 700 - Search in a Binary Search Tree【Binary Search Tree】 Deque # Leetcode 239 - Sliding Window Maximum【Deque】【Sliding Window】 Leetcode 641 - Design Circular Deque【Deque】 Sliding Window # Leetcode 239 - Sliding Window Maximum【Deque】【Sliding Window】 Monotonic Stack # Leetcode 739 - Daily Temperatures【Stack】【Monotonic Stack】 Prefix Sum # Leetcode 238 - Product of Array Except Self【Array】【Prefix Sum】 Dynamic Programming # Leetcode 121 - Best Time to Buy and Sell Stock【Array】【Dynamic Programming】 Recursion # Leetcode 2 - Add Two Numbers【Linked List】【Recursion】【Math】 Leetcode 21 - Merge Two Sorted Lists【Linked List】【Recursion】 Leetcode 22 - Generate Parentheses【Depth-First Search】【Backtracking】 Leetcode 203 - Remove Linked List Elements【Linked List】【Recursion】 Leetcode 206 - Reverse Linked List【Linked List】【Recursion】 Hash Table # Leetcode 1 - Two Sum【Array】【Hash Table】 Leetcode 3 - Longest Substring Without Repeating Characters【String】【Hash Table】 Leetcode 49 - Group Anagrams【String】【Hash Table】 Leetcode 138 - Copy List with Random Pointer【Linked List】【Hash Table】 Leetcode 141 - Linked List Cycle【Linked List】【Hash Table】 Leetcode 160 - Intersection of Two Linked Lists【Linked List】【Hash Table】 Leetcode 349 - Intersection of Two Arrays【Array】【Hash Table】 "}]