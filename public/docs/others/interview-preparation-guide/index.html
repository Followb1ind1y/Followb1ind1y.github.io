<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  面试准备大纲
  #



  基础理论
  #



  数学与统计学
  #


✅ 线性代数（矩阵运算、特征值分解）
✅ 概率论与统计（贝叶斯定理、分布、假设检验）
✅ 优化方法（梯度下降、Adam、学习率调度）



  机器学习基础
  #


✅ 监督学习（线性回归、决策树、SVM、集成学习）
✅ 评估指标（准确率、召回率、F1、AUC-ROC）
✅ 过拟合与正则化（L1/L2、Dropout）



  深度学习基础
  #


⭐ 神经网络（前向传播、反向传播、激活函数）


⭐ RNN（序列建模、自回归、语言模型）
  
⚠️ 序列模型（Sequence Models）和传统模型有什么区别？
  
    序列模型（Sequence Models）和传统模型有什么区别？
序列模型（Sequence Models）和传统模型的主要区别在于它们处理数据的方式。传统模型，如线性回归（Linear Regression），通常 假设输入数据是独立同分布（i.i.d., independent and identically distributed）的，即每个输入数据点的顺序不影响其他数据点。在这些模型中，数据被处理为独立的特征向量，通常是静态的，缺乏时间或顺序的关联。
序列模型，尤其是在自然语言处理领域，强调对数据中 时间或顺序信息的建模。他们往往能够通过其递归结构捕捉序列中前后数据点之间的依赖关系。序列模型在处理时考虑到顺序，因此它们在语言建模（Language Modeling）、语音识别（Speech Recognition）等任务中表现优异，因为这些任务 依赖于上下文信息和时间序列的动态变化。
  

⚠️ 什么是自回归（Autoregression）和自回归模型？
  
    什么是自回归（Autoregression）和自回归模型？
自回归（Autoregression）是一种统计模型，用于预测时间序列数据中的未来值。它基于一个假设：当前的观测值（或输出）与过去的观测值有直接关系。自回归模型通过 使用历史数据点作为输入，预测下一个时间步的值。在这种模型中，当前的输出是历史数据的线性组合，具体来说，当前的值是由过去若干个数据点的加权和构成。
在自然语言处理（NLP）中的生成式语言模型（如自回归语言模型）中，自回归模型的原理类似于时间序列分析。自回归语言模型在生成文本时，将每个词作为输入，并依赖于已经生成的词序列来预测下一个词。在这种情况下，模型的预测依赖于前一个或前几个词，因此可以用自回归模型的框架来描述这一过程。它通过逐步生成下一个输出（例如，词或值）并基于之前的生成结果来调整预测，确保每一步都与前面的步骤密切相关。
  

⚠️ 如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？
  
    如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？
在自然语言处理（NLP）中，语言模型（Language Model, LM） 是一种用于预测句子或文本中单词序列的模型。语言模型的核心目标是 估计一个给定单词序列出现的概率。具体来说，对于一个给定的单词序列，语言模型的任务是计算该序列的联合概率：



  \[
P(x_1, \ldots, x_T) 
\]

我们可以通过链式法则将序列的联合概率分解成条件概率的乘积，从而将问题转化为自回归预测（autoregressive prediction）："><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/others/interview-preparation-guide/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Interview Preparation Guide"><meta property="og:description" content="面试准备大纲 # 基础理论 # 数学与统计学 # ✅ 线性代数（矩阵运算、特征值分解） ✅ 概率论与统计（贝叶斯定理、分布、假设检验） ✅ 优化方法（梯度下降、Adam、学习率调度） 机器学习基础 # ✅ 监督学习（线性回归、决策树、SVM、集成学习） ✅ 评估指标（准确率、召回率、F1、AUC-ROC） ✅ 过拟合与正则化（L1/L2、Dropout） 深度学习基础 # ⭐ 神经网络（前向传播、反向传播、激活函数） ⭐ RNN（序列建模、自回归、语言模型） ⚠️ 序列模型（Sequence Models）和传统模型有什么区别？ 序列模型（Sequence Models）和传统模型有什么区别？ 序列模型（Sequence Models）和传统模型的主要区别在于它们处理数据的方式。传统模型，如线性回归（Linear Regression），通常 假设输入数据是独立同分布（i.i.d., independent and identically distributed）的，即每个输入数据点的顺序不影响其他数据点。在这些模型中，数据被处理为独立的特征向量，通常是静态的，缺乏时间或顺序的关联。
序列模型，尤其是在自然语言处理领域，强调对数据中 时间或顺序信息的建模。他们往往能够通过其递归结构捕捉序列中前后数据点之间的依赖关系。序列模型在处理时考虑到顺序，因此它们在语言建模（Language Modeling）、语音识别（Speech Recognition）等任务中表现优异，因为这些任务 依赖于上下文信息和时间序列的动态变化。
⚠️ 什么是自回归（Autoregression）和自回归模型？ 什么是自回归（Autoregression）和自回归模型？ 自回归（Autoregression）是一种统计模型，用于预测时间序列数据中的未来值。它基于一个假设：当前的观测值（或输出）与过去的观测值有直接关系。自回归模型通过 使用历史数据点作为输入，预测下一个时间步的值。在这种模型中，当前的输出是历史数据的线性组合，具体来说，当前的值是由过去若干个数据点的加权和构成。
在自然语言处理（NLP）中的生成式语言模型（如自回归语言模型）中，自回归模型的原理类似于时间序列分析。自回归语言模型在生成文本时，将每个词作为输入，并依赖于已经生成的词序列来预测下一个词。在这种情况下，模型的预测依赖于前一个或前几个词，因此可以用自回归模型的框架来描述这一过程。它通过逐步生成下一个输出（例如，词或值）并基于之前的生成结果来调整预测，确保每一步都与前面的步骤密切相关。
⚠️ 如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？ 如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？ 在自然语言处理（NLP）中，语言模型（Language Model, LM） 是一种用于预测句子或文本中单词序列的模型。语言模型的核心目标是 估计一个给定单词序列出现的概率。具体来说，对于一个给定的单词序列，语言模型的任务是计算该序列的联合概率：
\[ P(x_1, \ldots, x_T) \] 我们可以通过链式法则将序列的联合概率分解成条件概率的乘积，从而将问题转化为自回归预测（autoregressive prediction）："><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Interview Preparation Guide | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/others/interview-preparation-guide/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.03fb8739d2d9ad5e6f40161228d9d16f41298217d47681e808feb3c079bf4c66.js integrity="sha256-A/uHOdLZrV5vQBYSKNnRb0EpghfUdoHoCP6zwHm/TGY=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/others/interview-preparation-guide/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><input type=checkbox id=section-d0dd931d60033c220ecd4cd60b7c9170 class=toggle>
<label for=section-d0dd931d60033c220ecd4cd60b7c9170 class="flex justify-between"><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a></label><ul><li><a href=/docs/deep-learning/convolutional-neural-networks/modern-convolutional-neural-networks/>Modern Convolutional Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-a3019bfa8037cc33ed6405d1589b6219 class=toggle>
<label for=section-a3019bfa8037cc33ed6405d1589b6219 class="flex justify-between"><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a></label><ul><li><a href=/docs/deep-learning/recurrent-neural-networks/modern-recurrent-neural-networks/>Modern Recurrent Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-0a43584c16258b228ae9aa8d70efc320 class=toggle>
<label for=section-0a43584c16258b228ae9aa8d70efc320 class="flex justify-between"><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a></label><ul><li><a href=/docs/deep-learning/attention-and-transformers/large-scale-pretraining-with-transformers/>Large-Scale Pretraining with Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/modern-large-language-models-copy/>Modern Large Language Models</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92e8358c45c96009753cf4227e9daea8 class=toggle>
<label for=section-92e8358c45c96009753cf4227e9daea8 class="flex justify-between"><a href=/docs/deep-learning/llm-pipelines/>LLM Pipelines</a></label><ul><li><a href=/docs/deep-learning/llm-pipelines/llm-inference-and-deployment/>LLM Inference and Deployment</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle checked>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul><li><a href=/docs/others/interview-preparation-guide/ class=active>Interview Preparation Guide</a><ul></ul></li></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Interview Preparation Guide</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#基础理论><strong>基础理论</strong></a><ul><li><a href=#数学与统计学><strong>数学与统计学</strong></a></li><li><a href=#机器学习基础><strong>机器学习基础</strong></a></li><li><a href=#深度学习基础><strong>深度学习基础</strong></a></li></ul></li><li><a href=#llm-核心技术><strong>LLM 核心技术</strong></a><ul><li><a href=#llm-基础组件><strong>LLM 基础组件</strong></a></li><li><a href=#大语言模型llm><strong>大语言模型（LLM）</strong></a></li><li><a href=#工具与框架><strong>工具与框架</strong></a></li></ul></li><li><a href=#应用开发与工程化><strong>应用开发与工程化</strong></a><ul><li><a href=#应用开发框架><strong>应用开发框架</strong></a></li><li><a href=#检索增强生成rag><strong>检索增强生成（RAG）</strong></a></li><li><a href=#部署与运维><strong>部署与运维</strong></a></li></ul></li><li><a href=#模型训练与优化><strong>模型训练与优化</strong></a><ul><li><a href=#分布式训练><strong>分布式训练</strong></a></li><li><a href=#高效微调技术><strong>高效微调技术</strong></a></li><li><a href=#模型评估与调优><strong>模型评估与调优</strong></a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=面试准备大纲><strong>面试准备大纲</strong>
<a class=anchor href=#%e9%9d%a2%e8%af%95%e5%87%86%e5%a4%87%e5%a4%a7%e7%ba%b2>#</a></h1><hr><h2 id=基础理论><strong>基础理论</strong>
<a class=anchor href=#%e5%9f%ba%e7%a1%80%e7%90%86%e8%ae%ba>#</a></h2><hr><h3 id=数学与统计学><strong>数学与统计学</strong>
<a class=anchor href=#%e6%95%b0%e5%ad%a6%e4%b8%8e%e7%bb%9f%e8%ae%a1%e5%ad%a6>#</a></h3><ul><li>✅ 线性代数（矩阵运算、特征值分解）</li><li>✅ 概率论与统计（贝叶斯定理、分布、假设检验）</li><li>✅ 优化方法（梯度下降、Adam、学习率调度）</li></ul><hr><h3 id=机器学习基础><strong>机器学习基础</strong>
<a class=anchor href=#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80>#</a></h3><ul><li>✅ 监督学习（线性回归、决策树、SVM、集成学习）</li><li>✅ 评估指标（准确率、召回率、F1、AUC-ROC）</li><li>✅ 过拟合与正则化（L1/L2、Dropout）</li></ul><hr><h3 id=深度学习基础><strong>深度学习基础</strong>
<a class=anchor href=#%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80>#</a></h3><ul><li>⭐ 神经网络（前向传播、反向传播、激活函数）</li></ul><details><summary><strong class=custom-details-title>⭐ RNN（序列建模、自回归、语言模型）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⚠️ 序列模型（Sequence Models）和传统模型有什么区别？</strong></summary><div class=markdown-inner><h2><b>序列模型（Sequence Models）和传统模型有什么区别？</b></h2><p>序列模型（Sequence Models）和传统模型的主要区别在于它们处理数据的方式。传统模型，如线性回归（Linear Regression），通常 <strong>假设输入数据是独立同分布（i.i.d., independent and identically distributed）的</strong>，即每个输入数据点的顺序不影响其他数据点。在这些模型中，数据被处理为<strong>独立的特征向量，通常是静态的，缺乏时间或顺序的关联</strong>。</p><p>序列模型，尤其是在自然语言处理领域，强调对数据中 <strong>时间或顺序信息的建模</strong>。他们往往能够通过其递归结构捕捉序列中前后数据点之间的依赖关系。序列模型在处理时考虑到顺序，因此它们在语言建模（Language Modeling）、语音识别（Speech Recognition）等任务中表现优异，因为这些任务 <strong>依赖于上下文信息和时间序列的动态变化</strong>。</p></div></details><details><summary><strong class=custom-details-title>⚠️ 什么是自回归（Autoregression）和自回归模型？</strong></summary><div class=markdown-inner><h2><b>什么是自回归（Autoregression）和自回归模型？</b></h2><p>自回归（Autoregression）是一种统计模型，用于预测时间序列数据中的未来值。它基于一个假设：<strong>当前的观测值（或输出）与过去的观测值有直接关系</strong>。自回归模型通过 <strong>使用历史数据点作为输入，预测下一个时间步的值</strong>。在这种模型中，当前的输出是历史数据的线性组合，具体来说，当前的值是由过去若干个数据点的加权和构成。</p><p>在自然语言处理（NLP）中的生成式语言模型（如自回归语言模型）中，自回归模型的原理类似于时间序列分析。自回归语言模型在生成文本时，将每个词作为输入，并依赖于已经生成的词序列来预测下一个词。在这种情况下，模型的预测依赖于前一个或前几个词，因此可以用自回归模型的框架来描述这一过程。<strong>它通过逐步生成下一个输出（例如，词或值）并基于之前的生成结果来调整预测，确保每一步都与前面的步骤密切相关</strong>。</p></div></details><details><summary><strong class=custom-details-title>⚠️ 如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？</strong></summary><div class=markdown-inner><h2><b>如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？</b></h2><p>在自然语言处理（NLP）中，语言模型（Language Model, LM） 是一种用于预测句子或文本中单词序列的模型。语言模型的核心目标是 <strong>估计一个给定单词序列出现的概率</strong>。具体来说，对于一个给定的单词序列，语言模型的任务是计算该序列的联合概率：</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[
P(x_1, \ldots, x_T)
\]</span><p>我们可以通过链式法则将序列的联合概率分解成条件概率的乘积，从而将问题转化为自回归预测（autoregressive prediction）：</p><span>\[
P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1)
\]</span><p>在语言模型中，最常见的假设是 马尔科夫假设（Markov Assumption），即 <strong>当前单词的出现只依赖于前一个或前几个单词</strong>。基于这一假设，<strong>n-gram模型（n-gram model）</strong> 是一种常见的语言模型，它通过计算某个单词在给定其前 n-1 个单词的条件下出现的概率来进行预测。例如：</p><span>\[
P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1})
\]</span></div></details><details><summary><strong class=custom-details-title>⚠️ 如何衡量语言模型质量？什么是困惑度（Perplexity）？</strong></summary><div class=markdown-inner><h2><b>什么是困惑度（Perplexity）？</b></h2><p>衡量语言模型质量的一种方法是评估其对文本的预测能力。一个好的语言模型 <strong>能够以较高的准确性预测下一个词（token）</strong>。</p><span>\[
P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1)
\]</span><p>为了量化模型质量，可以通过计算序列的似然（likelihood）。然而，直接比较似然值并不直观，<strong>因为较短的序列通常有更高的似然值</strong>。例如我们用交叉熵（cross-entropy）衡量模型对序列的预测能力。</p><span>\[
\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1)
\]</span><p>为了解决：“较短的序列通常有更高的似然值”，自然语言处理领域通常使用<strong>困惑度（Perplexity）</strong> 作为评价标准，它是交叉熵损失的指数形式：</p><span>\[
\text{Perplexity} = \exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right)
\]</span><p><strong>困惑度可以理解为我们在选择下一个词时平均可用的真实选项数的倒数。困惑度越低，模型质量越高，表明其对文本序列的预测能力越强。</strong></p></div></details><details><summary><strong class=custom-details-title>⚠️ 在 RNN 中是如何读取长序列数据（Partitioning Sequences）的？</strong></summary><div class=markdown-inner><h2><b>在 RNN 中是如何读取长序列数据（Partitioning Sequences）的？</b></h2><p>由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。为了高效处理数据，模型通常以固定长度的序列小批量（minibatch）为单位进行训练。一个关键问题是 <strong>如何从数据集中随机读取输入序列和目标序列的小批量</strong>。处理长序列数据时，常用的 Partitioning Sequences 方法主要包括以下几种：</p><ol><li><strong>固定长度切分（Fixed-length Splitting）</strong> 是最常见的方法之一，其中将长序列分割成固定大小的子序列。这种方法简单且易于实现，但可能会丢失跨子序列的上下文信息。</li><li><strong>滑动窗口（Sliding Window）</strong> 方法通过定义一个窗口大小并在长序列中滑动该窗口来划分数据。每次滑动时，窗口会覆盖一定数量的词汇，并且每次滑动的步长通常为窗口大小的一部分，确保子序列之间有重叠，从而保持一定的上下文信息。</li></ol></div></details><details><summary><strong class=custom-details-title>⚠️ 什么是RNN？描述RNN的基本结构？？</strong></summary><div class=markdown-inner><h2><b>什么是RNN？描述RNN的基本结构？？</b></h2><p>在用马尔可夫模型和n-gram模型用于语言建模时，这些模型中某一时刻的词（token）只依赖于前 n 个词。如果我们希望将更早时间步的词对当前词的影响考虑进来，就需要增加 n 的值。然而，随着 n 增加，模型参数的数量也会呈指数级增长，因为需要为词汇表中的每个词存储对应的参数。因此， 因此与其将历史信息模型化，不如使用<strong>隐变量模型（latent variable model）</strong>：</p><span>\[
P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid h_{t-1})
\]</span><p>潜在变量模型的核心思想是通过 <strong>引入一个隐藏状态（hidden state）</strong>，它保存了直到当前时间步的序列信息。具体来说，隐藏状态在任意时间步可以通过 <strong>当前输入</strong> 和 <strong>上一个隐藏状态</strong> 来计算：</p><span>\[
h_t = f(x_{t}, h_{t-1})
\]</span><p><strong>循环神经网络（RNN） 就是通过隐藏状态来建模序列数据的神经网络</strong>。RNN 由三个主要部分组成：</p><ol><li><strong>输入层（Input layer）</strong>：接收序列数据，通常表示为词向量（word embeddings）或特征向量（feature vectors）。</li><li><strong>隐藏层（Hidden layer）</strong>：核心部分，由 隐藏状态（hidden state） 组成，每个时间步的隐藏状态不仅依赖于当前输入 x_t ，还依赖于前一个时间步的隐藏状态 h_{t-1} 。<strong>隐藏状态就是网络当前时刻的”记忆”</strong>。更新公式如下：</li></ol><span>\[
\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh} + \mathbf{b}_h)
\]</span><ol start=3><li><strong>输出层（Output layer）</strong>：用于预测目标值 y_t ，通常通过 全连接层（fully connected layer） 和 Softmax 激活函数（Softmax activation function） 计算类别概率：</li></ol><span>\[
\mathbf{O} = \mathbf{H} \mathbf{W}_{hq} + \mathbf{b}_q,
\]</span><p>在标准的RNN模型中，<strong>隐藏单元（hidden state）的权重是共享的</strong>，<strong>即相同的权重矩阵 W_xh（从输入到隐藏状态的权重）和 W_hh（从上一个时间步的隐藏状态到当前隐藏状态的权重）在每个时间步都会被复用</strong>。这意味着，RNN通过不断更新隐藏状态，并依赖同一组权重来捕捉序列中的时序信息。尽管RNN在每个时间步都会计算新的隐藏状态，但在基础模型中没有涉及多层结构的概念。</p></div></details><details><summary><strong class=custom-details-title>⚠️ RNN 在 inference 阶段的解码 (Decoding) 是怎么实现的？</strong></summary><div class=markdown-inner><h2><b>RNN 在 inference 阶段的解码 (Decoding) 是怎么实现的？</b></h2><p>在训练好语言模型后，模型不仅可以预测下一个 token，还可以通过将前一个预测的 token 作为输入，连续地预测后续的 token。标准的 RNN 解码过程的步骤可以总结为：</p><ol><li><strong>Warm-up阶段</strong>：<ul><li>解码开始时，将输入序列（已知 token ）输入到模型中，不输出任何结果。</li><li>目的是通过传递隐藏状态（hidden state），初始化模型内部状态以适应上下文。</li></ul></li><li><strong>续写生成</strong>：<ul><li>在输入完前缀后，模型开始生成后续字符。</li><li>每次生成一个字符，将其作为下一个时间步的输入，依次循环生成目标长度的文本。</li></ul></li><li><strong>输入和输出映射</strong>：<ul><li>通过输出层预测字符分布，并选择概率最大的字符作为结果。</li></ul></li></ol></div></details><details><summary><strong class=custom-details-title>⚠️ RNN训练时的主要挑战是什么？</strong></summary><div class=markdown-inner><h2><b>RNN训练时的主要挑战是什么？</b></h2><p>在训练 循环神经网络（Recurrent Neural Network, RNN） 时，主要面临以下几个挑战：</p><ol><li><strong>梯度消失（Vanishing Gradient）和梯度爆炸（Exploding Gradient）</strong>：由于 RNN 依赖于 <strong>时间步（time step）上的递归计算，在反向传播（Backpropagation Through Time, BPTT）</strong> 时，梯度会随着时间步数的增加不断衰减或增长。如果梯度值指数级减小，会导致模型在长序列上的学习能力受限，难以捕捉远距离依赖（long-range dependencies）；如果梯度值指数级增大，则会导致梯度爆炸，使得模型参数更新过大，训练变得不稳定。</li><li><strong>长期依赖问题（Long-Term Dependency Problem）</strong>：RNN 通过隐藏状态（hidden state） 传递信息，但当序列较长时，<strong>早期输入的信息会逐渐被后续时间步的信息覆盖</strong>，导致模型难以捕获远距离的上下文关系。</li><li><strong>计算效率低（Sequential Computation Bottleneck）</strong>：RNN 的计算是顺序的（sequential），<strong>即当前时间步的计算依赖于前一个时间步的计算结果，因此难以并行化</strong>。这使得 RNN 的训练和推理速度远低于 Transformer 这种可以全并行计算的架构。</li></ol></div></details><details><summary><strong class=custom-details-title>⚠️ 什么是梯度裁剪（Gradient Clipping）？为什么它被运用在 RNN 训练过程中？</strong></summary><div class=markdown-inner><h2><b>什么是梯度裁剪（Gradient Clipping）？为什么它被运用在 RNN 训练过程中？</b></h2><p><strong>梯度裁剪（Gradient Clipping）</strong> 是一种用于 <strong>防止梯度爆炸（Gradient Explosion）</strong> 的技术，主要在训练 循环神经网络（Recurrent Neural Networks, RNNs） 时使用。由于 RNN 需要进行反向传播，当序列较长时，梯度可能会在传播过程中指数级增长，导致 <strong>参数更新过大</strong>，进而影响模型的稳定性。</p><p>梯度裁剪的核心思想是在反向传播时，<strong>如果梯度的范数（Norm）超过了某个预设阈值（Threshold），则对梯度进行缩放（Scaling），限制梯度范数不超过一定范围</strong>，即：</p><span>\[
\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}
\]</span></div></details></div></details><details open><summary><strong class=custom-details-title>⭐ 经典序列模型（LSTM、GRU、Seq2Seq）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⚠️ LSTM（Long Short-Term Memory）的核心架构和原理是什么？</strong></summary><div class=markdown-inner><h2><b>LSTM 的核心架构和原理是什么？</b></h2><p>LSTM（Long Short-Term Memory）相比于传统 RNN 的主要改进在于它引入了 门控机制（Gating Mechanism），有效缓解了 梯度消失（Gradient Vanishing） 和 梯度爆炸（Gradient Explosion） 问题，使得模型能够捕捉长期依赖信息（Long-Term Dependencies）。</p><p>LSTM 由 遗忘门（Forget Gate）、输入门（Input Gate） 和 输出门（Output Gate） 组成，每个时间步通过这些门控单元来控制信息的流动。其中，<strong>遗忘门</strong> 负责决定遗忘多少过去的信息，<strong>输入门</strong> 控制新信息的写入，<strong>输出门</strong> 影响隐藏状态（Hidden State）的更新。</p><span>\[
\begin{split}\begin{aligned}
\mathbf{I}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xi} + \mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i),\\
\mathbf{F}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xf} + \mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f),\\
\mathbf{O}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xo} + \mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o),
\end{aligned}\end{split}
\]</span><blockquote class="book-hint warning"><p><strong>Note：</strong> 尽管公式形式类似，LSTM 中的门和一般 RNN 中的 Hidden state 的<strong>核心作用完全不同</strong>：</p><ol><li><strong>门的作用是“控制流动”</strong>：<ul><li>门本质上是一种控制开关，它的输出必须在 [0, 1] 之间，这样才能<strong>直观地表示“通过的信息比例”</strong>。所以门的激活函数使用的是 Sigmoid。</li><li>它们主要用于<strong>调节信息的流动</strong>，而不是直接参与信息存储。</li></ul></li><li><strong>Hidden State 的作用是“存储和传递信息”</strong>：<ul><li>而 Hidden state 的激活函数使用的是 tanh，其范围为 [-1, 1]，<strong>更适合表示信息本身的动态特征</strong>。</li><li>它不仅受门控机制影响，还通过非线性变换从记忆单元中提取信息。</li></ul></li></ol></blockquote><p>此外，LSTM 还维护了一个额外的 细胞状态（Cell State, C_t），用于长期存储信息。<strong>它可以被视为截止至当前时刻 t 的综合记忆信息</strong>。</p><p>新输入数据结合了当前时刻的输入 X 和上一个时间步的 Hidden State，可以被表示为：</p><span>\[
\tilde{\mathbf{C}}_t = \textrm{tanh}(\mathbf{X}_t \mathbf{W}_{\textrm{xc}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hc}} + \mathbf{b}_\textrm{c}),
\]</span><p>输入门控制我们在多大程度上考虑新输入数据，而遗忘门则决定了我们保留多少旧的记忆单元内部状态。通过使用Hadamard 积逐元素相乘）运算符，LSTM 的记忆单元内部状态的更新方程为：</p><span>\[
\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t
\]</span><p>最后，隐藏状态（Hidden State）定义了记忆单元的输出方式，它由输出门（Output Gate）控制。具体计算步骤如下：首先，对记忆单元的内部状态 应用 <code>tanh</code> 函数，使其值被规范化到 <code>(-1, 1)</code> 区间内。然后，将这一结果与输出门的值逐元素相乘，计算得到隐藏状态：</p><span>\[
\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t)
\]</span><p><strong>输出门（Output Gate） 的主要作用</strong>是控制 <strong>当前时刻的隐藏状态的输出</strong> 内容，从而决定 LSTM 的对外信息传递。Cell State 是 LSTM 的内部长期记忆，但它并不直接对外输出。输出门通过<strong>选择性地提取 Cell State 中的信息</strong>，并结合门控机制生成 <strong>新的Hidden State（短期记忆的表达）</strong>，作为当前时间步的隐藏状态对外输出。这种机制确保 LSTM 在对外传递信息时，<strong>不会将 Cell State 中的所有内容暴露出去</strong>，避免噪声干扰，同时保留最相关的信息。</p></div></details><details><summary><strong class=custom-details-title>⚠️ LSTM 解决的问题和原因？</strong></summary><div class=markdown-inner><h2><b>LSTM解决的问题和原因？</b></h2><p>LSTM（Long Short-Term Memory）主要解决了标准RNN在处理长序列时的 <strong>梯度消失（vanishing gradients）和梯度爆炸（exploding gradients）</strong> 问题。 这些问题导致普通RNN在处理长期依赖（long-term dependencies）时性能较差，无法有效捕获长时间跨度的信息。</p><ol><li><strong>细胞状态（Cell State）作为长期记忆的载体</strong><ul><li>LSTM引入了一个额外的细胞状态，它可以通过直通路径（&ldquo;constant error carousel&rdquo;）跨时间步传播信息，<strong>几乎不受梯度消失或梯度爆炸的影响</strong>。</li></ul></li><li><strong>梯度传播更稳定</strong><ul><li>普通RNN的梯度通过时间步传播时，会被<strong>反复乘以隐状态的权重矩阵</strong>。当权重矩阵的特征值远离 1 时，梯度会出现指数增长（梯度爆炸）或指数衰减（梯度消失）。</li><li>在LSTM中，<strong>细胞状态通过线性加权方式更新</strong>（不直接经过激活函数的非线性变换），从而避免了梯度的剧烈变化。门机制确保了信息流动的可控性，进一步减轻了梯度不稳定的问题。</li></ul></li><li><strong>更强的记忆能力</strong><ul><li>LSTM能<strong>同时捕获短期依赖和长期依赖（通过细胞状态）</strong>。在长时间序列中，它可以动态调整对不同时间跨度的信息的关注程度，使其既能够记住长期信息，又不会因记忆过多导致模型过载。</li></ul></li></ol></div></details><details><summary><strong class=custom-details-title>⚠️ 解释 GRU（Gated Recurrent Unit）的核心架构和原理？</strong></summary><div class=markdown-inner><h2><b>解释 GRU 的核心架构和原理？</b></h2><p>GRU（门控循环单元）是 <strong>LSTM记忆单元的简化版本</strong> 并保留内部状态和乘法门控机制的核心思想。相比LSTM，GRU在很多任务中可以达到相似的性能，但由于结构更加简单，计算速度更快。</p><p>在GRU（Gated Recurrent Unit）中，LSTM的三个门被替换为两个门：<strong>重置门（Reset Gate）</strong> 和 <strong>更新门（Update Gate）</strong>。这两个门使用了 <code>Sigmoid</code> 激活函数，输出值限制在区间 <code>[0, 1]</code> 内。</p><ul><li><strong>重置门</strong>：决定了当前状态需要记住多少之前隐藏状态的信息。</li><li><strong>更新门</strong>：控制新状态有多少是继承自旧状态的。</li></ul><span>\[
\begin{split}\begin{aligned}
\mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r),\\
\mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} + \mathbf{b}_z),
\end{aligned}\end{split}
\]</span><p>重置门（reset gate）与标准更新机制相结合，生成时间步 t 的 **候选隐藏状态（candidate hidden state），公式如下：</p><span>\[
\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} + \mathbf{b}_h),
\]</span><blockquote class="book-hint warning"><p><strong>Note：</strong> 重置门主要是对长期记忆进行筛选，在计算候选隐藏状态时，抑制过去隐藏状态中的某些部分，使得模型更多地依赖当前输入。这种机制让模型能够<strong>专注于短期依赖</strong>，通过结合当前输入<strong>产生一个更符合短期记忆的候选状态</strong>。</p></blockquote><p><strong>更新门（Update gate）决定了新隐藏状态</strong> 在多大程度上保留旧状态与新候选状态的信息。具体而言，控制了二者的加权组合，公式如下：</p><span>\[
\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1} + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t.
\]</span><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>更新门（update gate）</strong> 则决定 <strong>当前时刻的隐藏状态是由之前的隐藏状态（长期记忆）和当前输入（短期信息）以何种比例组合而成</strong>。更新门的值接近 1 时，模型保留大部分的长期记忆，接近 0 时则依赖更多的当前输入。这种机制帮助 GRU 捕捉长期依赖关系，因为更新门可以调节模型在序列中如何传递和利用过去的记忆。</p></blockquote></div></details><details><summary><strong class=custom-details-title>⚠️ 什么是深层 RNN（Deep RNNs），它与普通 RNN 有什么区别？</strong></summary><div class=markdown-inner><h2><b>什么是深层 RNN（Deep RNNs），它与普通 RNN 有什么区别？</b></h2><p>深层循环神经网络（Deep Recurrent Neural Networks, DRNN）是通过 <strong>堆叠多个RNN层</strong> 实现的。单隐藏层的RNN网络结构在捕捉时间步内部输入与输出之间复杂关系时存在局限。因此，为了同时增强 <strong>模型对时间依赖（temporal dependency）</strong> 和 <strong>时间步内部输入与输出关系</strong> 的建模能力，常会构造在时间方向和输入输出方向上都更深的RNN网络。这种深度的概念类似于在多层感知机（MLP）和深度卷积神经网络（CNN）中见到的层级加深方法。</p><p>在深层RNN中，每一时间步的隐藏单元 <strong>不仅依赖于同层前一个时间步的隐藏状态</strong>，还依赖于 <strong>前一层相同时间步的隐藏状态</strong>。这种结构使得深层RNN能够 <strong>同时捕获长期依赖关系（long-term dependency）和复杂的输入-输出关系</strong>。</p><p>在深层RNN的第l（l=1,&mldr;,L）个隐藏层Hidden state可以表示为:</p><span>\[
\mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)} + \mathbf{b}_h^{(l)}),
\]</span><p>最后，输出层的计算仅基于第<span>
(l)
</span>个隐藏层最终的隐状态：</p><span>\[
\mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q,
\]</span></div></details><details><summary><strong class=custom-details-title>⚠️ 什么是双向循环神经网络（BiRNN）？为什么在某些任务中它的的效果优于RNN？</strong></summary><div class=markdown-inner><h2><b>什么是双向循环神经网络（BiRNN）？为什么在某些任务中它的的效果优于RNN？</b></h2><p><strong>双向循环神经网络（Bidirectional Recurrent Neural Network, BiRNN）是一种扩展传统循环神经网络（Recurrent Neural Network, RNN）</strong> 的方法。与单向 RNN 仅从过去到未来处理序列数据不同，BiRNN 在每个时间步（Timestep）中同时计算两个方向的信息流：一个从前向后（Forward Direction），另一个从后向前（Backward Direction）。这种结构通过两个独立的隐藏层（Hidden Layers）分别处理时间序列的正向和反向信息，并在输出层（Output Layer）结合这两个隐藏状态（Hidden States），从而获得更丰富的上下文信息。前向和反向隐状态的更新如下：</p><span>\[
\begin{split}\begin{aligned}
\overrightarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{hh}^{(f)} + \mathbf{b}_h^{(f)}),\\
\overleftarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{hh}^{(b)} + \mathbf{b}_h^{(b)}),
\end{aligned}\end{split}
\]</span><p>接下来，将前向隐状态和反向隐状态连接起来，获得需要送入输出层的隐状态。在具有多个隐藏层的深度双向循环神经网络中，该信息作为输入传递到下一个双向层。</p><span>\[
\mathbf{H}_t = \begin{bmatrix} \overrightarrow{\mathbf{H}}_t \\ \overleftarrow{\mathbf{H}}_t \end{bmatrix}
\]</span><p>最后，输出层计算得到的输出为：</p><span>\[
\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q.
\]</span><p>在某些任务中，BiRNN 的效果优于单向 RNN，主要是因为它能够利用未来和过去的信息，而不仅仅依赖于当前时间步之前的历史数据。BiRNN 适用于 <strong>需要充分利用上下文信息的序列任务</strong>，特别是在 NLP 领域。具体来说，文本分类（Text Classification）、情感分析（Sentiment Analysis）、语音识别（Automatic Speech Recognition, ASR）等任务都可以从 BiRNN 的双向信息流中获益。</p></div></details><details><summary><strong class=custom-details-title>⚠️ 什么是 Encoder-Decoder 结构？它是如何工作的？</strong></summary><div class=markdown-inner><h2><b>什么是 Encoder-Decoder 结构？它是如何工作的？</b></h2><p>在序列到序列（sequence-to-sequence）问题中（如机器翻译），<strong>输入和输出通常具有不同的长度，且无法直接对齐</strong>。为了解决这一问题，通常采用编码器-解码器（encoder-decoder）架构。这个架构包括两个主要组件：</p><p><strong>编码器（Encoder）的作用是处理输入序列并将其转化为一个固定大小的表示（通常是一个向量，也叫做上下文向量（Context Vector））</strong>。在传统的 RNN（Recurrent Neural Network）或 LSTM（Long Short-Term Memory）网络中，编码器逐步读取输入序列的每个元素（如单词或字符），并通过递归地更新其隐藏状态（Hidden State）来捕捉输入的语义信息。最终，<strong>编码器输出的隐藏状态或最后一个时间步的隐藏状态</strong>（在一些变体中是所有时间步的隐藏状态）作为对输入序列的总结。</p><p><strong>解码器（Decoder）则是基于编码器的输出生成目标序列</strong>。解码器通常也是一个RNN或LSTM，它的工作方式是逐步预测输出序列中的每个元素。解码器首先接受编码器生成的上下文向量作为初始的隐藏状态，然后在生成每个目标词时，它根据当前隐藏状态以及之前生成的输出（或在训练时，使用教师强迫（Teacher Forcing），即真实的标签作为输入）来预测下一个词。这个过程一直持续到生成完整的目标序列。</p></div></details><details><summary><strong class=custom-details-title>⚠️ 什么是 强制教学（teacher forcing）？</strong></summary><div class=markdown-inner><h2><b>什么是 强制教学（teacher forcing）？</b></h2></div></details><details><summary><strong class=custom-details-title>⚠️ 如何使用 Beam Search 以提高生成文本的质量？</strong></summary><div class=markdown-inner><h2><b>如何使用 Beam Search 以提高生成文本的质量？</b></h2></div></details></div></details><hr><h2 id=llm-核心技术><strong>LLM 核心技术</strong>
<a class=anchor href=#llm-%e6%a0%b8%e5%bf%83%e6%8a%80%e6%9c%af>#</a></h2><hr><h3 id=llm-基础组件><strong>LLM 基础组件</strong>
<a class=anchor href=#llm-%e5%9f%ba%e7%a1%80%e7%bb%84%e4%bb%b6>#</a></h3><hr><details><summary><strong class=custom-details-title>⭐ Tokenization - 分词（BPE, WordPiece, SentencePiece）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⚠️ 什么是 Tokenization？为什么它对LLM至关重要？</strong></summary><div class=markdown-inner><h2><b>什么是 Tokenization？为什么它对LLM至关重要？</b></h2><p>Tokenization（分词）是将文本拆分成更小的单元（tokens）的过程，这些单元可以是单词、子词或字符。在自然语言处理中，分词是文本预处理的重要步骤。他直接影响模型对语义的理解能力、计算效率和内存占用。</p><ul><li><strong>Example</strong>：句子 “Hello, world!” 可被切分为 <code>["Hello", ",", "world", "!"]</code>（基于空格和标点）。</li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ 常见的 Tokenization 方法有哪些？它们的区别是什么？</strong></summary><div class=markdown-inner><h2><b>常见的 Tokenization 方法有哪些？它们的区别是什么</b></h2><ul><li><strong>Word-level</strong>：按词切分（如 <code>“natural language processing” → ["natural", "language", "processing"]</code>），但词表大且难以处理未登录词（OOV）。</li><li><strong>Subword-level（主流方法）</strong>：<ul><li><strong>BPE（Byte-Pair Encoding）</strong>：通过合并高频字符对生成子词（如GPT系列使用）。</li><li><strong>WordPiece</strong>：类似BPE，但基于概率合并（如BERT使用）。</li><li><strong>SentencePiece</strong>：无需预分词，直接处理原始文本（如T5使用）。</li></ul></li><li><strong>Character-level</strong>：按字符切分，词表极小但序列长且语义建模困难。</li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ BPE算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2><b>BPE 算法的工作原理是什么？请举例说明。</b></h2><p>BPE（Byte Pair Encoding）是一种子词分割（Subword Segmentation）算法，核心思想是基于 <strong>统计频率</strong> 的合并（Merge frequent pairs）。</p><ul><li><strong>工作原理</strong>：<ol><li>统计字符对（Byte Pair）频率，找到最常见的相邻字符对。<pre tabindex=0><code>[&#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34; &#34;, &#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34;e&#34;, &#34;r&#34;, &#34; &#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;]
</code></pre></li><li>合并最频繁的字符对，形成新的子词单元。<pre tabindex=0><code>(&#34;l&#34;, &#34;o&#34;) -&gt; 2次
(&#34;o&#34;, &#34;w&#34;) -&gt; 2次
    ...
</code></pre>（第一次）合并 <code>("l", "o") → "lo"</code>：<pre tabindex=0><code>[&#34;lo&#34;, &#34;w&#34;, &#34;lo&#34;, &#34;w&#34;, &#34;er&#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;st&#34;]
</code></pre>（第二次）合并 <code>("lo", "w") → "low"</code><pre tabindex=0><code>[&#34;low&#34;, &#34;low&#34;, &#34;er&#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;st&#34;]
</code></pre></li><li>重复步骤 1 和 2，直到达到预定的子词词汇量。<pre tabindex=0><code>[&#34;low&#34;, &#34;lower&#34;, &#34;er&#34;, &#34;newest&#34;]
</code></pre><pre tabindex=0><code>vocab = {&#34;low&#34;: 100, &#34;lower&#34;: 101, &#34;er&#34;: 102, &#34;newest&#34;: 103}
</code></pre></li></ol></li></ul><p>BPE的优点是它 <strong>能够有效地处理未登录词</strong>，并且在处理长尾词（rare words）时表现良好。比如，词”unhappiness”可以被分解为”un” + “happiness”，而不是完全看作一个新的词。</p></div></details><details><summary><strong class=custom-details-title>⚠️ WordPiece 算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2><b>WordPiece 算法的工作原理是什么？请举例说明</b></h2><p>WordPiece 的核心理念与BPE相似，但合并策略更注重语义完整性。其训练过程同样从基础单元（如字符）开始，但选择合并的标准并非单纯依赖频率，而是通过 <strong>计算合并后对语言模型概率的提升幅度</strong>，优先保留能够增强语义连贯性的子词。</p><ul><li><p>假设语言模型的目标是最大化训练数据的似然概率，在 WordPiece 中，简化为 通过子词频率估计概率（即一元语言模型）。通过计算合并后对语言模型概率的提升幅度进行合并：
<span>[
\begin{equation}
P(S)≈ \prod^{n}_{i=1}P(s_i)
\end{equation}
]</span></p></li><li><p><strong>工作原理</strong>：</p><ol><li><p>与BPE类似，首先将所有词分解为最小的单位（如字符）。</p><pre tabindex=0><code>[&#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34; &#34;, &#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34;e&#34;, &#34;r&#34;, &#34; &#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;, &#34; &#34;, &#34;w&#34;, &#34;i&#34;, &#34;d&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;]
</code></pre></li><li><p>统计所有可能的字符对（或子词对）在文本中的共现频率。</p></li><li><p>合并字符对，选择合并后能 <strong>最大化语言模型似然概率</strong> 的字符对。具体公式为：选择使 <code>score = freq(pair) / (freq(first) * freq(second)) </code>最大的字符对（<strong>与 BPE 不同，BPE 仅选择频率最高的对</strong>）。每次合并对语言模型概率提升最大的合并组合。</p><p>这里的 <code>##</code> 表示这个 token 只能作为后缀出现，不会单独存在。</p><pre tabindex=0><code>{&#34;low&#34;, &#34;##er&#34;, &#34;##ing&#34;, &#34;new&#34;, &#34;##est&#34;, &#34;wide&#34;, &#34;##st&#34;}
</code></pre></li><li><p>重复合并得分最高的字符对，直到达到预设的词汇表大小。</p><pre tabindex=0><code>vocab = {&#34;low&#34;: 100, &#34;##er&#34;: 101, &#34;##ing&#34;: 102, &#34;new&#34;: 103, &#34;##est&#34;: 104, &#34;wide&#34;: 105, &#34;##st&#34;: 106}
</code></pre></li></ol></li></ul><p>WordPiece 通过最大化语言模型概率合并子词，<strong>生成的子词更贴合语义需求</strong>。但计算复杂度更高，需多次评估合并得分。</p><ul><li>若需模型捕捉深层语义（如预训练任务），优先选择 WordPiece。</li><li>若需快速处理大规模数据且词汇表灵活，BPE 更合适。</li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ SentencePiece 算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2><b>SentencePiece 算法的工作原理是什么？请举例说明</b></h2><p>SentencePiece 是一种无监督的子词分割算法，其核心创新在于直接处理原始文本（包括空格和特殊符号），无需依赖预分词步骤。<strong>它支持两种底层算法：BPE 或 基于概率的Unigram Language Model</strong>。训练时，SentencePiece 将空格视为普通字符 <code>_</code>，可 <strong>直接处理多语言混合文本（如中英文混杂）</strong>，并自动学习跨语言的统一子词划分规则。</p><ul><li><strong>Example：</strong> <code>"Hello世界" → 编码为 ["▁He", "llo", "▁世", "界"]。</code></li></ul><p>SentencePiece 支持多语言（Multilingual）无需调整，统一处理空格与特殊符号。这一特性使其在需要多语言支持的场景（如T5模型）中表现突出，同时简化了数据处理流程，特别适合处理社交媒体文本等非规范化输入。</p></div></details><details><summary><strong class=custom-details-title>⚠️ 如何处理未登录词（OOV）？</strong></summary><div class=markdown-inner><h2><b>如何处理未登录词（OOV）？</b></h2><ul><li><strong>子词切分</strong>：将OOV（Out-of-Vocabulary）词拆分为已知子词（如 <code>“tokenization” → ["token", "ization"]</code>）。</li><li><strong>回退策略</strong>：使用特殊标记（如 <code>[UNK]</code>），但会损失信息。</li><li><strong>动态更新词表</strong>：在增量训练时扩展词表。</li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ Tokenization 如何影响模型性能？</strong></summary><div class=markdown-inner><h2><b>Tokenization 如何影响模型性能？</b></h2><ul><li><strong>词表过大</strong>：增加内存消耗，降低计算效率（Softmax 计算成本高）。</li><li><strong>词表过小</strong>：导致长序列和语义碎片化（如切分为无意义的子词）。</li><li><strong>语言适配性</strong>：中/日/韩等非空格语言需要特殊处理（如 SentencePiece）。</li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ 如何为多语言模型设计Tokenization方案？</strong></summary><div class=markdown-inner><h2><b>如何为多语言模型设计 Tokenization 方案？</b></h2><ul><li><strong>统一词表</strong>：使用 SentencePiece 跨语言训练（如mBERT）。</li><li><strong>平衡语种覆盖</strong>：根据语种数据量调整合并规则，避免小语种被淹没。</li><li><strong>特殊标记</strong>：添加语言ID（如 <code>[EN]</code>、<code>[ZH]</code>）引导模型区分语言。</li></ul></div></details></div></details><details><summary><strong class=custom-details-title>⭐ Word Embeddings - 词嵌入（Word2Vec、GloVe、FastText）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⚠️ 什么是词嵌入（Word Embeddings）？为什么它重要？</strong></summary><div class=markdown-inner><h2><b>什么是词嵌入（Word Embeddings）？为什么它重要？</b></h2><p>在自然语言处理中，Embedding（词嵌入）是将 <strong>离散的文本数据（如单词、短语或句子）映射到一个连续的、低维度的向量空间（vector space）的过程</strong>。他的作用包括：</p><ol><li><strong>捕捉语义关系（Semantic Relationships）</strong>：能表示同义词、类比关系（如 king - man + woman ≈ queen）。</li><li><strong>降维（Dimensionality Reduction）</strong>：将高维的 <strong>独热编码（One-hot Encoding）</strong> 转换为低维密集向量，提高计算效率。</li><li><strong>解决稀疏性问题（Handling Sparsity）</strong>：相比于独热编码，词嵌入具有更好的泛化能力（Generalization）。</li></ol></div></details><details><summary><strong class=custom-details-title>⚠️ 静态词向量 和 上下文动态词向量的区别？</strong></summary><div class=markdown-inner><h2><b>静态词向量 和 上下文动态词向量的区别？</b></h2><ul><li><strong>静态词向量（Static Word Embeddings）</strong> 的核心特点是 <strong>无论词语出现在何种上下文中，其向量表示均保持不变</strong>。这类方法通过大规模语料训练，捕捉词语间的语义和语法关系。静态词向量的优势在于训练高效、资源消耗低，且生成的向量可直观反映语义相似性（如“猫”和“狗”向量接近）；但其 <strong>局限性是无法处理多义词</strong>（如“苹果”在“水果”和“手机”场景中的不同含义），因为 <strong>每个词仅对应单一向量</strong>。代表模型包括：Word2Vec, GloVe。</li><li><strong>上下文动态词向量（Contextual Word Embeddings）</strong>：静态嵌入虽然可以表示词语的语义，但它们无法根据上下文动态调整，例如 “bank” 在 “river bank” 和 “bank account” 里的含义不同。而 动态词嵌入 解决了这个问题，代表性模型包括 ELMo、BERT 和 GPT。</li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ 解释 Word2Vec 的两种模型：Skip-gram 和 CBOW。</strong></summary><div class=markdown-inner><h2><b>解释 Word2Vec 的两种模型：Skip-gram 和 CBOW。</b></h2><ul><li><p><strong>跳元模型（Skip-gram）</strong> 假设 一个词可以用来在文本序列中生成其周围的单词。以文本序列 <code>“the”“man”“loves”“his”“son”</code> 为例。假设中心词选择 <code>“loves”</code>，并将上下文窗口设置为2，给定中心词 <code>“loves”</code>，跳元模型考虑生成上下文词 <code>“the”“man”“him”“son”</code> 的条件概率。最大化给定中心词时上下文词的条件概率：
$$
max{\sum log P(context_w|center_w)}
$$
Word2Vec 的核心是 <strong>一个浅层神经网络（Shallow Neural Network）</strong>，由一个输入层、一个隐藏层（线性变换层）、一个输出层（Softmax 或其他采样方法）组成:</p><ol><li><strong>输入示例</strong>：<ul><li>句子：<code>“I love natural language processing.”</code></li><li>若窗口大小为1，中心词为 <code>“natural”</code>，则上下文词为 <code>“love”</code> 和 <code>“language”</code>。</li></ul></li><li><strong>输入通过 One-Hot 编码 表示为一个稀疏向量</strong>。例如，若词汇表为 <code>["cat", "dog", "fish"]</code>，则<code>“dog”</code> 的输入编码为 <code>[0, 1, 0]</code>。</li><li><strong>输入层到隐藏层</strong>：输入向量与 输入权重矩阵相乘，得到中心词的嵌入向量。</li><li>通过 <strong>输出权重矩阵将隐层向量映射到输出概率</strong>：</li><li>使用 Softmax 归一化，通过梯度下降优化词向量，使得上下文词的概率最大化。</li></ol><p><strong>跳元模型（Skip-gram）特点</strong>：</p><ul><li><strong>擅长捕捉低频词</strong>：通过中心词预测多个上下文，低频词有更多训练机会。</li><li><strong>训练速度较慢</strong>：输出层需计算多个上下文词的概率。</li></ul></li></ul><hr><ul><li><p><strong>连续词袋（CBOW）</strong> 与 Skip-gram 相对，CBOW 的训练过程是 <strong>给定上下文词，预测中心词</strong>。这里的目标是将多个上下文词的向量平均起来，并通过它们来预测中心词。在文本序列 <code>“the”“man”“loves”“his”“son”</code> 中，在 <code>“loves”</code> 为中心词且上下文窗口为 2 的情况下，连续词袋模型考虑基于上下文词 <code>“the”“man”“him”“son”</code> 生成中心词 <code>“loves”</code> 的条件概率。最大化给定上下文时中心词的条件概率：
$$
max{\sum log P(center_w|context_w)}
$$
连续词袋（CBOW）的训练细节与 跳元模型（Skip-Gram）大部分类似，但是在输入 One-Hot 编码表示时，跳元模型（Skip-Gram）将中心词进行 One-Hot 编码，<strong>而连续词袋（CBOW）将上下文词进行 One-Hot 编码并取平均值</strong>。 例如，中心词为 <code>“natural”</code>，上下文词为 <code>“love”</code> 和 <code>“language”</code> 。输入为 <code>[0, 1, 0, 0, 0]</code>（<code>“love”</code>）和 <code>[0, 0, 0, 1, 0]</code>（<code>“language”</code>）的平均向量 <code>[0, 0.5, 0, 0.5, 0]</code>。此外，输出概率通过 Softmax 计算公式也有不同。</p><p><strong>连续词袋（CBOW）特点</strong>：</p><ul><li><strong>训练速度快</strong>：输入为多个词的均值向量，计算效率高。</li><li><strong>对高频词建模更好</strong>：上下文词共同贡献中心词预测。</li></ul></li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ Word2Vec 如何优化训练效率？</strong></summary><div class=markdown-inner><h2><b>Word2Vec 如何优化训练效率？</b></h2><p>由于 softmax 操作的性质，上下文词可以是词表中的任意项，但是，在一个词典上（通常有几十万或数百万个单词）<strong>求和的梯度的计算成本是巨大的</strong>！为了降低计算复杂度，可以采用两种近似训练方法：负采样（Negative Sampling）和层序softmax（Hierarchical Softmax）。</p><ul><li><p><strong>负采样（Negative Sampling）</strong>：</p><ul><li><strong>核心思想</strong>：将复杂的多分类问题（预测所有词的概率）简化为二分类问题，用少量负样本近似全词汇的Softmax计算。</li><li><strong>正负样本构建</strong>：<ul><li>对每个正样本（中心词与真实上下文词对），随机采样 K 个负样本（非上下文词）。</li><li>例如，中心词 <code>“apple”</code> 的真实上下文词为 <code>“fruit”</code>，则负样本可能是随机选择的 <code>“car”</code>,<code>“book”</code> 等无关词。</li></ul></li><li><strong>目标函数</strong>：最大化正样本对的相似度，同时最小化负样本对的相似度。</li></ul></li><li><p><strong>层序softmax（Hierarchical Softmax）</strong>：</p><ul><li><strong>核心思想</strong>：通过二叉树（如霍夫曼树）编码词汇表，将全局Softmax分解为路径上的二分类概率乘积，减少计算量。</li><li><strong>霍夫曼树构建</strong>：按词频从高到低排列词汇，高频词靠近根节点，形成最短路径。每个内部节点含一个可训练的向量参数。</li></ul></li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ 解释 GloVe 的原理？</strong></summary><div class=markdown-inner><h2><b>解释 GloVe 的原理？</b></h2><p>GloVe（Global Vectors for Word Representation）是一种基于全局统计信息的词向量训练方法，它通过构造整个语料库的共现矩阵（co-occurrence matrix）并进行矩阵分解来学习词的向量表示，其核心思想是 <strong>通过捕捉词与词之间的全局共现关系来学习语义信息</strong>，而不是像 Word2Vec 那样依赖于局部上下文窗口的预测方法。
具体而言，GloVe 首先统计语料库中的词对共现次数，构建一个共现矩阵 X ，其中 X_{ij} 表示词 i 在词 j 附近出现的频率，并计算共现概率:</p><span>\[
P(j|i) = \frac{X_{ij}}{\sum_k X_{ik}}
\]</span><p>GloVe 的核心目标是学习一个词向量映射，使得向量之间的点积能够近似这个共现概率的对数：</p><span>\[
\mathbf{u}_j^\top \mathbf{v}_i + b_i + c_j = \log(X_{ij})
\]</span><ul><li>v_i, u_j 是词 i 和词 j 的向量表示，<strong>每个词都由两个向量组成，一个是中心词向量 ，一个是上下文词向量</strong></li></ul><p>GloVe 并不依赖传统的神经网络，它的学习过程 <strong>更接近矩阵分解（Matrix Factorization）的优化方法</strong>，而非 Word2Vec 这样的前馈神经网络（Feedforward Neural Network）。GloVe <strong>不依赖反向传播（Backpropagation）</strong>，而是直接最小化共现概率对数的加权平方误差，来学习词向量。</p><p>GloVe 的优势在于它直接建模了全局的词共现信息，使得词向量能够更好地捕捉语义相似性和类比关系，如 <code>“king - man + woman ≈ queen”</code>，但由于依赖于整个共现矩阵，计算和存储成本较高，因此适用于大规模离线训练，而不是在线学习任务。</p></div></details><details><summary><strong class=custom-details-title>⚠️ 解释 FastText 的原理？</strong></summary><div class=markdown-inner><h2><b>解释 FastText 的原理？</b></h2><p>FastText 在 Word2Vec 的基础上进行了改进，能够更好地处理 OOV（Out-of-Vocabulary）问题，同时提高计算效率。FastText 的核心思想是 <strong>使用 n-gram 字符级子词（subword） 进行单词表示</strong>，而不是仅仅依赖于整个单词的词向量。它的训练过程类似于 Word2Vec 的 CBOW（Continuous Bag of Words）或 Skip-gram 模型，但 FastText 通过将一个单词拆分成多个 n-gram 片段（如 <code>“apple”</code> 可以被拆分为 <code>&lt;ap, app, ppl, ple, le></code>），然后通过这些 n-gram 子词的向量求和来表示整个单词，从而在处理未见单词时依然可以通过其子词获得较好的表示。</p></div></details><details><summary><strong class=custom-details-title>⚠️ 在实际项目中，如何选择不同的嵌入方法（Embedding Methods）？</strong></summary><div class=markdown-inner><h2><b>在实际项目中，如何选择不同的嵌入方法（Embedding Methods）？</b></h2><ol><li><strong>任务类型与语义需求</strong>：</li></ol><ul><li><strong>基础语义任务（如文本分类、简单相似度计算）</strong>：<ul><li>静态嵌入：Word2Vec、GloVe、FastText。<ul><li>优点：轻量高效，适合低资源场景。</li><li>示例：新闻分类任务中，预训练的 Word2Vec 向量足以捕捉主题关键词的语义。</li></ul></li></ul></li><li><strong>复杂语义任务（如问答、指代消解、多义词理解）</strong>：<ul><li>上下文嵌入：BERT、RoBERTa、XLNet。<ul><li>优点：动态生成上下文相关向量，解决一词多义。</li><li>示例：在医疗问答系统中，BERT可区分“Apple”指公司还是水果。</li></ul></li></ul></li></ul><ol start=2><li><strong>数据量与领域适配</strong>：</li></ol><ul><li><strong>小数据场景</strong>：<ul><li><strong>预训练静态嵌入 + 微调</strong>：使用公开预训练的 Word2Vec/GloVe，结合任务数据微调。</li><li><strong>轻量上下文模型</strong>：ALBERT或TinyBERT，降低训练成本。</li></ul></li><li><strong>大数据场景</strong>：<ul><li><strong>从头训练上下文模型</strong>：基于领域数据训练BERT或GPT，捕捉领域专属语义。</li><li><strong>领域适配</strong>：在金融/法律等领域，使用领域语料继续预训练（Domain-Adaptive Pretraining）。</li></ul></li></ul></div></details></div></details><ul><li>⭐ Transformer 架构核心（Self-Attention、多头注意力、位置编码）</li><li>✅ 预训练模型（BERT、GPT、T5的架构差异）</li></ul><hr><h3 id=大语言模型llm><strong>大语言模型（LLM）</strong>
<a class=anchor href=#%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8bllm>#</a></h3><ul><li>模型架构（Decoder-only vs Encoder-Decoder）</li><li>预训练任务（MLM、CLM、Span Corruption）</li><li>微调技术（全参数微调、LoRA、QLoRA、Prompt Tuning）</li><li>推理优化（KV Cache、量化、模型剪枝）</li></ul><hr><h3 id=工具与框架><strong>工具与框架</strong>
<a class=anchor href=#%e5%b7%a5%e5%85%b7%e4%b8%8e%e6%a1%86%e6%9e%b6>#</a></h3><ul><li>Hugging Face Transformers库（模型加载、Pipeline、Trainer）</li><li>LangChain（Agent、Chain、Tools设计）</li><li>LlamaIndex（文档索引、检索增强生成RAG）</li></ul><hr><h2 id=应用开发与工程化><strong>应用开发与工程化</strong>
<a class=anchor href=#%e5%ba%94%e7%94%a8%e5%bc%80%e5%8f%91%e4%b8%8e%e5%b7%a5%e7%a8%8b%e5%8c%96>#</a></h2><hr><h3 id=应用开发框架><strong>应用开发框架</strong>
<a class=anchor href=#%e5%ba%94%e7%94%a8%e5%bc%80%e5%8f%91%e6%a1%86%e6%9e%b6>#</a></h3><ul><li>LangChain高级用法（自定义Tools、Agent逻辑、流式输出）</li><li>前端集成（Streamlit、Gradio构建交互界面）</li><li>API开发（FastAPI/Flask构建RESTful服务）</li></ul><hr><h3 id=检索增强生成rag><strong>检索增强生成（RAG）</strong>
<a class=anchor href=#%e6%a3%80%e7%b4%a2%e5%a2%9e%e5%bc%ba%e7%94%9f%e6%88%90rag>#</a></h3><ul><li>向量数据库（Faiss、Pinecone、Chroma）</li><li>文档分块与嵌入策略（滑动窗口、语义分块）</li><li>检索优化（重排序、HyDE技术）</li></ul><hr><h3 id=部署与运维><strong>部署与运维</strong>
<a class=anchor href=#%e9%83%a8%e7%bd%b2%e4%b8%8e%e8%bf%90%e7%bb%b4>#</a></h3><ul><li>容器化（Docker、Kubernetes）</li><li>模型部署（ONNX、TensorRT、Triton Inference Server）</li><li>监控与日志（Prometheus、Grafana）</li><li>云服务（AWS SageMaker、GCP Vertex AI）</li></ul><hr><h2 id=模型训练与优化><strong>模型训练与优化</strong>
<a class=anchor href=#%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83%e4%b8%8e%e4%bc%98%e5%8c%96>#</a></h2><hr><h3 id=分布式训练><strong>分布式训练</strong>
<a class=anchor href=#%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83>#</a></h3><ul><li>数据并行 vs 模型并行</li><li>框架（DeepSpeed、PyTorch DDP、FSDP）</li><li>混合精度训练（FP16、BF16）</li></ul><hr><h3 id=高效微调技术><strong>高效微调技术</strong>
<a class=anchor href=#%e9%ab%98%e6%95%88%e5%be%ae%e8%b0%83%e6%8a%80%e6%9c%af>#</a></h3><ul><li>参数高效微调（LoRA、Adapter、Prefix Tuning）</li><li>低资源训练（QLoRA + 4-bit量化）</li><li>指令微调与对齐（RLHF、DPO）</li></ul><hr><h3 id=模型评估与调优><strong>模型评估与调优</strong>
<a class=anchor href=#%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0%e4%b8%8e%e8%b0%83%e4%bc%98>#</a></h3><ul><li>评估基准（GLUE、SuperGLUE、HELM）</li><li>超参数搜索（Optuna、Ray Tune）</li><li>可解释性（Attention可视化、LIME）</li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#基础理论><strong>基础理论</strong></a><ul><li><a href=#数学与统计学><strong>数学与统计学</strong></a></li><li><a href=#机器学习基础><strong>机器学习基础</strong></a></li><li><a href=#深度学习基础><strong>深度学习基础</strong></a></li></ul></li><li><a href=#llm-核心技术><strong>LLM 核心技术</strong></a><ul><li><a href=#llm-基础组件><strong>LLM 基础组件</strong></a></li><li><a href=#大语言模型llm><strong>大语言模型（LLM）</strong></a></li><li><a href=#工具与框架><strong>工具与框架</strong></a></li></ul></li><li><a href=#应用开发与工程化><strong>应用开发与工程化</strong></a><ul><li><a href=#应用开发框架><strong>应用开发框架</strong></a></li><li><a href=#检索增强生成rag><strong>检索增强生成（RAG）</strong></a></li><li><a href=#部署与运维><strong>部署与运维</strong></a></li></ul></li><li><a href=#模型训练与优化><strong>模型训练与优化</strong></a><ul><li><a href=#分布式训练><strong>分布式训练</strong></a></li><li><a href=#高效微调技术><strong>高效微调技术</strong></a></li><li><a href=#模型评估与调优><strong>模型评估与调优</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>