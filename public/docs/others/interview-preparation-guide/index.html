<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  面试准备大纲
  #



  基础理论
  #



  数学与统计学
  #


✅ 线性代数（矩阵运算、特征值分解）
✅ 概率论与统计（贝叶斯定理、分布、假设检验）
✅ 优化方法（梯度下降、Adam、学习率调度）



  机器学习基础
  #


✅ 监督学习（线性回归、决策树、SVM、集成学习）
✅ 评估指标（准确率、召回率、F1、AUC-ROC）
✅ 过拟合与正则化（L1/L2、Dropout）



  深度学习基础
  #


⭐ 神经网络（前向传播、反向传播、激活函数）


⭐ RNN（序列建模、自回归、语言模型）
  
⁉️ 序列模型（Sequence Models）和传统模型有什么区别？
  
    序列模型（Sequence Models）和传统模型有什么区别？
序列模型（Sequence Models）和传统模型的主要区别在于它们处理数据的方式。传统模型，如线性回归（Linear Regression），通常 假设输入数据是独立同分布（i.i.d., independent and identically distributed）的，即每个输入数据点的顺序不影响其他数据点。在这些模型中，数据被处理为独立的特征向量，通常是静态的，缺乏时间或顺序的关联。
序列模型，尤其是在自然语言处理领域，强调对数据中 时间或顺序信息的建模。他们往往能够通过其递归结构捕捉序列中前后数据点之间的依赖关系。序列模型在处理时考虑到顺序，因此它们在语言建模（Language Modeling）、语音识别（Speech Recognition）等任务中表现优异，因为这些任务 依赖于上下文信息和时间序列的动态变化。
  

⁉️ 什么是自回归（Autoregression）和自回归模型？
  
    什么是自回归（Autoregression）和自回归模型？
自回归（Autoregression）是一种统计模型，用于预测时间序列数据中的未来值。它基于一个假设：当前的观测值（或输出）与过去的观测值有直接关系。自回归模型通过 使用历史数据点作为输入，预测下一个时间步的值。在这种模型中，当前的输出是历史数据的线性组合，具体来说，当前的值是由过去若干个数据点的加权和构成。
在自然语言处理（NLP）中的生成式语言模型（如自回归语言模型）中，自回归模型的原理类似于时间序列分析。自回归语言模型在生成文本时，将每个词作为输入，并依赖于已经生成的词序列来预测下一个词。在这种情况下，模型的预测依赖于前一个或前几个词，因此可以用自回归模型的框架来描述这一过程。它通过逐步生成下一个输出（例如，词或值）并基于之前的生成结果来调整预测，确保每一步都与前面的步骤密切相关。
  

⁉️ 如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？
  
    如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？
在自然语言处理（NLP）中，语言模型（Language Model, LM） 是一种用于预测句子或文本中单词序列的模型。语言模型的核心目标是 估计一个给定单词序列出现的概率。具体来说，对于一个给定的单词序列，语言模型的任务是计算该序列的联合概率：



  \[
P(x_1, \ldots, x_T) 
\]

我们可以通过链式法则将序列的联合概率分解成条件概率的乘积，从而将问题转化为自回归预测（autoregressive prediction）："><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/others/interview-preparation-guide/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Interview Preparation Guide"><meta property="og:description" content="面试准备大纲 # 基础理论 # 数学与统计学 # ✅ 线性代数（矩阵运算、特征值分解） ✅ 概率论与统计（贝叶斯定理、分布、假设检验） ✅ 优化方法（梯度下降、Adam、学习率调度） 机器学习基础 # ✅ 监督学习（线性回归、决策树、SVM、集成学习） ✅ 评估指标（准确率、召回率、F1、AUC-ROC） ✅ 过拟合与正则化（L1/L2、Dropout） 深度学习基础 # ⭐ 神经网络（前向传播、反向传播、激活函数） ⭐ RNN（序列建模、自回归、语言模型） ⁉️ 序列模型（Sequence Models）和传统模型有什么区别？ 序列模型（Sequence Models）和传统模型有什么区别？ 序列模型（Sequence Models）和传统模型的主要区别在于它们处理数据的方式。传统模型，如线性回归（Linear Regression），通常 假设输入数据是独立同分布（i.i.d., independent and identically distributed）的，即每个输入数据点的顺序不影响其他数据点。在这些模型中，数据被处理为独立的特征向量，通常是静态的，缺乏时间或顺序的关联。
序列模型，尤其是在自然语言处理领域，强调对数据中 时间或顺序信息的建模。他们往往能够通过其递归结构捕捉序列中前后数据点之间的依赖关系。序列模型在处理时考虑到顺序，因此它们在语言建模（Language Modeling）、语音识别（Speech Recognition）等任务中表现优异，因为这些任务 依赖于上下文信息和时间序列的动态变化。
⁉️ 什么是自回归（Autoregression）和自回归模型？ 什么是自回归（Autoregression）和自回归模型？ 自回归（Autoregression）是一种统计模型，用于预测时间序列数据中的未来值。它基于一个假设：当前的观测值（或输出）与过去的观测值有直接关系。自回归模型通过 使用历史数据点作为输入，预测下一个时间步的值。在这种模型中，当前的输出是历史数据的线性组合，具体来说，当前的值是由过去若干个数据点的加权和构成。
在自然语言处理（NLP）中的生成式语言模型（如自回归语言模型）中，自回归模型的原理类似于时间序列分析。自回归语言模型在生成文本时，将每个词作为输入，并依赖于已经生成的词序列来预测下一个词。在这种情况下，模型的预测依赖于前一个或前几个词，因此可以用自回归模型的框架来描述这一过程。它通过逐步生成下一个输出（例如，词或值）并基于之前的生成结果来调整预测，确保每一步都与前面的步骤密切相关。
⁉️ 如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？ 如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？ 在自然语言处理（NLP）中，语言模型（Language Model, LM） 是一种用于预测句子或文本中单词序列的模型。语言模型的核心目标是 估计一个给定单词序列出现的概率。具体来说，对于一个给定的单词序列，语言模型的任务是计算该序列的联合概率：
\[ P(x_1, \ldots, x_T) \] 我们可以通过链式法则将序列的联合概率分解成条件概率的乘积，从而将问题转化为自回归预测（autoregressive prediction）："><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Interview Preparation Guide | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/others/interview-preparation-guide/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.01c68a70edc45771bbb42b4e05342b2b0290227460e1d8ccc696631fea5e1a02.js integrity="sha256-AcaKcO3EV3G7tCtOBTQrKwKQInRg4djMxpZjH+peGgI=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/others/interview-preparation-guide/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><input type=checkbox id=section-d0dd931d60033c220ecd4cd60b7c9170 class=toggle>
<label for=section-d0dd931d60033c220ecd4cd60b7c9170 class="flex justify-between"><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a></label><ul><li><a href=/docs/deep-learning/convolutional-neural-networks/modern-convolutional-neural-networks/>Modern Convolutional Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-a3019bfa8037cc33ed6405d1589b6219 class=toggle>
<label for=section-a3019bfa8037cc33ed6405d1589b6219 class="flex justify-between"><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a></label><ul><li><a href=/docs/deep-learning/recurrent-neural-networks/modern-recurrent-neural-networks/>Modern Recurrent Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-0a43584c16258b228ae9aa8d70efc320 class=toggle>
<label for=section-0a43584c16258b228ae9aa8d70efc320 class="flex justify-between"><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a></label><ul><li><a href=/docs/deep-learning/attention-and-transformers/large-scale-pretraining-with-transformers/>Large-Scale Pretraining with Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/modern-large-language-models-copy/>Modern Large Language Models</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92e8358c45c96009753cf4227e9daea8 class=toggle>
<label for=section-92e8358c45c96009753cf4227e9daea8 class="flex justify-between"><a href=/docs/deep-learning/llm-pipelines/>LLM Pipelines</a></label><ul><li><a href=/docs/deep-learning/llm-pipelines/llm-inference-and-deployment/>LLM Inference and Deployment</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle checked>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul><li><a href=/docs/others/interview-preparation-guide/ class=active>Interview Preparation Guide</a><ul></ul></li></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Interview Preparation Guide</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#基础理论><strong>基础理论</strong></a><ul><li><a href=#数学与统计学><strong>数学与统计学</strong></a></li><li><a href=#机器学习基础><strong>机器学习基础</strong></a></li><li><a href=#深度学习基础><strong>深度学习基础</strong></a></li></ul></li><li><a href=#llm-核心技术><strong>LLM 核心技术</strong></a><ul><li><a href=#llm-基础组件><strong>LLM 基础组件</strong></a></li><li><a href=#大语言模型llm><strong>大语言模型（LLM）</strong></a></li><li><a href=#工具与框架><strong>工具与框架</strong></a></li></ul></li><li><a href=#应用开发与工程化><strong>应用开发与工程化</strong></a><ul><li><a href=#应用开发框架><strong>应用开发框架</strong></a></li><li><a href=#检索增强生成rag><strong>检索增强生成（RAG）</strong></a></li><li><a href=#部署与运维><strong>部署与运维</strong></a></li></ul></li><li><a href=#模型训练与优化><strong>模型训练与优化</strong></a><ul><li><a href=#分布式训练><strong>分布式训练</strong></a></li><li><a href=#高效微调技术><strong>高效微调技术</strong></a></li><li><a href=#模型评估与调优><strong>模型评估与调优</strong></a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=面试准备大纲><strong>面试准备大纲</strong>
<a class=anchor href=#%e9%9d%a2%e8%af%95%e5%87%86%e5%a4%87%e5%a4%a7%e7%ba%b2>#</a></h1><hr><h2 id=基础理论><strong>基础理论</strong>
<a class=anchor href=#%e5%9f%ba%e7%a1%80%e7%90%86%e8%ae%ba>#</a></h2><hr><h3 id=数学与统计学><strong>数学与统计学</strong>
<a class=anchor href=#%e6%95%b0%e5%ad%a6%e4%b8%8e%e7%bb%9f%e8%ae%a1%e5%ad%a6>#</a></h3><ul><li>✅ 线性代数（矩阵运算、特征值分解）</li><li>✅ 概率论与统计（贝叶斯定理、分布、假设检验）</li><li>✅ 优化方法（梯度下降、Adam、学习率调度）</li></ul><hr><h3 id=机器学习基础><strong>机器学习基础</strong>
<a class=anchor href=#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80>#</a></h3><ul><li>✅ 监督学习（线性回归、决策树、SVM、集成学习）</li><li>✅ 评估指标（准确率、召回率、F1、AUC-ROC）</li><li>✅ 过拟合与正则化（L1/L2、Dropout）</li></ul><hr><h3 id=深度学习基础><strong>深度学习基础</strong>
<a class=anchor href=#%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80>#</a></h3><ul><li>⭐ 神经网络（前向传播、反向传播、激活函数）</li></ul><details><summary><strong class=custom-details-title>⭐ RNN（序列建模、自回归、语言模型）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ 序列模型（Sequence Models）和传统模型有什么区别？</strong></summary><div class=markdown-inner><h2><b>序列模型（Sequence Models）和传统模型有什么区别？</b></h2><p>序列模型（Sequence Models）和传统模型的主要区别在于它们处理数据的方式。传统模型，如线性回归（Linear Regression），通常 <strong>假设输入数据是独立同分布（i.i.d., independent and identically distributed）的</strong>，即每个输入数据点的顺序不影响其他数据点。在这些模型中，数据被处理为<strong>独立的特征向量，通常是静态的，缺乏时间或顺序的关联</strong>。</p><p>序列模型，尤其是在自然语言处理领域，强调对数据中 <strong>时间或顺序信息的建模</strong>。他们往往能够通过其递归结构捕捉序列中前后数据点之间的依赖关系。序列模型在处理时考虑到顺序，因此它们在语言建模（Language Modeling）、语音识别（Speech Recognition）等任务中表现优异，因为这些任务 <strong>依赖于上下文信息和时间序列的动态变化</strong>。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是自回归（Autoregression）和自回归模型？</strong></summary><div class=markdown-inner><h2><b>什么是自回归（Autoregression）和自回归模型？</b></h2><p>自回归（Autoregression）是一种统计模型，用于预测时间序列数据中的未来值。它基于一个假设：<strong>当前的观测值（或输出）与过去的观测值有直接关系</strong>。自回归模型通过 <strong>使用历史数据点作为输入，预测下一个时间步的值</strong>。在这种模型中，当前的输出是历史数据的线性组合，具体来说，当前的值是由过去若干个数据点的加权和构成。</p><p>在自然语言处理（NLP）中的生成式语言模型（如自回归语言模型）中，自回归模型的原理类似于时间序列分析。自回归语言模型在生成文本时，将每个词作为输入，并依赖于已经生成的词序列来预测下一个词。在这种情况下，模型的预测依赖于前一个或前几个词，因此可以用自回归模型的框架来描述这一过程。<strong>它通过逐步生成下一个输出（例如，词或值）并基于之前的生成结果来调整预测，确保每一步都与前面的步骤密切相关</strong>。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？</strong></summary><div class=markdown-inner><h2><b>如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？</b></h2><p>在自然语言处理（NLP）中，语言模型（Language Model, LM） 是一种用于预测句子或文本中单词序列的模型。语言模型的核心目标是 <strong>估计一个给定单词序列出现的概率</strong>。具体来说，对于一个给定的单词序列，语言模型的任务是计算该序列的联合概率：</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[
P(x_1, \ldots, x_T)
\]</span><p>我们可以通过链式法则将序列的联合概率分解成条件概率的乘积，从而将问题转化为自回归预测（autoregressive prediction）：</p><span>\[
P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1)
\]</span><p>在语言模型中，最常见的假设是 马尔科夫假设（Markov Assumption），即 <strong>当前单词的出现只依赖于前一个或前几个单词</strong>。基于这一假设，<strong>n-gram模型（n-gram model）</strong> 是一种常见的语言模型，它通过计算某个单词在给定其前 n-1 个单词的条件下出现的概率来进行预测。例如：</p><span>\[
P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1})
\]</span></div></details><details><summary><strong class=custom-details-title>⁉️ 如何衡量语言模型质量？什么是困惑度（Perplexity）？</strong></summary><div class=markdown-inner><h2><b>什么是困惑度（Perplexity）？</b></h2><p>衡量语言模型质量的一种方法是评估其对文本的预测能力。一个好的语言模型 <strong>能够以较高的准确性预测下一个词（token）</strong>。</p><span>\[
P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1)
\]</span><p>为了量化模型质量，可以通过计算序列的似然（likelihood）。然而，直接比较似然值并不直观，<strong>因为较短的序列通常有更高的似然值</strong>。例如我们用交叉熵（cross-entropy）衡量模型对序列的预测能力。</p><span>\[
\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1)
\]</span><p>为了解决：“较短的序列通常有更高的似然值”，自然语言处理领域通常使用<strong>困惑度（Perplexity）</strong> 作为评价标准，它是交叉熵损失的指数形式：</p><span>\[
\text{Perplexity} = \exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right)
\]</span><p><strong>困惑度可以理解为我们在选择下一个词时平均可用的真实选项数的倒数。困惑度越低，模型质量越高，表明其对文本序列的预测能力越强。</strong></p></div></details><details><summary><strong class=custom-details-title>⁉️ 在 RNN 中是如何读取长序列数据（Partitioning Sequences）的？</strong></summary><div class=markdown-inner><h2><b>在 RNN 中是如何读取长序列数据（Partitioning Sequences）的？</b></h2><p>由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。为了高效处理数据，模型通常以固定长度的序列小批量（minibatch）为单位进行训练。一个关键问题是 <strong>如何从数据集中随机读取输入序列和目标序列的小批量</strong>。处理长序列数据时，常用的 Partitioning Sequences 方法主要包括以下几种：</p><ol><li><strong>固定长度切分（Fixed-length Splitting）</strong> 是最常见的方法之一，其中将长序列分割成固定大小的子序列。这种方法简单且易于实现，但可能会丢失跨子序列的上下文信息。</li><li><strong>滑动窗口（Sliding Window）</strong> 方法通过定义一个窗口大小并在长序列中滑动该窗口来划分数据。每次滑动时，窗口会覆盖一定数量的词汇，并且每次滑动的步长通常为窗口大小的一部分，确保子序列之间有重叠，从而保持一定的上下文信息。</li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是RNN？描述RNN的基本结构？？</strong></summary><div class=markdown-inner><h2><b>什么是RNN？描述RNN的基本结构？？</b></h2><p>在用马尔可夫模型和n-gram模型用于语言建模时，这些模型中某一时刻的词（token）只依赖于前 n 个词。如果我们希望将更早时间步的词对当前词的影响考虑进来，就需要增加 n 的值。然而，随着 n 增加，模型参数的数量也会呈指数级增长，因为需要为词汇表中的每个词存储对应的参数。因此， 因此与其将历史信息模型化，不如使用<strong>隐变量模型（latent variable model）</strong>：</p><span>\[
P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid h_{t-1})
\]</span><p>潜在变量模型的核心思想是通过 <strong>引入一个隐藏状态（hidden state）</strong>，它保存了直到当前时间步的序列信息。具体来说，隐藏状态在任意时间步可以通过 <strong>当前输入</strong> 和 <strong>上一个隐藏状态</strong> 来计算：</p><span>\[
h_t = f(x_{t}, h_{t-1})
\]</span><p><strong>循环神经网络（RNN） 就是通过隐藏状态来建模序列数据的神经网络</strong>。RNN 由三个主要部分组成：</p><ol><li><strong>输入层（Input layer）</strong>：接收序列数据，通常表示为词向量（word embeddings）或特征向量（feature vectors）。</li><li><strong>隐藏层（Hidden layer）</strong>：核心部分，由 隐藏状态（hidden state） 组成，每个时间步的隐藏状态不仅依赖于当前输入 x_t ，还依赖于前一个时间步的隐藏状态 h_{t-1} 。<strong>隐藏状态就是网络当前时刻的”记忆”</strong>。更新公式如下：</li></ol><span>\[
\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh} + \mathbf{b}_h)
\]</span><ol start=3><li><strong>输出层（Output layer）</strong>：用于预测目标值 y_t ，通常通过 全连接层（fully connected layer） 和 Softmax 激活函数（Softmax activation function） 计算类别概率：</li></ol><span>\[
\mathbf{O} = \mathbf{H} \mathbf{W}_{hq} + \mathbf{b}_q,
\]</span><p>在标准的RNN模型中，<strong>隐藏单元（hidden state）的权重是共享的</strong>，<strong>即相同的权重矩阵 W_xh（从输入到隐藏状态的权重）和 W_hh（从上一个时间步的隐藏状态到当前隐藏状态的权重）在每个时间步都会被复用</strong>。这意味着，RNN通过不断更新隐藏状态，并依赖同一组权重来捕捉序列中的时序信息。尽管RNN在每个时间步都会计算新的隐藏状态，但在基础模型中没有涉及多层结构的概念。</p></div></details><details><summary><strong class=custom-details-title>⁉️ RNN 在 inference 阶段的解码 (Decoding) 是怎么实现的？</strong></summary><div class=markdown-inner><h2><b>RNN 在 inference 阶段的解码 (Decoding) 是怎么实现的？</b></h2><p>在训练好语言模型后，模型不仅可以预测下一个 token，还可以通过将前一个预测的 token 作为输入，连续地预测后续的 token。标准的 RNN 解码过程的步骤可以总结为：</p><ol><li><strong>Warm-up阶段</strong>：<ul><li>解码开始时，将输入序列（已知 token ）输入到模型中，不输出任何结果。</li><li>目的是通过传递隐藏状态（hidden state），初始化模型内部状态以适应上下文。</li></ul></li><li><strong>续写生成</strong>：<ul><li>在输入完前缀后，模型开始生成后续字符。</li><li>每次生成一个字符，将其作为下一个时间步的输入，依次循环生成目标长度的文本。</li></ul></li><li><strong>输入和输出映射</strong>：<ul><li>通过输出层预测字符分布，并选择概率最大的字符作为结果。</li></ul></li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ RNN训练时的主要挑战是什么？</strong></summary><div class=markdown-inner><h2><b>RNN训练时的主要挑战是什么？</b></h2><p>在训练 循环神经网络（Recurrent Neural Network, RNN） 时，主要面临以下几个挑战：</p><ol><li><strong>梯度消失（Vanishing Gradient）和梯度爆炸（Exploding Gradient）</strong>：由于 RNN 依赖于 <strong>时间步（time step）上的递归计算，在反向传播（Backpropagation Through Time, BPTT）</strong> 时，梯度会随着时间步数的增加不断衰减或增长。如果梯度值指数级减小，会导致模型在长序列上的学习能力受限，难以捕捉远距离依赖（long-range dependencies）；如果梯度值指数级增大，则会导致梯度爆炸，使得模型参数更新过大，训练变得不稳定。</li><li><strong>长期依赖问题（Long-Term Dependency Problem）</strong>：RNN 通过隐藏状态（hidden state） 传递信息，但当序列较长时，<strong>早期输入的信息会逐渐被后续时间步的信息覆盖</strong>，导致模型难以捕获远距离的上下文关系。</li><li><strong>计算效率低（Sequential Computation Bottleneck）</strong>：RNN 的计算是顺序的（sequential），<strong>即当前时间步的计算依赖于前一个时间步的计算结果，因此难以并行化</strong>。这使得 RNN 的训练和推理速度远低于 Transformer 这种可以全并行计算的架构。</li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是梯度裁剪（Gradient Clipping）？为什么它被运用在 RNN 训练过程中？</strong></summary><div class=markdown-inner><h2><b>什么是梯度裁剪（Gradient Clipping）？为什么它被运用在 RNN 训练过程中？</b></h2><p><strong>梯度裁剪（Gradient Clipping）</strong> 是一种用于 <strong>防止梯度爆炸（Gradient Explosion）</strong> 的技术，主要在训练 循环神经网络（Recurrent Neural Networks, RNNs） 时使用。由于 RNN 需要进行反向传播，当序列较长时，梯度可能会在传播过程中指数级增长，导致 <strong>参数更新过大</strong>，进而影响模型的稳定性。</p><p>梯度裁剪的核心思想是在反向传播时，<strong>如果梯度的范数（Norm）超过了某个预设阈值（Threshold），则对梯度进行缩放（Scaling），限制梯度范数不超过一定范围</strong>，即：</p><span>\[
\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}
\]</span></div></details></div></details><details><summary><strong class=custom-details-title>⭐ 经典序列模型（LSTM、GRU、Seq2Seq）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ LSTM（Long Short-Term Memory）的核心架构和原理是什么？</strong></summary><div class=markdown-inner><h2><b>LSTM 的核心架构和原理是什么？</b></h2><p>LSTM（Long Short-Term Memory）相比于传统 RNN 的主要改进在于它引入了 门控机制（Gating Mechanism），有效缓解了 梯度消失（Gradient Vanishing） 和 梯度爆炸（Gradient Explosion） 问题，使得模型能够捕捉长期依赖信息（Long-Term Dependencies）。</p><p>LSTM 由 遗忘门（Forget Gate）、输入门（Input Gate） 和 输出门（Output Gate） 组成，每个时间步通过这些门控单元来控制信息的流动。其中，<strong>遗忘门</strong> 负责决定遗忘多少过去的信息，<strong>输入门</strong> 控制新信息的写入，<strong>输出门</strong> 影响隐藏状态（Hidden State）的更新。</p><span>\[
\begin{split}\begin{aligned}
\mathbf{I}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xi} + \mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i),\\
\mathbf{F}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xf} + \mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f),\\
\mathbf{O}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xo} + \mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o),
\end{aligned}\end{split}
\]</span><blockquote class="book-hint warning"><p><strong>Note：</strong> 尽管公式形式类似，LSTM 中的门和一般 RNN 中的 Hidden state 的<strong>核心作用完全不同</strong>：</p><ol><li><strong>门的作用是“控制流动”</strong>：<ul><li>门本质上是一种控制开关，它的输出必须在 [0, 1] 之间，这样才能<strong>直观地表示“通过的信息比例”</strong>。所以门的激活函数使用的是 Sigmoid。</li><li>它们主要用于<strong>调节信息的流动</strong>，而不是直接参与信息存储。</li></ul></li><li><strong>Hidden State 的作用是“存储和传递信息”</strong>：<ul><li>而 Hidden state 的激活函数使用的是 tanh，其范围为 [-1, 1]，<strong>更适合表示信息本身的动态特征</strong>。</li><li>它不仅受门控机制影响，还通过非线性变换从记忆单元中提取信息。</li></ul></li></ol></blockquote><p>此外，LSTM 还维护了一个额外的 细胞状态（Cell State, C_t），用于长期存储信息。<strong>它可以被视为截止至当前时刻 t 的综合记忆信息</strong>。</p><p>新输入数据结合了当前时刻的输入 X 和上一个时间步的 Hidden State，可以被表示为：</p><span>\[
\tilde{\mathbf{C}}_t = \textrm{tanh}(\mathbf{X}_t \mathbf{W}_{\textrm{xc}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hc}} + \mathbf{b}_\textrm{c}),
\]</span><p>输入门控制我们在多大程度上考虑新输入数据，而遗忘门则决定了我们保留多少旧的记忆单元内部状态。通过使用Hadamard 积逐元素相乘）运算符，LSTM 的记忆单元内部状态的更新方程为：</p><span>\[
\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t
\]</span><p>最后，隐藏状态（Hidden State）定义了记忆单元的输出方式，它由输出门（Output Gate）控制。具体计算步骤如下：首先，对记忆单元的内部状态 应用 <code>tanh</code> 函数，使其值被规范化到 <code>(-1, 1)</code> 区间内。然后，将这一结果与输出门的值逐元素相乘，计算得到隐藏状态：</p><span>\[
\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t)
\]</span><p><strong>输出门（Output Gate） 的主要作用</strong>是控制 <strong>当前时刻的隐藏状态的输出</strong> 内容，从而决定 LSTM 的对外信息传递。Cell State 是 LSTM 的内部长期记忆，但它并不直接对外输出。输出门通过<strong>选择性地提取 Cell State 中的信息</strong>，并结合门控机制生成 <strong>新的Hidden State（短期记忆的表达）</strong>，作为当前时间步的隐藏状态对外输出。这种机制确保 LSTM 在对外传递信息时，<strong>不会将 Cell State 中的所有内容暴露出去</strong>，避免噪声干扰，同时保留最相关的信息。</p></div></details><details><summary><strong class=custom-details-title>⁉️ LSTM 解决的问题和原因？</strong></summary><div class=markdown-inner><h2><b>LSTM解决的问题和原因？</b></h2><p>LSTM（Long Short-Term Memory）主要解决了标准RNN在处理长序列时的 <strong>梯度消失（vanishing gradients）和梯度爆炸（exploding gradients）</strong> 问题。 这些问题导致普通RNN在处理长期依赖（long-term dependencies）时性能较差，无法有效捕获长时间跨度的信息。</p><ol><li><strong>细胞状态（Cell State）作为长期记忆的载体</strong><ul><li>LSTM引入了一个额外的细胞状态，它可以通过直通路径（&ldquo;constant error carousel&rdquo;）跨时间步传播信息，<strong>几乎不受梯度消失或梯度爆炸的影响</strong>。</li></ul></li><li><strong>梯度传播更稳定</strong><ul><li>普通RNN的梯度通过时间步传播时，会被<strong>反复乘以隐状态的权重矩阵</strong>。当权重矩阵的特征值远离 1 时，梯度会出现指数增长（梯度爆炸）或指数衰减（梯度消失）。</li><li>在LSTM中，<strong>细胞状态通过线性加权方式更新</strong>（不直接经过激活函数的非线性变换），从而避免了梯度的剧烈变化。门机制确保了信息流动的可控性，进一步减轻了梯度不稳定的问题。</li></ul></li><li><strong>更强的记忆能力</strong><ul><li>LSTM能<strong>同时捕获短期依赖和长期依赖（通过细胞状态）</strong>。在长时间序列中，它可以动态调整对不同时间跨度的信息的关注程度，使其既能够记住长期信息，又不会因记忆过多导致模型过载。</li></ul></li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ 解释 GRU（Gated Recurrent Unit）的核心架构和原理？</strong></summary><div class=markdown-inner><h2><b>解释 GRU 的核心架构和原理？</b></h2><p>GRU（门控循环单元）是 <strong>LSTM记忆单元的简化版本</strong> 并保留内部状态和乘法门控机制的核心思想。相比LSTM，GRU在很多任务中可以达到相似的性能，但由于结构更加简单，计算速度更快。</p><p>在GRU（Gated Recurrent Unit）中，LSTM的三个门被替换为两个门：<strong>重置门（Reset Gate）</strong> 和 <strong>更新门（Update Gate）</strong>。这两个门使用了 <code>Sigmoid</code> 激活函数，输出值限制在区间 <code>[0, 1]</code> 内。</p><ul><li><strong>重置门</strong>：决定了当前状态需要记住多少之前隐藏状态的信息。</li><li><strong>更新门</strong>：控制新状态有多少是继承自旧状态的。</li></ul><span>\[
\begin{split}\begin{aligned}
\mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r),\\
\mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} + \mathbf{b}_z),
\end{aligned}\end{split}
\]</span><p>重置门（reset gate）与标准更新机制相结合，生成时间步 t 的 **候选隐藏状态（candidate hidden state），公式如下：</p><span>\[
\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} + \mathbf{b}_h),
\]</span><blockquote class="book-hint warning"><p><strong>Note：</strong> 重置门主要是对长期记忆进行筛选，在计算候选隐藏状态时，抑制过去隐藏状态中的某些部分，使得模型更多地依赖当前输入。这种机制让模型能够<strong>专注于短期依赖</strong>，通过结合当前输入<strong>产生一个更符合短期记忆的候选状态</strong>。</p></blockquote><p><strong>更新门（Update gate）决定了新隐藏状态</strong> 在多大程度上保留旧状态与新候选状态的信息。具体而言，控制了二者的加权组合，公式如下：</p><span>\[
\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1} + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t.
\]</span><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>更新门（update gate）</strong> 则决定 <strong>当前时刻的隐藏状态是由之前的隐藏状态（长期记忆）和当前输入（短期信息）以何种比例组合而成</strong>。更新门的值接近 1 时，模型保留大部分的长期记忆，接近 0 时则依赖更多的当前输入。这种机制帮助 GRU 捕捉长期依赖关系，因为更新门可以调节模型在序列中如何传递和利用过去的记忆。</p></blockquote></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是深层 RNN（Deep RNNs），它与普通 RNN 有什么区别？</strong></summary><div class=markdown-inner><h2><b>什么是深层 RNN（Deep RNNs），它与普通 RNN 有什么区别？</b></h2><p>深层循环神经网络（Deep Recurrent Neural Networks, DRNN）是通过 <strong>堆叠多个RNN层</strong> 实现的。单隐藏层的RNN网络结构在捕捉时间步内部输入与输出之间复杂关系时存在局限。因此，为了同时增强 <strong>模型对时间依赖（temporal dependency）</strong> 和 <strong>时间步内部输入与输出关系</strong> 的建模能力，常会构造在时间方向和输入输出方向上都更深的RNN网络。这种深度的概念类似于在多层感知机（MLP）和深度卷积神经网络（CNN）中见到的层级加深方法。</p><p>在深层RNN中，每一时间步的隐藏单元 <strong>不仅依赖于同层前一个时间步的隐藏状态</strong>，还依赖于 <strong>前一层相同时间步的隐藏状态</strong>。这种结构使得深层RNN能够 <strong>同时捕获长期依赖关系（long-term dependency）和复杂的输入-输出关系</strong>。</p><p>在深层RNN的第l（l=1,&mldr;,L）个隐藏层Hidden state可以表示为:</p><span>\[
\mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)} + \mathbf{b}_h^{(l)}),
\]</span><p>最后，输出层的计算仅基于第<span>
(l)
</span>个隐藏层最终的隐状态：</p><span>\[
\mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q,
\]</span></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是双向循环神经网络（BiRNN）？为什么在某些任务中它的的效果优于RNN？</strong></summary><div class=markdown-inner><h2><b>什么是双向循环神经网络（BiRNN）？为什么在某些任务中它的的效果优于RNN？</b></h2><p><strong>双向循环神经网络（Bidirectional Recurrent Neural Network, BiRNN）是一种扩展传统循环神经网络（Recurrent Neural Network, RNN）</strong> 的方法。与单向 RNN 仅从过去到未来处理序列数据不同，BiRNN 在每个时间步（Timestep）中同时计算两个方向的信息流：一个从前向后（Forward Direction），另一个从后向前（Backward Direction）。这种结构通过两个独立的隐藏层（Hidden Layers）分别处理时间序列的正向和反向信息，并在输出层（Output Layer）结合这两个隐藏状态（Hidden States），从而获得更丰富的上下文信息。前向和反向隐状态的更新如下：</p><span>\[
\begin{split}\begin{aligned}
\overrightarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{hh}^{(f)} + \mathbf{b}_h^{(f)}),\\
\overleftarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{hh}^{(b)} + \mathbf{b}_h^{(b)}),
\end{aligned}\end{split}
\]</span><p>接下来，将前向隐状态和反向隐状态连接起来，获得需要送入输出层的隐状态。在具有多个隐藏层的深度双向循环神经网络中，该信息作为输入传递到下一个双向层。</p><span>\[
\mathbf{H}_t = \begin{bmatrix} \overrightarrow{\mathbf{H}}_t \\ \overleftarrow{\mathbf{H}}_t \end{bmatrix}
\]</span><p>最后，输出层计算得到的输出为：</p><span>\[
\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q.
\]</span><p>在某些任务中，BiRNN 的效果优于单向 RNN，主要是因为它能够利用未来和过去的信息，而不仅仅依赖于当前时间步之前的历史数据。BiRNN 适用于 <strong>需要充分利用上下文信息的序列任务</strong>，特别是在 NLP 领域。具体来说，文本分类（Text Classification）、情感分析（Sentiment Analysis）、语音识别（Automatic Speech Recognition, ASR）等任务都可以从 BiRNN 的双向信息流中获益。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是 Encoder-Decoder 结构？它是如何工作的？</strong></summary><div class=markdown-inner><h2><b>什么是 Encoder-Decoder 结构？它是如何工作的？</b></h2><p>在序列到序列（sequence-to-sequence）问题中（如机器翻译），<strong>输入和输出通常具有不同的长度，且无法直接对齐</strong>。为了解决这一问题，通常采用编码器-解码器（encoder-decoder）架构。这个架构包括两个主要组件：</p><p><strong>编码器（Encoder）的作用是处理输入序列并将其转化为一个固定大小的表示（通常是一个向量，也叫做上下文向量（Context Vector））</strong>。在传统的 RNN（Recurrent Neural Network）或 LSTM（Long Short-Term Memory）网络中，编码器逐步读取输入序列的每个元素（如单词或字符），并通过递归地更新其隐藏状态（Hidden State）来捕捉输入的语义信息。最终，<strong>编码器输出的隐藏状态或最后一个时间步的隐藏状态</strong>（在一些变体中是所有时间步的隐藏状态）作为对输入序列的总结。</p><p><strong>解码器（Decoder）则是基于编码器的输出生成目标序列</strong>。解码器通常也是一个RNN或LSTM，它的工作方式是逐步预测输出序列中的每个元素。解码器首先接受编码器生成的上下文向量作为初始的隐藏状态，然后在生成每个目标词时，它根据当前隐藏状态以及之前生成的输出（或在训练时，使用教师强迫（Teacher Forcing），即真实的标签作为输入）来预测下一个词。这个过程一直持续到生成完整的目标序列。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是 seq2seq 模型？</strong></summary><div class=markdown-inner><h2><b>什么是 seq2seq 模型？</b></h2><p>序列到序列（Sequence-to-Sequence, Seq2Seq）模型是一种用于处理输入序列（Input Sequence）到输出序列（Output Sequence）转换的深度学习架构（<strong>输入和输出都是变长的、未对齐的序列</strong>）。Seq2Seq 模型的核心结构是 <strong>编码器-解码器（Encoder-Decoder）架构</strong>，其中编码器（Encoder）的主要作用是将一个 <strong>长度可变的输入序列</strong> 转换为 <strong>固定形状的上下文变量（context variable）</strong>。这一过程可表示为：</p><span>\[
\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1}).
\]</span><p>Encoder 会利用自定义的函数 g 将所有时间步的隐藏状态转换为一个固定形状的上下文变量：</p><span>\[
\mathbf{c} = q(\mathbf{h}_1, \ldots, \mathbf{h}_T).
\]</span><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>Encoder 的设计目的</strong>：</p><ol><li><strong>压缩输入信息</strong>：将输入序列的所有信息压缩到一个低维表示中，确保模型能够以固定大小的特征表示处理任意长度的输入。</li><li><strong>捕捉序列的全局语义</strong>： Encoder 会通过递归网络（如 RNN、GRU 或 LSTM）处理输入序列，将序列中的时序依赖关系和语义信息编码到隐藏状态中。</li><li><strong>作为中间表示</strong>： Encoder 的输出（隐藏状态或上下文变量）提供了一种抽象的、高效的输入表示，适合传递给其他模块（如 Decoder）或用于分类、翻译等下游任务。</li></ol></blockquote><p><strong>解码器（decoder）负责根据目标输出序列</strong>，在每个时间步 t 预测下一步的输出。解码器的核心是基于目标序列中前一时间步的输出、前一时间步的隐藏状态和上下文变量来计算当前时间步的隐藏状态。公式如下：</p><span>\[
\mathbf{s}_{t} = g(y_{t}, \mathbf{c}, \mathbf{s}_{t}).
\]</span><p>在得到当前时间步的隐藏状态 <span>(\mathbf{s}_t)
</span>后，通过输出层和 softmax 操作计算下一步的输出 <span>(y_t)
</span>的概率分布:</p><span>\[
P(y_{t} \mid y_1, \ldots, y_{t}, \mathbf{c})
\]</span><blockquote class="book-hint warning"><p><strong>Note：</strong> 在 seq2seq 中，特定的 <code>&lt;eos></code> 表示序列结束词元。 一旦输出序列生成此词元，模型就会停止预测。在设计中，通常有两个特别的设计决策：首先，每个输入序列开始时都会有一个特殊的序列开始标记（<code>&lt;bos></code>），它是解码器的输入序列的第一个词元；其次，使用循环神经网络编码器 <strong>最终的隐状态来初始化解码器的隐状态</strong>。</p></blockquote></div></details><details><summary><strong class=custom-details-title>⁉️ 为什么解码器在训练时需要强制教学（teacher forcing）？</strong></summary><div class=markdown-inner><h2><b>为什么解码器在训练时需要强制教学（teacher forcing）？</b></h2><p>强制教学（Teacher Forcing） 是一种用于 序列到序列（Seq2Seq） 任务的训练技巧，主要应用于 递归神经网络（RNN） 及其变体。在标准的 Seq2Seq 训练过程中，解码器（Decoder）需要根据之前的输出逐步预测下一个单词，而 强制教学（Teacher Forcing） 的核心思想是在训练时，<strong>不使用解码器自身的预测结果作为下一步的输入，而是直接使用真实的目标序列（Ground Truth）作为输入，从而减少误差的累积</strong>。</p><p>在这种方法中，解码器的 <strong>输入使用的是目标序列 (target sequence) 的原始标签</strong>。具体来说，解码器的输入由特殊的起始标记 <code>&lt;bos></code> 和目标序列（去掉最后一个标记）拼接而成，而解码器的输出（用于训练的标签）是原始目标序列 <strong>向右偏移一个标记</strong>。例如：</p><ul><li>输入: <code>&lt;bos></code>, “Ils”, “regardent”, “.”</li><li>输出: “Ils”, “regardent”, “.”, <code>&lt;eos></code></li></ul><p>解码器在训练时需要 强制教学（Teacher Forcing），主要是为了 <strong>加速收敛并稳定训练过程</strong>。在没有 强制教学（Teacher Forcing） 的情况下，如果解码器的某一步预测错误，那么错误的输出会被作为下一步的输入，这可能导致错误被进一步放大，从而使整个序列的预测质量下降。通过使用真实目标序列作为输入，解码器可以更快学习到正确的模式，并减少梯度传播中的误差累积问题。</p><p>相比之下，<strong>编码器（Encoder） 并不需要 强制教学（Teacher Forcing）</strong>，因为编码器的作用是将整个输入序列编码成一个固定长度的隐状态（Hidden State），然后传递给解码器。编码器的输入是完整的源语言序列，因此它的计算不涉及前一步的预测误差传播。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 在解码阶段如何进行 Greedy Search 和 Beam Search？两者的优缺点是什么？</strong></summary><div class=markdown-inner><h2><b>在解码阶段如何进行 Greedy Search 和 Beam Search？两者的优缺点是什么？</b></h2><p>Greedy Search 是一种简单但次优的解码方法，<strong>每一步都选择当前概率最高的词作为输出</strong>，而不考虑全局最优。例如，在翻译任务中，若模型预测 “I love deep learning” 时，Greedy Search 可能会选择最高概率的词 “love” 作为第二个词，而不会评估其他可能的组合。这种方法的优势是计算速度快、实现简单，<strong>但容易陷入局部最优（Local Optimum），导致整体生成结果质量不佳</strong>。</p><p>Beam Search 是 Greedy Search 的改进方法，<strong>它在解码过程中维护 K（Beam Width） 个最优候选序列</strong>，而不是仅选择概率最高的词。例如，若 K=3，模型会同时跟踪三个最可能的翻译路径，在每个时间步计算所有可能扩展的概率，并仅保留 K 个最高概率的候选路径。Beam Search 能有效避免局部最优，并提高序列生成质量。然而，<strong>它的计算复杂度较高</strong>，较大的 K 值会显著增加计算量。此外，Beam Search 仍然无法保证找到全局最优解，并且可能导致重复生成（Repetitive Generation）的问题。</p><p>在实际应用中，Greedy Search 适用于低计算资源的环境，如实时应用（Real-time Applications），而 Beam Search 在机器翻译（Machine Translation）、文本摘要（Text Summarization）等任务中更常见，以提高生成文本的连贯性和流畅性。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 如何使用 BLEU 进行预测序列的评估 ？</strong></summary><div class=markdown-inner><h2><b>如何使用 BLEU 进行预测序列的评估 ？</b></h2><p>在自然语言生成任务（Natural Language Generation, NLG）中，预测序列的评估通常使用自动化指标来衡量 <strong>生成文本与参考文本（Ground Truth）之间的相似度</strong>，其中 BLEU（Bilingual Evaluation Understudy） 是最常用的指标之一。BLEU 主要用于机器翻译（Machine Translation, MT）等任务，通过计算 n-gram 之间的匹配度来评估生成文本的质量。</p><p>BLEU 的核心思想是计算 预测文本（Hypothesis） 和 参考文本（Reference） 之间的 n-gram 精确匹配率（n-gram Precision），并结合 惩罚因子（Brevity Penalty, BP） 以防止模型生成过短的句子。其计算过程如下：</p><span>\[
\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},\]</span><ul><li><strong>n-gram 精确匹配（n-gram Precision）</strong>: 计算预测文本中的 1-gram, 2-gram, 3-gram, 4-gram 等短语，在参考文本中是否出现，并计算匹配比例。</li><li><strong>惩罚因子（Brevity Penalty, BP）</strong>: 当预测文本过短（即比参考文本短）时，BLEU 会施加惩罚，避免通过只生成短而匹配的文本来提高得分。</li></ul><p>BLEU 的优点是计算简单，适用于大规模评测，并且与人类评分有一定相关性。然而，它的缺点是 <strong>缺乏语义理解（Semantic Understanding），无法衡量文本的可读性和流畅性。</strong></p></div></details></div></details><hr><h2 id=llm-核心技术><strong>LLM 核心技术</strong>
<a class=anchor href=#llm-%e6%a0%b8%e5%bf%83%e6%8a%80%e6%9c%af>#</a></h2><hr><h3 id=llm-基础组件><strong>LLM 基础组件</strong>
<a class=anchor href=#llm-%e5%9f%ba%e7%a1%80%e7%bb%84%e4%bb%b6>#</a></h3><hr><details><summary><strong class=custom-details-title>⭐ Tokenization - 分词（BPE, WordPiece, SentencePiece）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ 什么是 Tokenization？为什么它对LLM至关重要？</strong></summary><div class=markdown-inner><h2><b>什么是 Tokenization？为什么它对LLM至关重要？</b></h2><p>Tokenization（分词）是将文本拆分成更小的单元（tokens）的过程，这些单元可以是单词、子词或字符。在自然语言处理中，分词是文本预处理的重要步骤。他直接影响模型对语义的理解能力、计算效率和内存占用。</p><ul><li><strong>Example</strong>：句子 “Hello, world!” 可被切分为 <code>["Hello", ",", "world", "!"]</code>（基于空格和标点）。</li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ 常见的 Tokenization 方法有哪些？它们的区别是什么？</strong></summary><div class=markdown-inner><h2><b>常见的 Tokenization 方法有哪些？它们的区别是什么</b></h2><ul><li><strong>Word-level</strong>：按词切分（如 <code>“natural language processing” → ["natural", "language", "processing"]</code>），但词表大且难以处理未登录词（OOV）。</li><li><strong>Subword-level（主流方法）</strong>：<ul><li><strong>BPE（Byte-Pair Encoding）</strong>：通过合并高频字符对生成子词（如GPT系列使用）。</li><li><strong>WordPiece</strong>：类似BPE，但基于概率合并（如BERT使用）。</li><li><strong>SentencePiece</strong>：无需预分词，直接处理原始文本（如T5使用）。</li></ul></li><li><strong>Character-level</strong>：按字符切分，词表极小但序列长且语义建模困难。</li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ BPE算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2><b>BPE 算法的工作原理是什么？请举例说明。</b></h2><p>BPE（Byte Pair Encoding）是一种子词分割（Subword Segmentation）算法，核心思想是基于 <strong>统计频率</strong> 的合并（Merge frequent pairs）。</p><ul><li><strong>工作原理</strong>：<ol><li>统计字符对（Byte Pair）频率，找到最常见的相邻字符对。<pre tabindex=0><code>[&#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34; &#34;, &#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34;e&#34;, &#34;r&#34;, &#34; &#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;]
</code></pre></li><li>合并最频繁的字符对，形成新的子词单元。<pre tabindex=0><code>(&#34;l&#34;, &#34;o&#34;) -&gt; 2次
(&#34;o&#34;, &#34;w&#34;) -&gt; 2次
    ...
</code></pre>（第一次）合并 <code>("l", "o") → "lo"</code>：<pre tabindex=0><code>[&#34;lo&#34;, &#34;w&#34;, &#34;lo&#34;, &#34;w&#34;, &#34;er&#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;st&#34;]
</code></pre>（第二次）合并 <code>("lo", "w") → "low"</code><pre tabindex=0><code>[&#34;low&#34;, &#34;low&#34;, &#34;er&#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;st&#34;]
</code></pre></li><li>重复步骤 1 和 2，直到达到预定的子词词汇量。<pre tabindex=0><code>[&#34;low&#34;, &#34;lower&#34;, &#34;er&#34;, &#34;newest&#34;]
</code></pre><pre tabindex=0><code>vocab = {&#34;low&#34;: 100, &#34;lower&#34;: 101, &#34;er&#34;: 102, &#34;newest&#34;: 103}
</code></pre></li></ol></li></ul><p>BPE的优点是它 <strong>能够有效地处理未登录词</strong>，并且在处理长尾词（rare words）时表现良好。比如，词”unhappiness”可以被分解为”un” + “happiness”，而不是完全看作一个新的词。</p></div></details><details><summary><strong class=custom-details-title>⁉️ WordPiece 算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2><b>WordPiece 算法的工作原理是什么？请举例说明</b></h2><p>WordPiece 的核心理念与BPE相似，但合并策略更注重语义完整性。其训练过程同样从基础单元（如字符）开始，但选择合并的标准并非单纯依赖频率，而是通过 <strong>计算合并后对语言模型概率的提升幅度</strong>，优先保留能够增强语义连贯性的子词。</p><ul><li><p>假设语言模型的目标是最大化训练数据的似然概率，在 WordPiece 中，简化为 通过子词频率估计概率（即一元语言模型）。通过计算合并后对语言模型概率的提升幅度进行合并：
<span>[
\begin{equation}
P(S)≈ \prod^{n}_{i=1}P(s_i)
\end{equation}
]</span></p></li><li><p><strong>工作原理</strong>：</p><ol><li><p>与BPE类似，首先将所有词分解为最小的单位（如字符）。</p><pre tabindex=0><code>[&#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34; &#34;, &#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34;e&#34;, &#34;r&#34;, &#34; &#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;, &#34; &#34;, &#34;w&#34;, &#34;i&#34;, &#34;d&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;]
</code></pre></li><li><p>统计所有可能的字符对（或子词对）在文本中的共现频率。</p></li><li><p>合并字符对，选择合并后能 <strong>最大化语言模型似然概率</strong> 的字符对。具体公式为：选择使 <code>score = freq(pair) / (freq(first) * freq(second)) </code>最大的字符对（<strong>与 BPE 不同，BPE 仅选择频率最高的对</strong>）。每次合并对语言模型概率提升最大的合并组合。</p><p>这里的 <code>##</code> 表示这个 token 只能作为后缀出现，不会单独存在。</p><pre tabindex=0><code>{&#34;low&#34;, &#34;##er&#34;, &#34;##ing&#34;, &#34;new&#34;, &#34;##est&#34;, &#34;wide&#34;, &#34;##st&#34;}
</code></pre></li><li><p>重复合并得分最高的字符对，直到达到预设的词汇表大小。</p><pre tabindex=0><code>vocab = {&#34;low&#34;: 100, &#34;##er&#34;: 101, &#34;##ing&#34;: 102, &#34;new&#34;: 103, &#34;##est&#34;: 104, &#34;wide&#34;: 105, &#34;##st&#34;: 106}
</code></pre></li></ol></li></ul><p>WordPiece 通过最大化语言模型概率合并子词，<strong>生成的子词更贴合语义需求</strong>。但计算复杂度更高，需多次评估合并得分。</p><ul><li>若需模型捕捉深层语义（如预训练任务），优先选择 WordPiece。</li><li>若需快速处理大规模数据且词汇表灵活，BPE 更合适。</li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ SentencePiece 算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2><b>SentencePiece 算法的工作原理是什么？请举例说明</b></h2><p>SentencePiece 是一种无监督的子词分割算法，其核心创新在于直接处理原始文本（包括空格和特殊符号），无需依赖预分词步骤。<strong>它支持两种底层算法：BPE 或 基于概率的Unigram Language Model</strong>。训练时，SentencePiece 将空格视为普通字符 <code>_</code>，可 <strong>直接处理多语言混合文本（如中英文混杂）</strong>，并自动学习跨语言的统一子词划分规则。</p><ul><li><strong>Example：</strong> <code>"Hello世界" → 编码为 ["▁He", "llo", "▁世", "界"]。</code></li></ul><p>SentencePiece 支持多语言（Multilingual）无需调整，统一处理空格与特殊符号。这一特性使其在需要多语言支持的场景（如T5模型）中表现突出，同时简化了数据处理流程，特别适合处理社交媒体文本等非规范化输入。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 如何处理未登录词（OOV）？</strong></summary><div class=markdown-inner><h2><b>如何处理未登录词（OOV）？</b></h2><ul><li><strong>子词切分</strong>：将OOV（Out-of-Vocabulary）词拆分为已知子词（如 <code>“tokenization” → ["token", "ization"]</code>）。</li><li><strong>回退策略</strong>：使用特殊标记（如 <code>[UNK]</code>），但会损失信息。</li><li><strong>动态更新词表</strong>：在增量训练时扩展词表。</li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ Tokenization 如何影响模型性能？</strong></summary><div class=markdown-inner><h2><b>Tokenization 如何影响模型性能？</b></h2><ul><li><strong>词表过大</strong>：增加内存消耗，降低计算效率（Softmax 计算成本高）。</li><li><strong>词表过小</strong>：导致长序列和语义碎片化（如切分为无意义的子词）。</li><li><strong>语言适配性</strong>：中/日/韩等非空格语言需要特殊处理（如 SentencePiece）。</li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ 如何为多语言模型设计Tokenization方案？</strong></summary><div class=markdown-inner><h2><b>如何为多语言模型设计 Tokenization 方案？</b></h2><ul><li><strong>统一词表</strong>：使用 SentencePiece 跨语言训练（如mBERT）。</li><li><strong>平衡语种覆盖</strong>：根据语种数据量调整合并规则，避免小语种被淹没。</li><li><strong>特殊标记</strong>：添加语言ID（如 <code>[EN]</code>、<code>[ZH]</code>）引导模型区分语言。</li></ul></div></details></div></details><details><summary><strong class=custom-details-title>⭐ Word Embeddings - 词嵌入（Word2Vec、GloVe、FastText）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ 什么是词嵌入（Word Embeddings）？为什么它重要？</strong></summary><div class=markdown-inner><h2><b>什么是词嵌入（Word Embeddings）？为什么它重要？</b></h2><p>在自然语言处理中，Embedding（词嵌入）是将 <strong>离散的文本数据（如单词、短语或句子）映射到一个连续的、低维度的向量空间（vector space）的过程</strong>。他的作用包括：</p><ol><li><strong>捕捉语义关系（Semantic Relationships）</strong>：能表示同义词、类比关系（如 king - man + woman ≈ queen）。</li><li><strong>降维（Dimensionality Reduction）</strong>：将高维的 <strong>独热编码（One-hot Encoding）</strong> 转换为低维密集向量，提高计算效率。</li><li><strong>解决稀疏性问题（Handling Sparsity）</strong>：相比于独热编码，词嵌入具有更好的泛化能力（Generalization）。</li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ 静态词向量 和 上下文动态词向量的区别？</strong></summary><div class=markdown-inner><h2><b>静态词向量 和 上下文动态词向量的区别？</b></h2><ul><li><strong>静态词向量（Static Word Embeddings）</strong> 的核心特点是 <strong>无论词语出现在何种上下文中，其向量表示均保持不变</strong>。这类方法通过大规模语料训练，捕捉词语间的语义和语法关系。静态词向量的优势在于训练高效、资源消耗低，且生成的向量可直观反映语义相似性（如“猫”和“狗”向量接近）；但其 <strong>局限性是无法处理多义词</strong>（如“苹果”在“水果”和“手机”场景中的不同含义），因为 <strong>每个词仅对应单一向量</strong>。代表模型包括：Word2Vec, GloVe。</li><li><strong>上下文动态词向量（Contextual Word Embeddings）</strong>：静态嵌入虽然可以表示词语的语义，但它们无法根据上下文动态调整，例如 “bank” 在 “river bank” 和 “bank account” 里的含义不同。而 动态词嵌入 解决了这个问题，代表性模型包括 ELMo、BERT 和 GPT。</li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ 解释 Word2Vec 的两种模型：Skip-gram 和 CBOW。</strong></summary><div class=markdown-inner><h2><b>解释 Word2Vec 的两种模型：Skip-gram 和 CBOW。</b></h2><ul><li><p><strong>跳元模型（Skip-gram）</strong> 假设 一个词可以用来在文本序列中生成其周围的单词。以文本序列 <code>“the”“man”“loves”“his”“son”</code> 为例。假设中心词选择 <code>“loves”</code>，并将上下文窗口设置为2，给定中心词 <code>“loves”</code>，跳元模型考虑生成上下文词 <code>“the”“man”“him”“son”</code> 的条件概率。最大化给定中心词时上下文词的条件概率：
$$
max{\sum log P(context_w|center_w)}
$$
Word2Vec 的核心是 <strong>一个浅层神经网络（Shallow Neural Network）</strong>，由一个输入层、一个隐藏层（线性变换层）、一个输出层（Softmax 或其他采样方法）组成:</p><ol><li><strong>输入示例</strong>：<ul><li>句子：<code>“I love natural language processing.”</code></li><li>若窗口大小为1，中心词为 <code>“natural”</code>，则上下文词为 <code>“love”</code> 和 <code>“language”</code>。</li></ul></li><li><strong>输入通过 One-Hot 编码 表示为一个稀疏向量</strong>。例如，若词汇表为 <code>["cat", "dog", "fish"]</code>，则<code>“dog”</code> 的输入编码为 <code>[0, 1, 0]</code>。</li><li><strong>输入层到隐藏层</strong>：输入向量与 输入权重矩阵相乘，得到中心词的嵌入向量。</li><li>通过 <strong>输出权重矩阵将隐层向量映射到输出概率</strong>：</li><li>使用 Softmax 归一化，通过梯度下降优化词向量，使得上下文词的概率最大化。</li></ol><p><strong>跳元模型（Skip-gram）特点</strong>：</p><ul><li><strong>擅长捕捉低频词</strong>：通过中心词预测多个上下文，低频词有更多训练机会。</li><li><strong>训练速度较慢</strong>：输出层需计算多个上下文词的概率。</li></ul></li></ul><hr><ul><li><p><strong>连续词袋（CBOW）</strong> 与 Skip-gram 相对，CBOW 的训练过程是 <strong>给定上下文词，预测中心词</strong>。这里的目标是将多个上下文词的向量平均起来，并通过它们来预测中心词。在文本序列 <code>“the”“man”“loves”“his”“son”</code> 中，在 <code>“loves”</code> 为中心词且上下文窗口为 2 的情况下，连续词袋模型考虑基于上下文词 <code>“the”“man”“him”“son”</code> 生成中心词 <code>“loves”</code> 的条件概率。最大化给定上下文时中心词的条件概率：
$$
max{\sum log P(center_w|context_w)}
$$
连续词袋（CBOW）的训练细节与 跳元模型（Skip-Gram）大部分类似，但是在输入 One-Hot 编码表示时，跳元模型（Skip-Gram）将中心词进行 One-Hot 编码，<strong>而连续词袋（CBOW）将上下文词进行 One-Hot 编码并取平均值</strong>。 例如，中心词为 <code>“natural”</code>，上下文词为 <code>“love”</code> 和 <code>“language”</code> 。输入为 <code>[0, 1, 0, 0, 0]</code>（<code>“love”</code>）和 <code>[0, 0, 0, 1, 0]</code>（<code>“language”</code>）的平均向量 <code>[0, 0.5, 0, 0.5, 0]</code>。此外，输出概率通过 Softmax 计算公式也有不同。</p><p><strong>连续词袋（CBOW）特点</strong>：</p><ul><li><strong>训练速度快</strong>：输入为多个词的均值向量，计算效率高。</li><li><strong>对高频词建模更好</strong>：上下文词共同贡献中心词预测。</li></ul></li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ Word2Vec 如何优化训练效率？</strong></summary><div class=markdown-inner><h2><b>Word2Vec 如何优化训练效率？</b></h2><p>由于 softmax 操作的性质，上下文词可以是词表中的任意项，但是，在一个词典上（通常有几十万或数百万个单词）<strong>求和的梯度的计算成本是巨大的</strong>！为了降低计算复杂度，可以采用两种近似训练方法：负采样（Negative Sampling）和层序softmax（Hierarchical Softmax）。</p><ul><li><p><strong>负采样（Negative Sampling）</strong>：</p><ul><li><strong>核心思想</strong>：将复杂的多分类问题（预测所有词的概率）简化为二分类问题，用少量负样本近似全词汇的Softmax计算。</li><li><strong>正负样本构建</strong>：<ul><li>对每个正样本（中心词与真实上下文词对），随机采样 K 个负样本（非上下文词）。</li><li>例如，中心词 <code>“apple”</code> 的真实上下文词为 <code>“fruit”</code>，则负样本可能是随机选择的 <code>“car”</code>,<code>“book”</code> 等无关词。</li></ul></li><li><strong>目标函数</strong>：最大化正样本对的相似度，同时最小化负样本对的相似度。</li></ul></li><li><p><strong>层序softmax（Hierarchical Softmax）</strong>：</p><ul><li><strong>核心思想</strong>：通过二叉树（如霍夫曼树）编码词汇表，将全局Softmax分解为路径上的二分类概率乘积，减少计算量。</li><li><strong>霍夫曼树构建</strong>：按词频从高到低排列词汇，高频词靠近根节点，形成最短路径。每个内部节点含一个可训练的向量参数。</li></ul></li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ 解释 GloVe 的原理？</strong></summary><div class=markdown-inner><h2><b>解释 GloVe 的原理？</b></h2><p>GloVe（Global Vectors for Word Representation）是一种基于全局统计信息的词向量训练方法，它通过构造整个语料库的共现矩阵（co-occurrence matrix）并进行矩阵分解来学习词的向量表示，其核心思想是 <strong>通过捕捉词与词之间的全局共现关系来学习语义信息</strong>，而不是像 Word2Vec 那样依赖于局部上下文窗口的预测方法。
具体而言，GloVe 首先统计语料库中的词对共现次数，构建一个共现矩阵 X ，其中 X_{ij} 表示词 i 在词 j 附近出现的频率，并计算共现概率:</p><span>\[
P(j|i) = \frac{X_{ij}}{\sum_k X_{ik}}
\]</span><p>GloVe 的核心目标是学习一个词向量映射，使得向量之间的点积能够近似这个共现概率的对数：</p><span>\[
\mathbf{u}_j^\top \mathbf{v}_i + b_i + c_j = \log(X_{ij})
\]</span><ul><li>v_i, u_j 是词 i 和词 j 的向量表示，<strong>每个词都由两个向量组成，一个是中心词向量 ，一个是上下文词向量</strong></li></ul><p>GloVe 并不依赖传统的神经网络，它的学习过程 <strong>更接近矩阵分解（Matrix Factorization）的优化方法</strong>，而非 Word2Vec 这样的前馈神经网络（Feedforward Neural Network）。GloVe <strong>不依赖反向传播（Backpropagation）</strong>，而是直接最小化共现概率对数的加权平方误差，来学习词向量。</p><p>GloVe 的优势在于它直接建模了全局的词共现信息，使得词向量能够更好地捕捉语义相似性和类比关系，如 <code>“king - man + woman ≈ queen”</code>，但由于依赖于整个共现矩阵，计算和存储成本较高，因此适用于大规模离线训练，而不是在线学习任务。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 解释 FastText 的原理？</strong></summary><div class=markdown-inner><h2><b>解释 FastText 的原理？</b></h2><p>FastText 在 Word2Vec 的基础上进行了改进，能够更好地处理 OOV（Out-of-Vocabulary）问题，同时提高计算效率。FastText 的核心思想是 <strong>使用 n-gram 字符级子词（subword） 进行单词表示</strong>，而不是仅仅依赖于整个单词的词向量。它的训练过程类似于 Word2Vec 的 CBOW（Continuous Bag of Words）或 Skip-gram 模型，但 FastText 通过将一个单词拆分成多个 n-gram 片段（如 <code>“apple”</code> 可以被拆分为 <code>&lt;ap, app, ppl, ple, le></code>），然后通过这些 n-gram 子词的向量求和来表示整个单词，从而在处理未见单词时依然可以通过其子词获得较好的表示。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 在实际项目中，如何选择不同的嵌入方法（Embedding Methods）？</strong></summary><div class=markdown-inner><h2><b>在实际项目中，如何选择不同的嵌入方法（Embedding Methods）？</b></h2><ol><li><strong>任务类型与语义需求</strong>：</li></ol><ul><li><strong>基础语义任务（如文本分类、简单相似度计算）</strong>：<ul><li>静态嵌入：Word2Vec、GloVe、FastText。<ul><li>优点：轻量高效，适合低资源场景。</li><li>示例：新闻分类任务中，预训练的 Word2Vec 向量足以捕捉主题关键词的语义。</li></ul></li></ul></li><li><strong>复杂语义任务（如问答、指代消解、多义词理解）</strong>：<ul><li>上下文嵌入：BERT、RoBERTa、XLNet。<ul><li>优点：动态生成上下文相关向量，解决一词多义。</li><li>示例：在医疗问答系统中，BERT可区分“Apple”指公司还是水果。</li></ul></li></ul></li></ul><ol start=2><li><strong>数据量与领域适配</strong>：</li></ol><ul><li><strong>小数据场景</strong>：<ul><li><strong>预训练静态嵌入 + 微调</strong>：使用公开预训练的 Word2Vec/GloVe，结合任务数据微调。</li><li><strong>轻量上下文模型</strong>：ALBERT或TinyBERT，降低训练成本。</li></ul></li><li><strong>大数据场景</strong>：<ul><li><strong>从头训练上下文模型</strong>：基于领域数据训练BERT或GPT，捕捉领域专属语义。</li><li><strong>领域适配</strong>：在金融/法律等领域，使用领域语料继续预训练（Domain-Adaptive Pretraining）。</li></ul></li></ul></div></details></div></details><details><summary><strong class=custom-details-title>⭐ Attention - 注意力机制（QKV，Self-Attention、多头注意力）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ 什么是注意力机制（Attention Mechanism）？其中的Q，K，V都代表什么？</strong></summary><div class=markdown-inner><h2><b>什么是注意力机制（Attention Mechanism）？其中的Q，K，V都代表什么？</b></h2><p><strong>注意力机制（Attention Mechanism）</strong> 的核心思想是将 <strong>输入看作键-值对的数据库</strong>，并 <strong>基于查询计算注意力权重 (attention weights)</strong>，可以动态地选择哪些输入部分（例如词语或特征）最为重要。注意力机制定义如下：</p><span>\[
\textrm{Attention}(\mathbf{q}, \mathcal{D}) \stackrel{\textrm{def}}{=} \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i,
\]</span><ul><li>这里的 q，<strong>查询（Query）</strong> 是当前模型试图“寻找”或“关注”的目标。它是一个向量，代表了你想匹配的信息。</li><li>公式中的 k，<strong>键（key）</strong> 是所有潜在匹配目标的特征表示。每个键对应于输入中的一个元素，表示这个元素的特性或身份。</li><li>其中的 v，<strong>值（value）</strong> 是和键一起存储的信息，也是最终被提取的信息。注意力机制的目标是通过查询和键找到最相关的值。</li></ul><blockquote class="book-hint warning"><p>注意力机制的一般步骤为：</p><ol><li><strong>对查询和每个键计算相似度</strong>。</li><li>对这些相似度进行归一化（通常使用 Softmax 函数）。归一化后的结果称为注意力权重（Attention Weights）。</li><li>将注意力权重与对应的值相乘，得到一个加权求和结果。这个结果就是当前查询的输出。</li></ol></blockquote><p>在 transformer 中，Query (Q)、Key (K) 和 Value (V) 都是从输入嵌入（embedding）中线性变换得到的。它们的计算方式如下：</p><span>\[
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
\]</span><p><strong>得到 Q，K，V 的过程 相当于经历了一次线性变换</strong>。Attention不直接使用 X 而是使用经过矩阵乘法生成的这三个矩阵，因为使用三个可训练的参数矩阵，可增强模型的拟合能力。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是自注意力（Self-Attention）机制？它与传统注意力有何区别？</strong></summary><div class=markdown-inner><h2><b>什么是自注意力（Self-Attention）机制？它与传统注意力有何区别？</b></h2><p>Self-Attention（自注意力） 和 一般 Attention（注意力机制） 的 <strong>核心计算原理是相同的</strong>，都是通过 Query（Q） 和 Key（K） 计算相似度分数，再对 Value（V） 进行加权求和。但它们的区别在于作用目标不同：</p><ul><li><strong>Self-Attention（自注意力）</strong><ul><li><strong>Q、K、V 都来自同一个输入序列 X ，即 自身内部计算注意力</strong>，挖掘序列中不同位置之间的关系。例如，在 Transformer 的 Encoder 里，每个单词都和句子中的所有单词计算注意力。</li></ul></li><li><strong>General Attention（通用注意力，通常用于 Seq2Seq 结构）</strong><ul><li><strong>Q 和 K、V 来自不同的地方</strong>，通常是 Q 来自 Decoder，而 K、V 来自 Encoder，用于建立 Encoder 和 Decoder 之间的联系。例如，在机器翻译中，Decoder 生成当前词时，会对 Encoder 编码的所有词计算注意力，从而获取最相关的信息。</li></ul></li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是缩放点积注意力（Scaled Dot-Product Attention）？为什么要除以 √d？</strong></summary><div class=markdown-inner><h2><b>什么是缩放点积注意力（Scaled Dot-Product Attention）？为什么要除以 √d？</b></h2><p>缩放点积注意力（Scaled Dot-Product Attention） 是 Transformer 结构中的核心机制之一，它用于计算查询（Query）、键（Key）和值（Value）之间的注意力分数，以捕捉序列中不同位置的关联性。在计算过程中，首先对查询矩阵 Q 和键矩阵 K 进行点积（Dot Product），得到注意力得分（Attention Scores）。<strong>这个点积运算的本质是衡量 查询向量（Query） 和 键向量（Key） 之间的相似度。</strong> 之后，<strong>Softmax 作用于Q，K计算出的相似度得分，以将其转换为概率分布</strong>，使其满足：</p><ul><li><strong>归一化（Normalization）</strong>：确保所有注意力权重总和为 1，便于解释。</li><li>放大差异（Sharpening）：通过指数运算增强高相关性词的权重，抑制低相关性词。</li></ul><p>然而，点积的结果可能会随着 <strong>d_k（Key 维度的大小）增加而变大</strong>。将点积作为输入传递给 Softmax 函数时，Softmax 对大数值特别敏感，因为它会根据输入的相对大小来计算概率值。如果点积值变得非常大，<strong>Softmax 会让其中一些值的输出接近 1，而其他值接近 0</strong>，这会 导致计算不稳定或梯度消失等问题。</p><p>因此，在应用 Softmax 之前，需要对注意力得分进行缩放，即除以 √d_k，这样可以防止梯度消失或梯度爆炸问题，提高训练稳定性。<strong>这个缩放的主要目的是 将点积的方差控制在一个合理的范围，使其不随向量维度的增加而变得过大。</strong> 数学公式如下：</p><span>\[
\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}.
\]</span></div></details><details><summary><strong class=custom-details-title>⁉️ 计算自注意力机制的时间和空间复杂度，分析其瓶颈。</strong></summary><div class=markdown-inner><h2><b>计算自注意力机制的时间和空间复杂度，分析其瓶颈。</b></h2><p>自注意力机制（Self-Attention Mechanism）的时间复杂度（Time Complexity）和空间复杂度（Space Complexity）主要受输入序列长度 n 影响。在标准的 Transformer 结构中，每个 Self-Attention Layer 计算 注意力权重（Attention Weights） 需要进行矩阵乘法，计算 Query Q 和 Key K 之间的点积并进行 Softmax 归一化。</p><p>其中， Q 和 K 的维度均为 (n x d_k) ，计算 QK^T 需要 O(n^2 d_k) 次乘法运算，而应用 Softmax 需要 O(n^2) 的额外计算，因此 <strong>整体时间复杂度为</strong>：</p><span>\[
O(n^2 d_k)
\]</span><p>Self-Attention 计算过程中，需要存储 <strong>注意力权重矩阵（ n x n ），此外还需要存储 中间结果（如 Softmax 输出、梯度）</strong>，使得 <strong>空间复杂度达到</strong>：</p><span>\[
O(n^2 + n d_k)
\]</span><ul><li><strong>瓶颈分析（Bottleneck Analysis）</strong><ol><li><strong>计算瓶颈（Computational Bottleneck）</strong>：由于 Self-Attention 需要 O(n^2 d_k) 的计算量，因此在超长文本（如 10K 以上 Token）上，计算成本极高，推理速度变慢。</li><li><strong>内存瓶颈（Memory Bottleneck）</strong>：存储 O(n^2) 的注意力权重矩阵会 占用大量显存（VRAM），限制了可处理的最大序列长度。</li><li><strong>长序列扩展性差（Scalability for Long Sequences）</strong>：当 n 增大时，Transformer 计算复杂度随 n^2 级增长，难以应用于长文本建模。</li></ol></li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ 为什么需要多头注意力（Multi-Head Attention）？多头设计如何提升模型表达能力？</strong></summary><div class=markdown-inner><h2><b>为什么需要多头注意力（Multi-Head Attention）？多头设计如何提升模型表达能力？</b></h2><p>多头注意力（Multi-Head Attention）是 Transformer 结构中的关键组件，它通过多个独立的注意力头来提升模型的表达能力。其核心思想是 <strong>让模型在不同的子空间（Subspaces）中独立学习不同的特征表示，而不是仅依赖单一注意力机制</strong>。例如可以关注不同类型的关系（如语法关系、语义关系、长距离依赖等）。</p><div align=center><img src=/images/multi-head-attention.svg width=450px/></div><p>在计算过程中，输入序列的特征矩阵首先经过线性变换，生成查询（Query, Q）、键（Key, K）、和值（Value, V）。然后，<strong>每个注意力头都会独立地对 Q、K、V 进行投影</strong>，将其拆分成多个低维子空间，即：</p><span>\[
\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v},
\]</span><p>其中 W_i^q, W_i^k, W_i^v 是可训练的投影矩阵，每个头都对应一组独立的参数。随后，每个头分别执行 Scaled Dot-Product Attention（缩放点积注意力）。计算完成后，各个头的注意力输出会被拼接（Concatenation），然后通过一个最终的线性变换矩阵 W^o 进行映射：</p><span>\[
\begin{split}\mathbf W_o \begin{bmatrix}\mathbf h_1\\\vdots\\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}.\end{split}
\]</span><p>这样，多头注意力的最终输出仍然保持与输入相同的维度，同时融合了来自多个注意力头的信息，提高了模型对不同层次语义的建模能力。</p></div></details></div></details><details open><summary><strong class=custom-details-title>⭐ Positional Encoding - 位置编码（绝对位置，相对位置，RoPE）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ 为什么 Transformer 需要位置编码？纯 Self-Attention 为何无法感知位置信息？</strong></summary><div class=markdown-inner><h2><b>为什么 Transformer 需要位置编码？纯 Self-Attention 为何无法感知位置信息？</b></h2><p>在 Transformer 模型中，位置编码（Position Encoding）是用于注入位置信息的关键机制，因为模型本身的 Self-Attention 机制无法感知输入序列中元素的顺序或位置。Transformer 通过 Self-Attention 计算序列中各元素之间的关系，每个元素的表示（representation）由其与其他所有元素的相互作用决定。然而， <strong>Self-Attention 本身是位置无关的（position-independent）</strong>，即它并不考虑元素在序列中的相对或绝对位置。因此，如果不显式地引入位置编码，<strong>模型就无法了解输入序列的顺序信息</strong>。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 绝对位置编码（Absolute PE）和相对位置编码（Relative PE）的核心区别是什么？</strong></summary><div class=markdown-inner><h2><b>绝对位置编码（Absolute PE）和相对位置编码（Relative PE）的核心区别是什么？</b></h2><p>绝对位置编码（Absolute Position Encoding, Absolute PE）和相对位置编码（Relative Position Encoding, Relative PE）的核心区别在于它们对序列中单词位置的表示方式。<strong>绝对位置编码是基于序列中单词的固定位置来定义每个单词的位置编码</strong>，这些编码是通过对每个位置进行显式编码（例如使用正弦和余弦函数）来获得的。这意味着 <strong>每个位置的编码是固定的，与其他词汇之间的相对关系无关</strong>。简单来说，<strong>绝对位置编码的设计是通过为每个位置分配唯一的标识符来捕捉顺序信息</strong>。绝对位置编码被广泛用于 Transformer 模型中，如原始的 Transformer 和 BERT，这些模型通过对输入的词汇序列和其位置编码的加和来保留词汇的顺序信息。</p><p>相对位置编码则是通过 <strong>考虑单词之间的相对位置来计算每个单词的编码，而不是单纯地依赖于其绝对位置</strong>。在这种方法中，<strong>位置编码的更新基于词语之间的相对距离，因此它能捕捉到不同词之间的相对关系</strong>，而不仅仅是它们在序列中的固定位置。相对位置编码的一个例子是 Transformer-XL 模型，它通过引入相对位置编码来克服标准 Transformer 在处理长序列时存在的记忆限制问题，从而提升了对长距离依赖的建模能力。</p><p>尽管在某些情况下，相对位置编码可以通过绝对位置得到（例如，简单地计算位置差），但这种方法仍然有限。<strong>相对位置编码有以下优势</strong>：</p><ol><li><strong>灵活性和泛化性</strong>：相对位置编码使得模型能够处理不同长度的输入，而绝对位置编码依赖于固定的输入长度。这意味着在不同任务或不同数据集上，使用相对位置编码的模型能够更好地进行泛化，尤其是在处理较长序列时。</li><li><strong>更好的长距离依赖建模</strong>：相对位置编码能够更有效地捕捉长距离的依赖关系，因为它直接反映了词汇间的相对关系，而绝对位置编码则对远距离的依赖建模较弱，尤其是在长序列的上下文中。</li><li><strong>减少位置编码的冗余</strong>：在传统的绝对位置编码中，序列中的每个位置都有唯一的编码，且这些编码是全局固定的，而相对位置编码只关心词汇间的相对位置，从而避免了位置编码的冗余，尤其是在处理非常长的序列时。</li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ Sinusoidal 位置编码的公式是什么？为什么选择正弦/余弦函数组合？</strong></summary><div class=markdown-inner><h2><b>Sinusoidal 位置编码的公式是什么？为什么选择正弦/余弦函数组合？</b></h2><p>Sinusoidal 位置编码（Sinusoidal Positional Encoding） 是 Transformer 模型中用于捕捉序列中单词位置的一种方法，是常见的绝对位置编码（Absolute Position Encoding）方法。Sinusoidal 位置编码通过正弦和余弦函数的组合来生成每个位置的唯一向量，这些向量与输入的词嵌入（Word Embedding）相加，从而使模型能够学习到每个单词在序列中的位置。Sinusoidal 位置编码使用相同形状的位置嵌入矩阵 P 输出 X+P，其元素按以下公式生成：</p><span>\[
\begin{split}\begin{aligned} p_{i, 2j} &= \sin\left(\frac{i}{10000^{2j/d}}\right),\\p_{i, 2j+1} &= \cos\left(\frac{i}{10000^{2j/d}}\right).\end{aligned}\end{split}
\]</span><ul><li><strong>为什么选择正弦/余弦函数组合？</strong><ol><li><strong>不同频率的周期性</strong>：正弦和余弦函数有不同的频率，使得每个位置的编码在不同维度上具有不同的周期。这种周期性使得模型可以通过不同频率的变化来学习相对位置关系。通过正弦和余弦函数的组合，位置编码能够覆盖较长序列的不同范围，模型可以捕捉到全局和局部的位置信息。</li><li><strong>无重复的唯一表示</strong>：正弦和余弦函数的组合能够确保每个位置有一个独特的编码，这些编码在向量空间中是可区分的，能够提供丰富的位置信息。而且由于这两种函数的周期性和无穷制性质，不同位置的编码不会重复。</li><li><strong>容易计算和扩展</strong>：正弦和余弦函数的计算非常简单且高效。它们无需额外的学习参数，且可以通过简单的公式根据位置直接计算得出。这样的位置编码方式能够在大规模数据中有效应用，同时支持较长序列的处理。</li><li><strong>支持相对位置关系</strong>：这种编码方法能够通过比较不同位置的编码来推测它们之间的相对距离和顺序，尤其是在模型学习到的位置编码与实际任务（如机器翻译、文本生成）相关时，正弦/余弦函数的变化有助于保持序列的结构和信息流动。</li></ol></li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ 为什么位置编码可以直接与词向量逐元素相加？位置编码会破坏词向量的语义空间吗？</strong></summary><div class=markdown-inner><h2><b>为什么位置编码可以直接与word embedding逐元素相加？位置编码会破坏词向量的语义空间吗？</b></h2><p>Word embedding 主要表示单词的语义信息。Positional Encoding 提供额外的位置信息，以弥补 Transformer 结构中缺少序列顺序感的问题。相加的效果 是让同一个词（如 “apple”）在不同的位置有略微不同的表示，但仍保留其主要的语义信息。</p><p>虽然位置嵌入矩阵 P 与词向量 X 直接相加，但在 transformer 获得 Query (Q)、Key (K) 和 Value (V)的线形变化过程中（i.e. Q = XW_q），在学习 Weight 的过程中会将语义和位置信息分别投射在不同的维度上。Positional Encoding 并不需要通过训练来学习，<strong>它是固定的、基于位置的函数，因此不干扰原本的语义信息</strong>。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是可学习的位置编码（Learned PE）？</strong></summary><div class=markdown-inner><h2><b>什么是可学习的位置编码（Learned PE）？</b></h2><p>可学习的位置编码（Learned Positional Encoding, Learned PE）是一种在模型训练过程中通过优化学习得到的位置编码方法。与传统的 Sinusoidal Positional Encoding（正弦波位置编码）不同，Learned PE 不使用固定的数学公式来表示位置，<strong>而是通过神经网络的训练自动学习每个位置的编码表示</strong>。通常，这些位置编码是通过与输入的 词嵌入（Word Embedding） 相加来为模型提供位置信息，从而使模型能够捕捉到输入序列中各个元素的顺序。具体来说，Learned Positional Encoding 是 <strong>通过一个嵌入层来生成的</strong>。这个过程如下：</p><ol><li><strong>位置嵌入（Position Embedding）</strong>：每个位置（序列中的每个元素）都会被映射到一个可学习的向量。对于输入序列中的每个位置 i，我们为其分配一个嵌入向量 P_i ，这个向量是通过一个嵌入层学习得到的。</li><li><strong>添加到词嵌入（Word Embedding）</strong>：这些位置嵌入向量会与对应的词嵌入（Word Embedding）向量相加。假设某个词 w_i 在序列中的位置为 i，那么该词的最终输入向量为：</li></ol><span>\[
\mathbf{X}_i = \mathbf{X}_i + \mathbf{P}_i
\]</span><p>由于每个位置的编码表示是一个可训练的向量，它会在训练过程中和词嵌入（Word Embedding）一起作为输入传递到模型中。然后，<strong>模型通过反向传播算法更新这些可训练的参数</strong>，以便它们能够更好地捕捉到任务相关的位置信息。</p><p>可学习的位置编码的优点主要体现在 <strong>灵活性</strong>，由于位置编码是通过训练学习的，因此它可以在不同的任务和数据集上找到最优的表示，而不依赖于固定的模式（如正弦波的频率和相位）。这种灵活性使得它能够更好地适应各种复杂的数据模式和任务需求。</p><p>但它 <strong>需要更多的参数</strong>，Learned PE 需要为每个位置学习一个独立的参数，这使得模型的参数量增加，尤其是在处理长序列时，这可能会导致显著的计算和存储开销。同时也会有 <strong>过拟合风险</strong>：由于 Learnable PE 是基于数据学习的，它可能会过度拟合训练数据中的位置模式，尤其是在数据量较少的情况下，从而影响模型的泛化能力。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 写出 RoPE（Rotary Position Embedding）的数学公式，并解释其如何融合相对位置信息。</strong></summary><div class=markdown-inner><h2><b>写出 RoPE（Rotary Position Embedding）的数学公式，并解释其如何融合相对位置信息。</b></h2></div></details><details><summary><strong class=custom-details-title>⁉️ 为什么相对位置编码在长文本任务（如文本生成）中表现更好？</strong></summary><div class=markdown-inner><h2><b>为什么相对位置编码在长文本任务（如文本生成）中表现更好？</b></h2></div></details><details><summary><strong class=custom-details-title>⁉️ 长度外推（Length Extrapolation）问题的本质是什么？哪些位置编码方法能缓解该问题？</strong></summary><div class=markdown-inner><h2><b>长度外推（Length Extrapolation）问题的本质是什么？哪些位置编码方法能缓解该问题？</b></h2></div></details></div></details><details><summary><strong class=custom-details-title>⭐ Transformer 模型架构细节（FFN，Layer Norm，激活函数）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ 序列模型（Sequence Models）和传统模型有什么区别？</strong></summary><div class=markdown-inner><h2><b>序列模型（Sequence Models）和传统模型有什么区别？</b></h2></div></details></div></details><ul><li>✅ 预训练模型（BERT、GPT、T5的架构差异）</li></ul><hr><h3 id=大语言模型llm><strong>大语言模型（LLM）</strong>
<a class=anchor href=#%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8bllm>#</a></h3><ul><li>模型架构（Decoder-only vs Encoder-Decoder）</li><li>预训练任务（MLM、CLM、Span Corruption）</li><li>微调技术（全参数微调、LoRA、QLoRA、Prompt Tuning）</li><li>推理优化（KV Cache、量化、模型剪枝）</li></ul><hr><h3 id=工具与框架><strong>工具与框架</strong>
<a class=anchor href=#%e5%b7%a5%e5%85%b7%e4%b8%8e%e6%a1%86%e6%9e%b6>#</a></h3><ul><li>Hugging Face Transformers库（模型加载、Pipeline、Trainer）</li><li>LangChain（Agent、Chain、Tools设计）</li><li>LlamaIndex（文档索引、检索增强生成RAG）</li></ul><hr><h2 id=应用开发与工程化><strong>应用开发与工程化</strong>
<a class=anchor href=#%e5%ba%94%e7%94%a8%e5%bc%80%e5%8f%91%e4%b8%8e%e5%b7%a5%e7%a8%8b%e5%8c%96>#</a></h2><hr><h3 id=应用开发框架><strong>应用开发框架</strong>
<a class=anchor href=#%e5%ba%94%e7%94%a8%e5%bc%80%e5%8f%91%e6%a1%86%e6%9e%b6>#</a></h3><ul><li>LangChain高级用法（自定义Tools、Agent逻辑、流式输出）</li><li>前端集成（Streamlit、Gradio构建交互界面）</li><li>API开发（FastAPI/Flask构建RESTful服务）</li></ul><hr><h3 id=检索增强生成rag><strong>检索增强生成（RAG）</strong>
<a class=anchor href=#%e6%a3%80%e7%b4%a2%e5%a2%9e%e5%bc%ba%e7%94%9f%e6%88%90rag>#</a></h3><ul><li>向量数据库（Faiss、Pinecone、Chroma）</li><li>文档分块与嵌入策略（滑动窗口、语义分块）</li><li>检索优化（重排序、HyDE技术）</li></ul><hr><h3 id=部署与运维><strong>部署与运维</strong>
<a class=anchor href=#%e9%83%a8%e7%bd%b2%e4%b8%8e%e8%bf%90%e7%bb%b4>#</a></h3><ul><li>容器化（Docker、Kubernetes）</li><li>模型部署（ONNX、TensorRT、Triton Inference Server）</li><li>监控与日志（Prometheus、Grafana）</li><li>云服务（AWS SageMaker、GCP Vertex AI）</li></ul><hr><h2 id=模型训练与优化><strong>模型训练与优化</strong>
<a class=anchor href=#%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83%e4%b8%8e%e4%bc%98%e5%8c%96>#</a></h2><hr><h3 id=分布式训练><strong>分布式训练</strong>
<a class=anchor href=#%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83>#</a></h3><ul><li>数据并行 vs 模型并行</li><li>框架（DeepSpeed、PyTorch DDP、FSDP）</li><li>混合精度训练（FP16、BF16）</li></ul><hr><h3 id=高效微调技术><strong>高效微调技术</strong>
<a class=anchor href=#%e9%ab%98%e6%95%88%e5%be%ae%e8%b0%83%e6%8a%80%e6%9c%af>#</a></h3><ul><li>参数高效微调（LoRA、Adapter、Prefix Tuning）</li><li>低资源训练（QLoRA + 4-bit量化）</li><li>指令微调与对齐（RLHF、DPO）</li></ul><hr><h3 id=模型评估与调优><strong>模型评估与调优</strong>
<a class=anchor href=#%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0%e4%b8%8e%e8%b0%83%e4%bc%98>#</a></h3><ul><li>评估基准（GLUE、SuperGLUE、HELM）</li><li>超参数搜索（Optuna、Ray Tune）</li><li>可解释性（Attention可视化、LIME）</li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#基础理论><strong>基础理论</strong></a><ul><li><a href=#数学与统计学><strong>数学与统计学</strong></a></li><li><a href=#机器学习基础><strong>机器学习基础</strong></a></li><li><a href=#深度学习基础><strong>深度学习基础</strong></a></li></ul></li><li><a href=#llm-核心技术><strong>LLM 核心技术</strong></a><ul><li><a href=#llm-基础组件><strong>LLM 基础组件</strong></a></li><li><a href=#大语言模型llm><strong>大语言模型（LLM）</strong></a></li><li><a href=#工具与框架><strong>工具与框架</strong></a></li></ul></li><li><a href=#应用开发与工程化><strong>应用开发与工程化</strong></a><ul><li><a href=#应用开发框架><strong>应用开发框架</strong></a></li><li><a href=#检索增强生成rag><strong>检索增强生成（RAG）</strong></a></li><li><a href=#部署与运维><strong>部署与运维</strong></a></li></ul></li><li><a href=#模型训练与优化><strong>模型训练与优化</strong></a><ul><li><a href=#分布式训练><strong>分布式训练</strong></a></li><li><a href=#高效微调技术><strong>高效微调技术</strong></a></li><li><a href=#模型评估与调优><strong>模型评估与调优</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>