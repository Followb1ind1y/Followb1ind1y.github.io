<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  面试准备大纲
  #



  基础理论
  #



  数学与统计学
  #


✅ 线性代数（矩阵运算、特征值分解）
✅ 概率论与统计（贝叶斯定理、分布、假设检验）
✅ 优化方法（梯度下降、Adam、学习率调度）



  机器学习基础
  #


✅ 监督学习（线性回归、决策树、SVM、集成学习）
✅ 评估指标（准确率、召回率、F1、AUC-ROC）
✅ 过拟合与正则化（L1/L2、Dropout）



  深度学习基础
  #


⭐ 神经网络（前向传播、反向传播、激活函数）


⭐ RNN（序列建模、自回归、语言模型）
  
⁉️ 序列模型（Sequence Models）和传统模型有什么区别？
  
    序列模型（Sequence Models）和传统模型有什么区别？
序列模型（Sequence Models）和传统模型的主要区别在于它们处理数据的方式。传统模型，如线性回归（Linear Regression），通常 假设输入数据是独立同分布（i.i.d., independent and identically distributed）的，即每个输入数据点的顺序不影响其他数据点。在这些模型中，数据被处理为独立的特征向量，通常是静态的，缺乏时间或顺序的关联。
序列模型，尤其是在自然语言处理领域，强调对数据中 时间或顺序信息的建模。他们往往能够通过其递归结构捕捉序列中前后数据点之间的依赖关系。序列模型在处理时考虑到顺序，因此它们在语言建模（Language Modeling）、语音识别（Speech Recognition）等任务中表现优异，因为这些任务 依赖于上下文信息和时间序列的动态变化。
  

⁉️ 什么是自回归（Autoregression）和自回归模型？
  
    什么是自回归（Autoregression）和自回归模型？
自回归（Autoregression）是一种统计模型，用于预测时间序列数据中的未来值。它基于一个假设：当前的观测值（或输出）与过去的观测值有直接关系。自回归模型通过 使用历史数据点作为输入，预测下一个时间步的值。在这种模型中，当前的输出是历史数据的线性组合，具体来说，当前的值是由过去若干个数据点的加权和构成。
在自然语言处理（NLP）中的生成式语言模型（如自回归语言模型）中，自回归模型的原理类似于时间序列分析。自回归语言模型在生成文本时，将每个词作为输入，并依赖于已经生成的词序列来预测下一个词。在这种情况下，模型的预测依赖于前一个或前几个词，因此可以用自回归模型的框架来描述这一过程。它通过逐步生成下一个输出（例如，词或值）并基于之前的生成结果来调整预测，确保每一步都与前面的步骤密切相关。
  

⁉️ 如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？
  
    如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？
在自然语言处理（NLP）中，语言模型（Language Model, LM） 是一种用于预测句子或文本中单词序列的模型。语言模型的核心目标是 估计一个给定单词序列出现的概率。具体来说，对于一个给定的单词序列，语言模型的任务是计算该序列的联合概率：



  \[
P(x_1, \ldots, x_T) 
\]

我们可以通过链式法则将序列的联合概率分解成条件概率的乘积，从而将问题转化为自回归预测（autoregressive prediction）："><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/others/interview-preparation-guide/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Interview Preparation Guide"><meta property="og:description" content="面试准备大纲 # 基础理论 # 数学与统计学 # ✅ 线性代数（矩阵运算、特征值分解） ✅ 概率论与统计（贝叶斯定理、分布、假设检验） ✅ 优化方法（梯度下降、Adam、学习率调度） 机器学习基础 # ✅ 监督学习（线性回归、决策树、SVM、集成学习） ✅ 评估指标（准确率、召回率、F1、AUC-ROC） ✅ 过拟合与正则化（L1/L2、Dropout） 深度学习基础 # ⭐ 神经网络（前向传播、反向传播、激活函数） ⭐ RNN（序列建模、自回归、语言模型） ⁉️ 序列模型（Sequence Models）和传统模型有什么区别？ 序列模型（Sequence Models）和传统模型有什么区别？ 序列模型（Sequence Models）和传统模型的主要区别在于它们处理数据的方式。传统模型，如线性回归（Linear Regression），通常 假设输入数据是独立同分布（i.i.d., independent and identically distributed）的，即每个输入数据点的顺序不影响其他数据点。在这些模型中，数据被处理为独立的特征向量，通常是静态的，缺乏时间或顺序的关联。
序列模型，尤其是在自然语言处理领域，强调对数据中 时间或顺序信息的建模。他们往往能够通过其递归结构捕捉序列中前后数据点之间的依赖关系。序列模型在处理时考虑到顺序，因此它们在语言建模（Language Modeling）、语音识别（Speech Recognition）等任务中表现优异，因为这些任务 依赖于上下文信息和时间序列的动态变化。
⁉️ 什么是自回归（Autoregression）和自回归模型？ 什么是自回归（Autoregression）和自回归模型？ 自回归（Autoregression）是一种统计模型，用于预测时间序列数据中的未来值。它基于一个假设：当前的观测值（或输出）与过去的观测值有直接关系。自回归模型通过 使用历史数据点作为输入，预测下一个时间步的值。在这种模型中，当前的输出是历史数据的线性组合，具体来说，当前的值是由过去若干个数据点的加权和构成。
在自然语言处理（NLP）中的生成式语言模型（如自回归语言模型）中，自回归模型的原理类似于时间序列分析。自回归语言模型在生成文本时，将每个词作为输入，并依赖于已经生成的词序列来预测下一个词。在这种情况下，模型的预测依赖于前一个或前几个词，因此可以用自回归模型的框架来描述这一过程。它通过逐步生成下一个输出（例如，词或值）并基于之前的生成结果来调整预测，确保每一步都与前面的步骤密切相关。
⁉️ 如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？ 如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？ 在自然语言处理（NLP）中，语言模型（Language Model, LM） 是一种用于预测句子或文本中单词序列的模型。语言模型的核心目标是 估计一个给定单词序列出现的概率。具体来说，对于一个给定的单词序列，语言模型的任务是计算该序列的联合概率：
\[ P(x_1, \ldots, x_T) \] 我们可以通过链式法则将序列的联合概率分解成条件概率的乘积，从而将问题转化为自回归预测（autoregressive prediction）："><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Interview Preparation Guide | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/others/interview-preparation-guide/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a83786cf336fcc6d79fb0510bd001cac287af8c82ef854b4ca13b14d1506776d.js integrity="sha256-qDeGzzNvzG15+wUQvQAcrCh6+Mgu+FS0yhOxTRUGd20=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/others/interview-preparation-guide/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><input type=checkbox id=section-d0dd931d60033c220ecd4cd60b7c9170 class=toggle>
<label for=section-d0dd931d60033c220ecd4cd60b7c9170 class="flex justify-between"><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a></label><ul><li><a href=/docs/deep-learning/convolutional-neural-networks/modern-convolutional-neural-networks/>Modern Convolutional Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-a3019bfa8037cc33ed6405d1589b6219 class=toggle>
<label for=section-a3019bfa8037cc33ed6405d1589b6219 class="flex justify-between"><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a></label><ul><li><a href=/docs/deep-learning/recurrent-neural-networks/modern-recurrent-neural-networks/>Modern Recurrent Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-0a43584c16258b228ae9aa8d70efc320 class=toggle>
<label for=section-0a43584c16258b228ae9aa8d70efc320 class="flex justify-between"><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a></label><ul><li><a href=/docs/deep-learning/attention-and-transformers/large-scale-pretraining-with-transformers/>Large-Scale Pretraining with Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/modern-large-language-models-copy/>Modern Large Language Models</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92e8358c45c96009753cf4227e9daea8 class=toggle>
<label for=section-92e8358c45c96009753cf4227e9daea8 class="flex justify-between"><a href=/docs/deep-learning/llm-pipelines/>LLM Pipelines</a></label><ul><li><a href=/docs/deep-learning/llm-pipelines/llm-inference-and-deployment/>LLM Inference and Deployment</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle checked>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul><li><a href=/docs/others/interview-preparation-guide/ class=active>Interview Preparation Guide</a><ul></ul></li></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Interview Preparation Guide</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#基础理论><strong>基础理论</strong></a><ul><li><a href=#数学与统计学><strong>数学与统计学</strong></a></li><li><a href=#机器学习基础><strong>机器学习基础</strong></a></li><li><a href=#深度学习基础><strong>深度学习基础</strong></a></li></ul></li><li><a href=#llm-核心技术><strong>LLM 核心技术</strong></a><ul><li><a href=#llm-基础组件><strong>LLM 基础组件</strong></a></li><li><a href=#大语言模型llm><strong>大语言模型（LLM）</strong></a></li><li><a href=#工具与框架><strong>工具与框架</strong></a></li></ul></li><li><a href=#应用开发与工程化><strong>应用开发与工程化</strong></a><ul><li><a href=#应用开发框架><strong>应用开发框架</strong></a></li><li><a href=#检索增强生成rag><strong>检索增强生成（RAG）</strong></a></li><li><a href=#部署与运维><strong>部署与运维</strong></a></li></ul></li><li><a href=#模型训练与优化><strong>模型训练与优化</strong></a><ul><li><a href=#分布式训练><strong>分布式训练</strong></a></li><li><a href=#高效微调技术><strong>高效微调技术</strong></a></li><li><a href=#模型评估与调优><strong>模型评估与调优</strong></a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=面试准备大纲><strong>面试准备大纲</strong>
<a class=anchor href=#%e9%9d%a2%e8%af%95%e5%87%86%e5%a4%87%e5%a4%a7%e7%ba%b2>#</a></h1><hr><h2 id=基础理论><strong>基础理论</strong>
<a class=anchor href=#%e5%9f%ba%e7%a1%80%e7%90%86%e8%ae%ba>#</a></h2><hr><h3 id=数学与统计学><strong>数学与统计学</strong>
<a class=anchor href=#%e6%95%b0%e5%ad%a6%e4%b8%8e%e7%bb%9f%e8%ae%a1%e5%ad%a6>#</a></h3><ul><li>✅ 线性代数（矩阵运算、特征值分解）</li><li>✅ 概率论与统计（贝叶斯定理、分布、假设检验）</li><li>✅ 优化方法（梯度下降、Adam、学习率调度）</li></ul><hr><h3 id=机器学习基础><strong>机器学习基础</strong>
<a class=anchor href=#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80>#</a></h3><ul><li>✅ 监督学习（线性回归、决策树、SVM、集成学习）</li><li>✅ 评估指标（准确率、召回率、F1、AUC-ROC）</li><li>✅ 过拟合与正则化（L1/L2、Dropout）</li></ul><hr><h3 id=深度学习基础><strong>深度学习基础</strong>
<a class=anchor href=#%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80>#</a></h3><ul><li>⭐ 神经网络（前向传播、反向传播、激活函数）</li></ul><details><summary><strong class=custom-details-title>⭐ RNN（序列建模、自回归、语言模型）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ 序列模型（Sequence Models）和传统模型有什么区别？</strong></summary><div class=markdown-inner><h2><b>序列模型（Sequence Models）和传统模型有什么区别？</b></h2><p>序列模型（Sequence Models）和传统模型的主要区别在于它们处理数据的方式。传统模型，如线性回归（Linear Regression），通常 <strong>假设输入数据是独立同分布（i.i.d., independent and identically distributed）的</strong>，即每个输入数据点的顺序不影响其他数据点。在这些模型中，数据被处理为<strong>独立的特征向量，通常是静态的，缺乏时间或顺序的关联</strong>。</p><p>序列模型，尤其是在自然语言处理领域，强调对数据中 <strong>时间或顺序信息的建模</strong>。他们往往能够通过其递归结构捕捉序列中前后数据点之间的依赖关系。序列模型在处理时考虑到顺序，因此它们在语言建模（Language Modeling）、语音识别（Speech Recognition）等任务中表现优异，因为这些任务 <strong>依赖于上下文信息和时间序列的动态变化</strong>。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是自回归（Autoregression）和自回归模型？</strong></summary><div class=markdown-inner><h2><b>什么是自回归（Autoregression）和自回归模型？</b></h2><p>自回归（Autoregression）是一种统计模型，用于预测时间序列数据中的未来值。它基于一个假设：<strong>当前的观测值（或输出）与过去的观测值有直接关系</strong>。自回归模型通过 <strong>使用历史数据点作为输入，预测下一个时间步的值</strong>。在这种模型中，当前的输出是历史数据的线性组合，具体来说，当前的值是由过去若干个数据点的加权和构成。</p><p>在自然语言处理（NLP）中的生成式语言模型（如自回归语言模型）中，自回归模型的原理类似于时间序列分析。自回归语言模型在生成文本时，将每个词作为输入，并依赖于已经生成的词序列来预测下一个词。在这种情况下，模型的预测依赖于前一个或前几个词，因此可以用自回归模型的框架来描述这一过程。<strong>它通过逐步生成下一个输出（例如，词或值）并基于之前的生成结果来调整预测，确保每一步都与前面的步骤密切相关</strong>。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？</strong></summary><div class=markdown-inner><h2><b>如何定义语言模型？统计角度在解决的问题？什么是 n-gram模型？</b></h2><p>在自然语言处理（NLP）中，语言模型（Language Model, LM） 是一种用于预测句子或文本中单词序列的模型。语言模型的核心目标是 <strong>估计一个给定单词序列出现的概率</strong>。具体来说，对于一个给定的单词序列，语言模型的任务是计算该序列的联合概率：</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[
P(x_1, \ldots, x_T)
\]</span><p>我们可以通过链式法则将序列的联合概率分解成条件概率的乘积，从而将问题转化为自回归预测（autoregressive prediction）：</p><span>\[
P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1)
\]</span><p>在语言模型中，最常见的假设是 马尔科夫假设（Markov Assumption），即 <strong>当前单词的出现只依赖于前一个或前几个单词</strong>。基于这一假设，<strong>n-gram模型（n-gram model）</strong> 是一种常见的语言模型，它通过计算某个单词在给定其前 n-1 个单词的条件下出现的概率来进行预测。例如：</p><span>\[
P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1})
\]</span></div></details><details><summary><strong class=custom-details-title>⁉️ 如何衡量语言模型质量？什么是困惑度（Perplexity）？</strong></summary><div class=markdown-inner><h2><b>什么是困惑度（Perplexity）？</b></h2><p>衡量语言模型质量的一种方法是评估其对文本的预测能力。一个好的语言模型 <strong>能够以较高的准确性预测下一个词（token）</strong>。</p><span>\[
P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1)
\]</span><p>为了量化模型质量，可以通过计算序列的似然（likelihood）。然而，直接比较似然值并不直观，<strong>因为较短的序列通常有更高的似然值</strong>。例如我们用交叉熵（cross-entropy）衡量模型对序列的预测能力。</p><span>\[
\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1)
\]</span><p>为了解决：“较短的序列通常有更高的似然值”，自然语言处理领域通常使用<strong>困惑度（Perplexity）</strong> 作为评价标准，它是交叉熵损失的指数形式：</p><span>\[
\text{Perplexity} = \exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right)
\]</span><p><strong>困惑度可以理解为我们在选择下一个词时平均可用的真实选项数的倒数。困惑度越低，模型质量越高，表明其对文本序列的预测能力越强。</strong></p></div></details><details><summary><strong class=custom-details-title>⁉️ 在 RNN 中是如何读取长序列数据（Partitioning Sequences）的？</strong></summary><div class=markdown-inner><h2><b>在 RNN 中是如何读取长序列数据（Partitioning Sequences）的？</b></h2><p>由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。为了高效处理数据，模型通常以固定长度的序列小批量（minibatch）为单位进行训练。一个关键问题是 <strong>如何从数据集中随机读取输入序列和目标序列的小批量</strong>。处理长序列数据时，常用的 Partitioning Sequences 方法主要包括以下几种：</p><ol><li><strong>固定长度切分（Fixed-length Splitting）</strong> 是最常见的方法之一，其中将长序列分割成固定大小的子序列。这种方法简单且易于实现，但可能会丢失跨子序列的上下文信息。</li><li><strong>滑动窗口（Sliding Window）</strong> 方法通过定义一个窗口大小并在长序列中滑动该窗口来划分数据。每次滑动时，窗口会覆盖一定数量的词汇，并且每次滑动的步长通常为窗口大小的一部分，确保子序列之间有重叠，从而保持一定的上下文信息。</li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是RNN？描述RNN的基本结构？？</strong></summary><div class=markdown-inner><h2><b>什么是RNN？描述RNN的基本结构？？</b></h2><p>在用马尔可夫模型和n-gram模型用于语言建模时，这些模型中某一时刻的词（token）只依赖于前 n 个词。如果我们希望将更早时间步的词对当前词的影响考虑进来，就需要增加 n 的值。然而，随着 n 增加，模型参数的数量也会呈指数级增长，因为需要为词汇表中的每个词存储对应的参数。因此， 因此与其将历史信息模型化，不如使用<strong>隐变量模型（latent variable model）</strong>：</p><span>\[
P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid h_{t-1})
\]</span><p>潜在变量模型的核心思想是通过 <strong>引入一个隐藏状态（hidden state）</strong>，它保存了直到当前时间步的序列信息。具体来说，隐藏状态在任意时间步可以通过 <strong>当前输入</strong> 和 <strong>上一个隐藏状态</strong> 来计算：</p><span>\[
h_t = f(x_{t}, h_{t-1})
\]</span><p><strong>循环神经网络（RNN） 就是通过隐藏状态来建模序列数据的神经网络</strong>。RNN 由三个主要部分组成：</p><ol><li><strong>输入层（Input layer）</strong>：接收序列数据，通常表示为词向量（word embeddings）或特征向量（feature vectors）。</li><li><strong>隐藏层（Hidden layer）</strong>：核心部分，由 隐藏状态（hidden state） 组成，每个时间步的隐藏状态不仅依赖于当前输入 x_t ，还依赖于前一个时间步的隐藏状态 h_{t-1} 。<strong>隐藏状态就是网络当前时刻的”记忆”</strong>。更新公式如下：</li></ol><span>\[
\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh} + \mathbf{b}_h)
\]</span><ol start=3><li><strong>输出层（Output layer）</strong>：用于预测目标值 y_t ，通常通过 全连接层（fully connected layer） 和 Softmax 激活函数（Softmax activation function） 计算类别概率：</li></ol><span>\[
\mathbf{O} = \mathbf{H} \mathbf{W}_{hq} + \mathbf{b}_q,
\]</span><p>在标准的RNN模型中，<strong>隐藏单元（hidden state）的权重是共享的</strong>，<strong>即相同的权重矩阵 W_xh（从输入到隐藏状态的权重）和 W_hh（从上一个时间步的隐藏状态到当前隐藏状态的权重）在每个时间步都会被复用</strong>。这意味着，RNN通过不断更新隐藏状态，并依赖同一组权重来捕捉序列中的时序信息。尽管RNN在每个时间步都会计算新的隐藏状态，但在基础模型中没有涉及多层结构的概念。</p></div></details><details><summary><strong class=custom-details-title>⁉️ RNN 在 inference 阶段的解码 (Decoding) 是怎么实现的？</strong></summary><div class=markdown-inner><h2><b>RNN 在 inference 阶段的解码 (Decoding) 是怎么实现的？</b></h2><p>在训练好语言模型后，模型不仅可以预测下一个 token，还可以通过将前一个预测的 token 作为输入，连续地预测后续的 token。标准的 RNN 解码过程的步骤可以总结为：</p><ol><li><strong>Warm-up阶段</strong>：<ul><li>解码开始时，将输入序列（已知 token ）输入到模型中，不输出任何结果。</li><li>目的是通过传递隐藏状态（hidden state），初始化模型内部状态以适应上下文。</li></ul></li><li><strong>续写生成</strong>：<ul><li>在输入完前缀后，模型开始生成后续字符。</li><li>每次生成一个字符，将其作为下一个时间步的输入，依次循环生成目标长度的文本。</li></ul></li><li><strong>输入和输出映射</strong>：<ul><li>通过输出层预测字符分布，并选择概率最大的字符作为结果。</li></ul></li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ RNN训练时的主要挑战是什么？</strong></summary><div class=markdown-inner><h2><b>RNN训练时的主要挑战是什么？</b></h2><p>在训练 循环神经网络（Recurrent Neural Network, RNN） 时，主要面临以下几个挑战：</p><ol><li><strong>梯度消失（Vanishing Gradient）和梯度爆炸（Exploding Gradient）</strong>：由于 RNN 依赖于 <strong>时间步（time step）上的递归计算，在反向传播（Backpropagation Through Time, BPTT）</strong> 时，梯度会随着时间步数的增加不断衰减或增长。如果梯度值指数级减小，会导致模型在长序列上的学习能力受限，难以捕捉远距离依赖（long-range dependencies）；如果梯度值指数级增大，则会导致梯度爆炸，使得模型参数更新过大，训练变得不稳定。</li><li><strong>长期依赖问题（Long-Term Dependency Problem）</strong>：RNN 通过隐藏状态（hidden state） 传递信息，但当序列较长时，<strong>早期输入的信息会逐渐被后续时间步的信息覆盖</strong>，导致模型难以捕获远距离的上下文关系。</li><li><strong>计算效率低（Sequential Computation Bottleneck）</strong>：RNN 的计算是顺序的（sequential），<strong>即当前时间步的计算依赖于前一个时间步的计算结果，因此难以并行化</strong>。这使得 RNN 的训练和推理速度远低于 Transformer 这种可以全并行计算的架构。</li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是梯度裁剪（Gradient Clipping）？为什么它被运用在 RNN 训练过程中？</strong></summary><div class=markdown-inner><h2><b>什么是梯度裁剪（Gradient Clipping）？为什么它被运用在 RNN 训练过程中？</b></h2><p><strong>梯度裁剪（Gradient Clipping）</strong> 是一种用于 <strong>防止梯度爆炸（Gradient Explosion）</strong> 的技术，主要在训练 循环神经网络（Recurrent Neural Networks, RNNs） 时使用。由于 RNN 需要进行反向传播，当序列较长时，梯度可能会在传播过程中指数级增长，导致 <strong>参数更新过大</strong>，进而影响模型的稳定性。</p><p>梯度裁剪的核心思想是在反向传播时，<strong>如果梯度的范数（Norm）超过了某个预设阈值（Threshold），则对梯度进行缩放（Scaling），限制梯度范数不超过一定范围</strong>，即：</p><span>\[
\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}
\]</span></div></details></div></details><details><summary><strong class=custom-details-title>⭐ 经典序列模型（LSTM、GRU、Seq2Seq）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ LSTM（Long Short-Term Memory）的核心架构和原理是什么？</strong></summary><div class=markdown-inner><h2><b>LSTM 的核心架构和原理是什么？</b></h2><p>LSTM（Long Short-Term Memory）相比于传统 RNN 的主要改进在于它引入了 门控机制（Gating Mechanism），有效缓解了 梯度消失（Gradient Vanishing） 和 梯度爆炸（Gradient Explosion） 问题，使得模型能够捕捉长期依赖信息（Long-Term Dependencies）。</p><p>LSTM 由 遗忘门（Forget Gate）、输入门（Input Gate） 和 输出门（Output Gate） 组成，每个时间步通过这些门控单元来控制信息的流动。其中，<strong>遗忘门</strong> 负责决定遗忘多少过去的信息，<strong>输入门</strong> 控制新信息的写入，<strong>输出门</strong> 影响隐藏状态（Hidden State）的更新。</p><span>\[
\begin{split}\begin{aligned}
\mathbf{I}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xi} + \mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i),\\
\mathbf{F}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xf} + \mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f),\\
\mathbf{O}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xo} + \mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o),
\end{aligned}\end{split}
\]</span><blockquote class="book-hint warning"><p><strong>Note：</strong> 尽管公式形式类似，LSTM 中的门和一般 RNN 中的 Hidden state 的<strong>核心作用完全不同</strong>：</p><ol><li><strong>门的作用是“控制流动”</strong>：<ul><li>门本质上是一种控制开关，它的输出必须在 [0, 1] 之间，这样才能<strong>直观地表示“通过的信息比例”</strong>。所以门的激活函数使用的是 Sigmoid。</li><li>它们主要用于<strong>调节信息的流动</strong>，而不是直接参与信息存储。</li></ul></li><li><strong>Hidden State 的作用是“存储和传递信息”</strong>：<ul><li>而 Hidden state 的激活函数使用的是 tanh，其范围为 [-1, 1]，<strong>更适合表示信息本身的动态特征</strong>。</li><li>它不仅受门控机制影响，还通过非线性变换从记忆单元中提取信息。</li></ul></li></ol></blockquote><p>此外，LSTM 还维护了一个额外的 细胞状态（Cell State, C_t），用于长期存储信息。<strong>它可以被视为截止至当前时刻 t 的综合记忆信息</strong>。</p><p>新输入数据结合了当前时刻的输入 X 和上一个时间步的 Hidden State，可以被表示为：</p><span>\[
\tilde{\mathbf{C}}_t = \textrm{tanh}(\mathbf{X}_t \mathbf{W}_{\textrm{xc}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hc}} + \mathbf{b}_\textrm{c}),
\]</span><p>输入门控制我们在多大程度上考虑新输入数据，而遗忘门则决定了我们保留多少旧的记忆单元内部状态。通过使用Hadamard 积逐元素相乘）运算符，LSTM 的记忆单元内部状态的更新方程为：</p><span>\[
\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t
\]</span><p>最后，隐藏状态（Hidden State）定义了记忆单元的输出方式，它由输出门（Output Gate）控制。具体计算步骤如下：首先，对记忆单元的内部状态 应用 <code>tanh</code> 函数，使其值被规范化到 <code>(-1, 1)</code> 区间内。然后，将这一结果与输出门的值逐元素相乘，计算得到隐藏状态：</p><span>\[
\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t)
\]</span><p><strong>输出门（Output Gate） 的主要作用</strong>是控制 <strong>当前时刻的隐藏状态的输出</strong> 内容，从而决定 LSTM 的对外信息传递。Cell State 是 LSTM 的内部长期记忆，但它并不直接对外输出。输出门通过<strong>选择性地提取 Cell State 中的信息</strong>，并结合门控机制生成 <strong>新的Hidden State（短期记忆的表达）</strong>，作为当前时间步的隐藏状态对外输出。这种机制确保 LSTM 在对外传递信息时，<strong>不会将 Cell State 中的所有内容暴露出去</strong>，避免噪声干扰，同时保留最相关的信息。</p></div></details><details><summary><strong class=custom-details-title>⁉️ LSTM 解决的问题和原因？</strong></summary><div class=markdown-inner><h2><b>LSTM解决的问题和原因？</b></h2><p>LSTM（Long Short-Term Memory）主要解决了标准RNN在处理长序列时的 <strong>梯度消失（vanishing gradients）和梯度爆炸（exploding gradients）</strong> 问题。 这些问题导致普通RNN在处理长期依赖（long-term dependencies）时性能较差，无法有效捕获长时间跨度的信息。</p><ol><li><strong>细胞状态（Cell State）作为长期记忆的载体</strong><ul><li>LSTM引入了一个额外的细胞状态，它可以通过直通路径（&ldquo;constant error carousel&rdquo;）跨时间步传播信息，<strong>几乎不受梯度消失或梯度爆炸的影响</strong>。</li></ul></li><li><strong>梯度传播更稳定</strong><ul><li>普通RNN的梯度通过时间步传播时，会被<strong>反复乘以隐状态的权重矩阵</strong>。当权重矩阵的特征值远离 1 时，梯度会出现指数增长（梯度爆炸）或指数衰减（梯度消失）。</li><li>在LSTM中，<strong>细胞状态通过线性加权方式更新</strong>（不直接经过激活函数的非线性变换），从而避免了梯度的剧烈变化。门机制确保了信息流动的可控性，进一步减轻了梯度不稳定的问题。</li></ul></li><li><strong>更强的记忆能力</strong><ul><li>LSTM能<strong>同时捕获短期依赖和长期依赖（通过细胞状态）</strong>。在长时间序列中，它可以动态调整对不同时间跨度的信息的关注程度，使其既能够记住长期信息，又不会因记忆过多导致模型过载。</li></ul></li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ 解释 GRU（Gated Recurrent Unit）的核心架构和原理？</strong></summary><div class=markdown-inner><h2><b>解释 GRU 的核心架构和原理？</b></h2><p>GRU（门控循环单元）是 <strong>LSTM记忆单元的简化版本</strong> 并保留内部状态和乘法门控机制的核心思想。相比LSTM，GRU在很多任务中可以达到相似的性能，但由于结构更加简单，计算速度更快。</p><p>在GRU（Gated Recurrent Unit）中，LSTM的三个门被替换为两个门：<strong>重置门（Reset Gate）</strong> 和 <strong>更新门（Update Gate）</strong>。这两个门使用了 <code>Sigmoid</code> 激活函数，输出值限制在区间 <code>[0, 1]</code> 内。</p><ul><li><strong>重置门</strong>：决定了当前状态需要记住多少之前隐藏状态的信息。</li><li><strong>更新门</strong>：控制新状态有多少是继承自旧状态的。</li></ul><span>\[
\begin{split}\begin{aligned}
\mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r),\\
\mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} + \mathbf{b}_z),
\end{aligned}\end{split}
\]</span><p>重置门（reset gate）与标准更新机制相结合，生成时间步 t 的 **候选隐藏状态（candidate hidden state），公式如下：</p><span>\[
\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} + \mathbf{b}_h),
\]</span><blockquote class="book-hint warning"><p><strong>Note：</strong> 重置门主要是对长期记忆进行筛选，在计算候选隐藏状态时，抑制过去隐藏状态中的某些部分，使得模型更多地依赖当前输入。这种机制让模型能够<strong>专注于短期依赖</strong>，通过结合当前输入<strong>产生一个更符合短期记忆的候选状态</strong>。</p></blockquote><p><strong>更新门（Update gate）决定了新隐藏状态</strong> 在多大程度上保留旧状态与新候选状态的信息。具体而言，控制了二者的加权组合，公式如下：</p><span>\[
\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1} + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t.
\]</span><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>更新门（update gate）</strong> 则决定 <strong>当前时刻的隐藏状态是由之前的隐藏状态（长期记忆）和当前输入（短期信息）以何种比例组合而成</strong>。更新门的值接近 1 时，模型保留大部分的长期记忆，接近 0 时则依赖更多的当前输入。这种机制帮助 GRU 捕捉长期依赖关系，因为更新门可以调节模型在序列中如何传递和利用过去的记忆。</p></blockquote></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是深层 RNN（Deep RNNs），它与普通 RNN 有什么区别？</strong></summary><div class=markdown-inner><h2><b>什么是深层 RNN（Deep RNNs），它与普通 RNN 有什么区别？</b></h2><p>深层循环神经网络（Deep Recurrent Neural Networks, DRNN）是通过 <strong>堆叠多个RNN层</strong> 实现的。单隐藏层的RNN网络结构在捕捉时间步内部输入与输出之间复杂关系时存在局限。因此，为了同时增强 <strong>模型对时间依赖（temporal dependency）</strong> 和 <strong>时间步内部输入与输出关系</strong> 的建模能力，常会构造在时间方向和输入输出方向上都更深的RNN网络。这种深度的概念类似于在多层感知机（MLP）和深度卷积神经网络（CNN）中见到的层级加深方法。</p><p>在深层RNN中，每一时间步的隐藏单元 <strong>不仅依赖于同层前一个时间步的隐藏状态</strong>，还依赖于 <strong>前一层相同时间步的隐藏状态</strong>。这种结构使得深层RNN能够 <strong>同时捕获长期依赖关系（long-term dependency）和复杂的输入-输出关系</strong>。</p><p>在深层RNN的第l（l=1,&mldr;,L）个隐藏层Hidden state可以表示为:</p><span>\[
\mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)} + \mathbf{b}_h^{(l)}),
\]</span><p>最后，输出层的计算仅基于第<span>
(l)
</span>个隐藏层最终的隐状态：</p><span>\[
\mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q,
\]</span></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是双向循环神经网络（BiRNN）？为什么在某些任务中它的的效果优于RNN？</strong></summary><div class=markdown-inner><h2><b>什么是双向循环神经网络（BiRNN）？为什么在某些任务中它的的效果优于RNN？</b></h2><p><strong>双向循环神经网络（Bidirectional Recurrent Neural Network, BiRNN）是一种扩展传统循环神经网络（Recurrent Neural Network, RNN）</strong> 的方法。与单向 RNN 仅从过去到未来处理序列数据不同，BiRNN 在每个时间步（Timestep）中同时计算两个方向的信息流：一个从前向后（Forward Direction），另一个从后向前（Backward Direction）。这种结构通过两个独立的隐藏层（Hidden Layers）分别处理时间序列的正向和反向信息，并在输出层（Output Layer）结合这两个隐藏状态（Hidden States），从而获得更丰富的上下文信息。前向和反向隐状态的更新如下：</p><span>\[
\begin{split}\begin{aligned}
\overrightarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{hh}^{(f)} + \mathbf{b}_h^{(f)}),\\
\overleftarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{hh}^{(b)} + \mathbf{b}_h^{(b)}),
\end{aligned}\end{split}
\]</span><p>接下来，将前向隐状态和反向隐状态连接起来，获得需要送入输出层的隐状态。在具有多个隐藏层的深度双向循环神经网络中，该信息作为输入传递到下一个双向层。</p><span>\[
\mathbf{H}_t = \begin{bmatrix} \overrightarrow{\mathbf{H}}_t \\ \overleftarrow{\mathbf{H}}_t \end{bmatrix}
\]</span><p>最后，输出层计算得到的输出为：</p><span>\[
\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q.
\]</span><p>在某些任务中，BiRNN 的效果优于单向 RNN，主要是因为它能够利用未来和过去的信息，而不仅仅依赖于当前时间步之前的历史数据。BiRNN 适用于 <strong>需要充分利用上下文信息的序列任务</strong>，特别是在 NLP 领域。具体来说，文本分类（Text Classification）、情感分析（Sentiment Analysis）、语音识别（Automatic Speech Recognition, ASR）等任务都可以从 BiRNN 的双向信息流中获益。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是 Encoder-Decoder 结构？它是如何工作的？</strong></summary><div class=markdown-inner><h2><b>什么是 Encoder-Decoder 结构？它是如何工作的？</b></h2><p>在序列到序列（sequence-to-sequence）问题中（如机器翻译），<strong>输入和输出通常具有不同的长度，且无法直接对齐</strong>。为了解决这一问题，通常采用编码器-解码器（encoder-decoder）架构。这个架构包括两个主要组件：</p><p><strong>编码器（Encoder）的作用是处理输入序列并将其转化为一个固定大小的表示（通常是一个向量，也叫做上下文向量（Context Vector））</strong>。在传统的 RNN（Recurrent Neural Network）或 LSTM（Long Short-Term Memory）网络中，编码器逐步读取输入序列的每个元素（如单词或字符），并通过递归地更新其隐藏状态（Hidden State）来捕捉输入的语义信息。最终，<strong>编码器输出的隐藏状态或最后一个时间步的隐藏状态</strong>（在一些变体中是所有时间步的隐藏状态）作为对输入序列的总结。</p><p><strong>解码器（Decoder）则是基于编码器的输出生成目标序列</strong>。解码器通常也是一个RNN或LSTM，它的工作方式是逐步预测输出序列中的每个元素。解码器首先接受编码器生成的上下文向量作为初始的隐藏状态，然后在生成每个目标词时，它根据当前隐藏状态以及之前生成的输出（或在训练时，使用教师强迫（Teacher Forcing），即真实的标签作为输入）来预测下一个词。这个过程一直持续到生成完整的目标序列。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是 seq2seq 模型？</strong></summary><div class=markdown-inner><h2><b>什么是 seq2seq 模型？</b></h2><p>序列到序列（Sequence-to-Sequence, Seq2Seq）模型是一种用于处理输入序列（Input Sequence）到输出序列（Output Sequence）转换的深度学习架构（<strong>输入和输出都是变长的、未对齐的序列</strong>）。Seq2Seq 模型的核心结构是 <strong>编码器-解码器（Encoder-Decoder）架构</strong>，其中编码器（Encoder）的主要作用是将一个 <strong>长度可变的输入序列</strong> 转换为 <strong>固定形状的上下文变量（context variable）</strong>。这一过程可表示为：</p><span>\[
\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1}).
\]</span><p>Encoder 会利用自定义的函数 g 将所有时间步的隐藏状态转换为一个固定形状的上下文变量：</p><span>\[
\mathbf{c} = q(\mathbf{h}_1, \ldots, \mathbf{h}_T).
\]</span><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>Encoder 的设计目的</strong>：</p><ol><li><strong>压缩输入信息</strong>：将输入序列的所有信息压缩到一个低维表示中，确保模型能够以固定大小的特征表示处理任意长度的输入。</li><li><strong>捕捉序列的全局语义</strong>： Encoder 会通过递归网络（如 RNN、GRU 或 LSTM）处理输入序列，将序列中的时序依赖关系和语义信息编码到隐藏状态中。</li><li><strong>作为中间表示</strong>： Encoder 的输出（隐藏状态或上下文变量）提供了一种抽象的、高效的输入表示，适合传递给其他模块（如 Decoder）或用于分类、翻译等下游任务。</li></ol></blockquote><p><strong>解码器（decoder）负责根据目标输出序列</strong>，在每个时间步 t 预测下一步的输出。解码器的核心是基于目标序列中前一时间步的输出、前一时间步的隐藏状态和上下文变量来计算当前时间步的隐藏状态。公式如下：</p><span>\[
\mathbf{s}_{t} = g(y_{t}, \mathbf{c}, \mathbf{s}_{t}).
\]</span><p>在得到当前时间步的隐藏状态 <span>(\mathbf{s}_t)
</span>后，通过输出层和 softmax 操作计算下一步的输出 <span>(y_t)
</span>的概率分布:</p><span>\[
P(y_{t} \mid y_1, \ldots, y_{t}, \mathbf{c})
\]</span><blockquote class="book-hint warning"><p><strong>Note：</strong> 在 seq2seq 中，特定的 <code>&lt;eos></code> 表示序列结束词元。 一旦输出序列生成此词元，模型就会停止预测。在设计中，通常有两个特别的设计决策：首先，每个输入序列开始时都会有一个特殊的序列开始标记（<code>&lt;bos></code>），它是解码器的输入序列的第一个词元；其次，使用循环神经网络编码器 <strong>最终的隐状态来初始化解码器的隐状态</strong>。</p></blockquote></div></details><details><summary><strong class=custom-details-title>⁉️ 为什么解码器在训练时需要强制教学（teacher forcing）？</strong></summary><div class=markdown-inner><h2><b>为什么解码器在训练时需要强制教学（teacher forcing）？</b></h2><p>强制教学（Teacher Forcing） 是一种用于 序列到序列（Seq2Seq） 任务的训练技巧，主要应用于 递归神经网络（RNN） 及其变体。在标准的 Seq2Seq 训练过程中，解码器（Decoder）需要根据之前的输出逐步预测下一个单词，而 强制教学（Teacher Forcing） 的核心思想是在训练时，<strong>不使用解码器自身的预测结果作为下一步的输入，而是直接使用真实的目标序列（Ground Truth）作为输入，从而减少误差的累积</strong>。</p><p>在这种方法中，解码器的 <strong>输入使用的是目标序列 (target sequence) 的原始标签</strong>。具体来说，解码器的输入由特殊的起始标记 <code>&lt;bos></code> 和目标序列（去掉最后一个标记）拼接而成，而解码器的输出（用于训练的标签）是原始目标序列 <strong>向右偏移一个标记</strong>。例如：</p><ul><li>输入: <code>&lt;bos></code>, “Ils”, “regardent”, “.”</li><li>输出: “Ils”, “regardent”, “.”, <code>&lt;eos></code></li></ul><p>解码器在训练时需要 强制教学（Teacher Forcing），主要是为了 <strong>加速收敛并稳定训练过程</strong>。在没有 强制教学（Teacher Forcing） 的情况下，如果解码器的某一步预测错误，那么错误的输出会被作为下一步的输入，这可能导致错误被进一步放大，从而使整个序列的预测质量下降。通过使用真实目标序列作为输入，解码器可以更快学习到正确的模式，并减少梯度传播中的误差累积问题。</p><p>相比之下，<strong>编码器（Encoder） 并不需要 强制教学（Teacher Forcing）</strong>，因为编码器的作用是将整个输入序列编码成一个固定长度的隐状态（Hidden State），然后传递给解码器。编码器的输入是完整的源语言序列，因此它的计算不涉及前一步的预测误差传播。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 在解码阶段如何进行 Greedy Search 和 Beam Search？两者的优缺点是什么？</strong></summary><div class=markdown-inner><h2><b>在解码阶段如何进行 Greedy Search 和 Beam Search？两者的优缺点是什么？</b></h2><p>Greedy Search 是一种简单但次优的解码方法，<strong>每一步都选择当前概率最高的词作为输出</strong>，而不考虑全局最优。例如，在翻译任务中，若模型预测 “I love deep learning” 时，Greedy Search 可能会选择最高概率的词 “love” 作为第二个词，而不会评估其他可能的组合。这种方法的优势是计算速度快、实现简单，<strong>但容易陷入局部最优（Local Optimum），导致整体生成结果质量不佳</strong>。</p><p>Beam Search 是 Greedy Search 的改进方法，<strong>它在解码过程中维护 K（Beam Width） 个最优候选序列</strong>，而不是仅选择概率最高的词。例如，若 K=3，模型会同时跟踪三个最可能的翻译路径，在每个时间步计算所有可能扩展的概率，并仅保留 K 个最高概率的候选路径。Beam Search 能有效避免局部最优，并提高序列生成质量。然而，<strong>它的计算复杂度较高</strong>，较大的 K 值会显著增加计算量。此外，Beam Search 仍然无法保证找到全局最优解，并且可能导致重复生成（Repetitive Generation）的问题。</p><p>在实际应用中，Greedy Search 适用于低计算资源的环境，如实时应用（Real-time Applications），而 Beam Search 在机器翻译（Machine Translation）、文本摘要（Text Summarization）等任务中更常见，以提高生成文本的连贯性和流畅性。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 如何使用 BLEU 进行预测序列的评估 ？</strong></summary><div class=markdown-inner><h2><b>如何使用 BLEU 进行预测序列的评估 ？</b></h2><p>在自然语言生成任务（Natural Language Generation, NLG）中，预测序列的评估通常使用自动化指标来衡量 <strong>生成文本与参考文本（Ground Truth）之间的相似度</strong>，其中 BLEU（Bilingual Evaluation Understudy） 是最常用的指标之一。BLEU 主要用于机器翻译（Machine Translation, MT）等任务，通过计算 n-gram 之间的匹配度来评估生成文本的质量。</p><p>BLEU 的核心思想是计算 预测文本（Hypothesis） 和 参考文本（Reference） 之间的 n-gram 精确匹配率（n-gram Precision），并结合 惩罚因子（Brevity Penalty, BP） 以防止模型生成过短的句子。其计算过程如下：</p><span>\[
\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},\]</span><ul><li><strong>n-gram 精确匹配（n-gram Precision）</strong>: 计算预测文本中的 1-gram, 2-gram, 3-gram, 4-gram 等短语，在参考文本中是否出现，并计算匹配比例。</li><li><strong>惩罚因子（Brevity Penalty, BP）</strong>: 当预测文本过短（即比参考文本短）时，BLEU 会施加惩罚，避免通过只生成短而匹配的文本来提高得分。</li></ul><p>BLEU 的优点是计算简单，适用于大规模评测，并且与人类评分有一定相关性。然而，它的缺点是 <strong>缺乏语义理解（Semantic Understanding），无法衡量文本的可读性和流畅性。</strong></p></div></details></div></details><hr><h2 id=llm-核心技术><strong>LLM 核心技术</strong>
<a class=anchor href=#llm-%e6%a0%b8%e5%bf%83%e6%8a%80%e6%9c%af>#</a></h2><hr><h3 id=llm-基础组件><strong>LLM 基础组件</strong>
<a class=anchor href=#llm-%e5%9f%ba%e7%a1%80%e7%bb%84%e4%bb%b6>#</a></h3><hr><details><summary><strong class=custom-details-title>⭐ Tokenization - 分词（BPE, WordPiece, SentencePiece）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ 什么是 Tokenization？为什么它对LLM至关重要？</strong></summary><div class=markdown-inner><h2><b>什么是 Tokenization？为什么它对LLM至关重要？</b></h2><p>Tokenization（分词）是将文本拆分成更小的单元（tokens）的过程，这些单元可以是单词、子词或字符。在自然语言处理中，分词是文本预处理的重要步骤。他直接影响模型对语义的理解能力、计算效率和内存占用。</p><ul><li><strong>Example</strong>：句子 “Hello, world!” 可被切分为 <code>["Hello", ",", "world", "!"]</code>（基于空格和标点）。</li></ul><blockquote class="book-hint warning"><p><strong>Note</strong>：简而言之，Tokenization 是一个 <strong>将输入文本拆解为 tokens（可能是词、子词或字符）的过程</strong>，而词表是一个 <strong>包含所有可能 tokens 的集合</strong>，它定义了 token 到数字 ID 的映射。Tokenization 使用词表来将文本转换为模型可以处理的数字序列。我们会使用不同的 Tokenization 方法构建词表，这个词表包含了词或子词的常见组合。当输入一个句子时，Tokenizer 会根据这个词表将输入的文本转换为 tokens，再传递给模型进行处理。</p></blockquote></div></details><details><summary><strong class=custom-details-title>⁉️ 常见的 Tokenization 方法有哪些？它们的区别是什么？</strong></summary><div class=markdown-inner><h2><b>常见的 Tokenization 方法有哪些？它们的区别是什么</b></h2><ul><li><strong>Word-level</strong>：按词切分（如 <code>“natural language processing” → ["natural", "language", "processing"]</code>），但词表大且难以处理未登录词（OOV）。</li><li><strong>Subword-level（主流方法）</strong>：<ul><li><strong>BPE（Byte-Pair Encoding）</strong>：通过合并高频字符对生成子词（如GPT系列使用）。</li><li><strong>WordPiece</strong>：类似BPE，但基于概率合并（如BERT使用）。</li><li><strong>SentencePiece</strong>：无需预分词，直接处理原始文本（如T5使用）。</li></ul></li><li><strong>Character-level</strong>：按字符切分，词表极小但序列长且语义建模困难。</li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ BPE算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2><b>BPE 算法的工作原理是什么？请举例说明。</b></h2><p>BPE（Byte Pair Encoding）是一种子词分割（Subword Segmentation）算法，核心思想是基于 <strong>统计频率</strong> 的合并（Merge frequent pairs）。</p><ul><li><strong>工作原理</strong>：<ol><li>统计字符对（Byte Pair）频率，找到最常见的相邻字符对。<pre tabindex=0><code>[&#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34; &#34;, &#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34;e&#34;, &#34;r&#34;, &#34; &#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;]
</code></pre></li><li>合并最频繁的字符对，形成新的子词单元。<pre tabindex=0><code>(&#34;l&#34;, &#34;o&#34;) -&gt; 2次
(&#34;o&#34;, &#34;w&#34;) -&gt; 2次
    ...
</code></pre>（第一次）合并 <code>("l", "o") → "lo"</code>：<pre tabindex=0><code>[&#34;lo&#34;, &#34;w&#34;, &#34;lo&#34;, &#34;w&#34;, &#34;er&#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;st&#34;]
</code></pre>（第二次）合并 <code>("lo", "w") → "low"</code><pre tabindex=0><code>[&#34;low&#34;, &#34;low&#34;, &#34;er&#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;st&#34;]
</code></pre></li><li>重复步骤 1 和 2，直到达到预定的子词词汇量。<pre tabindex=0><code>[&#34;low&#34;, &#34;lower&#34;, &#34;er&#34;, &#34;newest&#34;]
</code></pre><pre tabindex=0><code>vocab = {&#34;low&#34;: 100, &#34;lower&#34;: 101, &#34;er&#34;: 102, &#34;newest&#34;: 103}
</code></pre></li></ol></li></ul><p>BPE的优点是它 <strong>能够有效地处理未登录词</strong>，并且在处理长尾词（rare words）时表现良好。比如，词”unhappiness”可以被分解为”un” + “happiness”，而不是完全看作一个新的词。</p></div></details><details><summary><strong class=custom-details-title>⁉️ WordPiece 算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2><b>WordPiece 算法的工作原理是什么？请举例说明</b></h2><p>WordPiece 的核心理念与BPE相似，但合并策略更注重语义完整性。其训练过程同样从基础单元（如字符）开始，但选择合并的标准并非单纯依赖频率，而是通过 <strong>计算合并后对语言模型概率的提升幅度</strong>，优先保留能够增强语义连贯性的子词。</p><ul><li><p>假设语言模型的目标是最大化训练数据的似然概率，在 WordPiece 中，简化为 通过子词频率估计概率（即一元语言模型）。通过计算合并后对语言模型概率的提升幅度进行合并：
<span>[
\begin{equation}
P(S)≈ \prod^{n}_{i=1}P(s_i)
\end{equation}
]</span></p></li><li><p><strong>工作原理</strong>：</p><ol><li><p>与BPE类似，首先将所有词分解为最小的单位（如字符）。</p><pre tabindex=0><code>[&#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34; &#34;, &#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34;e&#34;, &#34;r&#34;, &#34; &#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;, &#34; &#34;, &#34;w&#34;, &#34;i&#34;, &#34;d&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;]
</code></pre></li><li><p>统计所有可能的字符对（或子词对）在文本中的共现频率。</p></li><li><p>合并字符对，选择合并后能 <strong>最大化语言模型似然概率</strong> 的字符对。具体公式为：选择使 <code>score = freq(pair) / (freq(first) * freq(second)) </code>最大的字符对（<strong>与 BPE 不同，BPE 仅选择频率最高的对</strong>）。每次合并对语言模型概率提升最大的合并组合。</p><p>这里的 <code>##</code> 表示这个 token 只能作为后缀出现，不会单独存在。</p><pre tabindex=0><code>{&#34;low&#34;, &#34;##er&#34;, &#34;##ing&#34;, &#34;new&#34;, &#34;##est&#34;, &#34;wide&#34;, &#34;##st&#34;}
</code></pre></li><li><p>重复合并得分最高的字符对，直到达到预设的词汇表大小。</p><pre tabindex=0><code>vocab = {&#34;low&#34;: 100, &#34;##er&#34;: 101, &#34;##ing&#34;: 102, &#34;new&#34;: 103, &#34;##est&#34;: 104, &#34;wide&#34;: 105, &#34;##st&#34;: 106}
</code></pre></li></ol></li></ul><p>WordPiece 通过最大化语言模型概率合并子词，<strong>生成的子词更贴合语义需求</strong>。但计算复杂度更高，需多次评估合并得分。</p><ul><li>若需模型捕捉深层语义（如预训练任务），优先选择 WordPiece。</li><li>若需快速处理大规模数据且词汇表灵活，BPE 更合适。</li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ SentencePiece 算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2><b>SentencePiece 算法的工作原理是什么？请举例说明</b></h2><p>SentencePiece 是一种无监督的子词分割算法，其核心创新在于直接处理原始文本（包括空格和特殊符号），无需依赖预分词步骤。<strong>它支持两种底层算法：BPE 或 基于概率的Unigram Language Model</strong>。训练时，SentencePiece 将空格视为普通字符 <code>_</code>，可 <strong>直接处理多语言混合文本（如中英文混杂）</strong>，并自动学习跨语言的统一子词划分规则。</p><ul><li><strong>Example：</strong> <code>"Hello世界" → 编码为 ["▁He", "llo", "▁世", "界"]。</code></li></ul><p>SentencePiece 支持多语言（Multilingual）无需调整，统一处理空格与特殊符号。这一特性使其在需要多语言支持的场景（如T5模型）中表现突出，同时简化了数据处理流程，特别适合处理社交媒体文本等非规范化输入。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 如何处理未登录词（OOV）？</strong></summary><div class=markdown-inner><h2><b>如何处理未登录词（OOV）？</b></h2><ul><li><strong>子词切分</strong>：将OOV（Out-of-Vocabulary）词拆分为已知子词（如 <code>“tokenization” → ["token", "ization"]</code>）。</li><li><strong>回退策略</strong>：使用特殊标记（如 <code>[UNK]</code>），但会损失信息。</li><li><strong>动态更新词表</strong>：在增量训练时扩展词表。</li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ Tokenization 如何影响模型性能？</strong></summary><div class=markdown-inner><h2><b>Tokenization 如何影响模型性能？</b></h2><ul><li><strong>词表过大</strong>：增加内存消耗，降低计算效率（Softmax 计算成本高）。</li><li><strong>词表过小</strong>：导致长序列和语义碎片化（如切分为无意义的子词）。</li><li><strong>语言适配性</strong>：中/日/韩等非空格语言需要特殊处理（如 SentencePiece）。</li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ 如何为多语言模型设计Tokenization方案？</strong></summary><div class=markdown-inner><h2><b>如何为多语言模型设计 Tokenization 方案？</b></h2><ul><li><strong>统一词表</strong>：使用 SentencePiece 跨语言训练（如mBERT）。</li><li><strong>平衡语种覆盖</strong>：根据语种数据量调整合并规则，避免小语种被淹没。</li><li><strong>特殊标记</strong>：添加语言ID（如 <code>[EN]</code>、<code>[ZH]</code>）引导模型区分语言。</li></ul></div></details></div></details><details><summary><strong class=custom-details-title>⭐ Word Embeddings - 词嵌入（Word2Vec、GloVe、FastText）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ 什么是词嵌入（Word Embeddings）？为什么它重要？</strong></summary><div class=markdown-inner><h2><b>什么是词嵌入（Word Embeddings）？为什么它重要？</b></h2><p>在自然语言处理中，Embedding（词嵌入）是将 <strong>离散的文本数据（如单词、短语或句子）映射到一个连续的、低维度的向量空间（vector space）的过程</strong>。他的作用包括：</p><ol><li><strong>捕捉语义关系（Semantic Relationships）</strong>：能表示同义词、类比关系（如 king - man + woman ≈ queen）。</li><li><strong>降维（Dimensionality Reduction）</strong>：将高维的 <strong>独热编码（One-hot Encoding）</strong> 转换为低维密集向量，提高计算效率。</li><li><strong>解决稀疏性问题（Handling Sparsity）</strong>：相比于独热编码，词嵌入具有更好的泛化能力（Generalization）。</li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ 静态词向量 和 上下文动态词向量的区别？</strong></summary><div class=markdown-inner><h2><b>静态词向量 和 上下文动态词向量的区别？</b></h2><ul><li><strong>静态词向量（Static Word Embeddings）</strong> 的核心特点是 <strong>无论词语出现在何种上下文中，其向量表示均保持不变</strong>。这类方法通过大规模语料训练，捕捉词语间的语义和语法关系。静态词向量的优势在于训练高效、资源消耗低，且生成的向量可直观反映语义相似性（如“猫”和“狗”向量接近）；但其 <strong>局限性是无法处理多义词</strong>（如“苹果”在“水果”和“手机”场景中的不同含义），因为 <strong>每个词仅对应单一向量</strong>。代表模型包括：Word2Vec, GloVe。</li><li><strong>上下文动态词向量（Contextual Word Embeddings）</strong>：静态嵌入虽然可以表示词语的语义，但它们无法根据上下文动态调整，例如 “bank” 在 “river bank” 和 “bank account” 里的含义不同。而 动态词嵌入 解决了这个问题，代表性模型包括 ELMo、BERT 和 GPT。</li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ 解释 Word2Vec 的两种模型：Skip-gram 和 CBOW。</strong></summary><div class=markdown-inner><h2><b>解释 Word2Vec 的两种模型：Skip-gram 和 CBOW。</b></h2><ul><li><p><strong>跳元模型（Skip-gram）</strong> 假设 一个词可以用来在文本序列中生成其周围的单词。以文本序列 <code>“the”“man”“loves”“his”“son”</code> 为例。假设中心词选择 <code>“loves”</code>，并将上下文窗口设置为2，给定中心词 <code>“loves”</code>，跳元模型考虑生成上下文词 <code>“the”“man”“him”“son”</code> 的条件概率。最大化给定中心词时上下文词的条件概率：
$$
max{\sum log P(context_w|center_w)}
$$
Word2Vec 的核心是 <strong>一个浅层神经网络（Shallow Neural Network）</strong>，由一个输入层、一个隐藏层（线性变换层）、一个输出层（Softmax 或其他采样方法）组成:</p><ol><li><strong>输入示例</strong>：<ul><li>句子：<code>“I love natural language processing.”</code></li><li>若窗口大小为1，中心词为 <code>“natural”</code>，则上下文词为 <code>“love”</code> 和 <code>“language”</code>。</li></ul></li><li><strong>输入通过 One-Hot 编码 表示为一个稀疏向量</strong>。例如，若词汇表为 <code>["cat", "dog", "fish"]</code>，则<code>“dog”</code> 的输入编码为 <code>[0, 1, 0]</code>。</li><li><strong>输入层到隐藏层</strong>：输入向量与 输入权重矩阵相乘，得到中心词的嵌入向量。</li><li>通过 <strong>输出权重矩阵将隐层向量映射到输出概率</strong>：</li><li>使用 Softmax 归一化，通过梯度下降优化词向量，使得上下文词的概率最大化。</li></ol><p><strong>跳元模型（Skip-gram）特点</strong>：</p><ul><li><strong>擅长捕捉低频词</strong>：通过中心词预测多个上下文，低频词有更多训练机会。</li><li><strong>训练速度较慢</strong>：输出层需计算多个上下文词的概率。</li></ul></li></ul><hr><ul><li><p><strong>连续词袋（CBOW）</strong> 与 Skip-gram 相对，CBOW 的训练过程是 <strong>给定上下文词，预测中心词</strong>。这里的目标是将多个上下文词的向量平均起来，并通过它们来预测中心词。在文本序列 <code>“the”“man”“loves”“his”“son”</code> 中，在 <code>“loves”</code> 为中心词且上下文窗口为 2 的情况下，连续词袋模型考虑基于上下文词 <code>“the”“man”“him”“son”</code> 生成中心词 <code>“loves”</code> 的条件概率。最大化给定上下文时中心词的条件概率：
$$
max{\sum log P(center_w|context_w)}
$$
连续词袋（CBOW）的训练细节与 跳元模型（Skip-Gram）大部分类似，但是在输入 One-Hot 编码表示时，跳元模型（Skip-Gram）将中心词进行 One-Hot 编码，<strong>而连续词袋（CBOW）将上下文词进行 One-Hot 编码并取平均值</strong>。 例如，中心词为 <code>“natural”</code>，上下文词为 <code>“love”</code> 和 <code>“language”</code> 。输入为 <code>[0, 1, 0, 0, 0]</code>（<code>“love”</code>）和 <code>[0, 0, 0, 1, 0]</code>（<code>“language”</code>）的平均向量 <code>[0, 0.5, 0, 0.5, 0]</code>。此外，输出概率通过 Softmax 计算公式也有不同。</p><p><strong>连续词袋（CBOW）特点</strong>：</p><ul><li><strong>训练速度快</strong>：输入为多个词的均值向量，计算效率高。</li><li><strong>对高频词建模更好</strong>：上下文词共同贡献中心词预测。</li></ul></li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ Word2Vec 如何优化训练效率？</strong></summary><div class=markdown-inner><h2><b>Word2Vec 如何优化训练效率？</b></h2><p>由于 softmax 操作的性质，上下文词可以是词表中的任意项，但是，在一个词典上（通常有几十万或数百万个单词）<strong>求和的梯度的计算成本是巨大的</strong>！为了降低计算复杂度，可以采用两种近似训练方法：负采样（Negative Sampling）和层序softmax（Hierarchical Softmax）。</p><ul><li><p><strong>负采样（Negative Sampling）</strong>：</p><ul><li><strong>核心思想</strong>：将复杂的多分类问题（预测所有词的概率）简化为二分类问题，用少量负样本近似全词汇的Softmax计算。</li><li><strong>正负样本构建</strong>：<ul><li>对每个正样本（中心词与真实上下文词对），随机采样 K 个负样本（非上下文词）。</li><li>例如，中心词 <code>“apple”</code> 的真实上下文词为 <code>“fruit”</code>，则负样本可能是随机选择的 <code>“car”</code>,<code>“book”</code> 等无关词。</li></ul></li><li><strong>目标函数</strong>：最大化正样本对的相似度，同时最小化负样本对的相似度。</li></ul></li><li><p><strong>层序softmax（Hierarchical Softmax）</strong>：</p><ul><li><strong>核心思想</strong>：通过二叉树（如霍夫曼树）编码词汇表，将全局Softmax分解为路径上的二分类概率乘积，减少计算量。</li><li><strong>霍夫曼树构建</strong>：按词频从高到低排列词汇，高频词靠近根节点，形成最短路径。每个内部节点含一个可训练的向量参数。</li></ul></li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ 解释 GloVe 的原理？</strong></summary><div class=markdown-inner><h2><b>解释 GloVe 的原理？</b></h2><p>GloVe（Global Vectors for Word Representation）是一种基于全局统计信息的词向量训练方法，它通过构造整个语料库的共现矩阵（co-occurrence matrix）并进行矩阵分解来学习词的向量表示，其核心思想是 <strong>通过捕捉词与词之间的全局共现关系来学习语义信息</strong>，而不是像 Word2Vec 那样依赖于局部上下文窗口的预测方法。
具体而言，GloVe 首先统计语料库中的词对共现次数，构建一个共现矩阵 X ，其中 X_{ij} 表示词 i 在词 j 附近出现的频率，并计算共现概率:</p><span>\[
P(j|i) = \frac{X_{ij}}{\sum_k X_{ik}}
\]</span><p>GloVe 的核心目标是学习一个词向量映射，使得向量之间的点积能够近似这个共现概率的对数：</p><span>\[
\mathbf{u}_j^\top \mathbf{v}_i + b_i + c_j = \log(X_{ij})
\]</span><ul><li>v_i, u_j 是词 i 和词 j 的向量表示，<strong>每个词都由两个向量组成，一个是中心词向量 ，一个是上下文词向量</strong></li></ul><p>GloVe 并不依赖传统的神经网络，它的学习过程 <strong>更接近矩阵分解（Matrix Factorization）的优化方法</strong>，而非 Word2Vec 这样的前馈神经网络（Feedforward Neural Network）。GloVe <strong>不依赖反向传播（Backpropagation）</strong>，而是直接最小化共现概率对数的加权平方误差，来学习词向量。</p><p>GloVe 的优势在于它直接建模了全局的词共现信息，使得词向量能够更好地捕捉语义相似性和类比关系，如 <code>“king - man + woman ≈ queen”</code>，但由于依赖于整个共现矩阵，计算和存储成本较高，因此适用于大规模离线训练，而不是在线学习任务。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 解释 FastText 的原理？</strong></summary><div class=markdown-inner><h2><b>解释 FastText 的原理？</b></h2><p>FastText 在 Word2Vec 的基础上进行了改进，能够更好地处理 OOV（Out-of-Vocabulary）问题，同时提高计算效率。FastText 的核心思想是 <strong>使用 n-gram 字符级子词（subword） 进行单词表示</strong>，而不是仅仅依赖于整个单词的词向量。它的训练过程类似于 Word2Vec 的 CBOW（Continuous Bag of Words）或 Skip-gram 模型，但 FastText 通过将一个单词拆分成多个 n-gram 片段（如 <code>“apple”</code> 可以被拆分为 <code>&lt;ap, app, ppl, ple, le></code>），然后通过这些 n-gram 子词的向量求和来表示整个单词，从而在处理未见单词时依然可以通过其子词获得较好的表示。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 在实际项目中，如何选择不同的嵌入方法（Embedding Methods）？</strong></summary><div class=markdown-inner><h2><b>在实际项目中，如何选择不同的嵌入方法（Embedding Methods）？</b></h2><ol><li><strong>任务类型与语义需求</strong>：</li></ol><ul><li><strong>基础语义任务（如文本分类、简单相似度计算）</strong>：<ul><li>静态嵌入：Word2Vec、GloVe、FastText。<ul><li>优点：轻量高效，适合低资源场景。</li><li>示例：新闻分类任务中，预训练的 Word2Vec 向量足以捕捉主题关键词的语义。</li></ul></li></ul></li><li><strong>复杂语义任务（如问答、指代消解、多义词理解）</strong>：<ul><li>上下文嵌入：BERT、RoBERTa、XLNet。<ul><li>优点：动态生成上下文相关向量，解决一词多义。</li><li>示例：在医疗问答系统中，BERT可区分“Apple”指公司还是水果。</li></ul></li></ul></li></ul><ol start=2><li><strong>数据量与领域适配</strong>：</li></ol><ul><li><strong>小数据场景</strong>：<ul><li><strong>预训练静态嵌入 + 微调</strong>：使用公开预训练的 Word2Vec/GloVe，结合任务数据微调。</li><li><strong>轻量上下文模型</strong>：ALBERT或TinyBERT，降低训练成本。</li></ul></li><li><strong>大数据场景</strong>：<ul><li><strong>从头训练上下文模型</strong>：基于领域数据训练BERT或GPT，捕捉领域专属语义。</li><li><strong>领域适配</strong>：在金融/法律等领域，使用领域语料继续预训练（Domain-Adaptive Pretraining）。</li></ul></li></ul></div></details></div></details><details><summary><strong class=custom-details-title>⭐ Attention - 注意力机制（QKV，Self-Attention、多头注意力）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ 什么是注意力机制（Attention Mechanism）？其中的Q，K，V都代表什么？</strong></summary><div class=markdown-inner><h2><b>什么是注意力机制（Attention Mechanism）？其中的Q，K，V都代表什么？</b></h2><p><strong>注意力机制（Attention Mechanism）</strong> 的核心思想是将 <strong>输入看作键-值对的数据库</strong>，并 <strong>基于查询计算注意力权重 (attention weights)</strong>，可以动态地选择哪些输入部分（例如词语或特征）最为重要。注意力机制定义如下：</p><span>\[
\textrm{Attention}(\mathbf{q}, \mathcal{D}) \stackrel{\textrm{def}}{=} \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i,
\]</span><ul><li>这里的 q，<strong>查询（Query）</strong> 是当前模型试图“寻找”或“关注”的目标。它是一个向量，代表了你想匹配的信息。</li><li>公式中的 k，<strong>键（key）</strong> 是所有潜在匹配目标的特征表示。每个键对应于输入中的一个元素，表示这个元素的特性或身份。</li><li>其中的 v，<strong>值（value）</strong> 是和键一起存储的信息，也是最终被提取的信息。注意力机制的目标是通过查询和键找到最相关的值。</li></ul><blockquote class="book-hint warning"><p>注意力机制的一般步骤为：</p><ol><li><strong>对查询和每个键计算相似度</strong>。</li><li>对这些相似度进行归一化（通常使用 Softmax 函数）。归一化后的结果称为注意力权重（Attention Weights）。</li><li>将注意力权重与对应的值相乘，得到一个加权求和结果。这个结果就是当前查询的输出。</li></ol></blockquote><p>在 transformer 中，Query (Q)、Key (K) 和 Value (V) 都是从输入嵌入（embedding）中线性变换得到的。它们的计算方式如下：</p><span>\[
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
\]</span><p><strong>得到 Q，K，V 的过程 相当于经历了一次线性变换</strong>。Attention不直接使用 X 而是使用经过矩阵乘法生成的这三个矩阵，因为使用三个可训练的参数矩阵，可增强模型的拟合能力。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是自注意力（Self-Attention）机制？它与传统注意力有何区别？</strong></summary><div class=markdown-inner><h2><b>什么是自注意力（Self-Attention）机制？它与传统注意力有何区别？</b></h2><p>Self-Attention（自注意力） 和 一般 Attention（注意力机制） 的 <strong>核心计算原理是相同的</strong>，都是通过 Query（Q） 和 Key（K） 计算相似度分数，再对 Value（V） 进行加权求和。但它们的区别在于作用目标不同：</p><ul><li><strong>Self-Attention（自注意力）</strong><ul><li><strong>Q、K、V 都来自同一个输入序列 X ，即 自身内部计算注意力</strong>，挖掘序列中不同位置之间的关系。例如，在 Transformer 的 Encoder 里，每个单词都和句子中的所有单词计算注意力。</li></ul></li><li><strong>General Attention（通用注意力，通常用于 Seq2Seq 结构）</strong><ul><li><strong>Q 和 K、V 来自不同的地方</strong>，通常是 Q 来自 Decoder，而 K、V 来自 Encoder，用于建立 Encoder 和 Decoder 之间的联系。例如，在机器翻译中，Decoder 生成当前词时，会对 Encoder 编码的所有词计算注意力，从而获取最相关的信息。</li></ul></li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是缩放点积注意力（Scaled Dot-Product Attention）？为什么要除以 √d？</strong></summary><div class=markdown-inner><h2><b>什么是缩放点积注意力（Scaled Dot-Product Attention）？为什么要除以 √d？</b></h2><p>缩放点积注意力（Scaled Dot-Product Attention） 是 Transformer 结构中的核心机制之一，它用于计算查询（Query）、键（Key）和值（Value）之间的注意力分数，以捕捉序列中不同位置的关联性。在计算过程中，首先对查询矩阵 Q 和键矩阵 K 进行点积（Dot Product），得到注意力得分（Attention Scores）。<strong>这个点积运算的本质是衡量 查询向量（Query） 和 键向量（Key） 之间的相似度。</strong> 之后，<strong>Softmax 作用于Q，K计算出的相似度得分，以将其转换为概率分布</strong>，使其满足：</p><ul><li><strong>归一化（Normalization）</strong>：确保所有注意力权重总和为 1，便于解释。</li><li>放大差异（Sharpening）：通过指数运算增强高相关性词的权重，抑制低相关性词。</li></ul><p>然而，点积的结果可能会随着 <strong>d_k（Key 维度的大小）增加而变大</strong>。将点积作为输入传递给 Softmax 函数时，Softmax 对大数值特别敏感，因为它会根据输入的相对大小来计算概率值。如果点积值变得非常大，<strong>Softmax 会让其中一些值的输出接近 1，而其他值接近 0</strong>，这会 导致计算不稳定或梯度消失等问题。</p><p>因此，在应用 Softmax 之前，需要对注意力得分进行缩放，即除以 √d_k，这样可以防止梯度消失或梯度爆炸问题，提高训练稳定性。<strong>这个缩放的主要目的是 将点积的方差控制在一个合理的范围，使其不随向量维度的增加而变得过大。</strong> 数学公式如下：</p><span>\[
\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}.
\]</span></div></details><details><summary><strong class=custom-details-title>⁉️ 计算自注意力机制的时间和空间复杂度，分析其瓶颈。</strong></summary><div class=markdown-inner><h2><b>计算自注意力机制的时间和空间复杂度，分析其瓶颈。</b></h2><p>自注意力机制（Self-Attention Mechanism）的时间复杂度（Time Complexity）和空间复杂度（Space Complexity）主要受输入序列长度 n 影响。在标准的 Transformer 结构中，每个 Self-Attention Layer 计算 注意力权重（Attention Weights） 需要进行矩阵乘法，计算 Query Q 和 Key K 之间的点积并进行 Softmax 归一化。</p><p>其中， Q 和 K 的维度均为 (n x d_k) ，计算 QK^T 需要 O(n^2 d_k) 次乘法运算，而应用 Softmax 需要 O(n^2) 的额外计算，因此 <strong>整体时间复杂度为</strong>：</p><span>\[
O(n^2 d_k)
\]</span><p>Self-Attention 计算过程中，需要存储 <strong>注意力权重矩阵（ n x n ），此外还需要存储 中间结果（如 Softmax 输出、梯度）</strong>，使得 <strong>空间复杂度达到</strong>：</p><span>\[
O(n^2 + n d_k)
\]</span><ul><li><strong>瓶颈分析（Bottleneck Analysis）</strong><ol><li><strong>计算瓶颈（Computational Bottleneck）</strong>：由于 Self-Attention 需要 O(n^2 d_k) 的计算量，因此在超长文本（如 10K 以上 Token）上，计算成本极高，推理速度变慢。</li><li><strong>内存瓶颈（Memory Bottleneck）</strong>：存储 O(n^2) 的注意力权重矩阵会 占用大量显存（VRAM），限制了可处理的最大序列长度。</li><li><strong>长序列扩展性差（Scalability for Long Sequences）</strong>：当 n 增大时，Transformer 计算复杂度随 n^2 级增长，难以应用于长文本建模。</li></ol></li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ 为什么需要多头注意力（Multi-Head Attention）？多头设计如何提升模型表达能力？</strong></summary><div class=markdown-inner><h2><b>为什么需要多头注意力（Multi-Head Attention）？多头设计如何提升模型表达能力？</b></h2><p>多头注意力（Multi-Head Attention）是 Transformer 结构中的关键组件，它通过多个独立的注意力头来提升模型的表达能力。其核心思想是 <strong>让模型在不同的子空间（Subspaces）中独立学习不同的特征表示，而不是仅依赖单一注意力机制</strong>。例如可以关注不同类型的关系（如语法关系、语义关系、长距离依赖等）。</p><div align=center><img src=/images/multi-head-attention.svg width=450px/></div><p>在计算过程中，输入序列的特征矩阵首先经过线性变换，生成查询（Query, Q）、键（Key, K）、和值（Value, V）。然后，<strong>每个注意力头都会独立地对 Q、K、V 进行投影</strong>，将其拆分成多个低维子空间，即：</p><span>\[
\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v},
\]</span><p>其中 W_i^q, W_i^k, W_i^v 是可训练的投影矩阵，每个头都对应一组独立的参数。随后，每个头分别执行 Scaled Dot-Product Attention（缩放点积注意力）。计算完成后，各个头的注意力输出会被拼接（Concatenation），然后通过一个最终的线性变换矩阵 W^o 进行映射：</p><span>\[
\begin{split}\mathbf W_o \begin{bmatrix}\mathbf h_1\\\vdots\\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}.\end{split}
\]</span><p>这样，多头注意力的最终输出仍然保持与输入相同的维度，同时融合了来自多个注意力头的信息，提高了模型对不同层次语义的建模能力。</p></div></details></div></details><details><summary><strong class=custom-details-title>⭐ Positional Encoding - 位置编码（绝对位置，相对位置，RoPE）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ 为什么 Transformer 需要位置编码？纯 Self-Attention 为何无法感知位置信息？</strong></summary><div class=markdown-inner><h2><b>为什么 Transformer 需要位置编码？纯 Self-Attention 为何无法感知位置信息？</b></h2><p>在 Transformer 模型中，位置编码（Position Encoding）是用于注入位置信息的关键机制，因为模型本身的 Self-Attention 机制无法感知输入序列中元素的顺序或位置。Transformer 通过 Self-Attention 计算序列中各元素之间的关系，每个元素的表示（representation）由其与其他所有元素的相互作用决定。然而， <strong>Self-Attention 本身是位置无关的（position-independent）</strong>，即它并不考虑元素在序列中的相对或绝对位置。因此，如果不显式地引入位置编码，<strong>模型就无法了解输入序列的顺序信息</strong>。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 绝对位置编码（Absolute PE）和相对位置编码（Relative PE）的核心区别是什么？</strong></summary><div class=markdown-inner><h2><b>绝对位置编码（Absolute PE）和相对位置编码（Relative PE）的核心区别是什么？</b></h2><p>绝对位置编码（Absolute Position Encoding, Absolute PE）和相对位置编码（Relative Position Encoding, Relative PE）的核心区别在于它们对序列中单词位置的表示方式。<strong>绝对位置编码是基于序列中单词的固定位置来定义每个单词的位置编码</strong>，这些编码是通过对每个位置进行显式编码（例如使用正弦和余弦函数）来获得的。这意味着 <strong>每个位置的编码是固定的，与其他词汇之间的相对关系无关</strong>。简单来说，<strong>绝对位置编码的设计是通过为每个位置分配唯一的标识符来捕捉顺序信息</strong>。绝对位置编码被广泛用于 Transformer 模型中，如原始的 Transformer 和 BERT，这些模型通过对输入的词汇序列和其位置编码的加和来保留词汇的顺序信息。</p><p>相对位置编码则是通过 <strong>考虑单词之间的相对位置来计算每个单词的编码，而不是单纯地依赖于其绝对位置</strong>。在这种方法中，<strong>位置编码的更新基于词语之间的相对距离，因此它能捕捉到不同词之间的相对关系</strong>，而不仅仅是它们在序列中的固定位置。相对位置编码的一个例子是 Transformer-XL 模型，它通过引入相对位置编码来克服标准 Transformer 在处理长序列时存在的记忆限制问题，从而提升了对长距离依赖的建模能力。</p><p>尽管在某些情况下，相对位置编码可以通过绝对位置得到（例如，简单地计算位置差），但这种方法仍然有限。<strong>相对位置编码有以下优势</strong>：</p><ol><li><strong>灵活性和泛化性</strong>：相对位置编码使得模型能够处理不同长度的输入，而绝对位置编码依赖于固定的输入长度。这意味着在不同任务或不同数据集上，使用相对位置编码的模型能够更好地进行泛化，尤其是在处理较长序列时。</li><li><strong>更好的长距离依赖建模</strong>：相对位置编码能够更有效地捕捉长距离的依赖关系，因为它直接反映了词汇间的相对关系，而绝对位置编码则对远距离的依赖建模较弱，尤其是在长序列的上下文中。</li><li><strong>减少位置编码的冗余</strong>：在传统的绝对位置编码中，序列中的每个位置都有唯一的编码，且这些编码是全局固定的，而相对位置编码只关心词汇间的相对位置，从而避免了位置编码的冗余，尤其是在处理非常长的序列时。</li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ Sinusoidal 位置编码的公式是什么？为什么选择正弦/余弦函数组合？</strong></summary><div class=markdown-inner><h2><b>Sinusoidal 位置编码的公式是什么？为什么选择正弦/余弦函数组合？</b></h2><p>Sinusoidal 位置编码（Sinusoidal Positional Encoding） 是 Transformer 模型中用于捕捉序列中单词位置的一种方法，是常见的绝对位置编码（Absolute Position Encoding）方法。Sinusoidal 位置编码通过正弦和余弦函数的组合来生成每个位置的唯一向量，这些向量与输入的词嵌入（Word Embedding）相加，从而使模型能够学习到每个单词在序列中的位置。Sinusoidal 位置编码使用相同形状的位置嵌入矩阵 P 输出 X+P，其元素按以下公式生成：</p><span>\[
\begin{split}\begin{aligned} p_{i, 2j} &= \sin\left(\frac{i}{10000^{2j/d}}\right),\\p_{i, 2j+1} &= \cos\left(\frac{i}{10000^{2j/d}}\right).\end{aligned}\end{split}
\]</span><ul><li><strong>为什么选择正弦/余弦函数组合？</strong><ol><li><strong>不同频率的周期性</strong>：正弦和余弦函数有不同的频率，使得每个位置的编码在不同维度上具有不同的周期。这种周期性使得模型可以通过不同频率的变化来学习相对位置关系。通过正弦和余弦函数的组合，位置编码能够覆盖较长序列的不同范围，模型可以捕捉到全局和局部的位置信息。</li><li><strong>无重复的唯一表示</strong>：正弦和余弦函数的组合能够确保每个位置有一个独特的编码，这些编码在向量空间中是可区分的，能够提供丰富的位置信息。而且由于这两种函数的周期性和无穷制性质，不同位置的编码不会重复。</li><li><strong>容易计算和扩展</strong>：正弦和余弦函数的计算非常简单且高效。它们无需额外的学习参数，且可以通过简单的公式根据位置直接计算得出。这样的位置编码方式能够在大规模数据中有效应用，同时支持较长序列的处理。</li><li><strong>支持相对位置关系</strong>：这种编码方法能够通过比较不同位置的编码来推测它们之间的相对距离和顺序，尤其是在模型学习到的位置编码与实际任务（如机器翻译、文本生成）相关时，正弦/余弦函数的变化有助于保持序列的结构和信息流动。</li></ol></li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ 为什么位置编码可以直接与词向量逐元素相加？位置编码会破坏词向量的语义空间吗？</strong></summary><div class=markdown-inner><h2><b>为什么位置编码可以直接与word embedding逐元素相加？位置编码会破坏词向量的语义空间吗？</b></h2><p>Word embedding 主要表示单词的语义信息。Positional Encoding 提供额外的位置信息，以弥补 Transformer 结构中缺少序列顺序感的问题。相加的效果 是让同一个词（如 “apple”）在不同的位置有略微不同的表示，但仍保留其主要的语义信息。</p><p>虽然位置嵌入矩阵 P 与词向量 X 直接相加，但在 transformer 获得 Query (Q)、Key (K) 和 Value (V)的线形变化过程中（i.e. Q = XW_q），在学习 Weight 的过程中会将语义和位置信息分别投射在不同的维度上。Positional Encoding 并不需要通过训练来学习，<strong>它是固定的、基于位置的函数，因此不干扰原本的语义信息</strong>。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是可学习的位置编码（Learned PE）？</strong></summary><div class=markdown-inner><h2><b>什么是可学习的位置编码（Learned PE）？</b></h2><p>可学习的位置编码（Learned Positional Encoding, Learned PE）是一种在模型训练过程中通过优化学习得到的位置编码方法。与传统的 Sinusoidal Positional Encoding（正弦波位置编码）不同，Learned PE 不使用固定的数学公式来表示位置，<strong>而是通过神经网络的训练自动学习每个位置的编码表示</strong>。通常，这些位置编码是通过与输入的 词嵌入（Word Embedding） 相加来为模型提供位置信息，从而使模型能够捕捉到输入序列中各个元素的顺序。具体来说，Learned Positional Encoding 是 <strong>通过一个嵌入层来生成的</strong>。这个过程如下：</p><ol><li><strong>位置嵌入（Position Embedding）</strong>：每个位置（序列中的每个元素）都会被映射到一个可学习的向量。对于输入序列中的每个位置 i，我们为其分配一个嵌入向量 P_i ，这个向量是通过一个嵌入层学习得到的。</li><li><strong>添加到词嵌入（Word Embedding）</strong>：这些位置嵌入向量会与对应的词嵌入（Word Embedding）向量相加。假设某个词 w_i 在序列中的位置为 i，那么该词的最终输入向量为：</li></ol><span>\[
\mathbf{X}_i = \mathbf{X}_i + \mathbf{P}_i
\]</span><p>由于每个位置的编码表示是一个可训练的向量，它会在训练过程中和词嵌入（Word Embedding）一起作为输入传递到模型中。然后，<strong>模型通过反向传播算法更新这些可训练的参数</strong>，以便它们能够更好地捕捉到任务相关的位置信息。</p><p>可学习的位置编码的优点主要体现在 <strong>灵活性</strong>，由于位置编码是通过训练学习的，因此它可以在不同的任务和数据集上找到最优的表示，而不依赖于固定的模式（如正弦波的频率和相位）。这种灵活性使得它能够更好地适应各种复杂的数据模式和任务需求。</p><p>但它 <strong>需要更多的参数</strong>，Learned PE 需要为每个位置学习一个独立的参数，这使得模型的参数量增加，尤其是在处理长序列时，这可能会导致显著的计算和存储开销。同时也会有 <strong>过拟合风险</strong>：由于 Learnable PE 是基于数据学习的，它可能会过度拟合训练数据中的位置模式，尤其是在数据量较少的情况下，从而影响模型的泛化能力。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 写出 RoPE（Rotary Position Embedding）的数学公式，并解释其如何融合相对位置信息。</strong></summary><div class=markdown-inner><h2><b>写出 RoPE（Rotary Position Embedding）的数学公式，并解释其如何融合相对位置信息。</b></h2></div></details><details><summary><strong class=custom-details-title>⁉️ 为什么相对位置编码在长文本任务（如文本生成）中表现更好？</strong></summary><div class=markdown-inner><h2><b>为什么相对位置编码在长文本任务（如文本生成）中表现更好？</b></h2></div></details><details><summary><strong class=custom-details-title>⁉️ 长度外推（Length Extrapolation）问题的本质是什么？哪些位置编码方法能缓解该问题？</strong></summary><div class=markdown-inner><h2><b>长度外推（Length Extrapolation）问题的本质是什么？哪些位置编码方法能缓解该问题？</b></h2></div></details></div></details><details><summary><strong class=custom-details-title>⭐ Transformer 模型架构细节（FFN，Layer Norm，激活函数）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ 解释Transformer 模型架构细节？</strong></summary><div class=markdown-inner><h2><b>解释Transformer 模型架构细节？</b></h2><p>Transformer模型是一种 Encoder-Decoder 架构。Transformer的输入（源序列）和输出（目标序列）在送入编码器和解码器之前，会与 位置编码（positional encoding）相加。这种结构的编码器和解码器都基于 <strong>自注意力机制（self-attention）</strong>，并通过堆叠多个模块来实现。</p><div align=center><img src=/images/transformer.svg width=400px/></div><p>具体来说，Transformer 的编码器（Encoder）由多个相同的层堆叠而成，每一层包含两个子层：第一个是 多头自注意力（multi-head self-attention），第二个是 逐位置的前馈网络（positionwise feed-forward network）。在编码器的自注意力机制中，查询（queries）、键（keys）和值（values）都来自前一层的输出。每个子层都使用 残差连接（residual connection） 设计，并在其后进行 层归一化（layer normalization），确保模型的训练更稳定。最终，编码器为输入序列的每个位置输出一个 d-维向量表示。</p><p>Transformer的解码器与编码器类似，也是由多个相同的层组成，包含残差连接和层归一化。除了与编码器相同的两个子层外，解码器还加入了一个额外的子层，称为 <strong>编码器-解码器注意力（encoder-decoder attention）</strong>。在这个子层中，<strong>查询来自解码器自注意力子层的输出，而键和值来自编码器的输出</strong>。解码器中的自注意力机制中，查询、键和值都来自前一层的输出，但每个位置只能关注解码器中当前位置之前的所有位置，从而保留了自回归（autoregressive）特性，确保 预测仅依赖于已生成的输出标记。</p></div></details><details><summary><strong class=custom-details-title>⁉️ Transformer 中 Encoder 的理解？为什么 堆叠多个 TransformerEncoderBlock？</strong></summary><div class=markdown-inner><h2><b>Transformer 中 Encoder 的理解？为什么 堆叠多个 TransformerEncoderBlock？</b></h2><p><strong>Encoder 部分更关注于语言本身，Attention 找出词与词之间的全局关系（Q，K，V 均来自于自身），Positional FFN 深化词本身的含义，最终把所以词的理解综合到一个 matrix 当中。</strong></p><p>堆叠多个 TransformerEncoderBlock <strong>主要原因有</strong>：</p><ol><li><p>每个 <strong>TransformerEncoderBlock 都可以看作是一个特征提取器</strong>。通过堆叠多个 Block，模型能够从输入数据中提取出多层次的特征。随着层数的增加，模型能够捕捉更复杂的语义关系和全局依赖：</p><ul><li>浅层特征：语法、局部依赖。</li><li>中层特征：句法结构、短距离语义。</li><li>深层特征：全局语义、长距离依赖、抽象概念。</li></ul></li><li><p>每个 TransformerEncoderBlock 都包含一个自注意力机制和一个前馈神经网络（FFN），这些模块引入了非线性变换。通过堆叠多个 Block，模型可以<strong>逐步组合这些非线性变换，从而学习到更复杂的函数映射</strong>。深度模型（更多层）通常具有更强的表达能力，能够拟合更复杂的模式。</p></li><li><p>虽然自注意力机制理论上可以捕捉任意距离的依赖关系，但在实际中，单层的注意力机制可能仍然有限。通过堆叠多个 Block，<strong>模型可以在不同层次上反复处理信息，从而更好地捕捉长距离依赖</strong>。</p></li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ 如何理解 Decoder 中三个子层（sublayers）的作用？</strong></summary><div class=markdown-inner><h2><b>如何理解 Decoder 中三个子层（sublayers）的作用？</b></h2><p>Transformer解码器由多个相同的层（layers）组成，每一层包括三个子层（sublayers）：<strong>解码器自注意力（decoder self-attention）、编码器-解码器注意力（encoder-decoder attention） 和 逐位置前馈网络（positionwise feed-forward network）</strong>。每个子层都使用残差连接（residual connection）并紧接着进行层归一化（layer normalization）。</p><ol><li><p><strong>Masked Multi-Head Self-Attention Layer（解码器自注意力）</strong>：在这一步过程中，<strong>Q、K、V 全部来自目标序列的嵌入表示（即 Decoder 自身的输入）</strong>，与 Encoder 的输出无关。这一层的目的是让解码器 <strong>捕捉目标序列内部的依赖关系</strong>（例如语法结构、语义一致性），类似于 Encoder 的自注意力层捕捉输入序列的依赖关系。同时也确保 <strong>自回归特性（Autoregressive Property）</strong>：在生成目标序列时，解码器是自回归的，即每个 token 的生成依赖于之前已经生成的 token。解码器自注意力通过掩码（mask） 机制，确保在生成第 t 个 token 时，只能关注到第 1 到第 t−1 个 token，而不能“偷看”未来的 token。</p></li><li><p><strong>Multi-Head Attention Layer (Encoder-Decoder Attention)</strong>（编码器-解码器注意力）：该子层将解码器的输出与编码器的输出结合起来。通过这种方式，解码器能够 <strong>获取来自编码器的上下文信息</strong>，从而使得解码器能够 <strong>生成更相关的输出</strong>。在此过程中，解码器利用 编码器的输出（经过自注意力处理后的表示）来调整自己对目标序列的预测。</p><ul><li>编码器-解码器注意力中，Q（Query）：来自 <strong>解码器的当前状态（目标序列的嵌入表示）即“我需要关注什么”</strong>。K（Key） 和 V（Value）：<strong>来自 编码器的输出（即源序列的编码表示）</strong>。Key 表示源序列的特征，用于与 Query 计算相似度。Value 表示源序列的实际内容，用于加权求和。</li></ul></li><li><p><strong>Feed-Forward Neural Network Layer（前馈神经网络层）</strong>：该子层负责对每个位置的表示进行非线性变换和进一步的处理。它通常由两个全连接层（Fully Connected Layers）组成，其中一个是激活函数（通常是 ReLU/GELU）处理的隐藏层，另一个是输出层。前馈层提供了模型的表达能力，使得模型可以学习更复杂的特征。</p></li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ LayerNorm 和 BatchNorm 的核心区别是什么？为什么 Transformer 选择 LayerNorm？</strong></summary><div class=markdown-inner><h2><b>LayerNorm 和 BatchNorm 的核心区别是什么？为什么 Transformer 选择 LayerNorm？</b></h2><p><strong>Batch Normalization（BN）</strong> 在 batch 维度 上归一化，<strong>每个特征维度</strong> 独立计算均值和方差，统计 batch 内的样本均值 和方差。<strong>LayerNorm</strong> 在 <strong>特征维度（对每个样本的所有特征）</strong> 进行归一化，即在单个样本内部计算均值和方差，因此它不受 batch size 影响。</p><p>Transformer 选择 LayerNorm 而非 BatchNorm 的主要原因是 Transformer <strong>需要处理变长序列并进行自回归推理（Autoregressive Inference）</strong>，BatchNorm 在这种情况下无法正确归一化，而 LayerNorm 在每个时间步独立计算归一化统计量，避免了 batch 之间的依赖。</p><ul><li>Transformer 处理的是变长文本，例如 短句（“Hello”）和长句（“The weather is nice today”）可能共存于同一个 batch，但它们的 token 数不同。<strong>BatchNorm 无法直接在这些变长数据上计算 batch 统计量，因为不同长度的序列无法对齐进行批量归一化。</strong> 即使使用填充（Padding），这些填充值可能会影响均值和方差计算，导致不稳定的归一化效果。</li><li>BatchNorm 依赖于 整个 mini-batch 统计量 进行归一化，而在推理阶段（Inference），模型通常只能接收到一个 token 或一个小段文本，<strong>并无法获取完整 batch 进行归一化</strong>。因此，BatchNorm 统计量在 <strong>训练时计算的是 整个 batch 的均值和方差，但在推理时，batch size 可能是 1，导致统计量发生偏移</strong>，影响预测质量。</li></ul><p>此外，LayerNorm 在梯度流动上比 BatchNorm 更稳定，特别是对于深层 Transformer 模型，能够有效减少梯度消失（Gradient Vanishing）和梯度爆炸（Gradient Explosion）问题。</p></div></details><details><summary><strong class=custom-details-title>⁉️ Pre-LN 和 Post-LN 的区别是什么？为什么要在残差连接之后进行归一化（Post-LN）?</strong></summary><div class=markdown-inner><h2><b>Pre-LN 和 Post-LN 的区别是什么？为什么要在残差连接之后进行归一化（Post-LN）?</b></h2><p>Pre-Layer Normalization（Pre-LN）和 Post-Layer Normalization（Post-LN）是 Transformer 结构中两种不同的 Layer Normalization（LN，层归一化）策略。Pre-LN 在 <strong>Multi-Head Self-Attention（MHSA，多头自注意力） 和 Feed-Forward Network（FFN，前馈神经网络） 之前进行归一化</strong>，而 Post-LN 则在 <strong>残差连接（Residual Connection）之后进行归一化</strong>。</p><p>主要区别 在于梯度传播的方式：在 Pre-LN 结构中，归一化操作使得梯度 <strong>在深层网络中更加稳定</strong>，缓解了梯度消失（Gradient Vanishing）和梯度爆炸（Gradient Explosion）问题，因此 更适合深层 Transformer 训练。而 Post-LN 由于归一化在残差连接之后，会导致前期训练时梯度信号衰减，使得深度网络难以优化，尤其在 Transformer 层数较深时，梯度消失的问题更为严重。</p><p>然而，Post-LN 具有更好的优化表现，因为 <strong>它保留了每一层的特征分布</strong>，使得模型学习到的信息在归一化前不会被直接拉回到零均值单位方差的分布。因此，在某些场景下，如 小规模 Transformer 或浅层 Transformer，Post-LN 可能具有更好的收敛效果。</p></div></details><details><summary><strong class=custom-details-title>⁉️ 残差连接（Residual Connection）如何与 LayerNorm 协作缓解梯度消失？</strong></summary><div class=markdown-inner><h2><b>残差连接（Residual Connection）如何与 LayerNorm 协作缓解梯度消失?</b></h2><p>残差连接的核心思想是通过 <strong>跳跃连接（Skip Connection）</strong> 让信息能够绕过多个变换层，直接传递到更深的层，使梯度在反向传播（Backpropagation）过程中能够更稳定地传播。数学上，它通过加法操作，使输入 x 直接与变换后的输出 F(x) 相加，即 y = H(x) = x + F(x) ，从而保留原始信息并防止梯度过小。</p><ul><li>一个标准的网络层尝试学习一个复杂的映射 H(x)。</li><li>Residual Block 则将目标分解为 F(x) + x，其中：<ul><li>F(x) = H(x) - x：<strong>残差，即网络学习的部分</strong>。</li></ul></li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> 直接学习 H(x)（输入到输出的完整映射）可能是一个高度复杂的问题，<strong>而学习残差 F(x) = H(x) - x 相对简单得多</strong>。在许多实际任务中，输入 x 与目标 H(x) <strong>通常是接近的</strong>（例如图像分类任务中，特征提取后的信息不会发生剧烈变化）。通过学习残差 F(x)，<strong>网络只需关注输入与输出之间的细微差异</strong>，而不必重新建模整个映射。</p></blockquote><p>另一方面，LayerNorm 作用于每个时间步或通道上的神经元，对其均值和方差进行归一化，<strong>从而减小内部协变量偏移（Internal Covariate Shift），使梯度分布更加稳定</strong>，避免梯度消失或梯度爆炸（Gradient Explosion）。</p></div></details><details><summary><strong class=custom-details-title>⁉️ FFN 的结构是什么？为什么通常包含两层线性层和一个激活函数？</strong></summary><div class=markdown-inner><h2><b>FFN 的结构是什么？为什么通常包含两层线性层和一个激活函数？</b></h2><p>前馈神经网络（Feed-Forward Network, FFN）在深度学习模型（如 Transformer）中的结构通常包含两层线性变换（Linear Transformation）和一个非线性激活函数（Activation Function）组成：</p><span>\[
\text{FFN}(x) = \max(0, x W_1 + b_1) W_2 + b_2
\]</span><p>具体来说，FFN 的第一层是一个线性变换 W_1x + b_1 ，将 <strong>输入投影到一个更高维度的隐藏层</strong>，随后通过非线性激活函数增加模型的表示能力，最后通过第二个线性变换 W_2h + b_2 将高维特征映射回原始维度。这种结构的主要作用是增强模型的非线性表达能力，使其能够学习到更复杂的特征关系。两层线性变换的设计可以视为一种低维到高维再回归低维的映射，这样能够增加模型的容量，同时控制计算成本。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> 每个 token 经过 Self-Attention 计算后，得到的输出向量会被 <strong>独立地</strong> 传入 FFN，这个过程 <strong>不会跨 token 共享计算</strong>，即 <strong>每个位置的 token 独立通过相同的前馈网络 进行转换</strong>。可以理解为所有的 token（x）一起通过一个完全一样的 MLP，所以位置共用 MLP 中的一个 weight。</p><p><strong>在这个过程中会对每一个位置自身的特征信息进行加工，增加局部特征的表达力。而 Self-Attention 则负责关注全局的相互关系。</strong></p></blockquote></div></details><details><summary><strong class=custom-details-title>⁉️ 激活函数在 FFN 中的作用是什么？为什么常用 GELU 而非 ReLU？</strong></summary><div class=markdown-inner><h2><b>激活函数在 FFN 中的作用是什么？为什么常用 GELU 而非 ReLU？</b></h2><p>在前馈神经网络（Feedforward Neural Network, FFN）中，激活函数（Activation Function） 的主要作用是 <strong>引入非线性（Non-linearity）</strong>，使网络能够学习复杂的特征表示，而不仅仅是线性变换。</p><p>在大规模预训练语言模型（Large Language Models, LLMs）中，高斯误差线性单元（Gaussian Error Linear Unit, GELU） 常被用作 FFN 的激活函数，而不是传统的 修正线性单元（Rectified Linear Unit, ReLU），主要是因为 GELU 能够提供更平滑的非线性变换，从而提升梯度流的稳定性。GELU 的数学表达式为：</p><span>\[
\text{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2} \left( 1 + \text{erf} \left( \frac{x}{\sqrt{2}} \right) \right)
\]</span><div align=center><img src=/images/GELU.png width=500px/></div><p>RELU 的数学表达式为：</p><span>\[
\text{RELU}(x) = \max(0, x)
\]</span><p>与 ReLU 相比，GELU 在接近 0 的输入处具有平滑的 S 形曲线，而 ReLU 在 0 处存在不连续性（即当 x &lt; 0 时，输出恒为 0）。这种平滑性使 GELU <strong>在训练过程中能够更自然地保留和传播梯度，而不会像 ReLU 那样导致某些神经元完全死亡（Dead Neurons）的问题</strong>。此外，由于 GELU 允许输入以概率方式通过，而不是简单地进行硬阈值裁剪（如 ReLU 的 max(0, x) ），它在自注意力（Self-Attention）结构中表现更优，有助于 LLMs 更有效地捕捉复杂的语义关系。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> 在 自然语言处理（NLP）任务中，数据通常具有更复杂的特征表示，<strong>负数不一定意味着无用信息</strong>。（NLP 中的负值包含语义信息，特别是涉及到情感分析、语法结构等复杂任务时。负值可能代表对立的意思（如否定、反向情感等），因此 保留负值 变得尤为重要。）</p><p>ReLU 直接裁剪负数部分，意味着所有小于 0 的值都被映射为 0，相当于丢弃了部分信息。GELU 不是一个硬阈值，而是 <strong>基于概率平滑地裁剪输入，这意味着接近 0 的小负值仍有一定概率被保留</strong>，而不是完全消失。e.g. 假设某层神经元计算出的输出是 -0.1，在 ReLU 下，它会变成 0，而在 GELU 下，它可能仍然保持 -0.05 或其他较小值。<strong>这有助于模型保留更多信息，避免信息过早丢失</strong>。</p><p>ReLU 存在“神经元死亡（Dead Neurons）”问题：<strong>如果一个神经元的输入总是负数，那么它的输出始终是 0，对应的梯度也会一直是 0，这样该神经元可能永远无法被更新</strong>，从而降低模型的表达能力。GELU 由于其平滑的 S 形曲线，即使在负数区域仍然保持一定梯度，这样梯度可以更稳定地传播，减少“死神经元”问题。数学上看，GELU 的导数：</p><span>\[
\frac{d}{dx} GELU(x) = \Phi(x) + x \cdot \phi(x)
\]</span><p>Phi(x) 是标准正态分布的 CDF，表示激活的概率。phi(x) 是标准正态分布的 PDF，表示数据在某个点的概率密度。相比于 ReLU 的二值导数（1 或 0），GELU 的梯度在整个数轴上连续变化，更利于优化。</p></blockquote></div></details></div></details><details open><summary><strong class=custom-details-title>⭐ 预训练模型（BERT、GPT、T5）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ 什么是预训练？与传统监督学习的区别？</strong></summary><div class=markdown-inner><h2><b>什么是预训练？与传统监督学习的区别？</b></h2><p>预训练（Pretraining）是一种在大规模无标注数据（Unlabeled Data）上训练深度学习模型的技术，特别常用于自然语言处理（NLP）中的大规模语言模型。在预训练阶段，模型通常采用 <strong>自监督学习（Self-Supervised Learning）方法</strong>，通过预测被遮蔽的词（如 BERT 的掩码语言模型 Masked Language Model, MLM）或基于上下文预测下一个词（如 GPT 的自回归语言模型 Autoregressive Language Model, AR）来学习文本的统计特性和语义表示（Semantic Representation）。</p><p>相比传统的监督学习（Supervised Learning），预训练 <strong>不需要大量人工标注数据，而是利用大规模无标签语料</strong>，使模型具备广泛的语言理解能力。随后，模型可以通过 <strong>微调（Fine-Tuning）在小规模标注数据上进一步优化</strong>，以适应具体任务。这种方式相比传统监督学习更加高效，尤其适用于数据标注成本高的任务，同时提升模型的泛化能力（Generalization Ability）和适应性（Adaptability）。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：Pre-training 是在大规模无监督数据上训练模型，而 Fine-tuning 是在特定任务或数据集上对模型进行微调。</p></blockquote></div></details><details><summary><strong class=custom-details-title>⁉️ 解释Encoder-only，Encoder-Decoder，Decoder-only 三种模式的区别以及使用场景？</strong></summary><div class=markdown-inner><h2><b>解释Encoder-only，Encoder-Decoder，Decoder-only 三种模式的区别以及使用场景？</b></h2><ul><li><strong>Encoder-only 模式</strong>：在这种架构中，只有 <strong>编码器（Encoder）</strong> 部分用于处理输入数据，通常应用于 文本分类（Text Classification）、命名实体识别（Named Entity Recognition, NER） 和 情感分析（Sentiment Analysis） 等任务。该模式的主要任务是从输入序列中提取信息，生成固定长度的表示，适用于 <strong>需要理解输入而不生成输出</strong> 的任务。例如，BERT 就是一个典型的 Encoder-only 模型，通过 自注意力机制（Self-Attention） 学习输入文本的上下文信息并生成表示。</li><li><strong>Encoder-Decoder 模式</strong>：这种架构包含一个 编码器（Encoder） 和一个 解码器（Decoder），用于从输入生成输出。输入通过编码器进行处理，得到一个上下文表示，然后解码器根据这个表示生成最终输出。这种结构非常适合 <strong>序列到序列任务（Sequence-to-Sequence Tasks）</strong>，如 机器翻译（Machine Translation） 和 文本摘要（Text Summarization）。</li><li><strong>Decoder-only 模式</strong>：这种架构仅包含 <strong>解码器（Decoder）</strong>，通常用于 <strong>自回归生成任务（Autoregressive Generation Tasks）</strong>，例如 文本生成（Text Generation） 和 语言建模（Language Modeling）。在这种模式下，模型根据前面的输入和已经生成的词预测下一个词。一个典型的例子是 GPT 系列模型，它基于 Decoder-only 架构，通过不断预测下一个词来生成连贯的文本。此模式非常适用于 <strong>需要根据上下文生成输出</strong> 的任务。</li></ul></div></details><details><summary><strong class=custom-details-title>⁉️ BERT 的预训练目标有哪些？什么是 MLM 和 NSP？具体如何实现？</strong></summary><div class=markdown-inner><h2><b>BERT 的预训练目标有哪些？什么是 MLM 和 NSP？具体如何实现？</b></h2><p>BERT（Bidirectional Encoder Representations from Transformers）的预训练目标主要包括 <strong>Masked Language Modeling（MLM）</strong> 和 <strong>Next Sentence Prediction（NSP）</strong>。</p><p>在 MLM 任务 中，BERT <strong>随机遮盖（mask）</strong> 输入文本中的部分词汇（通常为 15%），并通过 Transformer Encoder 预测这些被遮盖的词。具体实现时，80% 的被遮盖词替换为 <code>[MASK]</code>，10% 保持不变，另 10% 替换为随机词，以增强模型的泛化能力。</p><p>NSP 任务 旨在学习句子级别的关系，BERT 通过给定的两个句子 <strong>判断它们是否为原始文本中的连续句（IsNext）或随机拼接的无关句（NotNext）</strong>。训练时，BERT 以 50% 的概率选择相邻句子作为正样本，另 50% 选择无关句子作为负样本，并通过 二分类损失函数（Binary Cross-Entropy Loss） 进行优化。<strong>MLM 使 BERT 能够学习上下文双向依赖关系，而 NSP 则有助于建模句子间的全局关系</strong>，这两个目标共同提升了 BERT 在 自然语言理解（Natural Language Understanding, NLU） 任务中的表现。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> BERT 的 <strong>Masked Language Modeling</strong> 本质上就是在做“完形填空”：预训练时，先将一部分词随机地盖住，经过模型的拟合，<strong>如果能够很好地预测那些盖住的词，模型就学到了文本的内在逻辑</strong>。这部分的主要作用是让模型 <strong>学到词汇和语法规则，提高语言理解能力</strong>。</p><p>除了“完形填空”，BERT还需要做 <strong>Next Sentence Prediction</strong> 任务：预测句子B是否为句子A的下一句。Next Sentence Prediction有点像英语考试中的“段落排序”题，只不过简化到只考虑两句话。<strong>如果模型无法正确地基于当前句子预测 Next Sentence，而是生硬地把两个不相关的句子拼到一起，两个句子在语义上是毫不相关的，说明模型没有读懂文本背后的意思。</strong> 这部分的主要作用是让模型 <strong>学习句子级别的语义关系</strong>。</p></blockquote></div></details><details><summary><strong class=custom-details-title>⁉️ 为什么 BERT 的输入需要添加 [CLS] 和 [SEP] 特殊标记？</strong></summary><div class=markdown-inner><h2><b>为什么 BERT 的输入需要添加 [CLS] 和 [SEP] 特殊标记？</b></h2><p>BERT 的输入需要添加 <code>[CLS]</code> 和 <code>[SEP]</code> 特殊标记，主要是用来表示 Next Sentence Prediction（NSP） 任务的输入格式，并增强模型的表示能力。例如：</p><pre tabindex=0><code>[CLS] 句子A [SEP] 句子B [SEP]
</code></pre><ul><li><strong><code>[CLS]</code>（Classification Token）</strong>：BERT 在输入序列的开头始终添加 <code>[CLS]</code>，它的最终隐藏状态（Hidden State）可以作为整个序列的表示，特别 <strong>适用于分类任务</strong>（如情感分析、自然语言推理 NLI）。即使不是分类任务，BERT 仍然会计算 <code>[CLS]</code> 的表示，因此它始终是输入的一部分。</li><li><strong><code>[SEP]</code>（Separator Token）</strong>：BERT 采用双向 Transformer，因此需要区分单句和双句任务。在单句任务（如情感分析）中，输入序列结尾会加 <code>[SEP]</code>，而在双句任务（如问答 QA 或文本匹配），<code>[SEP]</code> 用于分隔两个句子，帮助 BERT 处理跨句子的关系建模。</li></ul><p>在 微调阶段（Fine-Tuning），不同任务对 <code>[CLS]</code> 和 <code>[SEP]</code> 的使用方式略有不同。例如：</p><ul><li><strong>文本分类（如情感分析）</strong>：<code>[CLS]</code> 的最终表示输入到 Softmax 层进行分类。</li><li><strong>问答（QA）</strong>：<code>[SEP]</code> 作为问题和段落的分隔符，BERT 需要预测答案的起始和结束位置。</li><li><strong>命名实体识别（NER）</strong>：<code>[CLS]</code> 不是必须的，而是依赖 Token 级别的输出。</li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>对比 BERT 的 <code>[CLS]</code> 向量和平均池化获取句子表示的优缺点?</strong></p><ul><li><strong><code>[CLS]</code> 向量的优缺点</strong>：<ul><li><strong>简洁性</strong>：只需要一个向量（即 <code>[CLS]</code> 向量）来表示整个句子的语义，非常适用于分类任务，尤其是在输入句子较短时。</li><li><strong>端到端优化</strong>：由于 BERT 在预训练时优化了 <code>[CLS]</code> 向量，使其能够有效地聚合句子的语义信息，且通常与下游任务紧密相关。</li><li><strong>可能信息丢失</strong>：<code>[CLS]</code> 向量是通过加权和整个输入的 token 嵌入得到的，可能导致一些细节信息丢失，特别是当句子较长或复杂时。</li></ul></li><li><strong>平均池化（Mean Pooling）的优缺点</strong>：<ul><li><strong>信息保留</strong>：平均池化将所有 token 的表示进行平均，从而保留了句子中各个部分的信息，相比于 <code>[CLS]</code> 向量，它能保留更多的语义信息。</li><li><strong>缺乏上下文关注</strong>：平均池化忽略了 token 之间的复杂依赖关系，简单的加权平均可能无法捕捉到句子中不同部分的关联性，尤其是在多义词或句子结构复杂时。</li><li><strong>计算开销</strong>：对于长句子，平均池化需要计算所有 token 的平均值，可能增加计算开销，尤其在大规模数据集上。</li></ul></li></ul></blockquote></div></details><details><summary><strong class=custom-details-title>⁉️ BERT 微调的细节？</strong></summary><div class=markdown-inner><h2><b>BERT 微调的细节？</b></h2><p>BERT（Bidirectional Encoder Representations from Transformers）微调（Fine-tuning）通常是在预训练（Pre-training）后的基础上，将整个 BERT 模型与特定任务的分类头（Task-specific Head）一起训练，使其适应下游任务（Downstream Task）。</p><p>在微调过程中，输入文本经过分词（Tokenization）后，会被转换为对应的词嵌入（Token Embeddings）、位置嵌入（Position Embeddings）和分段嵌入（Segment Embeddings），然后输入 BERT 的 Transformer 层。模型通过多层双向自注意力（Bidirectional Self-Attention）计算上下文信息，并在最终的 <code>[CLS]</code>（分类标记）或其他适当的标记上添加任务特定的层，如全连接层（Fully Connected Layer）或 CRF（Conditional Random Field），然后使用任务相关的损失函数（Loss Function）进行优化，如交叉熵损失（Cross-Entropy Loss）用于分类任务。</p></div></details><details><summary><strong class=custom-details-title>⁉️ RoBERTa 的技术细节？它相比 BERT 做了哪些改进（如动态掩码、移除 NSP 任务）？</strong></summary><div class=markdown-inner><h2><b>RoBERTa 的技术细节？它相比 BERT 做了哪些改进（如动态掩码、移除 NSP 任务）？</b></h2><p><strong>RoBERTa（Robustly Optimized BERT Pretraining Approach）</strong> 在 BERT（Bidirectional Encoder Representations from Transformers）的基础上进行了多项优化，以提高模型的性能和泛化能力。</p><p>首先，RoBERTa 采用了 <strong>动态掩码（Dynamic Masking）</strong> 机制，即在每个训练 epoch 重新随机生成 Masked Language Model（MLM）掩码，而 BERT 仅在数据预处理阶段静态确定掩码。这种动态策略增加了模型学习的多样性，提高了其对不同掩码模式的适应能力。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>训练时使用的动态掩码（Dynamic Masking）与静态掩码有何区别？</strong></p><p>BERT 原始论文使用的是 静态掩码，即在数据预处理阶段，对训练数据进行一次性 Mask 处理，并将其存储起来。在训练过程中，每次使用该数据时，<strong>Mask 位置都是固定的</strong>。</p><p>RoBERTa 在 BERT 的基础上采用了 动态掩码，即在每次数据加载时，都会 <strong>随机重新选择 Mask 位置</strong>，确保同一输入文本在不同训练轮次中 Mask 位置不同。这提高了数据多样性，使得模型能够学习更丰富的上下文表示，而不会过度拟合某些固定的 Mask 位置。</p></blockquote><p>其次，RoBERTa <strong>移除了 NSP（Next Sentence Prediction）任务</strong>，BERT 在训练时采用了 NSP 任务以增强模型对句子关系的理解，<strong>但研究发现 NSP 任务并未显著提升下游任务的表现，甚至可能影响模型的学习效率</strong>。因此，RoBERTa 采用了更大规模的 <strong>连续文本（Longer Sequences of Text） 进行预训练，而不再强制区分句子关系</strong>。最后，RoBERTa 通过 增加 batch size 和训练数据量，并 采用更长的训练时间，进一步优化了 BERT 预训练过程，使模型能够更充分地学习语言特征。</p></div></details><details><summary><strong class=custom-details-title>⁉️ DeBERTa 的“解耦注意力”机制（Disentangled Attention）如何分离内容和位置信息？</strong></summary><div class=markdown-inner><h2><b>DeBERTa 的“解耦注意力”机制（Disentangled Attention）如何分离内容和位置信息？</b></h2><p><strong>DeBERTa（Decoding-enhanced BERT with Disentangled Attention）</strong> 相较于 BERT 主要在 <strong>解耦注意力（Disentangled Attention）</strong> 和 <strong>相对位置编码（Relative Position Encoding）</strong> 方面进行了改进，以提升模型的语言理解能力。BERT 采用标准的 Transformer 注意力机制（Self-Attention），其中查询（Query）、键（Key）、值（Value）向量均是基于相同的嵌入（Embedding），这意味着 内容信息（Content Information） 和 位置信息（Positional Information） 混合在一起，限制了模型对句子结构的建模能力。而 DeBERTa 通过解耦注意力机制，<strong>对内容和位置信息分别编码，使得模型在计算注意力时能够更精确地理解不同词语之间的相对关系</strong>。</p><p>具体来说，DeBERTa 在计算注意力权重时，不是直接基于词向量（Token Embedding），而是 <strong>分别计算基于内容（Content-based Attention）和基于位置（Position-based Attention）的注意力得分，然后再将两者加权合并</strong>。这种方式使得 DeBERTa 能够在更长的依赖关系建模上表现更好。此外，DeBERTa 还采用了 <strong>增强的相对位置编码（Enhanced Relative Position Encoding）</strong>，相比 BERT 的绝对位置编码（Absolute Positional Encoding），能够更自然地处理长文本结构。</p></div></details><details><summary><strong class=custom-details-title>⁉️ ALBERT 如何通过参数共享降低模型参数量？</strong></summary><div class=markdown-inner><h2><b>ALBERT 如何通过参数共享降低模型参数量？</b></h2><p><strong>ALBERT（A Lite BERT）是对 BERT 模型的轻量化改进</strong>，旨在通过降低模型参数量而不显著影响模型性能。其主要技术细节涉及两项关键的优化策略：参数分解嵌入（Factorized Embedding） 和 跨层参数共享（Cross-Layer Parameter Sharing）。</p><ol><li><strong>参数分解嵌入（Factorized Embedding）</strong>：BERT 使用了一个巨大的词嵌入矩阵（embedding matrix），其大小通常是词汇表的大小与隐藏层维度（hidden size）的乘积。而 ALBERT 采用了参数分解方法，<strong>将词嵌入矩阵分解为两个低维矩阵</strong>，分别是一个较小的 词汇嵌入矩阵（Word Embedding Matrix） 和一个较小的 隐藏层嵌入矩阵（Hidden Layer Embedding Matrix）。具体来说，词嵌入矩阵被分解为两个矩阵，一个低维的嵌入矩阵和一个较小的输出矩阵，这样就显著减少了参数数量。例如，假设词嵌入矩阵的维度为 V x H（V 是词汇表大小，H 是隐藏层大小），通过分解成两个矩阵 V x E 和 E x H（E 为较小的维度），可以大幅度减少计算复杂度和存储需求。</li><li><strong>跨层参数共享（Cross-Layer Parameter Sharing）</strong>：在标准的 BERT 中，每一层 Transformer 都有一组独立的参数，而 ALBERT 通过跨层共享参数，减少了每一层的独立参数。具体来说，ALBERT 将模型中多个 Transformer 层 的参数进行共享，<strong>所有层都使用相同的权重</strong>。这样，尽管 ALBERT 保留了更多的层数（如 BERT 的 12 层改为 12 层的 ALBERT），但通过共享权重，整体的参数数量大幅度减少。参数共享的核心思想是：<strong>每一层 Transformer 的前向传播和反向传播使用相同的参数，从而减少了每层的权重数量，降低了内存消耗。</strong></li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ T5 如何统一不同 NLP 任务的格式？其预训练任务（如 Span Corruption）具体如何实现？</strong></summary><div class=markdown-inner><h2><b>T5 如何统一不同 NLP 任务的格式？其预训练任务（如 Span Corruption）具体如何实现？</b></h2><p>T5（Text-to-Text Transfer Transformer）通过将所有 NLP 任务（如文本分类、机器翻译、问答、摘要生成等）<strong>统一转换为文本到文本（Text-to-Text）的格式</strong>，从而实现了一个通用的 NLP 框架。具体而言，无论是输入句子的分类任务还是填空任务，T5 都会将输入转换为文本序列，并要求模型生成相应的文本输出。例如，情感分析任务的输入可以是 <code>"sentiment: I love this movie"</code>，输出则是 <code>"positive"</code>，而机器翻译任务的输入可能是 <code>"translate English to French: How are you?"</code>，输出为 <code>"Comment ça va?"</code>。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：Span Corruption（Span-Masked Language Modeling, SMLM）与 Masked Language Modeling（MLM）的核心区别在于 Mask 的方式和学习目标的不同。</p><ul><li><strong>MLM（Masked Language Modeling，BERT 采用）</strong>：<ul><li>MLM 主要是随机选择 <strong>单个 token 进行遮蔽</strong>，然后让模型预测被遮蔽的 token。例如：<pre tabindex=0><code>Input: &#34;I love [MASK] learning&#34;
Target: &#34;deep&#34; 
</code></pre></li><li>由于每次仅遮蔽少量 token，BERT 可能 <strong>无法学习到更长跨度的依赖关系</strong>，特别是对完整的子句或短语的理解较弱。</li></ul></li><li><strong>SMLM（Span-Masked Language Modeling，T5 采用）</strong>：<ul><li>SMLM 采用 Span Corruption，即 <strong>一次遮蔽连续的多个 token</strong>，并用特殊标记 <code>&lt;extra_id_0></code> 来表示被遮蔽部分。例如：<pre tabindex=0><code>Input: &#34;I &lt;extra_id_0&gt; deep &lt;extra_id_1&gt;.&#34;
Target: &#34;&lt;extra_id_0&gt; love &lt;extra_id_1&gt; learning&#34;
</code></pre></li><li>能够更好地 <strong>学习长距离的依赖关系</strong>，适用于生成式任务（如摘要、翻译）。训练难度更高。</li></ul></li></ul></blockquote><p>T5 采用的主要预训练任务是 <strong>Span Corruption（Span-Masked Language Modeling, SMLM）</strong>，这是一种变体的掩码语言建模（Masked Language Modeling, MLM）。具体来说，该任务会在输入文本中随机选择若干个 span（即连续的子序列），用特殊的 <code>&lt;extra_id_X></code> 令牌替换它们，并要求模型预测被遮蔽的内容。例如，原始文本 <code>"The quick brown fox jumps over the lazy dog"</code> 可能会被转换为 <code>"The &lt;extra_id_0> fox jumps over the &lt;extra_id_1> dog"</code>，而模型需要输出 <code>"quick brown"</code> <code>&lt;extra_id_0></code> 和 <code>“lazy”</code> <code>&lt;extra_id_1></code>。这种方式比 BERT 的单词级别掩码更灵活，有助于学习更丰富的上下文信息，从而提升生成任务的能力。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：在 Span Corruption 预训练（Span Corruption Pretraining）中，被遮蔽的文本片段（Masked Span）的长度通常遵循 Zipf 分布（Zipf’s Law），<strong>即较短的片段更常见，而较长的片段较少，以模拟自然语言中的信息分布</strong>。具体而言，像 T5 这样的模型使用 <strong>几何分布（Geometric Distribution）</strong> 来采样 span 长度，以确保既有短范围的遮蔽，也有跨多个 token 的长范围遮蔽。</p><p>不同的遮蔽长度会影响模型的学习能力：较短的 span（例如 1-3 个 token）有助于模型学习局部语义填充能力（Local Context Understanding），而较长的 span（如 8-10 个 token 甚至更长）可以增强模型的全局推理能力（Global Reasoning）和段落级理解能力（Document-Level Comprehension）。如果 span 过短，模型可能更倾向于基于表面模式（Surface Patterns）预测，而非真正理解上下文；如果 span 过长，则可能导致学习任务过于困难，使得模型难以有效收敛。</p></blockquote></div></details><details><summary><strong class=custom-details-title>⁉️ 如何将预训练的 Encoder-Decoder 模型（如 T5）适配到具体下游任务？</strong></summary><div class=markdown-inner><h2><b>如何将预训练的 Encoder-Decoder 模型（如 T5）适配到具体下游任务？</b></h2><p>T5（Text-to-Text Transfer Transformer）模型与BERT相似，也需要通过在特定任务的数据上进行微调（fine-tuning）来完成下游任务。与BERT的微调有所不同，T5的主要特点包括：</p><ol><li><strong>任务描述（Task Prefix）</strong>：T5 的输入不仅包含原始文本，还需要附加一个任务描述。例如，在文本摘要（Text Summarization）任务中，输入可以是 &ldquo;summarize: 原文内容&rdquo;，而在问答（Question Answering）任务中，输入可以是 &ldquo;question: 问题内容 context: 相关文本&rdquo;。这种设计使 T5 能够以统一的 文本到文本（Text-to-Text） 形式处理不同任务。</li><li><strong>端到端序列生成（Sequence-to-Sequence Generation）</strong>：与 BERT 仅能进行分类或填空任务不同，T5 依赖其 Transformer 解码器（Transformer Decoder） 来生成完整的输出序列，使其适用于文本生成任务，如自动摘要、数据到文本转换（Data-to-Text Generation）等。</li><li><strong>无需额外层（No Task-Specific Layers）</strong>：在 BERT 微调时，通常需要在其顶层添加特定的任务头（Task-Specific Head），如分类层（Classification Layer）或 CRF（Conditional Random Field）层，而 T5 直接将任务作为输入提示（Prompt），并使用相同的模型结构进行训练，无需修改额外的网络层。</li></ol></div></details><details><summary><strong class=custom-details-title>⁉️ 什么是BART？BART 的预训练任务与 T5 有何异同？</strong></summary><div class=markdown-inner><h2><b>什么是BART？BART 的预训练任务（如 Text Infilling、Sentence Permutation）与 T5 的 Span Corruption 有何异同？</b></h2><p><strong>BART（Bidirectional and Auto-Regressive Transformers）</strong> 是一种结合了 BERT（Bidirectional Encoder Representations from Transformers）和 GPT（Generative Pretrained Transformer）优点的生成模型。BART 在预训练过程中采用了 <strong>自编码器（Autoencoder）</strong> 结构，其编码器部分像 BERT 一样使用双向编码，能够捕捉上下文信息，而解码器则是像 GPT 一样用于生成任务，通过自回归方式生成文本。</p><p>BART 的预训练任务包括 <strong>Text Infilling</strong> 和 <strong>Sentence Permutation</strong>：</p><ol><li><strong>Text Infilling</strong>：在这一任务中，模型需要从一段被掩盖部分的文本中恢复出被移除的词或词组。具体来说，给定一个输入文本，部分单词或短语被替换为掩码（mask），然后模型的任务是预测这些被掩盖的内容。这一任务类似于 BERT 的 Masked Language Model（MLM），但区别在于，BART 不仅仅是根据上下文来填充单一词汇，它的目标是生成整个缺失的文本片段。<pre tabindex=0><code>原始文本：“The quick brown fox jumps over the lazy dog in the park.”
<p>掩盖后的文本：“The quick [MASK] jumps over the lazy dog in the park.”
</code></pre></li></p></ol><blockquote class="book-hint warning"><p><strong>Note</strong>：在 MLM 中，模型的目标是预测被随机掩盖的单个词（或子词）。虽然 Text Infilling 看起来类似于 MLM，但 BART 的 Text Infilling 中，<strong>掩盖的部分不仅限于单个词，而是可能是一个较大的片段或短语</strong>。通常，模型会随机选择一个较长的文本片段（如一个短语或句子的一部分）并用一个掩码标记替换，然后模型需要预测整个被掩盖的文本片段。</p><p><strong>总结，MLM遮一个词，Text Infilling 遮一个片段，Span Corruption 遮多个片段。</strong></p></blockquote><ol start=2><li><strong>Sentence Permutation</strong>：在这一任务中，输入文本的句子顺序被打乱，模型的任务是根据上下文恢复正确的句子顺序。这个任务是为了帮助模型学习长文本的结构和上下文关系，使其能够生成连贯且符合语法规则的文本。它与 T5 中的 Span Corruption 有一定的相似性，因为两者都涉及对文本进行扰动，并要求模型根据扰动后的文本恢复原始文本。<pre tabindex=0><code>原始文本：
“The dog chased the ball. It was a sunny day.”
<p>打乱顺序后的文本：
“It was a sunny day. The dog chased the ball.”
</code></pre></li></p></ol></div></details></div></details><hr><h3 id=大语言模型llm><strong>大语言模型（LLM）</strong>
<a class=anchor href=#%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8bllm>#</a></h3><details open><summary><strong class=custom-details-title>⭐ 预训练流程细节（训练框架，混合精度训练，预训练评估）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⁉️ LLM 的预训练流程通常涉及到哪些环节？</strong></summary><div class=markdown-inner><h2><b>LLM 的预训练流程通常涉及到哪些环节？</b></h2><p>LLM的预训练流程通常包括多个关键环节，首先是 <strong>数据构建</strong>。通过爬取来自多源的文本数据（如网页、书籍、代码等），并进行严格的清洗（去重、去噪、质量过滤），然后根据不同领域或语言进行动态采样以优化数据的配比。这些数据随后用于 <strong>训练一个Tokenizer（分词器）</strong>，常用的算法包括BPE（Byte Pair Encoding）和WordPiece，它们生成适合任务的词汇表。</p><p>接下来，<strong>选择合适的模型架构和参数规模</strong> 是至关重要的。常见的架构包括Decoder-only的GPT类模型和Encoder-Decoder的T5类模型，具体选择依据任务需求而定。同时，模型的参数规模也需根据Chinchilla定律来平衡数据量、模型大小和计算资源，以便提高训练效率。为了应对海量数据的训练，通常 <strong>采用分布式训练框架（如DeepSpeed与Megatron-LM），并启用混合精度训练</strong>（如FP16或BF16），结合梯度缩放来加速计算过程。</p><p>在训练过程中，必须通过 <strong>监控损失曲线、计算资源的利用率等指标</strong>，确保训练稳定性和高效性。此外，<strong>定期保存Checkpoint（模型检查点）</strong> 以防止训练过程中断时丢失重要进展。训练的目标通常是基于MLM（Masked Language Modeling，掩码预测）或CLM（Causal Language Modeling，自回归生成）进行无监督学习，具体选择取决于模型设计和任务目标。</p><p>完成训练后，需要 <strong>通过困惑度（PPL，Perplexity）和领域Benchmark（如MMLU）等标准进行模型性能评估</strong>，并进行大规模的长文本生成测试（“大海捞针”测试）来考察模型的泛化能力。如果模型表现出良好的通用能力，接下来可以针对特定领域或长上下文需求进行进一步的预训练，以增强模型的专项性能。</p></div></details><details><summary><strong class=custom-details-title>⁉️ Tokenizer 在实际模型预训练阶段是如何被使用的？词表大小对模型性能的影响？</strong></summary><div class=markdown-inner><h2><b>Tokenizer 在实际模型预训练阶段是如何被使用的？词表大小对模型性能的影响？</b></h2><p>Tokenizer（分词器）的主要作用是 <strong>将原始文本转换为模型可理解的离散数值表示</strong>，即Token ID（标记序列）。这个过程通常包括分词（Tokenization）、映射（Mapping to Vocabulary） 和 填充/截断（Padding/Truncation）。在分词时，不同的 Tokenizer会根据预定义的 <strong>词表（Vocabulary）</strong> 将文本拆分成最优的子词单元。</p><p><strong>词表的大小决定了模型可识别的唯一 Token 数量</strong>，比如 LLaMA 采用了 32k 的词表，而 GPT-2 使用了 50k 词表。较大的词表允许模型以更少的 Token 表示相同文本，<strong>提高表达能力，但也增加了参数规模和计算复杂度</strong>；而较小的词表则 <strong>减少了计算需求，但可能导致序列变长，进而影响训练效率</strong>。因此，在预训练阶段，词表大小的选择会直接影响模型的记忆能力、计算成本以及推理速度。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：词表（Vocabulary）既可以直接使用预训练模型提供的标准词表，也可以根据自己的数据集重新训练一个词表，具体取决于应用需求：</p><ol><li><strong>直接使用预训练词表</strong>：如 GPT-3、LLaMA、T5 等开源模型的 Tokenizer 已经基于大规模文本语料（如 Common Crawl、Wikipedia）训练了词表，并随模型一起发布。直接使用这些词表能够确保与原始模型的 Token 方式一致，避免 Token 不匹配导致的性能下降。这种方法 <strong>适用于大多数 NLP 任务</strong>，特别是在迁移学习（Transfer Learning）场景下。</li><li><strong>基于自有数据训练新词表</strong>：如果 <strong>目标领域与通用 NLP 语料差异较大（如医学、法律、金融等专业领域），或者需要支持特定语言（如低资源语言或多语言任务）</strong>，可以使用 SentencePiece（支持 BPE、Unigram）或 Hugging Face Tokenizers 来从头训练词表。训练时通常会调整 词表大小（Vocabulary Size），使其适配目标任务。较大的词表可以减少 OOV（Out-Of-Vocabulary）问题，而较小的词表能减少计算复杂度，提高推理速度。</li></ol></blockquote></div></details><details><summary><strong class=custom-details-title>⁉️ 有哪些适合 LLM 训练的参数初始化策略（Parameter Initialization）？</strong></summary><div class=markdown-inner><h2><b>有哪些适合 LLM 训练的参数初始化策略（Parameter Initialization）？</b></h2><p>参数初始化（Parameter Initialization） 是神经网络训练中的一个关键步骤，旨在为 <strong>网络的权重（weights）和偏置（biases）赋予初始值</strong>。这些初始值对模型的训练收敛速度、稳定性及最终性能有着重要影响。<strong>合理的参数初始化策略能够避免梯度消失（Vanishing Gradient）或梯度爆炸（Exploding Gradient）等问题，进而提高训练效率</strong>。</p><p>对于 大规模语言模型（Large Language Models, LLM） 的训练，一些常见且适用的参数初始化策略包括：</p><ol><li><p><strong>Xavier 初始化（Xavier Initialization）</strong>：也叫做 Glorot 初始化，它通过考虑输入和输出的神经元数量来设置权重的方差。具体来说，权重的方差设置为：</p><span></li></ol><p>[
W_{i,j} \sim N\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)
]</span></p><p>其中 n_in 和 n_out 分别是当前层的输入和输出神经元数量。此策略通常用于 sigmoid 或 tanh 激活函数的网络，能够帮助缓解梯度消失问题。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>Xavier 初始化适用于激活函数是 sigmoid 或 tanh 的网络</strong>的原因是这些激活函数的<strong>导数容易趋于零</strong>，尤其是在输入值落入激活函数的<strong>饱和区（Sigmoid 的两侧平坦区域）</strong>。如果权重初始化过大，输入会快速进入饱和区，导致梯度消失。如果权重初始化过小，输出信号会逐层衰减，最终导致梯度消失。</p><p>Xavier 的初始化方法<strong>将权重分布限定在一个较小的范围内</strong>，使输入值主要分布在 Sigmoid 和 Tanh 的线性区，避免梯度消失。在较深的网络中，信号可能仍会因为层数的累积效应导致衰减或放大。</p></blockquote><ol start=2><li><strong>He 初始化（He Initialization）</strong>：类似于 Xavier 初始化，但特别适用于 ReLU 激活函数。它通过设置权重的方差，有效地解决了 ReLU 激活函数中常见的 dying ReLU 问题，即一些神经元始终不激活。</li></ol><span>\[
W_{i,j} \sim N\left(0, \frac{2}{n_{\text{in}}}\right)
\]</span><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>He 初始化适用于激活函数是ReLU及其变种</strong>的原因是对于 ReLU，当输入为负时，输出恒为 0；当输入为正时，输出为原值。由于一部分神经元输出会被截断为 0，导致<strong>有效的参与计算的神经元数量减少</strong>（称为“稀疏激活”现象）。如果初始化权重过小，信号会迅速减弱，导致梯度消失；而如果权重过大，信号会迅速放大，导致梯度爆炸。</p><p>He 初始化通过<strong>设定较大的方差</strong>，补偿了 ReLU 截断负值导致的信号损失。这样可以让激活值的分布更均衡，<strong>避免信号快速衰减或放大</strong>。He 初始化<strong>根据输入层大小调整权重的方差，使每层的输出方差保持相对稳定</strong>，即使网络层数增加，信号也不会显著衰减或爆炸。</p></blockquote><ol start=3><li><strong>Pretrained Initialization</strong>：对于 LLM，使用在大规模数据集上预训练的权重作为初始化参数（例如 BERT、GPT）是一种常见且有效的做法。通过 <strong>迁移学习（Transfer Learning）</strong>，这种初始化策略能够显著加速训练过程，并提升模型的性能。</li></ol></div></details></div></details><ul><li>微调技术（全参数微调、LoRA、QLoRA、Prompt Tuning）</li><li>推理优化（KV Cache、量化、模型剪枝）</li></ul><hr><h3 id=工具与框架><strong>工具与框架</strong>
<a class=anchor href=#%e5%b7%a5%e5%85%b7%e4%b8%8e%e6%a1%86%e6%9e%b6>#</a></h3><ul><li>Hugging Face Transformers库（模型加载、Pipeline、Trainer）</li><li>LangChain（Agent、Chain、Tools设计）</li><li>LlamaIndex（文档索引、检索增强生成RAG）</li></ul><hr><h2 id=应用开发与工程化><strong>应用开发与工程化</strong>
<a class=anchor href=#%e5%ba%94%e7%94%a8%e5%bc%80%e5%8f%91%e4%b8%8e%e5%b7%a5%e7%a8%8b%e5%8c%96>#</a></h2><hr><h3 id=应用开发框架><strong>应用开发框架</strong>
<a class=anchor href=#%e5%ba%94%e7%94%a8%e5%bc%80%e5%8f%91%e6%a1%86%e6%9e%b6>#</a></h3><ul><li>LangChain高级用法（自定义Tools、Agent逻辑、流式输出）</li><li>前端集成（Streamlit、Gradio构建交互界面）</li><li>API开发（FastAPI/Flask构建RESTful服务）</li></ul><hr><h3 id=检索增强生成rag><strong>检索增强生成（RAG）</strong>
<a class=anchor href=#%e6%a3%80%e7%b4%a2%e5%a2%9e%e5%bc%ba%e7%94%9f%e6%88%90rag>#</a></h3><ul><li>向量数据库（Faiss、Pinecone、Chroma）</li><li>文档分块与嵌入策略（滑动窗口、语义分块）</li><li>检索优化（重排序、HyDE技术）</li></ul><hr><h3 id=部署与运维><strong>部署与运维</strong>
<a class=anchor href=#%e9%83%a8%e7%bd%b2%e4%b8%8e%e8%bf%90%e7%bb%b4>#</a></h3><ul><li>容器化（Docker、Kubernetes）</li><li>模型部署（ONNX、TensorRT、Triton Inference Server）</li><li>监控与日志（Prometheus、Grafana）</li><li>云服务（AWS SageMaker、GCP Vertex AI）</li></ul><hr><h2 id=模型训练与优化><strong>模型训练与优化</strong>
<a class=anchor href=#%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83%e4%b8%8e%e4%bc%98%e5%8c%96>#</a></h2><hr><h3 id=分布式训练><strong>分布式训练</strong>
<a class=anchor href=#%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83>#</a></h3><ul><li>数据并行 vs 模型并行</li><li>框架（DeepSpeed、PyTorch DDP、FSDP）</li><li>混合精度训练（FP16、BF16）</li></ul><hr><h3 id=高效微调技术><strong>高效微调技术</strong>
<a class=anchor href=#%e9%ab%98%e6%95%88%e5%be%ae%e8%b0%83%e6%8a%80%e6%9c%af>#</a></h3><ul><li>参数高效微调（LoRA、Adapter、Prefix Tuning）</li><li>低资源训练（QLoRA + 4-bit量化）</li><li>指令微调与对齐（RLHF、DPO）</li></ul><hr><h3 id=模型评估与调优><strong>模型评估与调优</strong>
<a class=anchor href=#%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0%e4%b8%8e%e8%b0%83%e4%bc%98>#</a></h3><ul><li>评估基准（GLUE、SuperGLUE、HELM）</li><li>超参数搜索（Optuna、Ray Tune）</li><li>可解释性（Attention可视化、LIME）</li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#基础理论><strong>基础理论</strong></a><ul><li><a href=#数学与统计学><strong>数学与统计学</strong></a></li><li><a href=#机器学习基础><strong>机器学习基础</strong></a></li><li><a href=#深度学习基础><strong>深度学习基础</strong></a></li></ul></li><li><a href=#llm-核心技术><strong>LLM 核心技术</strong></a><ul><li><a href=#llm-基础组件><strong>LLM 基础组件</strong></a></li><li><a href=#大语言模型llm><strong>大语言模型（LLM）</strong></a></li><li><a href=#工具与框架><strong>工具与框架</strong></a></li></ul></li><li><a href=#应用开发与工程化><strong>应用开发与工程化</strong></a><ul><li><a href=#应用开发框架><strong>应用开发框架</strong></a></li><li><a href=#检索增强生成rag><strong>检索增强生成（RAG）</strong></a></li><li><a href=#部署与运维><strong>部署与运维</strong></a></li></ul></li><li><a href=#模型训练与优化><strong>模型训练与优化</strong></a><ul><li><a href=#分布式训练><strong>分布式训练</strong></a></li><li><a href=#高效微调技术><strong>高效微调技术</strong></a></li><li><a href=#模型评估与调优><strong>模型评估与调优</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>