<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='
  面试准备大纲
  #



  基础理论
  #



  数学与统计学
  #


线性代数（矩阵运算、特征值分解）
概率论与统计（贝叶斯定理、分布、假设检验）
优化方法（梯度下降、Adam、学习率调度）



  机器学习基础
  #


监督学习（线性回归、决策树、SVM、集成学习）
无监督学习（聚类、PCA、降维）
评估指标（准确率、召回率、F1、AUC-ROC）
过拟合与正则化（L1/L2、Dropout）



  深度学习基础
  #


神经网络（前向传播、反向传播、激活函数）
CNN（图像分类）、RNN/LSTM（序列建模）
注意力机制与Transformer架构（Self-Attention、多头注意力）



  NLP与LLM核心技术
  #



  经典NLP技术
  #


⭐ Tokenization - 分词（BPE, WordPiece, SentencePiece）
  
⚠️ 什么是 Tokenization？为什么它对LLM至关重要？
  
    什么是 Tokenization？为什么它对LLM至关重要？
Tokenization（分词）是将文本拆分成更小的单元（tokens）的过程，这些单元可以是单词、子词或字符。在自然语言处理中，分词是文本预处理的重要步骤。他直接影响模型对语义的理解能力、计算效率和内存占用。

Example：句子 “Hello, world!” 可被切分为 ["Hello", ",", "world", "!"]（基于空格和标点）。

  

⚠️ 常见的 Tokenization 方法有哪些？它们的区别是什么？
  
    
  常见的 Tokenization 方法有哪些？它们的区别是什么
  #


Word-level：按词切分（如 “natural language processing” → ["natural", "language", "processing"]），但词表大且难以处理未登录词（OOV）。
Subword-level（主流方法）：

BPE（Byte-Pair Encoding）：通过合并高频字符对生成子词（如GPT系列使用）。
WordPiece：类似BPE，但基于概率合并（如BERT使用）。
SentencePiece：无需预分词，直接处理原始文本（如T5使用）。


Character-level：按字符切分，词表极小但序列长且语义建模困难。

  

⚠️ BPE算法的工作原理是什么？请举例说明。
  
    
  BPE 算法的工作原理是什么？请举例说明。
  #

BPE（Byte Pair Encoding）是一种子词分割（Subword Segmentation）算法，核心思想是基于 统计频率 的合并（Merge frequent pairs）。'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/others/interview-preparation-guide/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Interview Preparation Guide"><meta property="og:description" content='面试准备大纲 # 基础理论 # 数学与统计学 # 线性代数（矩阵运算、特征值分解） 概率论与统计（贝叶斯定理、分布、假设检验） 优化方法（梯度下降、Adam、学习率调度） 机器学习基础 # 监督学习（线性回归、决策树、SVM、集成学习） 无监督学习（聚类、PCA、降维） 评估指标（准确率、召回率、F1、AUC-ROC） 过拟合与正则化（L1/L2、Dropout） 深度学习基础 # 神经网络（前向传播、反向传播、激活函数） CNN（图像分类）、RNN/LSTM（序列建模） 注意力机制与Transformer架构（Self-Attention、多头注意力） NLP与LLM核心技术 # 经典NLP技术 # ⭐ Tokenization - 分词（BPE, WordPiece, SentencePiece） ⚠️ 什么是 Tokenization？为什么它对LLM至关重要？ 什么是 Tokenization？为什么它对LLM至关重要？ Tokenization（分词）是将文本拆分成更小的单元（tokens）的过程，这些单元可以是单词、子词或字符。在自然语言处理中，分词是文本预处理的重要步骤。他直接影响模型对语义的理解能力、计算效率和内存占用。
Example：句子 “Hello, world!” 可被切分为 ["Hello", ",", "world", "!"]（基于空格和标点）。 ⚠️ 常见的 Tokenization 方法有哪些？它们的区别是什么？ 常见的 Tokenization 方法有哪些？它们的区别是什么 # Word-level：按词切分（如 “natural language processing” → ["natural", "language", "processing"]），但词表大且难以处理未登录词（OOV）。 Subword-level（主流方法）： BPE（Byte-Pair Encoding）：通过合并高频字符对生成子词（如GPT系列使用）。 WordPiece：类似BPE，但基于概率合并（如BERT使用）。 SentencePiece：无需预分词，直接处理原始文本（如T5使用）。 Character-level：按字符切分，词表极小但序列长且语义建模困难。 ⚠️ BPE算法的工作原理是什么？请举例说明。 BPE 算法的工作原理是什么？请举例说明。 # BPE（Byte Pair Encoding）是一种子词分割（Subword Segmentation）算法，核心思想是基于 统计频率 的合并（Merge frequent pairs）。'><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Interview Preparation Guide | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/others/interview-preparation-guide/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.cf556a4e8f68b45904ee5dbea91142cf9715658dc3de0b0795a7146b06d7e9bb.js integrity="sha256-z1VqTo9otFkE7l2+qRFCz5cVZY3D3gsHlacUawbX6bs=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/others/interview-preparation-guide/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><input type=checkbox id=section-d0dd931d60033c220ecd4cd60b7c9170 class=toggle>
<label for=section-d0dd931d60033c220ecd4cd60b7c9170 class="flex justify-between"><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a></label><ul><li><a href=/docs/deep-learning/convolutional-neural-networks/modern-convolutional-neural-networks/>Modern Convolutional Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-a3019bfa8037cc33ed6405d1589b6219 class=toggle>
<label for=section-a3019bfa8037cc33ed6405d1589b6219 class="flex justify-between"><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a></label><ul><li><a href=/docs/deep-learning/recurrent-neural-networks/modern-recurrent-neural-networks/>Modern Recurrent Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-0a43584c16258b228ae9aa8d70efc320 class=toggle>
<label for=section-0a43584c16258b228ae9aa8d70efc320 class="flex justify-between"><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a></label><ul><li><a href=/docs/deep-learning/attention-and-transformers/large-scale-pretraining-with-transformers/>Large-Scale Pretraining with Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/modern-large-language-models-copy/>Modern Large Language Models</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92e8358c45c96009753cf4227e9daea8 class=toggle>
<label for=section-92e8358c45c96009753cf4227e9daea8 class="flex justify-between"><a href=/docs/deep-learning/llm-pipelines/>LLM Pipelines</a></label><ul><li><a href=/docs/deep-learning/llm-pipelines/llm-inference-and-deployment/>LLM Inference and Deployment</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle checked>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul><li><a href=/docs/others/interview-preparation-guide/ class=active>Interview Preparation Guide</a><ul></ul></li></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Interview Preparation Guide</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#基础理论><strong>基础理论</strong></a><ul><li><a href=#数学与统计学><strong>数学与统计学</strong></a></li><li><a href=#机器学习基础><strong>机器学习基础</strong></a></li><li><a href=#深度学习基础><strong>深度学习基础</strong></a></li></ul></li><li><a href=#nlp与llm核心技术><strong>NLP与LLM核心技术</strong></a><ul><li><a href=#经典nlp技术><strong>经典NLP技术</strong></a></li><li><a href=#大语言模型llm><strong>大语言模型（LLM）</strong></a></li><li><a href=#工具与框架><strong>工具与框架</strong></a></li></ul></li><li><a href=#应用开发与工程化><strong>应用开发与工程化</strong></a><ul><li><a href=#应用开发框架><strong>应用开发框架</strong></a></li><li><a href=#检索增强生成rag><strong>检索增强生成（RAG）</strong></a></li><li><a href=#部署与运维><strong>部署与运维</strong></a></li></ul></li><li><a href=#模型训练与优化><strong>模型训练与优化</strong></a><ul><li><a href=#分布式训练><strong>分布式训练</strong></a></li><li><a href=#高效微调技术><strong>高效微调技术</strong></a></li><li><a href=#模型评估与调优><strong>模型评估与调优</strong></a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=面试准备大纲><strong>面试准备大纲</strong>
<a class=anchor href=#%e9%9d%a2%e8%af%95%e5%87%86%e5%a4%87%e5%a4%a7%e7%ba%b2>#</a></h1><hr><h2 id=基础理论><strong>基础理论</strong>
<a class=anchor href=#%e5%9f%ba%e7%a1%80%e7%90%86%e8%ae%ba>#</a></h2><hr><h3 id=数学与统计学><strong>数学与统计学</strong>
<a class=anchor href=#%e6%95%b0%e5%ad%a6%e4%b8%8e%e7%bb%9f%e8%ae%a1%e5%ad%a6>#</a></h3><ul><li>线性代数（矩阵运算、特征值分解）</li><li>概率论与统计（贝叶斯定理、分布、假设检验）</li><li>优化方法（梯度下降、Adam、学习率调度）</li></ul><hr><h3 id=机器学习基础><strong>机器学习基础</strong>
<a class=anchor href=#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80>#</a></h3><ul><li>监督学习（线性回归、决策树、SVM、集成学习）</li><li>无监督学习（聚类、PCA、降维）</li><li>评估指标（准确率、召回率、F1、AUC-ROC）</li><li>过拟合与正则化（L1/L2、Dropout）</li></ul><hr><h3 id=深度学习基础><strong>深度学习基础</strong>
<a class=anchor href=#%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80>#</a></h3><ul><li>神经网络（前向传播、反向传播、激活函数）</li><li>CNN（图像分类）、RNN/LSTM（序列建模）</li><li>注意力机制与Transformer架构（Self-Attention、多头注意力）</li></ul><hr><h2 id=nlp与llm核心技术><strong>NLP与LLM核心技术</strong>
<a class=anchor href=#nlp%e4%b8%8ellm%e6%a0%b8%e5%bf%83%e6%8a%80%e6%9c%af>#</a></h2><hr><h3 id=经典nlp技术><strong>经典NLP技术</strong>
<a class=anchor href=#%e7%bb%8f%e5%85%b8nlp%e6%8a%80%e6%9c%af>#</a></h3><hr><details><summary><strong class=custom-details-title>⭐ Tokenization - 分词（BPE, WordPiece, SentencePiece）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⚠️ 什么是 Tokenization？为什么它对LLM至关重要？</strong></summary><div class=markdown-inner><h2><b>什么是 Tokenization？为什么它对LLM至关重要？</b></h2><p>Tokenization（分词）是将文本拆分成更小的单元（tokens）的过程，这些单元可以是单词、子词或字符。在自然语言处理中，分词是文本预处理的重要步骤。他直接影响模型对语义的理解能力、计算效率和内存占用。</p><ul><li><strong>Example</strong>：句子 “Hello, world!” 可被切分为 <code>["Hello", ",", "world", "!"]</code>（基于空格和标点）。</li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ 常见的 Tokenization 方法有哪些？它们的区别是什么？</strong></summary><div class=markdown-inner><h2 id=常见的-tokenization-方法有哪些它们的区别是什么><strong>常见的 Tokenization 方法有哪些？它们的区别是什么</strong>
<a class=anchor href=#%e5%b8%b8%e8%a7%81%e7%9a%84-tokenization-%e6%96%b9%e6%b3%95%e6%9c%89%e5%93%aa%e4%ba%9b%e5%ae%83%e4%bb%ac%e7%9a%84%e5%8c%ba%e5%88%ab%e6%98%af%e4%bb%80%e4%b9%88>#</a></h2><ul><li><strong>Word-level</strong>：按词切分（如 <code>“natural language processing” → ["natural", "language", "processing"]</code>），但词表大且难以处理未登录词（OOV）。</li><li><strong>Subword-level（主流方法）</strong>：<ul><li><strong>BPE（Byte-Pair Encoding）</strong>：通过合并高频字符对生成子词（如GPT系列使用）。</li><li><strong>WordPiece</strong>：类似BPE，但基于概率合并（如BERT使用）。</li><li><strong>SentencePiece</strong>：无需预分词，直接处理原始文本（如T5使用）。</li></ul></li><li><strong>Character-level</strong>：按字符切分，词表极小但序列长且语义建模困难。</li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ BPE算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2 id=bpe-算法的工作原理是什么请举例说明><strong>BPE 算法的工作原理是什么？请举例说明。</strong>
<a class=anchor href=#bpe-%e7%ae%97%e6%b3%95%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86%e6%98%af%e4%bb%80%e4%b9%88%e8%af%b7%e4%b8%be%e4%be%8b%e8%af%b4%e6%98%8e>#</a></h2><p>BPE（Byte Pair Encoding）是一种子词分割（Subword Segmentation）算法，核心思想是基于 <strong>统计频率</strong> 的合并（Merge frequent pairs）。</p><ul><li><strong>工作原理</strong>：<ol><li>统计字符对（Byte Pair）频率，找到最常见的相邻字符对。<pre tabindex=0><code>[&#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34; &#34;, &#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34;e&#34;, &#34;r&#34;, &#34; &#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;]
</code></pre></li><li>合并最频繁的字符对，形成新的子词单元。<pre tabindex=0><code>(&#34;l&#34;, &#34;o&#34;) -&gt; 2次
(&#34;o&#34;, &#34;w&#34;) -&gt; 2次
    ...
</code></pre>（第一次）合并 <code>("l", "o") → "lo"</code>：<pre tabindex=0><code>[&#34;lo&#34;, &#34;w&#34;, &#34;lo&#34;, &#34;w&#34;, &#34;er&#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;st&#34;]
</code></pre>（第二次）合并 <code>("lo", "w") → "low"</code><pre tabindex=0><code>[&#34;low&#34;, &#34;low&#34;, &#34;er&#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;st&#34;]
</code></pre></li><li>重复步骤 1 和 2，直到达到预定的子词词汇量。<pre tabindex=0><code>[&#34;low&#34;, &#34;lower&#34;, &#34;er&#34;, &#34;newest&#34;]
</code></pre><pre tabindex=0><code>vocab = {&#34;low&#34;: 100, &#34;lower&#34;: 101, &#34;er&#34;: 102, &#34;newest&#34;: 103}
</code></pre></li></ol></li></ul><p>BPE的优点是它 <strong>能够有效地处理未登录词</strong>，并且在处理长尾词（rare words）时表现良好。比如，词”unhappiness”可以被分解为”un” + “happiness”，而不是完全看作一个新的词。</p></div></details><details><summary><strong class=custom-details-title>⚠️ WordPiece 算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2 id=wordpiece-算法的工作原理是什么请举例说明><strong>WordPiece 算法的工作原理是什么？请举例说明</strong>
<a class=anchor href=#wordpiece-%e7%ae%97%e6%b3%95%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86%e6%98%af%e4%bb%80%e4%b9%88%e8%af%b7%e4%b8%be%e4%be%8b%e8%af%b4%e6%98%8e>#</a></h2><p>WordPiece 的核心理念与BPE相似，但合并策略更注重语义完整性。其训练过程同样从基础单元（如字符）开始，但选择合并的标准并非单纯依赖频率，而是通过 <strong>计算合并后对语言模型概率的提升幅度</strong>，优先保留能够增强语义连贯性的子词。</p><ul><li>假设语言模型的目标是最大化训练数据的似然概率，在 WordPiece 中，简化为 通过子词频率估计概率（即一元语言模型）。通过计算合并后对语言模型概率的提升幅度进行合并：</li></ul><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[
\begin{equation}
P(S)≈ \prod^{n}_{i=1}P(s_i)
\end{equation}
\]</span><ul><li><strong>工作原理</strong>：<ol><li><p>与BPE类似，首先将所有词分解为最小的单位（如字符）。</p><pre tabindex=0><code>[&#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34; &#34;, &#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34;e&#34;, &#34;r&#34;, &#34; &#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;, &#34; &#34;, &#34;w&#34;, &#34;i&#34;, &#34;d&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;]
</code></pre></li><li><p>统计所有可能的字符对（或子词对）在文本中的共现频率。</p></li><li><p>合并字符对，选择合并后能 <strong>最大化语言模型似然概率</strong> 的字符对。具体公式为：选择使 <code>score = freq(pair) / (freq(first) * freq(second)) </code>最大的字符对（<strong>与 BPE 不同，BPE 仅选择频率最高的对</strong>）。每次合并对语言模型概率提升最大的合并组合。</p><p>这里的 <code>##</code> 表示这个 token 只能作为后缀出现，不会单独存在。</p><pre tabindex=0><code>{&#34;low&#34;, &#34;##er&#34;, &#34;##ing&#34;, &#34;new&#34;, &#34;##est&#34;, &#34;wide&#34;, &#34;##st&#34;}
</code></pre></li><li><p>重复合并得分最高的字符对，直到达到预设的词汇表大小。</p><pre tabindex=0><code>vocab = {&#34;low&#34;: 100, &#34;##er&#34;: 101, &#34;##ing&#34;: 102, &#34;new&#34;: 103, &#34;##est&#34;: 104, &#34;wide&#34;: 105, &#34;##st&#34;: 106}
</code></pre></li></ol></li></ul><p>WordPiece 通过最大化语言模型概率合并子词，<strong>生成的子词更贴合语义需求</strong>。但计算复杂度更高，需多次评估合并得分。</p><ul><li>若需模型捕捉深层语义（如预训练任务），优先选择 WordPiece。</li><li>若需快速处理大规模数据且词汇表灵活，BPE 更合适。</li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ SentencePiece 算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2 id=sentencepiece-算法的工作原理是什么请举例说明><strong>SentencePiece 算法的工作原理是什么？请举例说明</strong>
<a class=anchor href=#sentencepiece-%e7%ae%97%e6%b3%95%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86%e6%98%af%e4%bb%80%e4%b9%88%e8%af%b7%e4%b8%be%e4%be%8b%e8%af%b4%e6%98%8e>#</a></h2><p>SentencePiece 是一种无监督的子词分割算法，其核心创新在于直接处理原始文本（包括空格和特殊符号），无需依赖预分词步骤。<strong>它支持两种底层算法：BPE 或 基于概率的Unigram Language Model</strong>。训练时，SentencePiece 将空格视为普通字符 <code>_</code>，可 <strong>直接处理多语言混合文本（如中英文混杂）</strong>，并自动学习跨语言的统一子词划分规则。</p><ul><li><strong>Example：</strong> <code>"Hello世界" → 编码为 ["▁He", "llo", "▁世", "界"]。</code></li></ul><p>SentencePiece 支持多语言（Multilingual）无需调整，统一处理空格与特殊符号。这一特性使其在需要多语言支持的场景（如T5模型）中表现突出，同时简化了数据处理流程，特别适合处理社交媒体文本等非规范化输入。</p></div></details><details><summary><strong class=custom-details-title>⚠️ 如何处理未登录词（OOV）？</strong></summary><div class=markdown-inner><h2><b>如何处理未登录词（OOV）？</b></h2><ul><li><strong>子词切分</strong>：将OOV（Out-of-Vocabulary）词拆分为已知子词（如 <code>“tokenization” → ["token", "ization"]</code>）。</li><li><strong>回退策略</strong>：使用特殊标记（如 <code>[UNK]</code>），但会损失信息。</li><li><strong>动态更新词表</strong>：在增量训练时扩展词表。</li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ Tokenization 如何影响模型性能？</strong></summary><div class=markdown-inner><h2><b>Tokenization 如何影响模型性能？</b></h2><ul><li><strong>词表过大</strong>：增加内存消耗，降低计算效率（Softmax 计算成本高）。</li><li><strong>词表过小</strong>：导致长序列和语义碎片化（如切分为无意义的子词）。</li><li><strong>语言适配性</strong>：中/日/韩等非空格语言需要特殊处理（如 SentencePiece）。</li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ 如何为多语言模型设计Tokenization方案？</strong></summary><div class=markdown-inner><h2><b>如何为多语言模型设计 Tokenization 方案？</b></h2><ul><li><strong>统一词表</strong>：使用 SentencePiece 跨语言训练（如mBERT）。</li><li><strong>平衡语种覆盖</strong>：根据语种数据量调整合并规则，避免小语种被淹没。</li><li><strong>特殊标记</strong>：添加语言ID（如 <code>[EN]</code>、<code>[ZH]</code>）引导模型区分语言。</li></ul></div></details></div></details><details><summary><strong class=custom-details-title>⭐ Word Embeddings - 词嵌入（Word2Vec、GloVe、FastText）</strong></summary><div class=markdown-inner><details><summary><strong class=custom-details-title>⚠️ 什么是词嵌入（Word Embeddings）？为什么它重要？</strong></summary><div class=markdown-inner><h2><b>什么是词嵌入（Word Embeddings）？为什么它重要？</b></h2><p>在自然语言处理中，Embedding（词嵌入）是将 <strong>离散的文本数据（如单词、短语或句子）映射到一个连续的、低维度的向量空间（vector space）的过程</strong>。他的作用包括：</p><ol><li><strong>捕捉语义关系（Semantic Relationships）</strong>：能表示同义词、类比关系（如 king - man + woman ≈ queen）。</li><li><strong>降维（Dimensionality Reduction）</strong>：将高维的 <strong>独热编码（One-hot Encoding）</strong> 转换为低维密集向量，提高计算效率。</li><li><strong>解决稀疏性问题（Handling Sparsity）</strong>：相比于独热编码，词嵌入具有更好的泛化能力（Generalization）。</li></ol></div></details><details><summary><strong class=custom-details-title>⚠️ 静态词向量 和 上下文动态词向量的区别？</strong></summary><div class=markdown-inner><h2><b>静态词向量 和 上下文动态词向量的区别？</b></h2><ul><li><strong>静态词向量（Static Word Embeddings）</strong> 的核心特点是 <strong>无论词语出现在何种上下文中，其向量表示均保持不变</strong>。这类方法通过大规模语料训练，捕捉词语间的语义和语法关系。静态词向量的优势在于训练高效、资源消耗低，且生成的向量可直观反映语义相似性（如“猫”和“狗”向量接近）；但其 <strong>局限性是无法处理多义词</strong>（如“苹果”在“水果”和“手机”场景中的不同含义），因为 <strong>每个词仅对应单一向量</strong>。代表模型包括：Word2Vec, GloVe。</li><li><strong>上下文动态词向量（Contextual Word Embeddings）</strong>：静态嵌入虽然可以表示词语的语义，但它们无法根据上下文动态调整，例如 “bank” 在 “river bank” 和 “bank account” 里的含义不同。而 动态词嵌入 解决了这个问题，代表性模型包括 ELMo、BERT 和 GPT。</li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ 解释 Word2Vec 的两种模型：Skip-gram 和 CBOW。</strong></summary><div class=markdown-inner><h2><b>解释 Word2Vec 的两种模型：Skip-gram 和 CBOW。</b></h2><ul><li><p><strong>跳元模型（Skip-gram）</strong> 假设 一个词可以用来在文本序列中生成其周围的单词。以文本序列 <code>“the”“man”“loves”“his”“son”</code> 为例。假设中心词选择 <code>“loves”</code>，并将上下文窗口设置为2，给定中心词 <code>“loves”</code>，跳元模型考虑生成上下文词 <code>“the”“man”“him”“son”</code> 的条件概率。最大化给定中心词时上下文词的条件概率：
$$
max{\sum log P(context_w|center_w)}
$$
Word2Vec 的核心是 <strong>一个浅层神经网络（Shallow Neural Network）</strong>，由一个输入层、一个隐藏层（线性变换层）、一个输出层（Softmax 或其他采样方法）组成:</p><ol><li><strong>输入示例</strong>：<ul><li>句子：<code>“I love natural language processing.”</code></li><li>若窗口大小为1，中心词为 <code>“natural”</code>，则上下文词为 <code>“love”</code> 和 <code>“language”</code>。</li></ul></li><li><strong>输入通过 One-Hot 编码 表示为一个稀疏向量</strong>。例如，若词汇表为 <code>["cat", "dog", "fish"]</code>，则<code>“dog”</code> 的输入编码为 <code>[0, 1, 0]</code>。</li><li><strong>输入层到隐藏层</strong>：输入向量与 输入权重矩阵相乘，得到中心词的嵌入向量。</li><li>通过 <strong>输出权重矩阵将隐层向量映射到输出概率</strong>：</li><li>使用 Softmax 归一化，通过梯度下降优化词向量，使得上下文词的概率最大化。</li></ol><p><strong>跳元模型（Skip-gram）特点</strong>：</p><ul><li><strong>擅长捕捉低频词</strong>：通过中心词预测多个上下文，低频词有更多训练机会。</li><li><strong>训练速度较慢</strong>：输出层需计算多个上下文词的概率。</li></ul></li></ul><hr><ul><li><p><strong>连续词袋（CBOW）</strong> 与 Skip-gram 相对，CBOW 的训练过程是 <strong>给定上下文词，预测中心词</strong>。这里的目标是将多个上下文词的向量平均起来，并通过它们来预测中心词。在文本序列 <code>“the”“man”“loves”“his”“son”</code> 中，在 <code>“loves”</code> 为中心词且上下文窗口为 2 的情况下，连续词袋模型考虑基于上下文词 <code>“the”“man”“him”“son”</code> 生成中心词 <code>“loves”</code> 的条件概率。最大化给定上下文时中心词的条件概率：
$$
max{\sum log P(center_w|context_w)}
$$
连续词袋（CBOW）的训练细节与 跳元模型（Skip-Gram）大部分类似，但是在输入 One-Hot 编码表示时，跳元模型（Skip-Gram）将中心词进行 One-Hot 编码，<strong>而连续词袋（CBOW）将上下文词进行 One-Hot 编码并取平均值</strong>。 例如，中心词为 <code>“natural”</code>，上下文词为 <code>“love”</code> 和 <code>“language”</code> 。输入为 <code>[0, 1, 0, 0, 0]</code>（<code>“love”</code>）和 <code>[0, 0, 0, 1, 0]</code>（<code>“language”</code>）的平均向量 <code>[0, 0.5, 0, 0.5, 0]</code>。此外，输出概率通过 Softmax 计算公式也有不同。</p><p><strong>连续词袋（CBOW）特点</strong>：</p><ul><li><strong>训练速度快</strong>：输入为多个词的均值向量，计算效率高。</li><li><strong>对高频词建模更好</strong>：上下文词共同贡献中心词预测。</li></ul></li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ Word2Vec 如何优化训练效率？</strong></summary><div class=markdown-inner><h2><b>Word2Vec 如何优化训练效率？</b></h2><p>由于 softmax 操作的性质，上下文词可以是词表中的任意项，但是，在一个词典上（通常有几十万或数百万个单词）<strong>求和的梯度的计算成本是巨大的</strong>！为了降低计算复杂度，可以采用两种近似训练方法：负采样（Negative Sampling）和层序softmax（Hierarchical Softmax）。</p><ul><li><p><strong>负采样（Negative Sampling）</strong>：</p><ul><li><strong>核心思想</strong>：将复杂的多分类问题（预测所有词的概率）简化为二分类问题，用少量负样本近似全词汇的Softmax计算。</li><li><strong>正负样本构建</strong>：<ul><li>对每个正样本（中心词与真实上下文词对），随机采样 K 个负样本（非上下文词）。</li><li>例如，中心词 <code>“apple”</code> 的真实上下文词为 <code>“fruit”</code>，则负样本可能是随机选择的 <code>“car”</code>,<code>“book”</code> 等无关词。</li></ul></li><li><strong>目标函数</strong>：最大化正样本对的相似度，同时最小化负样本对的相似度。</li></ul></li><li><p><strong>层序softmax（Hierarchical Softmax）</strong>：</p><ul><li><strong>核心思想</strong>：通过二叉树（如霍夫曼树）编码词汇表，将全局Softmax分解为路径上的二分类概率乘积，减少计算量。</li><li><strong>霍夫曼树构建</strong>：按词频从高到低排列词汇，高频词靠近根节点，形成最短路径。每个内部节点含一个可训练的向量参数。</li></ul></li></ul></div></details><details><summary><strong class=custom-details-title>⚠️ 解释 GloVe 的原理？</strong></summary><div class=markdown-inner><h2><b>解释 GloVe 的原理？</b></h2><p>GloVe（Global Vectors for Word Representation）是一种基于全局统计信息的词向量训练方法，它通过构造整个语料库的共现矩阵（co-occurrence matrix）并进行矩阵分解来学习词的向量表示，其核心思想是 <strong>通过捕捉词与词之间的全局共现关系来学习语义信息</strong>，而不是像 Word2Vec 那样依赖于局部上下文窗口的预测方法。
具体而言，GloVe 首先统计语料库中的词对共现次数，构建一个共现矩阵 X ，其中 X_{ij} 表示词 i 在词 j 附近出现的频率，并计算共现概率:</p><span>\[
P(j|i) = \frac{X_{ij}}{\sum_k X_{ik}}
\]</span><p>GloVe 的核心目标是学习一个词向量映射，使得向量之间的点积能够近似这个共现概率的对数：</p><span>\[
\mathbf{u}_j^\top \mathbf{v}_i + b_i + c_j = \log(X_{ij})
\]</span><ul><li>v_i, u_j 是词 i 和词 j 的向量表示，<strong>每个词都由两个向量组成，一个是中心词向量 ，一个是上下文词向量</strong></li></ul><p>GloVe 并不依赖传统的神经网络，它的学习过程 <strong>更接近矩阵分解（Matrix Factorization）的优化方法</strong>，而非 Word2Vec 这样的前馈神经网络（Feedforward Neural Network）。GloVe <strong>不依赖反向传播（Backpropagation）</strong>，而是直接最小化共现概率对数的加权平方误差，来学习词向量。</p><p>GloVe 的优势在于它直接建模了全局的词共现信息，使得词向量能够更好地捕捉语义相似性和类比关系，如 <code>“king - man + woman ≈ queen”</code>，但由于依赖于整个共现矩阵，计算和存储成本较高，因此适用于大规模离线训练，而不是在线学习任务。</p></div></details><details><summary><strong class=custom-details-title>⚠️ 解释 FastText 的原理？</strong></summary><div class=markdown-inner><h2><b>解释 FastText 的原理？</b></h2><p>FastText 在 Word2Vec 的基础上进行了改进，能够更好地处理 OOV（Out-of-Vocabulary）问题，同时提高计算效率。FastText 的核心思想是 <strong>使用 n-gram 字符级子词（subword） 进行单词表示</strong>，而不是仅仅依赖于整个单词的词向量。它的训练过程类似于 Word2Vec 的 CBOW（Continuous Bag of Words）或 Skip-gram 模型，但 FastText 通过将一个单词拆分成多个 n-gram 片段（如 <code>“apple”</code> 可以被拆分为 <code>&lt;ap, app, ppl, ple, le></code>），然后通过这些 n-gram 子词的向量求和来表示整个单词，从而在处理未见单词时依然可以通过其子词获得较好的表示。</p></div></details><details><summary><strong class=custom-details-title>⚠️ 在实际项目中，如何选择不同的嵌入方法（Embedding Methods）？</strong></summary><div class=markdown-inner><h2><b>在实际项目中，如何选择不同的嵌入方法（Embedding Methods）？</b></h2><ol><li><strong>任务类型与语义需求</strong>：</li></ol><ul><li><strong>基础语义任务（如文本分类、简单相似度计算）</strong>：<ul><li>静态嵌入：Word2Vec、GloVe、FastText。<ul><li>优点：轻量高效，适合低资源场景。</li><li>示例：新闻分类任务中，预训练的 Word2Vec 向量足以捕捉主题关键词的语义。</li></ul></li></ul></li><li><strong>复杂语义任务（如问答、指代消解、多义词理解）</strong>：<ul><li>上下文嵌入：BERT、RoBERTa、XLNet。<ul><li>优点：动态生成上下文相关向量，解决一词多义。</li><li>示例：在医疗问答系统中，BERT可区分“Apple”指公司还是水果。</li></ul></li></ul></li></ul><ol start=2><li><strong>数据量与领域适配</strong>：</li></ol><ul><li><strong>小数据场景</strong>：<ul><li><strong>预训练静态嵌入 + 微调</strong>：使用公开预训练的 Word2Vec/GloVe，结合任务数据微调。</li><li><strong>轻量上下文模型</strong>：ALBERT或TinyBERT，降低训练成本。</li></ul></li><li><strong>大数据场景</strong>：<ul><li><strong>从头训练上下文模型</strong>：基于领域数据训练BERT或GPT，捕捉领域专属语义。</li><li><strong>领域适配</strong>：在金融/法律等领域，使用领域语料继续预训练（Domain-Adaptive Pretraining）。</li></ul></li></ul></div></details></div></details><ul><li><strong>词嵌入（Word2Vec、GloVe、FastText）</strong></li><li>序列模型（BiLSTM、CRF、Seq2Seq）</li><li>预训练模型（BERT、GPT、T5的架构差异）</li></ul><hr><h3 id=大语言模型llm><strong>大语言模型（LLM）</strong>
<a class=anchor href=#%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8bllm>#</a></h3><ul><li>模型架构（Decoder-only vs Encoder-Decoder）</li><li>预训练任务（MLM、CLM、Span Corruption）</li><li>微调技术（全参数微调、LoRA、QLoRA、Prompt Tuning）</li><li>推理优化（KV Cache、量化、模型剪枝）</li></ul><hr><h3 id=工具与框架><strong>工具与框架</strong>
<a class=anchor href=#%e5%b7%a5%e5%85%b7%e4%b8%8e%e6%a1%86%e6%9e%b6>#</a></h3><ul><li>Hugging Face Transformers库（模型加载、Pipeline、Trainer）</li><li>LangChain（Agent、Chain、Tools设计）</li><li>LlamaIndex（文档索引、检索增强生成RAG）</li></ul><hr><h2 id=应用开发与工程化><strong>应用开发与工程化</strong>
<a class=anchor href=#%e5%ba%94%e7%94%a8%e5%bc%80%e5%8f%91%e4%b8%8e%e5%b7%a5%e7%a8%8b%e5%8c%96>#</a></h2><hr><h3 id=应用开发框架><strong>应用开发框架</strong>
<a class=anchor href=#%e5%ba%94%e7%94%a8%e5%bc%80%e5%8f%91%e6%a1%86%e6%9e%b6>#</a></h3><ul><li>LangChain高级用法（自定义Tools、Agent逻辑、流式输出）</li><li>前端集成（Streamlit、Gradio构建交互界面）</li><li>API开发（FastAPI/Flask构建RESTful服务）</li></ul><hr><h3 id=检索增强生成rag><strong>检索增强生成（RAG）</strong>
<a class=anchor href=#%e6%a3%80%e7%b4%a2%e5%a2%9e%e5%bc%ba%e7%94%9f%e6%88%90rag>#</a></h3><ul><li>向量数据库（Faiss、Pinecone、Chroma）</li><li>文档分块与嵌入策略（滑动窗口、语义分块）</li><li>检索优化（重排序、HyDE技术）</li></ul><hr><h3 id=部署与运维><strong>部署与运维</strong>
<a class=anchor href=#%e9%83%a8%e7%bd%b2%e4%b8%8e%e8%bf%90%e7%bb%b4>#</a></h3><ul><li>容器化（Docker、Kubernetes）</li><li>模型部署（ONNX、TensorRT、Triton Inference Server）</li><li>监控与日志（Prometheus、Grafana）</li><li>云服务（AWS SageMaker、GCP Vertex AI）</li></ul><hr><h2 id=模型训练与优化><strong>模型训练与优化</strong>
<a class=anchor href=#%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83%e4%b8%8e%e4%bc%98%e5%8c%96>#</a></h2><hr><h3 id=分布式训练><strong>分布式训练</strong>
<a class=anchor href=#%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83>#</a></h3><ul><li>数据并行 vs 模型并行</li><li>框架（DeepSpeed、PyTorch DDP、FSDP）</li><li>混合精度训练（FP16、BF16）</li></ul><hr><h3 id=高效微调技术><strong>高效微调技术</strong>
<a class=anchor href=#%e9%ab%98%e6%95%88%e5%be%ae%e8%b0%83%e6%8a%80%e6%9c%af>#</a></h3><ul><li>参数高效微调（LoRA、Adapter、Prefix Tuning）</li><li>低资源训练（QLoRA + 4-bit量化）</li><li>指令微调与对齐（RLHF、DPO）</li></ul><hr><h3 id=模型评估与调优><strong>模型评估与调优</strong>
<a class=anchor href=#%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0%e4%b8%8e%e8%b0%83%e4%bc%98>#</a></h3><ul><li>评估基准（GLUE、SuperGLUE、HELM）</li><li>超参数搜索（Optuna、Ray Tune）</li><li>可解释性（Attention可视化、LIME）</li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#基础理论><strong>基础理论</strong></a><ul><li><a href=#数学与统计学><strong>数学与统计学</strong></a></li><li><a href=#机器学习基础><strong>机器学习基础</strong></a></li><li><a href=#深度学习基础><strong>深度学习基础</strong></a></li></ul></li><li><a href=#nlp与llm核心技术><strong>NLP与LLM核心技术</strong></a><ul><li><a href=#经典nlp技术><strong>经典NLP技术</strong></a></li><li><a href=#大语言模型llm><strong>大语言模型（LLM）</strong></a></li><li><a href=#工具与框架><strong>工具与框架</strong></a></li></ul></li><li><a href=#应用开发与工程化><strong>应用开发与工程化</strong></a><ul><li><a href=#应用开发框架><strong>应用开发框架</strong></a></li><li><a href=#检索增强生成rag><strong>检索增强生成（RAG）</strong></a></li><li><a href=#部署与运维><strong>部署与运维</strong></a></li></ul></li><li><a href=#模型训练与优化><strong>模型训练与优化</strong></a><ul><li><a href=#分布式训练><strong>分布式训练</strong></a></li><li><a href=#高效微调技术><strong>高效微调技术</strong></a></li><li><a href=#模型评估与调优><strong>模型评估与调优</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>