<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  LLM 模型大小估算与硬件选择（LLM Hardware and Model Size）
  #



  模型大小估算
  #


Note：什么是字节（Byte）？
字节（Byte） 是计算机中数据存储的基本单位之一。一个字节等于 8 位（bit）。字节用于表示一个字符或者基本的数据单元，在不同的数据类型中，它代表了数据的存储容量。

FP32（32-bit Floating Point）：标准的浮点数表示方法，广泛用于训练深度学习模型，尤其是在需要高精度的计算中。占用字节数：4 字节。
FP16（16-bit Floating Point）：通常用于加速推理和减少内存占用，特别是在现代 GPU（如 NVIDIA 的 Volta 和 Ampere 架构）中，FP16 被广泛应用。占用字节数：2 字节。
INT8（8-bit Integer）：常用于量化模型，将浮点数转换为 8 位整数，以减少内存和计算需求，特别是在边缘设备或移动设备上推理时。占用字节数：1 字节。
Char（Character）：通常用于表示 ASCII 或其他单字节字符编码。占用字节数：1 字节。



Note：字节（Byte）、MB（兆字节）和GB（千兆字节）之间的关系如下

1 字节 (Byte) 是计算机存储数据的基本单位。
1 千字节 (Kilobyte, KB) = 1,024 Byte
1 兆字节 (Megabyte, MB) = 1,024 KB = 1,024 × 1,024 Byte
1 千兆字节 (Gigabyte, GB) = 1,024 MB = 1,024 × 1,024 × 1,024 Byte
1 太字节 (Terabyte, TB) = 1,024 GB = 1,024 × 1,024 × 1,024 × 1,024 Byte



Note：假设一个 LLM 说它是 7B（如 LLaMA-7B），表示它有 7 Billion（70 亿, 7x10^9） 个参数。如果以 FP32 精度储存，则推理（inference）所需要的内存大致为："><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/deep-learning/llm-pipelines/llm-hardware-and-model-size/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="LLM Hardware and Model Size"><meta property="og:description" content="LLM 模型大小估算与硬件选择（LLM Hardware and Model Size） # 模型大小估算 # Note：什么是字节（Byte）？
字节（Byte） 是计算机中数据存储的基本单位之一。一个字节等于 8 位（bit）。字节用于表示一个字符或者基本的数据单元，在不同的数据类型中，它代表了数据的存储容量。
FP32（32-bit Floating Point）：标准的浮点数表示方法，广泛用于训练深度学习模型，尤其是在需要高精度的计算中。占用字节数：4 字节。 FP16（16-bit Floating Point）：通常用于加速推理和减少内存占用，特别是在现代 GPU（如 NVIDIA 的 Volta 和 Ampere 架构）中，FP16 被广泛应用。占用字节数：2 字节。 INT8（8-bit Integer）：常用于量化模型，将浮点数转换为 8 位整数，以减少内存和计算需求，特别是在边缘设备或移动设备上推理时。占用字节数：1 字节。 Char（Character）：通常用于表示 ASCII 或其他单字节字符编码。占用字节数：1 字节。 Note：字节（Byte）、MB（兆字节）和GB（千兆字节）之间的关系如下
1 字节 (Byte) 是计算机存储数据的基本单位。 1 千字节 (Kilobyte, KB) = 1,024 Byte 1 兆字节 (Megabyte, MB) = 1,024 KB = 1,024 × 1,024 Byte 1 千兆字节 (Gigabyte, GB) = 1,024 MB = 1,024 × 1,024 × 1,024 Byte 1 太字节 (Terabyte, TB) = 1,024 GB = 1,024 × 1,024 × 1,024 × 1,024 Byte Note：假设一个 LLM 说它是 7B（如 LLaMA-7B），表示它有 7 Billion（70 亿, 7x10^9） 个参数。如果以 FP32 精度储存，则推理（inference）所需要的内存大致为："><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>LLM Hardware and Model Size | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/deep-learning/llm-pipelines/llm-hardware-and-model-size/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.67f2d5ed31d7f45b86c5326ac415236048345d1a47f18baa0c66f13fc669df9f.js integrity="sha256-Z/LV7THX9FuGxTJqxBUjYEg0XRpH8YuqDGbxP8Zp358=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/deep-learning/llm-pipelines/llm-hardware-and-model-size/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li><li><a href=/docs/machine-learning/computational-performance/>Computational Performance</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><input type=checkbox id=section-d0dd931d60033c220ecd4cd60b7c9170 class=toggle>
<label for=section-d0dd931d60033c220ecd4cd60b7c9170 class="flex justify-between"><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a></label><ul><li><a href=/docs/deep-learning/convolutional-neural-networks/modern-convolutional-neural-networks/>Modern Convolutional Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-a3019bfa8037cc33ed6405d1589b6219 class=toggle>
<label for=section-a3019bfa8037cc33ed6405d1589b6219 class="flex justify-between"><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a></label><ul><li><a href=/docs/deep-learning/recurrent-neural-networks/modern-recurrent-neural-networks/>Modern Recurrent Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-0a43584c16258b228ae9aa8d70efc320 class=toggle>
<label for=section-0a43584c16258b228ae9aa8d70efc320 class="flex justify-between"><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a></label><ul><li><a href=/docs/deep-learning/attention-and-transformers/tokenization-and-word-embeddings/>Tokenization and Word Embeddings</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/transformer-architecture/>Transformer Architecture</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/large-scale-pretraining-with-transformers/>Large-Scale Pretraining with Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/modern-large-language-models/>Modern Large Language Models</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/post-training-large-language-models/>Post-training Large Language Models</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/multimodal-large-language-models/>Multimodal Large Language Models</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92e8358c45c96009753cf4227e9daea8 class=toggle checked>
<label for=section-92e8358c45c96009753cf4227e9daea8 class="flex justify-between"><a href=/docs/deep-learning/llm-pipelines/>LLM Pipelines</a></label><ul><li><a href=/docs/deep-learning/llm-pipelines/llm-hardware-and-model-size/ class=active>LLM Hardware and Model Size</a><ul></ul></li><li><a href=/docs/deep-learning/llm-pipelines/llm-inference-and-deployment/>LLM Inference and Deployment</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul><li><a href=/docs/others/interview-preparation-guide/>Interview Preparation Guide</a><ul></ul></li></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>LLM Hardware and Model Size</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#模型大小估算><strong>模型大小估算</strong></a><ul><li><a href=#训练阶段><strong>训练阶段</strong></a></li><li><a href=#推理阶段><strong>推理阶段</strong></a></li></ul></li><li><a href=#硬件选择><strong>硬件选择</strong></a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=llm-模型大小估算与硬件选择llm-hardware-and-model-size><strong>LLM 模型大小估算与硬件选择（LLM Hardware and Model Size）</strong>
<a class=anchor href=#llm-%e6%a8%a1%e5%9e%8b%e5%a4%a7%e5%b0%8f%e4%bc%b0%e7%ae%97%e4%b8%8e%e7%a1%ac%e4%bb%b6%e9%80%89%e6%8b%a9llm-hardware-and-model-size>#</a></h1><hr><h2 id=模型大小估算><strong>模型大小估算</strong>
<a class=anchor href=#%e6%a8%a1%e5%9e%8b%e5%a4%a7%e5%b0%8f%e4%bc%b0%e7%ae%97>#</a></h2><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>什么是字节（Byte）？</strong></p><p><strong>字节（Byte）</strong> 是计算机中数据存储的基本单位之一。<strong>一个字节等于 8 位（bit）</strong>。字节用于表示一个字符或者基本的数据单元，在不同的数据类型中，它代表了数据的存储容量。</p><ul><li><strong>FP32（32-bit Floating Point）</strong>：标准的浮点数表示方法，广泛用于训练深度学习模型，尤其是在需要高精度的计算中。<strong>占用字节数：4 字节。</strong></li><li><strong>FP16（16-bit Floating Point）</strong>：通常用于加速推理和减少内存占用，特别是在现代 GPU（如 NVIDIA 的 Volta 和 Ampere 架构）中，FP16 被广泛应用。<strong>占用字节数：2 字节。</strong></li><li><strong>INT8（8-bit Integer）</strong>：常用于量化模型，将浮点数转换为 8 位整数，以减少内存和计算需求，特别是在边缘设备或移动设备上推理时。<strong>占用字节数：1 字节。</strong></li><li><strong>Char（Character）</strong>：通常用于表示 ASCII 或其他单字节字符编码。<strong>占用字节数：1 字节。</strong></li></ul></blockquote><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>字节（Byte）、MB（兆字节）和GB（千兆字节）之间的关系如下</strong></p><ul><li>1 字节 (Byte) 是计算机存储数据的基本单位。</li><li>1 千字节 (Kilobyte, KB) = 1,024 Byte</li><li>1 兆字节 (Megabyte, MB) = 1,024 KB = 1,024 × 1,024 Byte</li><li>1 千兆字节 (Gigabyte, GB) = 1,024 MB = 1,024 × 1,024 × 1,024 Byte</li><li>1 太字节 (Terabyte, TB) = 1,024 GB = 1,024 × 1,024 × 1,024 × 1,024 Byte</li></ul></blockquote><blockquote class="book-hint warning"><p><strong>Note</strong>：假设一个 LLM 说它是 7B（如 LLaMA-7B），表示它有 7 Billion（70 亿, 7x10^9） 个参数。如果以 FP32 精度储存，则推理（inference）所需要的内存大致为：</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[
7 \times 10^9 \times 4 Bytes \approx 26 GB \approx 7B \times 4 Bytes = 28 GB
\]</span></blockquote><h3 id=训练阶段><strong>训练阶段</strong>
<a class=anchor href=#%e8%ae%ad%e7%bb%83%e9%98%b6%e6%ae%b5>#</a></h3><p>在估算大型语言模型（LLM）训练阶段的模型大小时，需要综合考虑以下因素及其物理意义：</p><ol><li><p><strong>模型参数（Parameters）</strong>：参数是模型的核心组成部分，直接影响模型的容量和内存占用。LLM的参数主要集中在：</p><ul><li>Transformer层的权重矩阵（自注意力层、前馈网络）</li><li>词嵌入矩阵（输入/输出嵌入）</li></ul><p>对于包含 <span>\(N\)
</span>个Transformer层的模型，参数总量可近似为：</p><span>\[
Params≈12Nd_{model}^2+Vd_{model}
\]</span><ul><li><span>\(d_{model}\)
</span>：隐藏层维度（如4096）</li><li><span>\(V\)
</span>：（如50,000）</li><li><strong>示例</strong>：GPT-3（<span>
\(N=96, d_{model}=12288, V=50257）\)
</span>：
<span>\[
Params=12×96×12288^2+50257×12288≈175亿
\]
</span>若参数用FP16存储（2字节/参数），则175B参数的模型需：
<span>\[
175×10^9×2字节=350 GB
\]</span></li></ul></li></ol><blockquote class="book-hint warning"><p><strong>Note</strong>：像 LLaMA-7B 这样的标注通常指的是 模型的参数量，也就是 <strong>训练好的权重（weights）和偏置（biases） 的总数量</strong>。它不包括 梯度（gradients）、优化器状态（optimizer states）或中间激活值（activations）等训练过程中需要的额外存储。</p></blockquote><ol start=2><li><p><strong>梯度（Gradients）</strong>：反向传播时需保存每个参数的梯度，<strong>其大小与参数数量一致</strong>。</p></li><li><p><strong>优化器状态（Optimizer States）</strong>：优化器（如Adam）需额外保存动量（momentum）和方差（variance），显存占用通常是参数的数倍。并且一般采用精度较高的 FP32 储存。</p><ul><li>例如，Adam优化器的显存需求：
<span>\[
优化器状态=Params×(2×4字节)(FP32存储m和v)
\]</span></li></ul></li><li><p><strong>中间激活值（Activations）</strong>：在 前向传播 和 反向传播 阶段，网络每一层的 <strong>激活值（即每一层的输出）</strong> 都需要存储。尤其是对于大规模模型，中间激活值 会占据大量的显存。</p><ul><li>内存需求：中间激活值的内存需求取决于模型的 <strong>输入数据大小（例如 batch size）</strong> 和每一层的输出维度。每一层的输出通常是 矩阵或张量，这些需要存储在内存中，直到反向传播结束。</li><li>影响因素：影响中间激活值大小的因素有 批大小（batch size） 和 每层的激活维度（例如 Transformer 模型的 <span>\(d_model\)
</span>大小）。</li></ul></li></ol><p><strong>​总训练显存估算可以总结为</strong>：
<span>\[
总显存=参数+梯度+优化器状态+激活值+其他
\]</span></p><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>假设我们有一个 7B参数的模型</strong>，使用 FP32存储参数，训练时使用 Adam优化器（假设使用了动量和平方梯度），批大小为 32，每层输出维度为 d_model=4096。</p><ol><li><strong>参数存储</strong>：</li></ol><span>\[
7B \times 4 \text{字节} = 28 \text{GB}
\]</span><ol start=2><li><strong>梯度存储</strong>：</li></ol><span>\[
7B \times 4 \text{字节} = 28 \text{GB}
\]</span><ol start=3><li><strong>优化器状态（假设Adam优化器）</strong>：</li></ol><span>\[
7B \times 2 \times 4 \text{字节} = 56 \text{GB}
\]</span><ol start=4><li><strong>激活存储</strong>（假设每层大小是 [batch_size, seq_len, d_model]，假设有 48 层和 batch_size = 32，seq_len = 2048）：</li></ol><span>\[
48 \times 32 \times 2048 \times 4096 \times 4 \text{字节} \approx 1.4 TB
\]</span><p><strong>总内存需求（仅计算主要内存需求，不考虑中间优化等）</strong>：</p><span>\[
28 \text{GB} + 28 \text{GB} + 56 \text{GB} + 1.4 \text{TB} = 1.48 \text{TB}
\]</span></blockquote><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>Checkpoint 通常保存以下关键信息，以便后续恢复训练或进行推理（Inference）</strong>。</p><span>\[
Checkpoint 大小 ≈ 模型参数 + 优化器状态 + 额外训练信息
\]</span><p>例如：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>checkpoint <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;model_state&#34;</span>: model<span style=color:#f92672>.</span>state_dict(),
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;optimizer_state&#34;</span>: optimizer<span style=color:#f92672>.</span>state_dict(),
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;scheduler_state&#34;</span>: scheduler<span style=color:#f92672>.</span>state_dict(),
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;epoch&#34;</span>: epoch
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>save(checkpoint, <span style=color:#e6db74>&#34;checkpoint.pth&#34;</span>)
</span></span></code></pre></div></blockquote><h3 id=推理阶段><strong>推理阶段</strong>
<a class=anchor href=#%e6%8e%a8%e7%90%86%e9%98%b6%e6%ae%b5>#</a></h3><p>在 LLM inference（推理） 阶段，估算 模型大小和显存需求 时，主要考虑 参数（parameters）、参数类型（precision）、梯度（gradients）、优化器状态（optimizer state）、中间激活值（activations）。</p><ul><li><strong>模型参数（Parameters）</strong>：参数（weights）是模型的核心部分，它们在推理阶段不更新，仅用于计算。</li><li><strong>中间激活值（Activations）</strong>：在推理（inference）过程中，激活值（activations）只需要保留当前计算所需的部分，而不需要像训练时那样保存所有层的激活值。这是因为推理阶段不需要进行反向传播（backpropagation），所以不需要存储完整的计算图和梯度信息。</li><li><strong>KV-Cache（Key-Value Cache）</strong></li></ul><p><strong>​总推理显存估算可以总结为</strong>：
<span>\[
总显存=参数+激活值+KQ Cache+其他
\]</span></p><p><strong>例如</strong>：</p><table><thead><tr><th><strong>部分</strong></th><th><strong>计算公式</strong></th><th><strong>显存需求</strong></th></tr></thead><tbody><tr><td>参数存储</td><td>7B × 2B</td><td>14GB</td></tr><tr><td>激活值</td><td>2048 × 4096 × 2B</td><td>16MB</td></tr><tr><td>KV-Cache</td><td>2 × 2048 × 32 × 128 × 2B</td><td>16MB</td></tr><tr><td>其他开销</td><td>OS + CUDA 预留</td><td>2GB</td></tr><tr><td><strong>合计</strong></td><td>—</td><td><strong>16GB（接近 RTX 4090 限制）</strong></td></tr></tbody></table><hr><h2 id=硬件选择><strong>硬件选择</strong>
<a class=anchor href=#%e7%a1%ac%e4%bb%b6%e9%80%89%e6%8b%a9>#</a></h2><ul><li><p><strong>硬件基础知识</strong></p><table><thead><tr><th>组件</th><th>作用</th><th>对 LLM 的影响</th></tr></thead><tbody><tr><td><strong>CPU（中央处理器）</strong></td><td>处理系统任务，调度 GPU 计算</td><td>影响数据加载、预处理速度</td></tr><tr><td><strong>GPU（图形处理器）</strong></td><td>执行并行计算，加速矩阵运算</td><td>影响 LLM 训练 & 推理速度</td></tr><tr><td><strong>TPU（张量处理单元）</strong></td><td>Google 专用 AI 计算芯片，比 GPU 更快</td><td>用于 Google Cloud LLM 训练</td></tr><tr><td><strong>RAM（内存）</strong></td><td>存储临时数据（不等于显存）</td><td>数据加载、训练时数据预处理</td></tr><tr><td><strong>VRAM（显存）</strong></td><td>GPU 的专用内存，存放 LLM 参数</td><td>影响 LLM 最大可运行模型大小</td></tr><tr><td><strong>存储（SSD/HDD）</strong></td><td>存放 LLM 训练数据、模型参数</td><td>训练时的 I/O 速度</td></tr><tr><td><strong>带宽（PCIe/NVLink）</strong></td><td>设备间数据传输速度</td><td>影响分布式训练效率</td></tr></tbody></table></li><li><p><strong>LLM 的 训练 和 推理（inference） 需要不同的硬件资源</strong>：</p><table><thead><tr><th>对比项</th><th>训练</th><th>推理</th></tr></thead><tbody><tr><td>计算量（FLOPs）</td><td>极高，多 GPU 并行训练</td><td>较低，单 GPU 可完成</td></tr><tr><td>显存需求</td><td>参数 + 梯度存储 + Adam 优化器</td><td>仅参数存储 + 计算缓存</td></tr><tr><td>适用 GPU</td><td>A100, H100, TPU</td><td>4090, A100</td></tr><tr><td>推荐存储</td><td>高速 NVMe SSD（>8TB）</td><td>适量存储（&lt;2TB）</td></tr><tr><td>推荐 CPU</td><td>高核心数（AMD EPYC 64 核）</td><td>普通 CPU（i9, Ryzen 9）</td></tr></tbody></table></li><li><p><strong>云计算 vs. 本地部署</strong></p><table><thead><tr><th>选项</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>云计算（AWS, Google Cloud）</td><td>高性能（H100, TPU）</td><td>成本高，GPU 计费</td></tr><tr><td>本地服务器（A100, 4090）</td><td>长期成本低</td><td>初期购置费用高</td></tr><tr><td>边缘 AI（M2, Jetson）</td><td>低功耗，适合移动端</td><td>只能运行小型模型</td></tr></tbody></table></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#模型大小估算><strong>模型大小估算</strong></a><ul><li><a href=#训练阶段><strong>训练阶段</strong></a></li><li><a href=#推理阶段><strong>推理阶段</strong></a></li></ul></li><li><a href=#硬件选择><strong>硬件选择</strong></a></li></ul></nav></div></aside></main></body></html>