<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  感知机和神经网络（Perceptrons and Neural Network）
  #

感知机（Perceptron）是神经网络（Neural Network）的雏形，基于单个或多个 神经元（Neuron） 通过线性组合和简单的激活函数（如阶跃函数）实现输入数据的分类。神经元是感知机的基本组成单元，其功能是对输入特征进行加权、偏置调整并生成输出。随着任务复杂性的增加，感知机被扩展为 多层感知机（Multilayer Perceptron, MLP），通过堆叠多个隐藏层并引入非线性激活函数，使网络能够表达复杂的映射关系，解决非线性问题，是前馈神经网络（Feedforward Neural Network）的核心架构。


  神经元（Neuron）
  #

神经元（Neuron）是神经网络的最基本单元，模拟生物神经元的行为。它的主要功能是 接收输入、加权处理并通过激活函数生成输出。




  \[
y = f\left(\sum_{i=1}^n w_i x_i + b\right)
\]


其中：


  \(x_i\)

 : 输入特征。

  \(w_i\)

 : 权重，衡量每个输入的影响程度。

  \(b\)

 : 偏置，用于调整输出的灵活性。

  \(f(\cdot)\)

 : 激活函数，增加非线性能力（如ReLU、Sigmoid）。

  \(y\)

 : 输出信号。






  感知机（Perceptrons）
  #

感知器（Perceptrons）是一种执行 二元分类（binary classification） 的神经网络，它将输入特征映射到输出决策，通常将数据分为两个类别之一，例如 0 或 1。感知器由一层输入节点（input nodes）组成，这些节点与一层输出节点（output nodes）完全连接（fully connected）。感知子可以用图像表示为：




单个感知机可以看作是一个最简单的神经元，专注于实现线性分类任务。单个神经元是感知机的扩展形式，通过引入更灵活的激活函数和多层结构，可以应用于更复杂的问题。"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/deep-learning/perceptrons-and-neural-network/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Perceptrons and Neural Network"><meta property="og:description" content="感知机和神经网络（Perceptrons and Neural Network） # 感知机（Perceptron）是神经网络（Neural Network）的雏形，基于单个或多个 神经元（Neuron） 通过线性组合和简单的激活函数（如阶跃函数）实现输入数据的分类。神经元是感知机的基本组成单元，其功能是对输入特征进行加权、偏置调整并生成输出。随着任务复杂性的增加，感知机被扩展为 多层感知机（Multilayer Perceptron, MLP），通过堆叠多个隐藏层并引入非线性激活函数，使网络能够表达复杂的映射关系，解决非线性问题，是前馈神经网络（Feedforward Neural Network）的核心架构。
神经元（Neuron） # 神经元（Neuron）是神经网络的最基本单元，模拟生物神经元的行为。它的主要功能是 接收输入、加权处理并通过激活函数生成输出。
\[ y = f\left(\sum_{i=1}^n w_i x_i + b\right) \] 其中：
\(x_i\) : 输入特征。 \(w_i\) : 权重，衡量每个输入的影响程度。 \(b\) : 偏置，用于调整输出的灵活性。 \(f(\cdot)\) : 激活函数，增加非线性能力（如ReLU、Sigmoid）。 \(y\) : 输出信号。 感知机（Perceptrons） # 感知器（Perceptrons）是一种执行 二元分类（binary classification） 的神经网络，它将输入特征映射到输出决策，通常将数据分为两个类别之一，例如 0 或 1。感知器由一层输入节点（input nodes）组成，这些节点与一层输出节点（output nodes）完全连接（fully connected）。感知子可以用图像表示为：
单个感知机可以看作是一个最简单的神经元，专注于实现线性分类任务。单个神经元是感知机的扩展形式，通过引入更灵活的激活函数和多层结构，可以应用于更复杂的问题。"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Perceptrons and Neural Network | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/deep-learning/perceptrons-and-neural-network/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.7322bcb2a59700245f40bcf446191085012705e76322af2dda3a70352945fdd3.js integrity="sha256-cyK8sqWXACRfQLz0RhkQhQEnBedjIq8t2jpwNSlF/dM=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/deep-learning/perceptrons-and-neural-network/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/ class=active>Perceptrons and Neural Network</a><ul></ul></li><li><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a><ul></ul></li><li><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/natural-language-processing/>Natural Language Processing</a><ul></ul></li><li><a href=/docs/deep-learning/computer-vision/>Computer Vision</a><ul></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Perceptrons and Neural Network</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#神经元neuron><strong>神经元（Neuron）</strong></a></li><li><a href=#感知机perceptrons><strong>感知机（Perceptrons）</strong></a><ul><li><a href=#模型结构><strong>模型结构</strong></a></li><li><a href=#损失函数><strong>损失函数</strong></a></li><li><a href=#更新规则><strong>更新规则</strong></a></li><li><a href=#感知机perceptrons和逻辑回归logistic-regression的区别><strong>感知机（Perceptrons）和逻辑回归（Logistic Regression）的区别</strong></a></li><li><a href=#感知机代码实现><strong>感知机代码实现</strong></a></li></ul></li><li><a href=#多层感知机multilayer-perceptronmlp><strong>多层感知机（Multilayer Perceptron，MLP）</strong></a><ul><li><a href=#输入层input-layer><strong>输入层（Input Layer）</strong></a></li><li><a href=#隐藏层hidden-layer><strong>隐藏层（Hidden Layer）</strong></a></li><li><a href=#输出层output-layer><strong>输出层（Output Layer）</strong></a></li></ul></li><li><a href=#神经网络运行流程和原理><strong>神经网络运行流程和原理</strong></a><ul><li><a href=#向前传播-forward-propagation><strong>向前传播 (Forward Propagation)</strong></a></li><li><a href=#向后传播-backward-propagation><strong>向后传播 (Backward Propagation)</strong></a></li></ul></li><li><a href=#简单神经网络代码实现><strong>简单神经网络代码实现</strong></a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=感知机和神经网络perceptrons-and-neural-network><strong>感知机和神经网络（Perceptrons and Neural Network）</strong>
<a class=anchor href=#%e6%84%9f%e7%9f%a5%e6%9c%ba%e5%92%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9cperceptrons-and-neural-network>#</a></h1><p>感知机（Perceptron）是神经网络（Neural Network）的雏形，基于单个或多个 <strong>神经元（Neuron）</strong> 通过线性组合和简单的激活函数（如阶跃函数）实现输入数据的分类。神经元是感知机的基本组成单元，其功能是对输入特征进行加权、偏置调整并生成输出。随着任务复杂性的增加，感知机被扩展为 <strong>多层感知机（Multilayer Perceptron, MLP）</strong>，通过堆叠多个隐藏层并引入非线性激活函数，使网络能够表达复杂的映射关系，解决非线性问题，是前馈神经网络（Feedforward Neural Network）的核心架构。</p><hr><h2 id=神经元neuron><strong>神经元（Neuron）</strong>
<a class=anchor href=#%e7%a5%9e%e7%bb%8f%e5%85%83neuron>#</a></h2><p>神经元（Neuron）是神经网络的最基本单元，模拟生物神经元的行为。它的主要功能是 <strong>接收输入、加权处理并通过激活函数生成输出</strong>。</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[
y = f\left(\sum_{i=1}^n w_i x_i + b\right)
\]</span><p>其中：</p><ul><li><span>\(x_i\)
</span>: 输入特征。</li><li><span>\(w_i\)
</span>: 权重，衡量每个输入的影响程度。</li><li><span>\(b\)
</span>: 偏置，用于调整输出的灵活性。</li><li><span>\(f(\cdot)\)
</span>: 激活函数，增加<strong>非线性能力</strong>（如<code>ReLU</code>、<code>Sigmoid</code>）。</li><li><span>\(y\)
</span>: 输出信号。</li></ul><div align=center><img src=/images/01_biological_neuron.jpeg width=600px/></div><hr><h2 id=感知机perceptrons><strong>感知机（Perceptrons）</strong>
<a class=anchor href=#%e6%84%9f%e7%9f%a5%e6%9c%baperceptrons>#</a></h2><p>感知器（Perceptrons）是一种执行 <strong>二元分类（binary classification）</strong> 的神经网络，它将输入特征映射到输出决策，通常将数据分为两个类别之一，例如 <code>0</code> 或 <code>1</code>。感知器由一层输入节点（input nodes）组成，这些节点与一层输出节点（output nodes）<strong>完全连接（fully connected）</strong>。感知子可以用图像表示为：</p><div align=center><img src=/images/01_Perceptron.PNG width=500px/></div><blockquote class="book-hint warning"><p><strong>单个感知机</strong>可以看作是一个<strong>最简单的神经元</strong>，专注于实现线性分类任务。单个神经元是感知机的扩展形式，通过引入<strong>更灵活的激活函数和多层结构</strong>，可以应用于更复杂的问题。</p></blockquote><hr><h3 id=模型结构><strong>模型结构</strong>
<a class=anchor href=#%e6%a8%a1%e5%9e%8b%e7%bb%93%e6%9e%84>#</a></h3><p>感知机的基本结构包括以下<strong>组成部分</strong>：</p><ul><li><strong>输入层（Input Layer）</strong>：接收输入特征向量 <span>\(x = [x_1, x_2, \dots, x_n]\)
</span>。</li><li><strong>权重向量（Weights）</strong>：每个输入特征 <span>\(x_i\)
</span>对应的权重 <span>\(w_i\)
</span>。</li><li><strong>偏置（Bias, b）</strong>：平移决策边界，增强模型的灵活性。<ul><li><strong>线性组合</strong>：
<span>\[
z = W^T X + b = \sum_{i=1}^n w_i x_i + b
\]</span></li></ul></li><li><strong>激活函数（Activation Function）</strong>：通常为符号函数 <span>\(\text{sign}(z)\)
</span>，用于将加权和映射为输出标签。<ul><li><strong>输出结果</strong>：
<span>\[
y = \text{sign}(z) =
\begin{cases}
+1, & \text{if } z \geq 0 \\
-1, & \text{if } z < 0
\end{cases}
\]</span></li></ul></li></ul><hr><h3 id=损失函数><strong>损失函数</strong>
<a class=anchor href=#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0>#</a></h3><p>感知机的损失函数本质上是用来惩罚误分类样本，从而引导模型学习到能够正确分类所有样本的权重参数。他的主要目标是找到一个<strong>超平面（hyperplane）</strong>：<span>
\(w^T x + b = 0\)
</span>。使得数据点能够被正确分类，即：</p><ul><li>如果 <span>\(y_i = +1\)
</span>，那么希望 <span>\(w^T x_i + b > 0\)
</span>；</li><li>如果 <span>\(y_i = -1\)
</span>，那么希望 <span>\(w^T x_i + b < 0\)
</span>。</li></ul><p>感知机的损失函数<strong>只关注那些被误分类的样本</strong>。对于误分类的样本，有：
<span>\[
y_i (w^T x_i + b) \leq 0
\]</span></p><p>损失函数定义为<strong>所有误分类样本</strong>的<strong>负边界距离</strong>的总和：
<span>\[
L(w, b) = -\sum_{i \in M} y_i (w^T x_i + b)
\]</span></p><ul><li><span>\(M\)
</span>表示所有<strong>被误分类的样本的集合</strong>；</li><li><span>\(y_i (w^T x_i + b)\)
</span>表示样本 <span>\(x_i\)
</span>到决策超平面的有符号距离。</li></ul><hr><h3 id=更新规则><strong>更新规则</strong>
<a class=anchor href=#%e6%9b%b4%e6%96%b0%e8%a7%84%e5%88%99>#</a></h3><p>严格来说，感知机本身<strong>并不使用梯度下降法</strong>进行优化，因为感知机的<strong>损失函数是分段的、非连续的</strong>，无法直接对其求导。感知机的权重更新公式是：
<span>\[
\begin{align*}
&w \leftarrow w + \eta \cdot y_i \cdot x_i \\
&b \leftarrow b + \eta \cdot y_i \\
\end{align*}
\]</span></p><p>感知机的权重更新可以看作是一种<strong>离散化、非平滑</strong>的近似梯度下降过程:</p><ol><li>每次只对一个误分类样本 <span>\(x_i\)
</span>更新权重和偏置；</li><li>更新方向为该样本的贡献（<span>
\(-y_i x_i\)
</span>的负梯度方向）；</li></ol><p>从几何角度看：如果一个样本被误分类，更新方向是沿着样本 <span>\(x_i\)
</span>的方向，并且朝着正确分类 <span>\(y_i\)
</span>的方向推进决策边界。</p><hr><h3 id=感知机perceptrons和逻辑回归logistic-regression的区别><strong>感知机（Perceptrons）和逻辑回归（Logistic Regression）的区别</strong>
<a class=anchor href=#%e6%84%9f%e7%9f%a5%e6%9c%baperceptrons%e5%92%8c%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92logistic-regression%e7%9a%84%e5%8c%ba%e5%88%ab>#</a></h3><p>感知机（Perceptrons）和逻辑回归（Logistic Regression）在表面上确实有很多相似之处，因为它们都属于线性模型，但它们的核心区别在于损失函数和输出目标。</p><ul><li><p><strong>激活函数区别</strong></p><ul><li>Perceptrons 使用符号函数（sign function）作为激活函数。这意味着感知机的输出是基于决策边界的二元结果，<strong>不提供概率信息</strong>。
<span>\[
y = \text{sign}(w^T x + b)
\]</span></li><li>Logistic Regression 使用逻辑函数（sigmoid function）将<strong>线性组合结果映射到概率范围</strong>：
<span>\[
P(y=1|x) = \sigma(w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}
\]</span></li></ul></li><li><p><strong>损失函数区别</strong></p><ul><li>Perceptrons 的学习目标是<strong>最小化误分类样本的数量</strong>，仅对被误分类的样本进行权重更新。
<span>\[
L(w, b) = -\sum_{i \in M} y_i (w^T x_i + b)
\]</span></li><li>Logistic Regression 通过<strong>最大化条件概率</strong> <span>\(P(y|x)\)
</span>的对数似然来优化参数：
<span>\[
L(w, b) = -\sum_{i=1}^N \left[y_i \log(\sigma(w^T x_i + b)) + (1 - y_i) \log(1 - \sigma(w^T x_i + b)) \right]
\]</span></li></ul></li><li><p><strong>决策边界区别</strong></p><p>感知机和逻辑回归都假设数据是线性可分的，因此其决策边界都是一个超平面：</p><ul><li>Perceptrons 直接依赖超平面将数据划分为两个类别，但<strong>没有提供关于样本距离边界的任何信息</strong>。</li><li>Logistic Regression 利用概率信息描述样本在边界两侧的信心度，决策边界定义为 <span>\(P(y=1|x) = 0.5\)
</span>。</li></ul></li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> Perceptrons 通常<strong>仅适用于线性可分的数据</strong>。算法在数据线性可分时会收敛；但如果数据线性不可分，则会陷入无限循环。Logistic Regression <strong>可处理线性不可分数据</strong>，即使数据线性不可分，也能找到最优的权重（通过拟合概率分布）。</p></blockquote><hr><h3 id=感知机代码实现><strong>感知机代码实现</strong>
<a class=anchor href=#%e6%84%9f%e7%9f%a5%e6%9c%ba%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># &lt;--- From scratch ---&gt;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>Perceptrons</span>(X, y, lr, max_iter): 
</span></span><span style=display:flex><span>    n_samples, n_features <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>shape 
</span></span><span style=display:flex><span>    weights <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(n_features)
</span></span><span style=display:flex><span>    bias <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(max_iter):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n_samples):
</span></span><span style=display:flex><span>            <span style=color:#75715e># 计算线性输出</span>
</span></span><span style=display:flex><span>            linear_output <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(X[i], weights) <span style=color:#f92672>+</span> bias
</span></span><span style=display:flex><span>            <span style=color:#75715e># 如果预测错误，则更新权重和偏置</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> y[i] <span style=color:#f92672>*</span> linear_output <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                weights <span style=color:#f92672>+=</span> lr <span style=color:#f92672>*</span> y[i] <span style=color:#f92672>*</span> X[i]
</span></span><span style=display:flex><span>                bias <span style=color:#f92672>+=</span> lr <span style=color:#f92672>*</span> y[i]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> weights, bias
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 模型预测</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(X, weights, bias):
</span></span><span style=display:flex><span>    linear_output <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(X, weights) <span style=color:#f92672>+</span> bias
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>where(linear_output <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 数据示例</span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>], [<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>], [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>], [<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], [<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], [<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>]])
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 模型训练</span>
</span></span><span style=display:flex><span>weights, bias <span style=color:#f92672>=</span> Perceptrons(X, y, lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>, max_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 输出结果</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;学习到的权重:&#34;</span>, weights)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;学习到的偏置:&#34;</span>, bias)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 进行预测</span>
</span></span><span style=display:flex><span>predictions <span style=color:#f92672>=</span> predict(X, weights, bias)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;预测结果:&#34;</span>, predictions)
</span></span></code></pre></div><hr><h2 id=多层感知机multilayer-perceptronmlp><strong>多层感知机（Multilayer Perceptron，MLP）</strong>
<a class=anchor href=#%e5%a4%9a%e5%b1%82%e6%84%9f%e7%9f%a5%e6%9c%bamultilayer-perceptronmlp>#</a></h2><p>多层感知机（MLP）是最常见的前馈神经网络（Feedforward Networks）之一，由多个全连接层组成。它是神经网络的基础结构，广泛用于分类、回归等任务。MLP的<strong>核心思想是通过隐藏层和非线性激活函数，提取输入数据的特征并映射到目标输出</strong>。</p><div align=center><img src=/images/The-structure-of-a-multi-layer-perceptronMLP-neural-network-to-model-absorption.png width=500px/></div><hr><h3 id=输入层input-layer><strong>输入层（Input Layer）</strong>
<a class=anchor href=#%e8%be%93%e5%85%a5%e5%b1%82input-layer>#</a></h3><p>输入层是多层感知机（MLP）的第一部分，用于接收外部输入数据并将其传递给网络的隐藏层。输入层的设计直接决定了模型对数据的适配能力。输入层可以视为数据和网络之间的接口：</p><ul><li><strong>数据接受</strong>：接收外部特征输入，通常以向量或矩阵的形式表示。</li><li><strong>维度映射</strong>：将原始数据的特征维度（<span>
\(d\)
</span>）映射到神经网络的内部表示维度。输入数据的特征维度需与输入层的线性变换参数兼容。</li><li><strong>数据传递</strong>：输入层通过线性变换（如 <span>\(xW + b\)
</span>）将输入映射到第一个隐藏层的维度。它仅负责将输入数据直接传递到隐藏层。</li></ul><ol><li><p><strong>神经元（Neuron）数量</strong>：输入层的神经元数量<strong>等于第一个隐藏层的神经元数量</strong>，即输入层通过线性变换将输入特征映射到第一个隐藏层的维度。输入数据的特征数量（<span>
\(d\)
</span>）决定了输入层每个神经元的权重数量：</p><ul><li>对于输入维度为 <span>\(d\)
</span>的数据，每个神经元会有 <span>\(d\)
</span>个权重加上一个偏置项。</li><li><strong>示例</strong>: 如果输入数据具有 4 个特征（如 <span>\([x_1, x_2, x_3, x_4]\)
</span>），且第一个隐藏层包含 10 个神经元，则输入层需要：<ul><li>10 个神经元（每个神经元与隐藏层的每个神经元一一对应）。</li><li>每个神经元包含 4 个权重（分别对应 4 个输入特征）和 1 个偏置项。</li></ul></li></ul></li><li><p><strong>输入数据格式</strong>：输入数据通常为一个向量或矩阵：</p><ul><li>单样本输入: 向量形式，如 <span>\([x_1, x_2, …, x_d]\)
</span>。</li><li>批量输入: 矩阵形式，形状为 <span>\((\text{batch size}, d)\)
</span>，其中 <span>\( \text{batch size} \)
</span>为每次输入的样本数， <span>\(d\)
</span>为特征数。</li></ul></li></ol><ul><li><strong>代码示例</strong>:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>SimpleInputLayer</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, input_dim, hidden_dim):
</span></span><span style=display:flex><span>        super(SimpleInputLayer, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(input_dim, hidden_dim)  <span style=color:#75715e># 从输入层到第一个隐藏层的线性变换</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>relu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ReLU()  <span style=color:#75715e># 激活函数（ReLU）</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc1(x)  <span style=color:#75715e># 输入数据经过线性变换</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>relu(x)  <span style=color:#75715e># 使用 ReLU 激活函数</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div></li></ul><hr><h3 id=隐藏层hidden-layer><strong>隐藏层（Hidden Layer）</strong>
<a class=anchor href=#%e9%9a%90%e8%97%8f%e5%b1%82hidden-layer>#</a></h3><p>隐藏层是神经网络中介于输入层和输出层之间的层，它是神经网络学习和表示复杂映射关系的关键部分。隐藏层通过层叠的神经元以及非线性激活函数，能够从数据中提取特征和学习模式，是深度学习的核心。隐藏层的主要功能有：</p><ul><li><strong>特征提取</strong>：隐藏层负责从输入数据中提取有用的特征，形成更高维、更抽象的表示。</li><li><strong>非线性映射</strong>：通过非线性激活函数（如 <code>ReLU</code>、<code>Sigmoid</code>、<code>Tanh</code>），隐藏层能够学习复杂的非线性关系，而非仅仅是简单的线性变换。</li><li><strong>数据处理</strong>：每个隐藏层接收上一个层的输出，经过线性变换和激活函数处理后，传递到下一层。</li></ul><div align=center><img src=/images/hidden_layer.png width=450px/></div><hr><p>每个隐藏层由以下<strong>三部分</strong>组成：</p><ol><li><strong>神经元</strong>：</li></ol><ul><li>每个神经元代表一个计算单元，接收来自上一层所有神经元的输入，加权求和后进行激活函数变换。</li><li>隐藏层的<strong>神经元数量是一个超参数</strong>，需要根据具体问题进行选择。</li></ul><ol start=2><li><strong>权重（Weights）和偏置（Biases）</strong>：</li></ol><ul><li>权重：连接两层之间的神经元，并决定输入的重要性。</li><li>偏置：用于调整激活函数的输出，增加模型的表达能力。</li><li>公式：<span>
\(z = xW + b\)
</span>，其中 <span>\(x\)
</span>是输入，<span>
\(W\)
</span>是权重矩阵，<span>
\(b\)
</span>是偏置向量。</li></ul><ol start=3><li><strong>激活函数</strong>：</li></ol><ul><li><p>激活函数引入<strong>非线性</strong>，使神经网络能够<strong>学习复杂的非线性映射</strong>。</p></li><li><p>常见激活函数：</p><p><strong>ReLU (Rectified Linear Unit)</strong></p><div align=center><img src=/images/1*fWjpaGDDQQd5NxAVGAWBuw.png width=550px/></div><span>\[
f(x) = \max(0, x)
\]</span><ul><li><strong>计算简单</strong>：仅比较输入值是否大于 0，操作速度快。</li><li><strong>非线性</strong>：尽管形式简单，但 ReLU 是非线性的，可帮助网络学习复杂特征。</li><li><strong>稀疏性</strong>：当输入小于 0 时，输出为 0，相当于让部分神经元不激活，提升模型稀疏性。</li><li><strong>问题</strong>：可能导致“神经元死亡”（当许多权重使输入始终小于 0，导致该神经元永不更新）。</li><li>ReLU 广泛应用于深层神经网络，一般情况下是<strong>默认选择</strong>，适合大多数场景。</li></ul><hr><p><strong>Sigmoid</strong></p><div align=center><img src="/images/Gradient Vanishing.png" width=450px/></div><span>\[
f(x) = \frac{1}{1 + e^{-x}}
\]</span><ul><li><strong>输出范围</strong>：<span>
\([0, 1]\)
</span>，适合表示概率值。</li><li><strong>单调性</strong>：对于输入增大，输出逐渐趋近于 1，但变化减缓。</li><li><strong>梯度消失问题</strong>：当 <span>\(x\)
</span>的绝对值较大时，函数的梯度趋近于 0，导致反向传播时权重更新困难。</li></ul><hr><p><strong>Tanh (双曲正切函数)</strong></p><div align=center><img src=/images/Tanh.png width=450px/></div><span>\[
f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]</span><ul><li><strong>输出范围</strong>：<span>
\([-1, 1]\)
</span>，比 Sigmoid 更对称，适合隐藏层激活函数。</li><li><strong>对称性</strong>：中心对称于原点，有助于加速收敛。</li><li><strong>梯度消失问题</strong>：与 Sigmoid 类似，当 <span>\(x\)
</span>的绝对值较大时，梯度会趋近于 0。</li></ul></li></ul><hr><p><strong>关于非线性</strong></p><ul><li><strong>线性关系</strong>：一个函数是线性的，如果它满足叠加性和齐次性，即：
<span>\(f(ax + by) = a \cdot f(x) + b \cdot f(y)\)
</span>例如：<span>
\(y = 2x + 3\)
</span>是线性函数。<strong>它的图形是一直线</strong>。</li><li><strong>非线性关系</strong>：非线性函数不满足上述性质。例如：<span>
\(y = x^2\)
</span>或 <span>\(y = \sin(x)\)
</span>是非线性的。它的图形可能是<strong>弯曲的、不规则的</strong>。</li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> 如果神经网络的每一层都只包含线性变换，那么即使增加多层隐藏层，整个网络<strong>仍然是一个线性函数的组合</strong>。非线性让网络能够学习复杂的映射关系，如分类非线性可分的数据、<strong>逼近复杂函数</strong>等。</p></blockquote><ul><li><p>激活<strong>函数的形状</strong>的确会影响它的性能，但<strong>并非关键因素</strong>。重要的是它能够引入 非线性特性，打破线性关系的局限性。</p></li><li><p>激活函数通常是 <strong>element-wise（逐元素）操作</strong>，逐元素操作<strong>不改变张量的形状，只改变其值</strong>，适合深度学习网络中逐层特征变换的需要。他的作用是提取特征和表示，而不是直接产生概率分布。</p></li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> 神经网络的目标 <strong>不是直接寻找一个显式的数学公式（如通用曲线）</strong>，而是构建一种复杂的非线性映射，将输入特征与输出目标连接起来。这种映射是通过网络参数的学习过程（如权重和偏置的调整）隐式表达的。虽然网络可以实现“圆形”或“复杂形状”的决策边界，但这不是通过显式地拟合一个圆的方程，而是通过层层<strong>线性变换与非线性激活的组合</strong>自动得出的结果。</p></blockquote><hr><p><strong>隐藏层理解</strong></p><p>隐藏层的作用可以类比为数据的逐步“翻译”或“加工”：</p><ul><li><strong>低层特征提取</strong>：第一层隐藏层处理输入数据的基本特征（如简单的边缘、颜色或频率等）。</li><li><strong>中层特征组合</strong>：中间层将低层特征组合成更高阶的特征（如形状、局部模式或局部结构）。</li><li><strong>高层特征整合</strong>：更深层次的隐藏层进一步整合复杂特征，用于最终分类或预测任务。</li></ul><p>每一层将数据映射到新的特征空间中，这种映射使得神经网络能够捕获从简单到复杂的模式。隐藏层维度的<strong>设计通常遵循“逐步扩展—再收缩</strong>”的模式：</p><ol><li><strong>增加维度</strong>：扩展特征空间<ul><li>增加维度可以让模型在更高维的特征空间中<strong>提取更加复杂和细粒度的模式</strong>。</li><li>例如，输入层可能包含较少的原始特征（如像素、频谱或特定数值），但这些特征经过线性变换和激活后，隐藏层可以生成更多维度的“隐含特征”。</li><li>扩展特征空间类似于“打开数据的潜力”，为网络提供更丰富的信息处理能力。</li></ul></li><li><strong>减少维度</strong>：聚合有用信息<ul><li>在后续隐藏层中减少维度是为了压缩特征表示，<strong>去除冗余信息</strong>，仅保留与任务相关的高质量特征。</li><li>这一过程可以防止模型过拟合，同时提高计算效率和泛化能力。</li><li>减少维度还可以实现对数据的进一步“压缩”，形成对输入数据的简洁而有力的表示。</li></ul></li></ol><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>为什么逐步增加维度（e.g. dim4 -> dim128 -> dim256 -> dim512）而不是直接扩展到高维（e.g. dim4 -> dim512）？</strong></p><p>直接扩展到高维，模型可能无法有效捕获特征层次，会使学习过程更加困难。需要的权重参数过多，特别是在数据量不足时效果较差。同时由于缺少逐步提取特征的过程，模型可能过于依赖输入数据的特定模式。</p></blockquote><ul><li><strong>代码示例</strong>：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>SimpleHiddenLayer</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, input_dim, hidden_dims):
</span></span><span style=display:flex><span>        super(SimpleHiddenLayer, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        layers <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(hidden_dims)):
</span></span><span style=display:flex><span>            in_dim <span style=color:#f92672>=</span> input_dim <span style=color:#66d9ef>if</span> i <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>else</span> hidden_dims[i <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>            out_dim <span style=color:#f92672>=</span> hidden_dims[i]
</span></span><span style=display:flex><span>            layers<span style=color:#f92672>.</span>append(nn<span style=color:#f92672>.</span>Linear(in_dim, out_dim))  <span style=color:#75715e># 线性变换</span>
</span></span><span style=display:flex><span>            layers<span style=color:#f92672>.</span>append(nn<span style=color:#f92672>.</span>ReLU())  <span style=color:#75715e># ReLU 激活函数</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>hidden_layers <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(<span style=color:#f92672>*</span>layers)  <span style=color:#75715e># 将所有隐藏层组合为一个模块</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>hidden_layers(x)  <span style=color:#75715e># 输入依次通过所有隐藏层</span>
</span></span></code></pre></div><hr><h3 id=输出层output-layer><strong>输出层（Output Layer）</strong>
<a class=anchor href=#%e8%be%93%e5%87%ba%e5%b1%82output-layer>#</a></h3><p>输出层是神经网络的最后一部分，其核心职责是<strong>根据模型的目标任务（Cost function），生成适合应用场景的输出</strong>。不同任务对输出层的设计要求不同，例如分类、回归或生成任务等。输出层的实现通常结合特定的单元（如线性、Sigmoid、Softmax）和损失函数，以适应不同类型的数据分布和学习目标。</p><ol><li><strong>线性单元（Linear Units）</strong></li></ol><ul><li><strong>适用场景</strong>: 连续型输出（如回归任务）。</li><li><strong>数学表达式</strong>: 输出值 <span>\(y = xW + b\)
</span>（无激活函数）。</li><li><strong>解释</strong>:<ul><li>线性单元生成实值输出，不引入非线性变换。</li><li>损失函数通常为<strong>均方误差（MSE）</strong>: <span>\(L = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\)
</span>，其中 <span>\(\hat{y}_i\)
</span>是预测值。</li></ul></li><li><strong>代码示例</strong>:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>LinearOutputLayer</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, input_dim):
</span></span><span style=display:flex><span>        super(LinearOutputLayer, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(input_dim, <span style=color:#ae81ff>1</span>)  <span style=color:#75715e># 线性变换（输出一个数值）</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc(x)  <span style=color:#75715e># 线性变换</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div></li></ul><ol start=2><li><strong>Sigmoid 单元</strong></li></ol><ul><li><strong>适用场景</strong>: 二分类任务。</li><li><strong>数学表达式</strong>: <span>\(y = \sigma(xW + b) = \frac{1}{1 + e^{-(xW + b)}}\)
</span>。</li><li><strong>解释</strong>:<ul><li>Sigmoid 单元将输出值压缩到区间 <span>\((0, 1)\)
</span>，解释为正样本的概率 <span>\(P(y=1|x)\)
</span>。</li><li>损失函数通常为<strong>二元交叉熵损失（Binary Cross-Entropy Loss）</strong>:
<span>\(L = - \frac{1}{n} \sum_{i=1}^n [y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]\)
</span>，其中 <span>\(\hat{y}_i\)
</span>是预测的概率值。</li></ul></li><li><strong>代码示例</strong>:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>BinaryOutputLayer</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, input_dim):
</span></span><span style=display:flex><span>        super(BinaryOutputLayer, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(input_dim, <span style=color:#ae81ff>1</span>)  <span style=color:#75715e># 线性层，将输入映射到 1 个输出（概率）</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>sigmoid <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sigmoid()  <span style=color:#75715e># Sigmoid 激活函数</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc(x)  <span style=color:#75715e># 线性变换</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>sigmoid(x)  <span style=color:#75715e># Sigmoid 激活函数</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div></li></ul><ol start=3><li><strong>Softmax 单元</strong></li></ol><ul><li><strong>适用场景</strong>: 多分类任务。</li><li><strong>数学表达式</strong>: <span>\(y_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}\)
</span>，其中 <span>\(z_j\)
</span>是第 <span>\(j\)
</span>类的得分。</li><li><strong>解释</strong>:<ul><li>Softmax 单元将多个输出值转换为概率分布，保证 <span>\(\sum_{j=1}^K y_j = 1\)
</span>，每个 <span>\(y_j\)
</span>表示属于第 <span>\(j\)
</span>类的概率。</li><li>损失函数通常为多<strong>分类交叉熵损失（Categorical Cross-Entropy Loss）</strong>:
<span>\(L = - \sum_{i=1}^n \sum_{j=1}^K y_{ij} \log(\hat{y}_{ij})\)
</span>，其中 <span>\(y_{ij}\)
</span>是真实标签， <span>\(\hat{y}_{ij}\)
</span>是预测概率。</li></ul></li><li><strong>代码示例</strong>:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MultiClassOutputLayer</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, input_dim, num_classes):
</span></span><span style=display:flex><span>        super(MultiClassOutputLayer, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(input_dim, num_classes)  <span style=color:#75715e># 输出类别数个神经元</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>softmax <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Softmax(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)  <span style=color:#75715e># Softmax 激活函数</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc(x)  <span style=color:#75715e># 线性变换</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>softmax(x)  <span style=color:#75715e># Softmax 激活函数</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div></li></ul><hr><h2 id=神经网络运行流程和原理><strong>神经网络运行流程和原理</strong>
<a class=anchor href=#%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%90%e8%a1%8c%e6%b5%81%e7%a8%8b%e5%92%8c%e5%8e%9f%e7%90%86>#</a></h2><h3 id=向前传播-forward-propagation><strong>向前传播 (Forward Propagation)</strong>
<a class=anchor href=#%e5%90%91%e5%89%8d%e4%bc%a0%e6%92%ad-forward-propagation>#</a></h3><p><strong>向前传播 (Forward Propagation)</strong> 通过网络层层传递数据，逐步对输入进行线性变换和非线性映射，提取特征并输出预测结果。</p><div align=center><img src=/images/forward_propagation.svg width=450px/></div><ul><li><p><strong>输入层到隐藏层</strong>：
假设输入层的输入为 <span>\( x \)
</span>，第一个隐藏层的权重矩阵为 <span>\( W^{(1)} \)
</span>，偏置为 <span>\( b^{(1)} \)
</span>。计算该层的线性变换：</p><ol><li>线性变换：
<span>\( z^{(1)} = W^{(1)} x + b^{(1)} \)
</span>其中，<span>
\( z^{(1)} \)
</span>是第一个隐藏层的线性变换输出。</li><li>激活函数：
经过激活函数 <span>\( \varphi(\cdot) \)
</span>后得到隐藏层的输出：
<span>\( h^{(1)} = \varphi(z^{(1)}) = \varphi(W^{(1)} x + b^{(1)}) \)
</span>其中，<span>
\( \varphi(\cdot) \)
</span>可以是常见的激活函数，如 ReLU 或 Sigmoid，具体的形式取决于任务需求。</li></ol></li><li><p><strong>隐藏层到输出层</strong>：
假设输出层的权重矩阵为 <span>\( W^{(2)} \)
</span>，偏置为 <span>\( b^{(2)} \)
</span>。该层的输出经过线性变换并得到最终的输出。</p><ol><li>线性变换：
<span>\( z^{(2)} = W^{(2)} h^{(1)} + b^{(2)} \)
</span>其中，<span>
\( h^{(1)} \)
</span>是上一层的输出，<span>
\( z^{(2)} \)
</span>是当前层的线性变换输出。</li><li>激活函数：
直接得到预测值（例如在回归问题中，输出层可能不使用激活函数）。</li></ol><ul><li>如果是分类问题：
<span>\( \hat{y} = \varphi(z^{(2)}) = \varphi(W^{(2)} h^{(1)} + b^{(2)}) \)</span></li><li>如果是回归问题（没有激活函数）：
<span>\( \hat{y} = W^{(2)} h^{(1)} + b^{(2)} \)
</span>其中，<span>
\( \hat{y} \)
</span>是最终输出。</li></ul></li><li><p><strong>损失函数的计算</strong>：
最终，我们计算输出与真实标签之间的损失，通常使用损失函数 <span>\( L = l(\hat{y} , y) \)
</span>，其中 <span>\( \hat{y} \)
</span>是网络的预测输出，<span>
\( y \)
</span>是实际标签。</p></li><li><p><strong>完整流程</strong>：
<span>\[
z^{(1)} = W^{(1)} x + b^{(1)} → h^{(1)} = \varphi(z^{(1)}) → z^{(2)} = W^{(2)}h^{(1)} + b^{(2)}→ \hat{y} = \varphi(z^{(2)}) → L = l(\hat{y}, y)
\]</span></p></li></ul><hr><h3 id=向后传播-backward-propagation><strong>向后传播 (Backward Propagation)</strong>
<a class=anchor href=#%e5%90%91%e5%90%8e%e4%bc%a0%e6%92%ad-backward-propagation>#</a></h3><p>向后传播是计算梯度并调整模型参数的过程，用于优化神经网络。通过链式法则，逐层计算损失函数对每个参数的偏导数。我们在向后传播中关注的<strong>重点在于计算权重（weight）和偏置（bias）的梯度</strong>。</p><blockquote><p><strong>链式法则的数学公式</strong>: 设有复合函数：<span>
\( y = f(g(h(x))) \)
</span>展开形式：<span>
\( \frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dh} \cdot \frac{dh}{dx} \)</span></p></blockquote><ul><li><p><strong>损失函数对输出的梯度</strong>：
首先，从损失函数出发，计算损失函数对输出层的梯度。</p><ul><li>损失函数：<span>
\( L = l(\hat{y}, y) \)
</span>其中，<span>
\( \hat{y} \)
</span>是模型的输出，<span>
\( y \)
</span>是真实标签。</li><li>损失函数关于输出层的梯度是：
<span>\[
\frac{\partial L}{\partial \hat{y}} = \frac{\partial l(\hat{y}, y)}{\partial \hat{y}} \]</span></li></ul></li><li><p><strong>输出层到隐藏层的梯度</strong>：
接下来，我们需要计算<strong>损失函数对第二层权重矩阵 <span>\( W^{(2)} \)
</span>和偏置项 <span>\( b^{(2)} \)
</span>的梯度 (i.e. <span>\( \frac{\partial L}{\partial W^{(2)}}\)
</span>, <span>\( \frac{\partial L}{\partial b^{(2)}}\)
</span>)</strong>。</p><ul><li>我们有<span>
\(\hat{y} = \varphi(z^{(2)})\)
</span>，由此可得输出层的梯度对第二层的加权输入 <span>\( z^{(2)} \)
</span>的偏导数：
<span>\[
\frac{\partial L}{\partial z^{(2)}} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(2)}}
\]</span></li><li>我们有<span>
\(z^{(2)} = W^{(2)}h^{(1)} + b^{(2)}\)
</span>，然后计算损失函数对第二层权重矩阵 <span>\( W^{(2)} \)
</span>和偏置项 <span>\( b^{(2)}\)
</span>的梯度：
<span>\[
\frac{\partial L}{\partial W^{(2)}} = \frac{\partial L}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial W^{(2)}} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial W^{(2)}}
\]
</span>以及 <span>\( \frac{\partial L}{\partial b^{(2)}} = \frac{\partial L}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial b^{(2)}} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial b^{(2)}} \)</span></li></ul></li><li><p><strong>隐藏层到输入层的梯度</strong>：
接着，我们需要计算损失函数对<strong>第一层的权重矩阵 <span>\( W^{(1)} \)
</span>和偏置项 <span>\( b^{(1)} \)
</span>的梯度</strong>。</p><ul><li>从<span>
\(z^{(2)} = W^{(2)}h^{(1)} + b^{(2)}\)
</span>和 <span>\(h^{(1)} = \varphi(z^{(1)})\)
</span>我们可以得到：
<span>\[
\frac{\partial L}{\partial z^{(1)}} = \frac{\partial L}{\partial h^{(1)}} \cdot \frac{\partial h^{(1)}}{\partial z^{(1)}} = \frac{\partial L}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial h^{(1)}} \cdot \frac{\partial h^{(1)}}{\partial z^{(1)}}
\]</span></li><li>然后从 <span>\(z^{(1)} = W^{(1)} x + b^{(1)}\)
</span>，计算损失函数对第一层权重矩阵 <span>\( W^{(1)} \)
</span>和偏置项 <span>\( b^{(1)}\)
</span>的梯度：
<span>\[
\frac{\partial L}{\partial W^{(1)}} = \frac{\partial L}{\partial z^{(1)}} \cdot \frac{\partial z^{(1)}}{\partial W^{(1)}}=\frac{\partial L}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial h^{(1)}} \cdot \frac{\partial h^{(1)}}{\partial z^{(1)}} \cdot \frac{\partial z^{(1)}}{\partial W^{(1)}}
\]</span></li></ul><p>以及 <span>\( \frac{\partial L}{\partial b^{(1)}} = \frac{\partial L}{\partial z^{(1)}} \cdot \frac{\partial z^{(1)}}{\partial b^{(1)}}=\frac{\partial L}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial h^{(1)}} \cdot \frac{\partial h^{(1)}}{\partial z^{(1)}} \cdot \frac{\partial z^{(1)}}{\partial b^{(1)}} \)</span></p></li><li><p><strong>参数更新</strong>：
在向后传播过程中，我们计算出了所有需要的梯度，接下来进行参数更新。在每次迭代中，通过梯度下降更新每个权重和偏置项：</p><ul><li>更新权重：
<span>\( W^{(1)} = W^{(1)} - \eta \frac{\partial L}{\partial W^{(1)}} \)
</span>，
<span>\( W^{(2)} = W^{(2)} - \eta \frac{\partial L}{\partial W^{(2)}} \)</span></li><li>更新偏置：
<span>\( b^{(1)} = b^{(1)} - \eta \frac{\partial L}{\partial b^{(1)}} \)
</span>，
<span>\( b^{(2)} = b^{(2)} - \eta \frac{\partial L}{\partial b^{(2)}} \)</span></li></ul></li></ul><blockquote class="book-hint warning"><p><strong>Note</strong>：激活值本身并<strong>不会直接被用于反向传播</strong>，而是激活值的导数和前一层的梯度共同作用于权重的更新。在反向传播中，计算每一层的梯度时需要用到<strong>激活函数的导数，而不是激活值本身</strong>。</p></blockquote><hr><h2 id=简单神经网络代码实现><strong>简单神经网络代码实现</strong>
<a class=anchor href=#%e7%ae%80%e5%8d%95%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># &lt;--- From scratch ---&gt;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>ReLu</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>maximum(<span style=color:#ae81ff>0</span>, x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>ReLU_derivative</span>(z):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (z <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>astype(float)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>Softmax</span>(x):
</span></span><span style=display:flex><span>    exp_x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>exp(x <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>max(x, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>))  <span style=color:#75715e># 防止溢出</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> exp_x <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>sum(exp_x, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 损失函数和其导数</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>Cross_entropy_loss</span>(y_hat, y):
</span></span><span style=display:flex><span>    m <span style=color:#f92672>=</span> y<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span>np<span style=color:#f92672>.</span>sum(y <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>log(y_hat <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-15</span>)) <span style=color:#f92672>/</span> m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>Cross_entropy_loss_derivative</span>(y_hat, y):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> y_hat <span style=color:#f92672>-</span> y 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>MLP</span>(X, y, max_iter, learning_rate):
</span></span><span style=display:flex><span>    input_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>    hidden_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>    output_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    W1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(hidden_dim, input_dim) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.01</span>
</span></span><span style=display:flex><span>    b1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((hidden_dim, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    W2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(output_dim, hidden_dim) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.01</span>
</span></span><span style=display:flex><span>    b2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((output_dim, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>Forward_Propagation</span>(X, W1, b1, W2, b2):
</span></span><span style=display:flex><span>        z1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(W1, X<span style=color:#f92672>.</span>T) <span style=color:#f92672>+</span> b1
</span></span><span style=display:flex><span>        h1 <span style=color:#f92672>=</span> ReLu(z1)
</span></span><span style=display:flex><span>        z2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(W2, h1<span style=color:#f92672>.</span>T) <span style=color:#f92672>+</span> b2
</span></span><span style=display:flex><span>        y_hat <span style=color:#f92672>=</span> Softmax(z2<span style=color:#f92672>.</span>T)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> z1, h1, z2, y_hat
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>Backward_Propagation</span>(X, y, z1, h1, z2, y_hat, W1, W2):
</span></span><span style=display:flex><span>        m <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        dz2 <span style=color:#f92672>=</span> cross_entropy_loss_derivative(y_hat, y)
</span></span><span style=display:flex><span>        dW2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(dz2<span style=color:#f92672>.</span>T, h1<span style=color:#f92672>.</span>T) <span style=color:#f92672>/</span> m
</span></span><span style=display:flex><span>        db2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(dz2<span style=color:#f92672>.</span>T, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) <span style=color:#f92672>/</span> m
</span></span><span style=display:flex><span>        dh1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(W2<span style=color:#f92672>.</span>T, dz2<span style=color:#f92672>.</span>T)
</span></span><span style=display:flex><span>        dz1 <span style=color:#f92672>=</span> dh1 <span style=color:#f92672>*</span> relu_derivative(z1)
</span></span><span style=display:flex><span>        dW1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(dz1, X) <span style=color:#f92672>/</span> m
</span></span><span style=display:flex><span>        db1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(dz1, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) <span style=color:#f92672>/</span> m
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> dW1, db1, dW2, db2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(max_iter):
</span></span><span style=display:flex><span>        z1, h1, z2, y_hat <span style=color:#f92672>=</span> Forward_Propagation(X, W1, b1, W2, b2)
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> Cross_entropy_loss(y_hat, y)
</span></span><span style=display:flex><span>        dW1, db1, dW2, db2 <span style=color:#f92672>=</span> Backward_Propagation(X, y, z1, h1, z2, y_hat, W1, W2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        W1 <span style=color:#f92672>-=</span> learning_rate <span style=color:#f92672>*</span> dW1
</span></span><span style=display:flex><span>        b1 <span style=color:#f92672>-=</span> learning_rate <span style=color:#f92672>*</span> db1
</span></span><span style=display:flex><span>        W2 <span style=color:#f92672>-=</span> learning_rate <span style=color:#f92672>*</span> dW2
</span></span><span style=display:flex><span>        b2 <span style=color:#f92672>-=</span> learning_rate <span style=color:#f92672>*</span> db2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> W1, b1, W2, b2
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># &lt;--- From pytorch ---&gt;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.optim <span style=color:#66d9ef>as</span> optim
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MLP</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, input_dim, hidden_dim, output_dim):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(input_dim, hidden_dim)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(hidden_dim, output_dim)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>relu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ReLU() 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>layer1(x))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer2(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>4</span>)  <span style=color:#75715e># 输入特征</span>
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>3</span>, (<span style=color:#ae81ff>100</span>,))  <span style=color:#75715e># 3 类标签</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 模型初始化</span>
</span></span><span style=display:flex><span>input_dim <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>hidden_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>output_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>max_iter <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> MLP(input_dim, hidden_dim, output_dim)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>CrossEntropyLoss()
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>Adam(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(max_iter):
</span></span><span style=display:flex><span>    y_hat <span style=color:#f92672>=</span> model(X)
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> criterion(y_hat, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> epoch <span style=color:#f92672>%</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Epoch </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74>, Loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#f92672>.</span>item()<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#神经元neuron><strong>神经元（Neuron）</strong></a></li><li><a href=#感知机perceptrons><strong>感知机（Perceptrons）</strong></a><ul><li><a href=#模型结构><strong>模型结构</strong></a></li><li><a href=#损失函数><strong>损失函数</strong></a></li><li><a href=#更新规则><strong>更新规则</strong></a></li><li><a href=#感知机perceptrons和逻辑回归logistic-regression的区别><strong>感知机（Perceptrons）和逻辑回归（Logistic Regression）的区别</strong></a></li><li><a href=#感知机代码实现><strong>感知机代码实现</strong></a></li></ul></li><li><a href=#多层感知机multilayer-perceptronmlp><strong>多层感知机（Multilayer Perceptron，MLP）</strong></a><ul><li><a href=#输入层input-layer><strong>输入层（Input Layer）</strong></a></li><li><a href=#隐藏层hidden-layer><strong>隐藏层（Hidden Layer）</strong></a></li><li><a href=#输出层output-layer><strong>输出层（Output Layer）</strong></a></li></ul></li><li><a href=#神经网络运行流程和原理><strong>神经网络运行流程和原理</strong></a><ul><li><a href=#向前传播-forward-propagation><strong>向前传播 (Forward Propagation)</strong></a></li><li><a href=#向后传播-backward-propagation><strong>向后传播 (Backward Propagation)</strong></a></li></ul></li><li><a href=#简单神经网络代码实现><strong>简单神经网络代码实现</strong></a></li></ul></nav></div></aside></main></body></html>