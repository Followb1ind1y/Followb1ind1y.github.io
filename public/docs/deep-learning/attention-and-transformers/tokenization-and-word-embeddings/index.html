<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='
  分词和词嵌入（Tokenization and Word Embeddings）
  #


  分词（Tokenization）
  #


  分词的基本概念
  #

⁉️ 什么是 Tokenization？为什么它对LLM至关重要？
  
什么是 Tokenization？为什么它对LLM至关重要？
Tokenization（分词）是将文本拆分成更小的单元（tokens）的过程，这些单元可以是单词、子词或字符。在自然语言处理中，分词是文本预处理的重要步骤。他直接影响模型对语义的理解能力、计算效率和内存占用。

Example：句子 “Hello, world!” 可被切分为 ["Hello", ",", "world", "!"]（基于空格和标点）。


  Note：简而言之，Tokenization 是一个 将输入文本拆解为 tokens（可能是词、子词或字符）的过程，而词表 是一个 包含所有可能 tokens 的集合，它定义了 token 到数字 ID 的映射。

  


⁉️ 解释什么是 Encoding？Text Encoding，Feature Encoding，Position Encoding 分别是什么？
  
解释什么是 Encoding？
Text Encoding 主要做的事情是 把文本转换成结构化的、可索引的格式，这样后续的模型或者算法可以进行查询和计算，这个过程类似于构建一个 “字典”（vocabulary），把文本转换成可以查询的 ID 表示。除了构建可查询的字典。所以说 分词（tokenization）可以理解为 encoding 的子步骤或前置步骤。
除了 Text Encoding，Encoding 还可以分为：

Text Encoding / Token Encoding - 输入编码 （如 BPE, WordPiece, Token → ID）
Feature Encoding - 特征构建 （如 TF-IDF, BoW, 词频向量等）
Position / Structural Encoding - 结构性编码 （如 Position Encoding in Transformers）

一些简单的 Feature Encoding 包括：'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/deep-learning/attention-and-transformers/tokenization-and-word-embeddings/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Tokenization and Word Embeddings"><meta property="og:description" content='分词和词嵌入（Tokenization and Word Embeddings） # 分词（Tokenization） # 分词的基本概念 # ⁉️ 什么是 Tokenization？为什么它对LLM至关重要？ 什么是 Tokenization？为什么它对LLM至关重要？ Tokenization（分词）是将文本拆分成更小的单元（tokens）的过程，这些单元可以是单词、子词或字符。在自然语言处理中，分词是文本预处理的重要步骤。他直接影响模型对语义的理解能力、计算效率和内存占用。
Example：句子 “Hello, world!” 可被切分为 ["Hello", ",", "world", "!"]（基于空格和标点）。 Note：简而言之，Tokenization 是一个 将输入文本拆解为 tokens（可能是词、子词或字符）的过程，而词表 是一个 包含所有可能 tokens 的集合，它定义了 token 到数字 ID 的映射。
⁉️ 解释什么是 Encoding？Text Encoding，Feature Encoding，Position Encoding 分别是什么？ 解释什么是 Encoding？ Text Encoding 主要做的事情是 把文本转换成结构化的、可索引的格式，这样后续的模型或者算法可以进行查询和计算，这个过程类似于构建一个 “字典”（vocabulary），把文本转换成可以查询的 ID 表示。除了构建可查询的字典。所以说 分词（tokenization）可以理解为 encoding 的子步骤或前置步骤。
除了 Text Encoding，Encoding 还可以分为：
Text Encoding / Token Encoding - 输入编码 （如 BPE, WordPiece, Token → ID） Feature Encoding - 特征构建 （如 TF-IDF, BoW, 词频向量等） Position / Structural Encoding - 结构性编码 （如 Position Encoding in Transformers） 一些简单的 Feature Encoding 包括：'><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Tokenization and Word Embeddings | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/deep-learning/attention-and-transformers/tokenization-and-word-embeddings/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.24532350eecdb297078016289fe973e07204be56bcb24c9b2dd6827d34161602.js integrity="sha256-JFMjUO7NspcHgBYon+lz4HIEvla8skybLdaCfTQWFgI=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/deep-learning/attention-and-transformers/tokenization-and-word-embeddings/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li><li><a href=/docs/machine-learning/computational-performance/>Computational Performance</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><input type=checkbox id=section-d0dd931d60033c220ecd4cd60b7c9170 class=toggle>
<label for=section-d0dd931d60033c220ecd4cd60b7c9170 class="flex justify-between"><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a></label><ul><li><a href=/docs/deep-learning/convolutional-neural-networks/modern-convolutional-neural-networks/>Modern Convolutional Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-a3019bfa8037cc33ed6405d1589b6219 class=toggle>
<label for=section-a3019bfa8037cc33ed6405d1589b6219 class="flex justify-between"><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a></label><ul><li><a href=/docs/deep-learning/recurrent-neural-networks/modern-recurrent-neural-networks/>Modern Recurrent Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-0a43584c16258b228ae9aa8d70efc320 class=toggle checked>
<label for=section-0a43584c16258b228ae9aa8d70efc320 class="flex justify-between"><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a></label><ul><li><a href=/docs/deep-learning/attention-and-transformers/tokenization-and-word-embeddings/ class=active>Tokenization and Word Embeddings</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/transformer-architecture/>Transformer Architecture</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/large-scale-pretraining-with-transformers/>Large-Scale Pretraining with Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/modern-large-language-models/>Modern Large Language Models</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/post-training-large-language-models/>Post-training Large Language Models</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/multimodal-large-language-models/>Multimodal Large Language Models</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92e8358c45c96009753cf4227e9daea8 class=toggle>
<label for=section-92e8358c45c96009753cf4227e9daea8 class="flex justify-between"><a href=/docs/deep-learning/llm-pipelines/>LLM Pipelines</a></label><ul><li><a href=/docs/deep-learning/llm-pipelines/llm-hardware-and-model-size/>LLM Hardware and Model Size</a><ul></ul></li><li><a href=/docs/deep-learning/llm-pipelines/llm-inference-and-deployment/>LLM Inference and Deployment</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul><li><a href=/docs/others/interview-preparation-guide/>Interview Preparation Guide</a><ul></ul></li></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Tokenization and Word Embeddings</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#分词tokenization><strong>分词（Tokenization）</strong></a><ul><li><a href=#分词的基本概念><strong>分词的基本概念</strong></a></li><li><a href=#分词方法bpe-wordpiece-sentencepiece><strong>分词方法（BPE, WordPiece, SentencePiece）</strong></a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=分词和词嵌入tokenization-and-word-embeddings><strong>分词和词嵌入（Tokenization and Word Embeddings）</strong>
<a class=anchor href=#%e5%88%86%e8%af%8d%e5%92%8c%e8%af%8d%e5%b5%8c%e5%85%a5tokenization-and-word-embeddings>#</a></h1><h2 id=分词tokenization><strong>分词（Tokenization）</strong>
<a class=anchor href=#%e5%88%86%e8%af%8dtokenization>#</a></h2><h3 id=分词的基本概念><strong>分词的基本概念</strong>
<a class=anchor href=#%e5%88%86%e8%af%8d%e7%9a%84%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5>#</a></h3><details><summary><strong class=custom-details-title>⁉️ 什么是 Tokenization？为什么它对LLM至关重要？</strong></summary><div class=markdown-inner><h2><b>什么是 Tokenization？为什么它对LLM至关重要？</b></h2><p>Tokenization（分词）是将文本拆分成更小的单元（tokens）的过程，这些单元可以是单词、子词或字符。在自然语言处理中，分词是文本预处理的重要步骤。他直接影响模型对语义的理解能力、计算效率和内存占用。</p><ul><li><strong>Example</strong>：句子 “Hello, world!” 可被切分为 <code>["Hello", ",", "world", "!"]</code>（基于空格和标点）。</li></ul><blockquote class="book-hint warning"><p><strong>Note</strong>：简而言之，Tokenization 是一个 <strong>将输入文本拆解为 tokens（可能是词、子词或字符）的过程</strong>，而<strong>词表</strong> 是一个 <strong>包含所有可能 tokens 的集合</strong>，它定义了 token 到数字 ID 的映射。</p></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 解释什么是 Encoding？Text Encoding，Feature Encoding，Position Encoding 分别是什么？</strong></summary><div class=markdown-inner><h2><b>解释什么是 Encoding？</b></h2><p>Text Encoding 主要做的事情是 <strong>把文本转换成结构化的、可索引的格式，这样后续的模型或者算法可以进行查询和计算</strong>，这个过程类似于构建一个 “字典”（vocabulary），把文本转换成可以查询的 ID 表示。除了构建可查询的字典。所以说 <strong>分词（tokenization）可以理解为 encoding 的子步骤或前置步骤。</strong></p><p>除了 Text Encoding，Encoding 还可以分为：</p><ul><li><strong>Text Encoding / Token Encoding - 输入编码</strong> （如 BPE, WordPiece, Token → ID）</li><li><strong>Feature Encoding - 特征构建</strong> （如 TF-IDF, BoW, 词频向量等）</li><li><strong>Position / Structural Encoding - 结构性编码</strong> （如 Position Encoding in Transformers）</li></ul><p>一些简单的 <strong>Feature Encoding</strong> 包括：</p><ul><li><p><a href=https://followb1ind1y.github.io/docs/machine-learning/data-preprocessing/#%e7%8b%ac%e7%83%ad%e7%bc%96%e7%a0%81one-hot-encoding>独热编码（One-Hot Encoding）</a></p></li><li><p><a href=https://followb1ind1y.github.io/docs/machine-learning/data-preprocessing/#bag-of-words-bow>Bag-of-Words (BoW)</a></p></li><li><p><a href=https://followb1ind1y.github.io/docs/machine-learning/data-preprocessing/#%e7%8b%ac%e7%83%ad%e7%bc%96%e7%a0%81one-hot-encoding>TF-IDF</a></p></li></ul><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>Text Encoding ≈ 构建词表 + 确定如何把文本映射成 token</strong>。也就是说，“encoding”这个词在 tokenizer.encode() 这种 API 里，通常指的是：</p><ul><li>把原始字符串 → token（比如 BPE 的子词切分）</li><li>把 token → token ID（索引在词表中的位置）</li></ul><p>这个过程中依赖词表（vocabulary），所以可以说“构建词表是 encoding 的核心部分之一”。</p><p><strong>Tokenization 是 encoding 的子步骤，Tokenizer 是执行这套 encoding 规则的工具。</strong></p></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 词表大小对模型性能的影响？</strong></summary><div class=markdown-inner><h2><b>词表大小对模型性能的影响？</b></h2><p><strong>词表的大小决定了模型可识别的唯一 Token 数量</strong>，比如 LLaMA 采用了 32k 的词表，而 GPT-2 使用了 50k 词表。较大的词表（包含子词、常用词，甚至整个句子）允许模型以更少的 Token 表示相同文本，<strong>提高表达能力，但也增加了参数规模和计算复杂度</strong>；而较小的词表（只包含字符 a-z, 标点等）则 <strong>减少了计算需求，但可能导致序列变长，进而影响训练效率</strong>。因此，在预训练阶段，词表大小的选择会直接影响模型的记忆能力、计算成本以及推理速度。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：词表（Vocabulary）既可以直接使用预训练模型提供的标准词表，也可以根据自己的数据集重新训练一个词表，具体取决于应用需求：</p><ol><li><strong>直接使用预训练词表</strong>：如 GPT-3、LLaMA、T5 等开源模型的 Tokenizer 已经基于大规模文本语料（如 Common Crawl、Wikipedia）训练了词表，并随模型一起发布。直接使用这些词表能够确保与原始模型的 Token 方式一致，避免 Token 不匹配导致的性能下降。这种方法 <strong>适用于大多数 NLP 任务</strong>，特别是在迁移学习（Transfer Learning）场景下。</li><li><strong>基于自有数据训练新词表</strong>：如果 <strong>目标领域与通用 NLP 语料差异较大（如医学、法律、金融等专业领域），或者需要支持特定语言（如低资源语言或多语言任务）</strong>，可以使用 SentencePiece（支持 BPE、Unigram）或 Hugging Face Tokenizers 来从头训练词表。训练时通常会调整 词表大小（Vocabulary Size），使其适配目标任务。较大的词表可以减少 OOV（Out-Of-Vocabulary）问题，而较小的词表能减少计算复杂度，提高推理速度。</li></ol></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ Tokenizer 在实际模型预训练阶段是如何被使用的？</strong></summary><div class=markdown-inner><h2><b>Tokenizer 在实际模型预训练阶段是如何被使用的？</b></h2><p><strong>Tokenizer（分词器）</strong> 的主要作用是 <strong>将原始文本转换为模型可理解的离散数值表示</strong>，即 <strong>Token ID（标记序列）</strong>。这个过程通常包括分词（Tokenization）、映射（Mapping to Vocabulary） 和 填充/截断（Padding/Truncation）。在分词时，不同的 Tokenizer会根据预定义的 <strong>词表（Vocabulary）</strong> 将文本拆分成最优的子词单元。</p><blockquote class="book-hint warning"><p>以 Hugging Face 的 tokenizer 或 OpenAI 的 tokenizer 为例，它通常会完成以下几个步骤：</p><ol><li><strong>预处理</strong>：清理文本，比如去掉空格、标准化 Unicode 等</li><li><strong>分词（Tokenization）</strong>：比如把句子 “ChatGPT 是谁？” 切成像 <code>['Chat', 'G', 'PT', ' 是', '谁', '?']</code></li><li><strong>映射成 ID（Encoding）</strong>：把这些 token 映射为对应的词汇表索引，例如 <code>[1345, 67, 9801, 345, 26, 9]</code></li><li><strong>加入特殊 token</strong>：比如 <code>[CLS]</code>、<code>[SEP]</code> 或 <code>&lt;|startoftext|></code> 这样的符号</li><li><strong>返回 tensor</strong>：供模型输入使用</li></ol></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ Tokenization 如何影响模型性能？</strong></summary><div class=markdown-inner><h2><b>Tokenization 如何影响模型性能？</b></h2><ul><li><p><strong>词表过大</strong>：增加内存消耗，降低计算效率（Softmax 计算成本高）。</p></li><li><p><strong>词表过小</strong>：导致长序列和语义碎片化（如切分为无意义的子词）。</p></li><li><p><strong>语言适配性</strong>：中/日/韩等非空格语言需要特殊处理（如 SentencePiece）。</p></div></li></ul></details><hr><details><summary><strong class=custom-details-title>⁉️ 如何为多语言模型设计Tokenization方案？</strong></summary><div class=markdown-inner><h2><b>如何为多语言模型设计 Tokenization 方案？</b></h2><ul><li><p><strong>统一词表</strong>：使用 SentencePiece 跨语言训练（如mBERT）。</p></li><li><p><strong>平衡语种覆盖</strong>：根据语种数据量调整合并规则，避免小语种被淹没。</p></li><li><p><strong>特殊标记</strong>：添加语言ID（如 <code>[EN]</code>、<code>[ZH]</code>）引导模型区分语言。</p></div></li></ul></details><h3 id=分词方法bpe-wordpiece-sentencepiece><strong>分词方法（BPE, WordPiece, SentencePiece）</strong>
<a class=anchor href=#%e5%88%86%e8%af%8d%e6%96%b9%e6%b3%95bpe-wordpiece-sentencepiece>#</a></h3><details><summary><strong class=custom-details-title>⁉️ 常见的 Tokenization 方法有哪些？它们的区别是什么？</strong></summary><div class=markdown-inner><h2><b>常见的 Tokenization 方法有哪些？它们的区别是什么</b></h2><ul><li><p><strong>Word-level</strong>：按词切分（如 <code>“natural language processing” → ["natural", "language", "processing"]</code>），但词表大且难以处理未登录词（OOV）。</p></li><li><p><strong>Subword-level（主流方法）</strong>：</p><ul><li><strong>BPE（Byte-Pair Encoding）</strong>：通过合并高频字符对生成子词（如GPT系列使用）。</li><li><strong>WordPiece</strong>：类似BPE，但基于概率合并（如BERT使用）。</li><li><strong>SentencePiece</strong>：无需预分词，直接处理原始文本（如T5使用）。</li></ul></li><li><p><strong>Character-level</strong>：按字符切分，词表极小但序列长且语义建模困难。</p></div></li></ul></details><hr><details><summary><strong class=custom-details-title>⁉️ BPE算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2><b>BPE 算法的工作原理是什么？请举例说明。</b></h2><p>BPE（Byte Pair Encoding）是一种子词分割（Subword Segmentation）算法，核心思想是基于 <strong>统计频率</strong> 的合并（Merge frequent pairs）。</p><ul><li><strong>工作原理</strong>：<ol><li>统计字符对（Byte Pair）频率，找到最常见的相邻字符对。<pre tabindex=0><code>[&#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34; &#34;, &#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34;e&#34;, &#34;r&#34;, &#34; &#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;]
</code></pre></li><li>合并最频繁的字符对，形成新的子词单元。<pre tabindex=0><code>(&#34;l&#34;, &#34;o&#34;) -&gt; 2次
(&#34;o&#34;, &#34;w&#34;) -&gt; 2次
    ...
</code></pre>（第一次）合并 <code>("l", "o") → "lo"</code>：<pre tabindex=0><code>[&#34;lo&#34;, &#34;w&#34;, &#34;lo&#34;, &#34;w&#34;, &#34;er&#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;st&#34;]
</code></pre>（第二次）合并 <code>("lo", "w") → "low"</code><pre tabindex=0><code>[&#34;low&#34;, &#34;low&#34;, &#34;er&#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;st&#34;]
</code></pre></li><li>重复步骤 1 和 2，直到达到预定的子词词汇量。<pre tabindex=0><code>[&#34;low&#34;, &#34;lower&#34;, &#34;er&#34;, &#34;newest&#34;]
</code></pre><pre tabindex=0><code>vocab = {&#34;low&#34;: 100, &#34;lower&#34;: 101, &#34;er&#34;: 102, &#34;newest&#34;: 103}
</code></pre></li></ol></li></ul><p>BPE的优点是它 <strong>能够有效地处理未登录词</strong>，并且在处理长尾词（rare words）时表现良好。比如，词”unhappiness”可以被分解为”un” + “happiness”，而不是完全看作一个新的词。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ WordPiece 算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2><b>WordPiece 算法的工作原理是什么？请举例说明</b></h2><p>WordPiece 的核心理念与BPE相似，但合并策略更注重语义完整性。其训练过程同样从基础单元（如字符）开始，但选择合并的标准并非单纯依赖频率，而是通过 <strong>计算合并后对语言模型概率的提升幅度</strong>，优先保留能够增强语义连贯性的子词。</p><ul><li>假设语言模型的目标是最大化训练数据的似然概率，在 WordPiece 中，简化为 通过子词频率估计概率（即一元语言模型）。通过计算合并后对语言模型概率的提升幅度进行合并：</li></ul><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[
\begin{equation}
P(S)≈ \prod^{n}_{i=1}P(s_i)
\end{equation}
\]</span><ul><li><strong>工作原理</strong>：<ol><li><p>与BPE类似，首先将所有词分解为最小的单位（如字符）。</p><pre tabindex=0><code>[&#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34; &#34;, &#34;l&#34;, &#34;o&#34;, &#34;w&#34;, &#34;e&#34;, &#34;r&#34;, &#34; &#34;, &#34;n&#34;, &#34;e&#34;, &#34;w&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;, &#34; &#34;, &#34;w&#34;, &#34;i&#34;, &#34;d&#34;, &#34;e&#34;, &#34;s&#34;, &#34;t&#34;]
</code></pre></li><li><p>统计所有可能的字符对（或子词对）在文本中的共现频率。</p></li><li><p>合并字符对，选择合并后能 <strong>最大化语言模型似然概率</strong> 的字符对。具体公式为：选择使 <code>score = freq(pair) / (freq(first) * freq(second)) </code>最大的字符对（<strong>与 BPE 不同，BPE 仅选择频率最高的对</strong>）。每次合并对语言模型概率提升最大的合并组合。</p><p>这里的 <code>##</code> 表示这个 token 只能作为后缀出现，不会单独存在。</p><pre tabindex=0><code>{&#34;low&#34;, &#34;##er&#34;, &#34;##ing&#34;, &#34;new&#34;, &#34;##est&#34;, &#34;wide&#34;, &#34;##st&#34;}
</code></pre></li><li><p>重复合并得分最高的字符对，直到达到预设的词汇表大小。</p><pre tabindex=0><code>vocab = {&#34;low&#34;: 100, &#34;##er&#34;: 101, &#34;##ing&#34;: 102, &#34;new&#34;: 103, &#34;##est&#34;: 104, &#34;wide&#34;: 105, &#34;##st&#34;: 106}
</code></pre></li></ol></li></ul><p>WordPiece 通过最大化语言模型概率合并子词，<strong>生成的子词更贴合语义需求</strong>。但计算复杂度更高，需多次评估合并得分。</p><ul><li><p>若需模型捕捉深层语义（如预训练任务），优先选择 WordPiece。</p></li><li><p>若需快速处理大规模数据且词汇表灵活，BPE 更合适。</p></div></li></ul></details><hr><details><summary><strong class=custom-details-title>⁉️ SentencePiece 算法的工作原理是什么？请举例说明。</strong></summary><div class=markdown-inner><h2><b>SentencePiece 算法的工作原理是什么？请举例说明</b></h2><p>SentencePiece 是一种无监督的子词分割算法，其核心创新在于直接处理原始文本（包括空格和特殊符号），无需依赖预分词步骤。<strong>它支持两种底层算法：BPE 或 基于概率的Unigram Language Model</strong>。训练时，SentencePiece 将空格视为普通字符 <code>_</code>，可 <strong>直接处理多语言混合文本（如中英文混杂）</strong>，并自动学习跨语言的统一子词划分规则。</p><ul><li><strong>Example：</strong> <code>"Hello世界" → 编码为 ["▁He", "llo", "▁世", "界"]。</code></li></ul><p>SentencePiece 支持多语言（Multilingual）无需调整，统一处理空格与特殊符号。这一特性使其在需要多语言支持的场景（如T5模型）中表现突出，同时简化了数据处理流程，特别适合处理社交媒体文本等非规范化输入。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 如何处理未登录词（OOV）？</strong></summary><div class=markdown-inner><h2><b>如何处理未登录词（OOV）？</b></h2><ul><li><p><strong>子词切分</strong>：将OOV（Out-of-Vocabulary）词拆分为已知子词（如 <code>“tokenization” → ["token", "ization"]</code>）。</p></li><li><p><strong>回退策略</strong>：使用特殊标记（如 <code>[UNK]</code>），但会损失信息。</p></li><li><p><strong>动态更新词表</strong>：在增量训练时扩展词表。</p></div></li></ul></details><hr></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#分词tokenization><strong>分词（Tokenization）</strong></a><ul><li><a href=#分词的基本概念><strong>分词的基本概念</strong></a></li><li><a href=#分词方法bpe-wordpiece-sentencepiece><strong>分词方法（BPE, WordPiece, SentencePiece）</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>