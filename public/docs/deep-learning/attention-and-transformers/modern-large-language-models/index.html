<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  现代大语言模型（Modern Large Language Models）
  #


  预训练基础知识
  #

⁉️ 什么是预训练？与传统监督学习的区别？
  
什么是预训练？与传统监督学习的区别？
预训练（Pretraining）是一种在大规模无标注数据（Unlabeled Data）上训练深度学习模型的技术，特别常用于自然语言处理（NLP）中的大规模语言模型。在预训练阶段，模型通常采用 自监督学习（Self-Supervised Learning）方法，通过预测被遮蔽的词（如 BERT 的掩码语言模型 Masked Language Model, MLM）或基于上下文预测下一个词（如 GPT 的自回归语言模型 Autoregressive Language Model, AR）来学习文本的统计特性和语义表示（Semantic Representation）。
相比传统的监督学习（Supervised Learning），预训练 不需要大量人工标注数据，而是利用大规模无标签语料，使模型具备广泛的语言理解能力。随后，模型可以通过 微调（Fine-Tuning）在小规模标注数据上进一步优化，以适应具体任务。这种方式相比传统监督学习更加高效，尤其适用于数据标注成本高的任务，同时提升模型的泛化能力（Generalization Ability）和适应性（Adaptability）。

  Note：Pre-training 是在 大规模无监督数据上训练模型，而 Fine-tuning 是在 特定任务或数据集上对模型进行微调。

  


⁉️ LLM 的预训练流程通常涉及到哪些环节？
  
LLM 的预训练流程通常涉及到哪些环节？
在 大规模语言模型（LLM, Large Language Model） 的 预训练（Pretraining） 过程中，整个流程可以从宏观的角度划分为 数据收集与预处理、模型架构设计、训练目标设定、优化与梯度更新、以及分布式训练 五个关键部分，每一部分在模型能力的提升上都起着至关重要的作用。

数据收集与预处理（Data Collection & Preprocessing）：预训练的第一步是收集大规模、高质量的数据集，通常包括 网络文本（web corpus）、书籍（books）、维基百科（Wikipedia）、代码数据（code repositories） 等多种来源。数据经过 去重（deduplication）、格式化（formatting）、分词（tokenization） 等预处理步骤，以确保输入的数据干净、标准化，并能有效地被神经网络处理。此外，还可能应用 文本过滤（text filtering） 和 去毒化（detoxification） 以减少偏见（bias mitigation）和不良内容。
模型架构设计（Model Architecture Design）：现代 LLM 主要采用 Transformer 架构，其核心是 自注意力机制（Self-Attention Mechanism），能够有效建模长距离依赖（long-range dependencies）。具体的设计可能包括 解码器（Decoder-only, 如 GPT 系列） 或 编码器-解码器（Encoder-Decoder, 如 T5） 结构，参数规模从数亿到数千亿不等。此外，还涉及 层数（number of layers）、隐藏维度（hidden size）、多头注意力（multi-head attention）、前馈网络（feedforward network） 等关键超参数的选择。
训练目标设定（Training Objectives）：LLM 的预训练目标通常基于 自监督学习（Self-Supervised Learning），主要分为：

自回归目标（Autoregressive Objective, AR）：如 GPT 系列使用的 因果语言建模（Causal Language Modeling, CLM），即通过已知的上下文预测下一个单词。
自编码目标（Autoencoding Objective, AE）：如 BERT 采用的 掩码语言建模（Masked Language Modeling, MLM），即在输入中随机掩盖（mask）部分单词，并让模型预测原始单词。
对比学习（Contrastive Learning） 和 指令调优（Instruction Tuning） 在部分预训练或微调阶段也会用到，以提升模型的表示能力。


优化与梯度更新（Optimization & Gradient Updates）：预训练的核心是基于 梯度下降（Gradient Descent） 进行参数优化，具体采用 AdamW（带权重衰减的 Adam 优化器）来减少过拟合（overfitting）。此外，由于 LLM 训练涉及超大规模的参数，通常会使用：

混合精度训练（Mixed Precision Training, FP16/BF16） 降低计算成本。
梯度裁剪（Gradient Clipping） 解决梯度爆炸（Gradient Explosion）。
学习率调度（Learning Rate Scheduling, 如 Cosine Decay, Warmup） 提升收敛效率。


分布式训练（Distributed Training）：由于 LLM 规模巨大，单机难以承载完整计算，因此需要分布式训练，包括：

数据并行（Data Parallelism, DP）：多个 GPU 处理相同的模型，但分配不同的数据批次（batches）。
模型并行（Model Parallelism, MP）：模型的不同部分分布在多个 GPU 计算，适用于超大参数模型。
流水线并行（Pipeline Parallelism） 和 张量并行（Tensor Parallelism） 结合使用，以提升大模型的计算效率。



  


⁉️ 解释Encoder-only，Encoder-Decoder，Decoder-only 三种模式的区别以及使用场景？
  
解释Encoder-only，Encoder-Decoder，Decoder-only 三种模式的区别以及使用场景？


Encoder-only 模式：在这种架构中，只有 编码器（Encoder） 部分用于处理输入数据，通常应用于 文本分类（Text Classification）、命名实体识别（Named Entity Recognition, NER） 和 情感分析（Sentiment Analysis） 等任务。该模式的主要任务是从输入序列中提取信息，生成固定长度的表示，适用于 需要理解输入而不生成输出 的任务。例如，BERT 就是一个典型的 Encoder-only 模型，通过 自注意力机制（Self-Attention） 学习输入文本的上下文信息并生成表示。"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/deep-learning/attention-and-transformers/modern-large-language-models/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Modern LLMs and Pre-Training"><meta property="og:description" content="现代大语言模型（Modern Large Language Models） # 预训练基础知识 # ⁉️ 什么是预训练？与传统监督学习的区别？ 什么是预训练？与传统监督学习的区别？ 预训练（Pretraining）是一种在大规模无标注数据（Unlabeled Data）上训练深度学习模型的技术，特别常用于自然语言处理（NLP）中的大规模语言模型。在预训练阶段，模型通常采用 自监督学习（Self-Supervised Learning）方法，通过预测被遮蔽的词（如 BERT 的掩码语言模型 Masked Language Model, MLM）或基于上下文预测下一个词（如 GPT 的自回归语言模型 Autoregressive Language Model, AR）来学习文本的统计特性和语义表示（Semantic Representation）。
相比传统的监督学习（Supervised Learning），预训练 不需要大量人工标注数据，而是利用大规模无标签语料，使模型具备广泛的语言理解能力。随后，模型可以通过 微调（Fine-Tuning）在小规模标注数据上进一步优化，以适应具体任务。这种方式相比传统监督学习更加高效，尤其适用于数据标注成本高的任务，同时提升模型的泛化能力（Generalization Ability）和适应性（Adaptability）。
Note：Pre-training 是在 大规模无监督数据上训练模型，而 Fine-tuning 是在 特定任务或数据集上对模型进行微调。
⁉️ LLM 的预训练流程通常涉及到哪些环节？ LLM 的预训练流程通常涉及到哪些环节？ 在 大规模语言模型（LLM, Large Language Model） 的 预训练（Pretraining） 过程中，整个流程可以从宏观的角度划分为 数据收集与预处理、模型架构设计、训练目标设定、优化与梯度更新、以及分布式训练 五个关键部分，每一部分在模型能力的提升上都起着至关重要的作用。
数据收集与预处理（Data Collection & Preprocessing）：预训练的第一步是收集大规模、高质量的数据集，通常包括 网络文本（web corpus）、书籍（books）、维基百科（Wikipedia）、代码数据（code repositories） 等多种来源。数据经过 去重（deduplication）、格式化（formatting）、分词（tokenization） 等预处理步骤，以确保输入的数据干净、标准化，并能有效地被神经网络处理。此外，还可能应用 文本过滤（text filtering） 和 去毒化（detoxification） 以减少偏见（bias mitigation）和不良内容。 模型架构设计（Model Architecture Design）：现代 LLM 主要采用 Transformer 架构，其核心是 自注意力机制（Self-Attention Mechanism），能够有效建模长距离依赖（long-range dependencies）。具体的设计可能包括 解码器（Decoder-only, 如 GPT 系列） 或 编码器-解码器（Encoder-Decoder, 如 T5） 结构，参数规模从数亿到数千亿不等。此外，还涉及 层数（number of layers）、隐藏维度（hidden size）、多头注意力（multi-head attention）、前馈网络（feedforward network） 等关键超参数的选择。 训练目标设定（Training Objectives）：LLM 的预训练目标通常基于 自监督学习（Self-Supervised Learning），主要分为： 自回归目标（Autoregressive Objective, AR）：如 GPT 系列使用的 因果语言建模（Causal Language Modeling, CLM），即通过已知的上下文预测下一个单词。 自编码目标（Autoencoding Objective, AE）：如 BERT 采用的 掩码语言建模（Masked Language Modeling, MLM），即在输入中随机掩盖（mask）部分单词，并让模型预测原始单词。 对比学习（Contrastive Learning） 和 指令调优（Instruction Tuning） 在部分预训练或微调阶段也会用到，以提升模型的表示能力。 优化与梯度更新（Optimization & Gradient Updates）：预训练的核心是基于 梯度下降（Gradient Descent） 进行参数优化，具体采用 AdamW（带权重衰减的 Adam 优化器）来减少过拟合（overfitting）。此外，由于 LLM 训练涉及超大规模的参数，通常会使用： 混合精度训练（Mixed Precision Training, FP16/BF16） 降低计算成本。 梯度裁剪（Gradient Clipping） 解决梯度爆炸（Gradient Explosion）。 学习率调度（Learning Rate Scheduling, 如 Cosine Decay, Warmup） 提升收敛效率。 分布式训练（Distributed Training）：由于 LLM 规模巨大，单机难以承载完整计算，因此需要分布式训练，包括： 数据并行（Data Parallelism, DP）：多个 GPU 处理相同的模型，但分配不同的数据批次（batches）。 模型并行（Model Parallelism, MP）：模型的不同部分分布在多个 GPU 计算，适用于超大参数模型。 流水线并行（Pipeline Parallelism） 和 张量并行（Tensor Parallelism） 结合使用，以提升大模型的计算效率。 ⁉️ 解释Encoder-only，Encoder-Decoder，Decoder-only 三种模式的区别以及使用场景？ 解释Encoder-only，Encoder-Decoder，Decoder-only 三种模式的区别以及使用场景？ Encoder-only 模式：在这种架构中，只有 编码器（Encoder） 部分用于处理输入数据，通常应用于 文本分类（Text Classification）、命名实体识别（Named Entity Recognition, NER） 和 情感分析（Sentiment Analysis） 等任务。该模式的主要任务是从输入序列中提取信息，生成固定长度的表示，适用于 需要理解输入而不生成输出 的任务。例如，BERT 就是一个典型的 Encoder-only 模型，通过 自注意力机制（Self-Attention） 学习输入文本的上下文信息并生成表示。"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Modern LLMs and Pre-Training | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/deep-learning/attention-and-transformers/modern-large-language-models/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.670555303dfbb7938e0816360e2c1564f40948efda7b217bb34a969fae5e3801.js integrity="sha256-ZwVVMD37t5OOCBY2DiwVZPQJSO/aeyF7s0qWn65eOAE=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/deep-learning/attention-and-transformers/modern-large-language-models/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li><li><a href=/docs/machine-learning/computational-performance/>Computational Performance</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><input type=checkbox id=section-d0dd931d60033c220ecd4cd60b7c9170 class=toggle>
<label for=section-d0dd931d60033c220ecd4cd60b7c9170 class="flex justify-between"><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a></label><ul><li><a href=/docs/deep-learning/convolutional-neural-networks/modern-convolutional-neural-networks/>Modern Convolutional Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-a3019bfa8037cc33ed6405d1589b6219 class=toggle>
<label for=section-a3019bfa8037cc33ed6405d1589b6219 class="flex justify-between"><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a></label><ul><li><a href=/docs/deep-learning/recurrent-neural-networks/modern-recurrent-neural-networks/>Modern Recurrent Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-0a43584c16258b228ae9aa8d70efc320 class=toggle checked>
<label for=section-0a43584c16258b228ae9aa8d70efc320 class="flex justify-between"><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a></label><ul><li><a href=/docs/deep-learning/attention-and-transformers/tokenization-and-word-embeddings/>Tokenization and Word Embeddings</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/transformer-architecture/>Transformer Architecture</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/modern-large-language-models/ class=active>Modern LLMs and Pre-Training</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/post-training-large-language-models/>Post-training LLMs</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/multimodal-large-language-models/>Multimodal Large Language Models</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92e8358c45c96009753cf4227e9daea8 class=toggle>
<label for=section-92e8358c45c96009753cf4227e9daea8 class="flex justify-between"><a href=/docs/deep-learning/llm-pipelines/>LLM Pipelines</a></label><ul><li><a href=/docs/deep-learning/llm-pipelines/llm-hardware-and-model-size/>LLM Hardware and Model Size</a><ul></ul></li><li><a href=/docs/deep-learning/llm-pipelines/large-scale-pretraining-with-transformers/>Large-Scale Pretraining with Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/llm-pipelines/llm-inference-and-deployment/>LLM Inference and Deployment</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul><li><a href=/docs/others/interview-preparation-guide/>Interview Preparation Guide</a><ul></ul></li></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Modern LLMs and Pre-Training</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#预训练基础知识><strong>预训练基础知识</strong></a></li><li><a href=#预训练细节><strong>预训练细节</strong></a></li><li><a href=#encoder-only><strong>Encoder-Only</strong></a></li><li><a href=#encoder-decoder><strong>Encoder-Decoder</strong></a></li><li><a href=#decoder-only><strong>Decoder-Only</strong></a></li><li><a href=#训练优化><strong>训练优化</strong></a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=现代大语言模型modern-large-language-models><strong>现代大语言模型（Modern Large Language Models）</strong>
<a class=anchor href=#%e7%8e%b0%e4%bb%a3%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8bmodern-large-language-models>#</a></h1><h2 id=预训练基础知识><strong>预训练基础知识</strong>
<a class=anchor href=#%e9%a2%84%e8%ae%ad%e7%bb%83%e5%9f%ba%e7%a1%80%e7%9f%a5%e8%af%86>#</a></h2><details><summary><strong class=custom-details-title>⁉️ 什么是预训练？与传统监督学习的区别？</strong></summary><div class=markdown-inner><h2><b>什么是预训练？与传统监督学习的区别？</b></h2><p>预训练（Pretraining）是一种在大规模无标注数据（Unlabeled Data）上训练深度学习模型的技术，特别常用于自然语言处理（NLP）中的大规模语言模型。在预训练阶段，模型通常采用 <strong>自监督学习（Self-Supervised Learning）方法</strong>，通过预测被遮蔽的词（如 BERT 的掩码语言模型 Masked Language Model, MLM）或基于上下文预测下一个词（如 GPT 的自回归语言模型 Autoregressive Language Model, AR）来学习文本的统计特性和语义表示（Semantic Representation）。</p><p>相比传统的监督学习（Supervised Learning），预训练 <strong>不需要大量人工标注数据，而是利用大规模无标签语料</strong>，使模型具备广泛的语言理解能力。随后，模型可以通过 <strong>微调（Fine-Tuning）在小规模标注数据上进一步优化</strong>，以适应具体任务。这种方式相比传统监督学习更加高效，尤其适用于数据标注成本高的任务，同时提升模型的泛化能力（Generalization Ability）和适应性（Adaptability）。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：Pre-training 是在 <strong>大规模无监督数据上训练模型</strong>，而 Fine-tuning 是在 <strong>特定任务或数据集上对模型进行微调</strong>。</p></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ LLM 的预训练流程通常涉及到哪些环节？</strong></summary><div class=markdown-inner><h2><b>LLM 的预训练流程通常涉及到哪些环节？</b></h2><p>在 大规模语言模型（LLM, Large Language Model） 的 预训练（Pretraining） 过程中，整个流程可以从宏观的角度划分为 数据收集与预处理、模型架构设计、训练目标设定、优化与梯度更新、以及分布式训练 五个关键部分，每一部分在模型能力的提升上都起着至关重要的作用。</p><ol><li><strong>数据收集与预处理（Data Collection & Preprocessing）</strong>：预训练的第一步是收集大规模、高质量的数据集，通常包括 网络文本（web corpus）、书籍（books）、维基百科（Wikipedia）、代码数据（code repositories） 等多种来源。数据经过 去重（deduplication）、格式化（formatting）、分词（tokenization） 等预处理步骤，以确保输入的数据干净、标准化，并能有效地被神经网络处理。此外，还可能应用 文本过滤（text filtering） 和 去毒化（detoxification） 以减少偏见（bias mitigation）和不良内容。</li><li><strong>模型架构设计（Model Architecture Design）</strong>：现代 LLM 主要采用 Transformer 架构，其核心是 自注意力机制（Self-Attention Mechanism），能够有效建模长距离依赖（long-range dependencies）。具体的设计可能包括 解码器（Decoder-only, 如 GPT 系列） 或 编码器-解码器（Encoder-Decoder, 如 T5） 结构，参数规模从数亿到数千亿不等。此外，还涉及 层数（number of layers）、隐藏维度（hidden size）、多头注意力（multi-head attention）、前馈网络（feedforward network） 等关键超参数的选择。</li><li><strong>训练目标设定（Training Objectives）</strong>：LLM 的预训练目标通常基于 自监督学习（Self-Supervised Learning），主要分为：<ul><li>自回归目标（Autoregressive Objective, AR）：如 GPT 系列使用的 因果语言建模（Causal Language Modeling, CLM），即通过已知的上下文预测下一个单词。</li><li>自编码目标（Autoencoding Objective, AE）：如 BERT 采用的 掩码语言建模（Masked Language Modeling, MLM），即在输入中随机掩盖（mask）部分单词，并让模型预测原始单词。</li><li>对比学习（Contrastive Learning） 和 指令调优（Instruction Tuning） 在部分预训练或微调阶段也会用到，以提升模型的表示能力。</li></ul></li><li><strong>优化与梯度更新（Optimization & Gradient Updates）</strong>：预训练的核心是基于 梯度下降（Gradient Descent） 进行参数优化，具体采用 AdamW（带权重衰减的 Adam 优化器）来减少过拟合（overfitting）。此外，由于 LLM 训练涉及超大规模的参数，通常会使用：<ul><li>混合精度训练（Mixed Precision Training, FP16/BF16） 降低计算成本。</li><li>梯度裁剪（Gradient Clipping） 解决梯度爆炸（Gradient Explosion）。</li><li>学习率调度（Learning Rate Scheduling, 如 Cosine Decay, Warmup） 提升收敛效率。</li></ul></li><li><strong>分布式训练（Distributed Training）</strong>：由于 LLM 规模巨大，单机难以承载完整计算，因此需要分布式训练，包括：<ul><li>数据并行（Data Parallelism, DP）：多个 GPU 处理相同的模型，但分配不同的数据批次（batches）。</li><li>模型并行（Model Parallelism, MP）：模型的不同部分分布在多个 GPU 计算，适用于超大参数模型。</li><li>流水线并行（Pipeline Parallelism） 和 张量并行（Tensor Parallelism） 结合使用，以提升大模型的计算效率。</li></ul></li></ol></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 解释Encoder-only，Encoder-Decoder，Decoder-only 三种模式的区别以及使用场景？</strong></summary><div class=markdown-inner><h2><b>解释Encoder-only，Encoder-Decoder，Decoder-only 三种模式的区别以及使用场景？</b></h2><ul><li><p><strong>Encoder-only 模式</strong>：在这种架构中，只有 <strong>编码器（Encoder）</strong> 部分用于处理输入数据，通常应用于 文本分类（Text Classification）、命名实体识别（Named Entity Recognition, NER） 和 情感分析（Sentiment Analysis） 等任务。该模式的主要任务是从输入序列中提取信息，生成固定长度的表示，适用于 <strong>需要理解输入而不生成输出</strong> 的任务。例如，BERT 就是一个典型的 Encoder-only 模型，通过 自注意力机制（Self-Attention） 学习输入文本的上下文信息并生成表示。</p></li><li><p><strong>Encoder-Decoder 模式</strong>：这种架构包含一个 编码器（Encoder） 和一个 解码器（Decoder），用于从输入生成输出。输入通过编码器进行处理，得到一个上下文表示，然后解码器根据这个表示生成最终输出。这种结构非常适合 <strong>序列到序列任务（Sequence-to-Sequence Tasks）</strong>，如 机器翻译（Machine Translation） 和 文本摘要（Text Summarization）。</p></li><li><p><strong>Decoder-only 模式</strong>：这种架构仅包含 <strong>解码器（Decoder）</strong>，通常用于 <strong>自回归生成任务（Autoregressive Generation Tasks）</strong>，例如 文本生成（Text Generation） 和 语言建模（Language Modeling）。在这种模式下，模型根据前面的输入和已经生成的词预测下一个词。一个典型的例子是 GPT 系列模型，它基于 Decoder-only 架构，通过不断预测下一个词来生成连贯的文本。此模式非常适用于 <strong>需要根据上下文生成输出</strong> 的任务。</p></div></li></ul></details><hr><h2 id=预训练细节><strong>预训练细节</strong>
<a class=anchor href=#%e9%a2%84%e8%ae%ad%e7%bb%83%e7%bb%86%e8%8a%82>#</a></h2><details><summary><strong class=custom-details-title>⁉️ 什么是参数初始化？有哪些适合 LLM 训练的参数初始化策略（Parameter Initialization）？</strong></summary><div class=markdown-inner><h2><b>什么是参数初始化？有哪些适合 LLM 训练的参数初始化策略（Parameter Initialization）？</b></h2><p>参数初始化（Parameter Initialization） 是神经网络训练中的一个关键步骤，旨在为 <strong>网络的权重（weights）和偏置（biases）赋予初始值</strong>。这些初始值对模型的训练收敛速度、稳定性及最终性能有着重要影响。<strong>合理的参数初始化策略能够避免梯度消失（Vanishing Gradient）或梯度爆炸（Exploding Gradient）等问题，进而提高训练效率</strong>。</p><p>对于 大规模语言模型（Large Language Models, LLM） 的训练，一些常见且适用的参数初始化策略包括：</p><ol><li><strong>Xavier 初始化（Xavier Initialization）</strong>：也叫做 Glorot 初始化，它通过考虑输入和输出的神经元数量来设置权重的方差。具体来说，权重的方差设置为：</li></ol><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[
W_{i,j} \sim N\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)
\]</span><p>其中 n_in 和 n_out 分别是当前层的输入和输出神经元数量。此策略通常用于 sigmoid 或 tanh 激活函数的网络，能够帮助缓解梯度消失问题。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>Xavier 初始化适用于激活函数是 sigmoid 或 tanh 的网络</strong>的原因是这些激活函数的<strong>导数容易趋于零</strong>，尤其是在输入值落入激活函数的<strong>饱和区（Sigmoid 的两侧平坦区域）</strong>。如果权重初始化过大，输入会快速进入饱和区，导致梯度消失。如果权重初始化过小，输出信号会逐层衰减，最终导致梯度消失。</p><p>Xavier 的初始化方法<strong>将权重分布限定在一个较小的范围内</strong>，使输入值主要分布在 Sigmoid 和 Tanh 的线性区，避免梯度消失。在较深的网络中，信号可能仍会因为层数的累积效应导致衰减或放大。</p></blockquote><ol start=2><li><strong>He 初始化（He Initialization）</strong>：类似于 Xavier 初始化，但特别适用于 ReLU 激活函数。它通过设置权重的方差，有效地解决了 ReLU 激活函数中常见的 dying ReLU 问题，即一些神经元始终不激活。</li></ol><span>\[
W_{i,j} \sim N\left(0, \frac{2}{n_{\text{in}}}\right)
\]</span><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>He 初始化适用于激活函数是ReLU及其变种</strong>的原因是对于 ReLU，当输入为负时，输出恒为 0；当输入为正时，输出为原值。由于一部分神经元输出会被截断为 0，导致<strong>有效的参与计算的神经元数量减少</strong>（称为“稀疏激活”现象）。如果初始化权重过小，信号会迅速减弱，导致梯度消失；而如果权重过大，信号会迅速放大，导致梯度爆炸。</p><p>He 初始化通过<strong>设定较大的方差</strong>，补偿了 ReLU 截断负值导致的信号损失。这样可以让激活值的分布更均衡，<strong>避免信号快速衰减或放大</strong>。He 初始化<strong>根据输入层大小调整权重的方差，使每层的输出方差保持相对稳定</strong>，即使网络层数增加，信号也不会显著衰减或爆炸。</p></blockquote><ol start=3><li><strong>Pretrained Initialization</strong>：对于 LLM，使用在大规模数据集上预训练的权重作为初始化参数（例如 BERT、GPT）是一种常见且有效的做法。通过 <strong>迁移学习（Transfer Learning）</strong>，这种初始化策略能够显著加速训练过程，并提升模型的性能。</li></ol></div></details><hr><details><summary><strong class=custom-details-title>⁉️ LLM 预训练中为什么常用 Adam 或 AdamW 优化器？</strong></summary><div class=markdown-inner><h2><b>LLM 预训练中为什么常用 Adam 或 AdamW 优化器？</b></h2><p>在大规模语言模型（Large Language Model, LLM）预训练中，Adam 和 AdamW 优化器被广泛采用，主要是因为它们具备高效处理 <strong>非平稳目标函数（non-stationary objective functions）和稀疏梯度（sparse gradients）</strong> 的能力，并能显著 <strong>提升训练稳定性与收敛速度</strong>。</p><ul><li><strong>Adam（Adaptive Moment Estimation）</strong> 优化器的核心思想是对每个参数维护一阶矩估计 <code>m_t</code> 和二阶矩估计 <code>v_t</code>，即梯度的一阶与二阶指数加权平均值，其更新公式如下：</li></ul><span>\[
\begin{aligned}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \\
\theta_{t+1} &= \theta_t - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{aligned}
\]</span><p>在大规模模型训练中，Adam 的自适应性特别关键：它为每个参数独立调整学习率，使训练过程对初始学习率的选择更加鲁棒，尤其在 early training phase 能更快收敛；同时一阶与二阶动量估计也减缓了梯度震荡，稳定了训练轨迹。</p><ul><li><strong>AdamW（Adam with decoupled weight decay）</strong> 是对 Adam 的一个重要改进。在原始 Adam 中引入 L2 正则项时，正则化项被错误地加入到了梯度更新路径中，违背了权重衰减（Weight Decay）应独立于梯度计算的原则。AdamW 正确地将权重衰减项从梯度更新中解耦出来：</li></ul><span>\[
\theta_{t+1} = \theta_t - \alpha \cdot \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \cdot \theta_t \right)
\]</span><p>这里 λ 是 weight decay 系数，直接对参数进行衰减，从而更有效地控制模型容量，减少过拟合。此策略被证明在 Transformer-based 模型（如 BERT、GPT、T5）预训练中效果更优，是现代预训练标准范式的一部分。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 什么是 Gradient Clipping？在预训练中它通常怎么设置？</strong></summary><div class=markdown-inner><h2><b>什么是 Gradient Clipping？在预训练中它通常怎么设置？</b></h2><p><strong>Gradient Clipping（梯度裁剪）</strong> 是一种用于稳定神经网络训练过程的技巧，尤其常用于训练大型模型如 Transformer 或 LLM（Large Language Models）。在深度网络中，尤其是带有大量参数和长序列依赖的模型中，反向传播过程中可能出现 <strong>梯度爆炸（Gradient Explosion）</strong> 的问题，即某些参数的梯度异常大，导致参数更新过猛，从而使 loss 发散甚至出现 NaN。Gradient Clipping 的作用就是在这种情况下对梯度进行限制，<strong>防止模型参数的更新幅度过大</strong>，从而提高训练的稳定性。</p><p>在数学上，假设当前参数 \theta 的总梯度向量是:</p><p>$$
g = \nabla_\theta \mathcal{L}(\theta)
$$</p><p>我们可以计算它的 L2 范数（Euclidean norm）</p><p>$$
||g||_2 = \sqrt{\sum_i g_i^2}
$$</p><p>然后我们设置一个阈值 τ（通常是一个小正数，比如 1.0 或 0.5），当梯度范数超过阈值时，我们将梯度按比例缩放，使得其范数不超过 τ，公式如下：</p><span>\[
g{\prime} = \begin{cases}
g & \text{if } \|g\|_2 \le \tau \\
\tau \cdot \frac{g}{\|g\|_2} & \text{if } \|g\|_2 > \tau
\end{cases}
\]</span><p>也就是说，如果梯度“太大”，我们就把它整个向量缩放到阈值 τ 的长度。这样的缩放是向量级别的，不会改变方向，但会限制更新步长，从而避免模型训练时的不稳定震荡。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：在实际的 LLM 预训练中，Gradient Clipping 是一个非常常见的稳定技巧，<strong>尤其是在使用 mixed precision training（如 FP16）时，浮点数表示范围更窄，更容易发生梯度爆炸</strong>。通常的设置方式是：</p><ul><li>使用 global norm clipping（全局范数裁剪），即不是每层单独裁剪，而是对所有参数梯度的联合向量做裁剪；</li><li>裁剪阈值设置为 1.0 是最常见的经验值；</li><li>在实现上，大多数框架（如 PyTorch）提供内置函数，如 <code>torch.nn.utils.clip_grad_norm_()</code> 来自动完成这一操作；</li><li>在 Transformer-based 模型（如 BERT、GPT、T5）的大规模预训练中，Gradient Clipping 常常与 learning rate scheduler、warm-up 策略等配合使用以确保训练稳定。</li></ul></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 为什么大模型预训练时通常需要 Warmup 学习率策略？</strong></summary><div class=markdown-inner><h2><b>为什么大模型预训练时通常需要 Warmup 学习率策略？</b></h2><p>在大规模语言模型（Large Language Models, 简称 LLM）预训练的初期，通常会采用 <strong>Warmup 学习率策略（Warmup Learning Rate Strategy）</strong>，其核心目的是为了防止训练初期参数剧烈震荡，提升模型稳定性和收敛速度。</p><p>具体来说，训练神经网络时，尤其是 Transformer 这类具有复杂残差结构（Residual Connections）和层归一化（Layer Normalization）的模型，如果在刚开始使用一个较大的学习率（Learning Rate），模型的参数尚未调整到合适的尺度，会导致梯度过大，引发梯度爆炸（Gradient Explosion）或者优化方向震荡，从而影响模型性能甚至训练失败。而 Warmup 策略通过在前若干步逐渐提升学习率，让网络在参数尚未稳定时“热启动”，逐步适应学习率的放大，从而缓解初期训练不稳定的问题。以 Transformer 原始论文中的学习率调度为例，其学习率调度公式如下：</p><span>\[
\text{lr}(t) = d_{\text{model}}^{-0.5} \cdot \min(t^{-0.5}, \ t \cdot \text{warmup\_steps}^{-1.5})
\]</span><p>整个公式的含义是：<strong>前 <code>warmup_steps</code> 步中，学习率呈线性上升；之后则以 <code>t^{-0.5}</code> 的速度衰减。</strong> 这种策略结合了 Warmup + Inverse Square Root Decay 的优势，使得训练初期更加平稳，训练后期又具有良好的收敛性能。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 如何理解训练中 Batch Size 和模型收敛的关系？</strong></summary><div class=markdown-inner><h2><b>如何理解训练中 Batch Size 和模型收敛的关系？</b></h2><p>在大型语言模型（LLM, Large Language Model）训练中，Batch Size（批大小）是影响模型收敛速度、性能表现和最终泛化能力的关键超参数之一。Batch Size 指的是 <strong>每次参数更新时使用的样本数量</strong>，它与 <strong>梯度估计的稳定性</strong> 和 <strong>优化路径的收敛特性</strong> 密切相关。</p><p>当使用较小的 batch size 时，梯度估计具有更高的方差（variance），训练过程较为“嘈杂”，这虽然可能导致训练过程不稳定，但也能更容易跳出局部最优，具有较强的正则化效果（即有助于泛化）。而使用较大的 batch size，梯度估计更加精确，训练曲线更平滑，可以更稳定地收敛，但也更容易陷入平稳鞍点（sharp minima），泛化能力下降。</p><p>一种常见的经验法则是：<strong>在扩大 batch size 的同时同步增大学习率（Learning Rate）。</strong></p><p>在 LLM 训练中，batch size 越大，优化器状态（如梯度一阶和二阶动量） 的稳定性越好，同时训练可以更高效并行化。值得注意的是，large-batch training 面临“generalization gap”问题：在相同训练 loss 下，较大的 batch size 通常泛化误差更高（test loss 更大）。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 困惑度（Perplexity）的物理意义是什么？为什么不能完全依赖它评估模型？</strong></summary><div class=markdown-inner><h2><b>困惑度（Perplexity）的物理意义是什么？为什么不能完全依赖它评估模型？</b></h2><p>困惑度（Perplexity）是衡量语言模型预测能力的经典指标，其物理意义可以理解为“模型在面对当前语言任务时的不确定性”或者“平均每一步预测所面临的选择数量”。数学上，如果一个模型对长度为 <code>N</code> 的真实序列 <code>x_1, x_2, …, x_N</code> 的联合概率为 <code>P(x_1, …, x_N)</code>，那么困惑度定义为：</p><p>$$
\text{Perplexity} = P(x_1, x_2, …, x_N)^{-\frac{1}{N}} = \exp\left( -\frac{1}{N} \sum_{i=1}^N \log P(x_i | x_1, …, x_{i-1}) \right)
$$</p><p>也就是说，困惑度本质上是平均负对数似然（Negative Log-Likelihood, NLL）的指数形式。困惑度越小，表示模型越“确信”自己的预测，说明语言建模能力越强。但在实际应用中，困惑度不能作为评估模型好坏的唯一标准，原因主要有以下几点：</p><p>首先，困惑度 <strong>只衡量了模型在训练集或验证集上的语言流畅性预测能力</strong>，它并不考虑任务完成度或输出的语义质量。比如一个困惑度很低的模型可能在生成对话时语法无误但语义空洞、脱离上下文，无法有效完成如问答、摘要等任务。</p><p>其次，<strong>困惑度具有不可跨模型或tokenizer比较的局限性</strong>。不同模型的词表（Vocabulary）大小不同，Tokenization策略不同（如BPE、WordPiece、SentencePiece），即使输入文本相同，模型面对的token序列结构也不同，导致困惑度不具备一致可比性。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 为什么大模型会出现幻觉（Hallucination）？如何检测和减少？</strong></summary><div class=markdown-inner><h2><b>为什么大模型会出现幻觉（Hallucination）？如何检测和减少？</b></h2><p>大语言模型（LLM）产生幻觉（Hallucination）的本质原因，是它在生成内容时不依赖事实性记忆或知识库，而是基于训练过程中学到的统计相关性模式（statistical co-occurrence patterns）。语言模型通常使用 maximum likelihood estimation（MLE） 训练目标，即最大化下一个词的条件概率：</p><p>$$
\max_\theta \sum_{t=1}^{T} \log P_\theta(w_t \mid w_1, \ldots, w_{t-1})
$$</p><p>这种训练方式鼓励模型在给定上下文下生成最可能的词，而非最“正确”的词。如果训练数据中存在矛盾、虚假、或低质量内容，模型可能会学到错误相关性，并在推理时“自信地编造”不存在的事实，从而产生幻觉。</p><p>此外，当前主流的 LLM 如 GPT 系列是 decoder-only 架构，缺乏内建的知识查询机制（如检索模块），也缺少对事实一致性的显式建模能力（例如逻辑一致性、实体对齐）。当输入提示（prompt）模糊、开放性高，或涉及冷门事实时，模型只能根据相似上下文生成“可能对但未必正确”的输出，幻觉风险大幅增加。</p><p>减少幻觉的有效方法包括：</p><ol><li><strong>强化训练数据质量</strong>：使用高质量、事实一致的文本进行预训练和微调；避免使用社交媒体等噪声源。</li><li><strong>加入外部知识源（Retrieval-Augmented Generation, RAG）</strong>：通过检索模块提供支持内容，再交由生成器解码，形成“生成-检索耦合系统”。</li><li><strong>后训练微调（Instruction tuning / RLHF）</strong>：用人类标注反馈指导模型偏向真实、有用的回答，尤其强化“拒答能力（refusal when unsure）”。</li></ol></div></details><hr><ul><li>LLM 中的 activation 函数</li><li>LLM 的evaluation Metrics</li></ul><h2 id=encoder-only><strong>Encoder-Only</strong>
<a class=anchor href=#encoder-only>#</a></h2><details><summary><strong class=custom-details-title>⁉️ Encoder-Only 在架构上和 transformer 有什么区别？</strong></summary><div class=markdown-inner><h2><b>Encoder-Only 在架构上和 transformer 有什么区别？</b></h2><p>Encoder-Only 架构只保留了 Transformer 的 Encoder 部分，完全去掉了 Decoder，所以它 <strong>只能用于特征提取或上下文建模（Representation Learning），而不是生成任务（Generation Tasks）</strong>。它的训练目标通常是掩码语言模型（Masked Language Modeling, MLM），而不是自回归语言模型（Autoregressive Language Modeling, AR）。换句话说，Encoder-Only模型的输入和输出都是“同时存在的完整句子”，<strong>模型学习的是如何理解和表征输入，而不是如何生成新的输出</strong>。</p><p>和完整的Transformer结构相比，缺少了解码器的部分，因此结构上更简单，适用的任务也偏向于分类（Classification）、特征抽取（Feature Extraction）、检索（Retrieval）等理解类任务（Understanding Tasks），而非文本生成（Text Generation）。</p><div align=center><img src=/images/BERT.png width=200px/></div></div></details><hr><details><summary><strong class=custom-details-title>⁉️ BERT 的预训练目标有哪些？什么是 MLM 和 NSP？具体如何实现？</strong></summary><div class=markdown-inner><h2><b>BERT 的预训练目标有哪些？什么是 MLM 和 NSP？具体如何实现？</b></h2><p>BERT（Bidirectional Encoder Representations from Transformers）的预训练目标主要包括 <strong>Masked Language Modeling（MLM）</strong> 和 <strong>Next Sentence Prediction（NSP）</strong>。</p><ul><li><p><strong>① Masked Language Modeling (MLM)</strong></p><p>Masked Language Modeling 是 BERT 预训练的核心目标，目的是让模型通过理解上下文，预测被遮盖（Masked）的单词。相比 GPT 的自回归预测，MLM 允许模型在训练时同时观察整个输入句子的左右两侧信息，是一种双向语言建模（Bidirectional Language Modeling）。</p><p>在 MLM 任务 中，BERT <strong>随机遮盖（mask）</strong> 输入文本中的部分词汇（通常为 15%），并通过 Transformer Encoder 预测这些被遮盖的词。具体实现时，80% 的被遮盖词替换为 <code>[MASK]</code>，10% 保持不变，另 10% 替换为随机词，以增强模型的泛化能力。<strong>模型接收完整的句子（包括 <code>[MASK]</code>）作为输入，目标是在输出端预测这些被遮盖位置的原始 token。</strong></p><p><strong>损失函数</strong> 通常使用 <strong>Cross-Entropy Loss</strong>，只对被 mask 的位置计算损失：</p><p>$$
L = - \sum_{i \in \text{Mask}} \log P_\theta (x_i | X_{\setminus i})
$$</p></li><li><p>② <strong>Next Sentence Prediction (NSP)</strong></p><p>NSP 任务 旨在学习句子级别的关系，BERT 通过给定的两个句子 <strong>判断它们是否为原始文本中的连续句（IsNext）或随机拼接的无关句（NotNext）</strong>。输入格式为：</p><pre tabindex=0><code>[CLS] Sentence A [SEP] Sentence B [SEP]
</code></pre><p>训练时，BERT 以 50% 的概率选择相邻句子作为正样本，另 50% 选择无关句子作为负样本，并通过 二分类损失函数（Binary Cross-Entropy Loss） 进行优化。最后的输出 <code>[CLS]</code> 位置的向量经过一个二分类器（通常是一个简单的全连接层+Softmax），预测 Sentence B 是否为 Sentence A 的下一个句子。</p><p><strong>损失函数</strong> 通常使用 <strong>二分类交叉熵损失（Binary Cross-Entropy）</strong>：</p><p>$$
L = - [ y \log p + (1 - y) \log (1 - p) ]
$$</p><p><strong>MLM 使 BERT 能够学习上下文双向依赖关系，而 NSP 则有助于建模句子间的全局关系</strong>，这两个目标共同提升了 BERT 在 自然语言理解（Natural Language Understanding, NLU） 任务中的表现。<strong>BERT 原版是通过将这两个目标一起训练的，最终损失是：</strong></p><p>$$
L_{\text{Total}} = L_{\text{MLM}} + L_{\text{NSP}}
$$</p></li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> BERT 的 <strong>Masked Language Modeling</strong> 本质上就是在做“完形填空”：预训练时，先将一部分词随机地盖住，经过模型的拟合，<strong>如果能够很好地预测那些盖住的词，模型就学到了文本的内在逻辑</strong>。这部分的主要作用是让模型 <strong>学到词汇和语法规则，提高语言理解能力</strong>。</p><p>除了“完形填空”，BERT还需要做 <strong>Next Sentence Prediction</strong> 任务：预测句子B是否为句子A的下一句。Next Sentence Prediction有点像英语考试中的“段落排序”题，只不过简化到只考虑两句话。<strong>如果模型无法正确地基于当前句子预测 Next Sentence，而是生硬地把两个不相关的句子拼到一起，两个句子在语义上是毫不相关的，说明模型没有读懂文本背后的意思。</strong> 这部分的主要作用是让模型 <strong>学习句子级别的语义关系</strong>。</p></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 为什么 BERT 的输入需要添加 [CLS] 和 [SEP] 特殊标记？</strong></summary><div class=markdown-inner><h2><b>为什么 BERT 的输入需要添加 [CLS] 和 [SEP] 特殊标记？</b></h2><p>BERT 的输入需要添加 <code>[CLS]</code> 和 <code>[SEP]</code> 特殊标记，主要是用来表示 Next Sentence Prediction（NSP） 任务的输入格式，并增强模型的表示能力。例如：</p><pre tabindex=0><code>[CLS] 句子A [SEP] 句子B [SEP]
</code></pre><ul><li><strong><code>[CLS]</code>（Classification Token）</strong>：BERT 在输入序列的开头始终添加 <code>[CLS]</code>，它的最终隐藏状态（Hidden State）可以作为整个序列的表示，特别 <strong>适用于分类任务</strong>（如情感分析、自然语言推理 NLI）。即使不是分类任务，BERT 仍然会计算 <code>[CLS]</code> 的表示，因此它始终是输入的一部分。</li><li><strong><code>[SEP]</code>（Separator Token）</strong>：BERT 采用双向 Transformer，因此需要区分单句和双句任务。在单句任务（如情感分析）中，输入序列结尾会加 <code>[SEP]</code>，而在双句任务（如问答 QA 或文本匹配），<code>[SEP]</code> 用于分隔两个句子，帮助 BERT 处理跨句子的关系建模。</li></ul><p>在 微调阶段（Fine-Tuning），不同任务对 <code>[CLS]</code> 和 <code>[SEP]</code> 的使用方式略有不同。例如：</p><ul><li><strong>文本分类（如情感分析）</strong>：<code>[CLS]</code> 的最终表示输入到 Softmax 层进行分类。</li><li><strong>问答（QA）</strong>：<code>[SEP]</code> 作为问题和段落的分隔符，BERT 需要预测答案的起始和结束位置。</li><li><strong>命名实体识别（NER）</strong>：<code>[CLS]</code> 不是必须的，而是依赖 Token 级别的输出。</li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>对比 BERT 的 <code>[CLS]</code> 向量和平均池化获取句子表示的优缺点?</strong></p><ul><li><strong><code>[CLS]</code> 向量的优缺点</strong>：<ul><li><strong>简洁性</strong>：只需要一个向量（即 <code>[CLS]</code> 向量）来表示整个句子的语义，非常适用于分类任务，尤其是在输入句子较短时。</li><li><strong>端到端优化</strong>：由于 BERT 在预训练时优化了 <code>[CLS]</code> 向量，使其能够有效地聚合句子的语义信息，且通常与下游任务紧密相关。</li><li><strong>可能信息丢失</strong>：<code>[CLS]</code> 向量是通过加权和整个输入的 token 嵌入得到的，可能导致一些细节信息丢失，特别是当句子较长或复杂时。</li></ul></li><li><strong>平均池化（Mean Pooling）的优缺点</strong>：<ul><li><strong>信息保留</strong>：平均池化将所有 token 的表示进行平均，从而保留了句子中各个部分的信息，相比于 <code>[CLS]</code> 向量，它能保留更多的语义信息。</li><li><strong>缺乏上下文关注</strong>：平均池化忽略了 token 之间的复杂依赖关系，简单的加权平均可能无法捕捉到句子中不同部分的关联性，尤其是在多义词或句子结构复杂时。</li><li><strong>计算开销</strong>：对于长句子，平均池化需要计算所有 token 的平均值，可能增加计算开销，尤其在大规模数据集上。</li></ul></li></ul></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 为什么BERT使用双向自注意力机制？这种设计如何影响下游任务？</strong></summary><div class=markdown-inner><h2><b>为什么BERT使用双向自注意力机制？这种设计如何影响下游任务？</b></h2><p>BERT之所以采用 <strong>双向自注意力机制（Bidirectional Self-Attention）</strong>，核心原因在于它 <strong>希望在预训练阶段同时捕捉上下文的完整信息（full context information）</strong>。在每一个Transformer层中，模型可以在同一时间考虑输入序列中 <strong>当前位置（current position）的左边和右边的所有词汇信息。这种设计通过 Masked Language Model（MLM）</strong> 的训练目标实现：随机遮挡输入中的一部分token，模型需要根据完整的上下文（包括被遮挡位置的左右两侧）去预测被遮挡的token，从而学到双向的信息整合能力。</p><p>这种机制对于下游任务的影响非常显著：</p><ol><li><strong>语义理解增强（Enhanced Semantic Understanding）</strong>：双向结构让模型能更准确捕捉句子中词与词之间的相互关系，尤其在句子语序复杂或有歧义时，能够有效消除偏见并提升上下文理解能力。例如，在情感分析（Sentiment Analysis）任务中，句尾的否定词“but”对整句话的情感倾向至关重要，BERT的双向机制能捕捉到这种句子尾部的反转信息。</li><li><strong>特征表达更加丰富（Richer Feature Representation）</strong>：相较于单向模型，双向自注意力产生的上下文特征向量（contextual embeddings）包含了全局信息，使得微调（Fine-tuning）在分类、序列标注等任务时，模型更容易收敛且表现更优。</li><li><strong>下游任务适配性更广（Better Downstream Adaptability）</strong>：许多自然语言处理任务，比如问答系统（Question Answering）、<strong>文本蕴含识别（Natural Language Inference, NLI）</strong> 等，需要模型理解整段文本的完整含义，而不仅仅是基于局部信息的预测。BERT的双向特性正好契合这类需求，能在不修改架构的前提下，通过不同的微调头（Task-specific Heads）适配各种任务。</li></ol></div></details><hr><details><summary><strong class=custom-details-title>⁉️ BERT 微调的细节？</strong></summary><div class=markdown-inner><h2><b>BERT 微调的细节？</b></h2><p>BERT（Bidirectional Encoder Representations from Transformers）微调（Fine-tuning）通常是在预训练（Pre-training）后的基础上，将整个 BERT 模型与特定任务的分类头（Task-specific Head）一起训练，使其适应 <strong>下游任务（Downstream Task）</strong>。</p><p>在微调过程中，输入文本经过分词（Tokenization）后，会被转换为对应的词嵌入（Token Embeddings）、位置嵌入（Position Embeddings）和分段嵌入（Segment Embeddings），然后输入 BERT 的 Transformer 层。模型通过多层 <strong>双向自注意力（Bidirectional Self-Attention）计算上下文信息</strong>，并在最终的 <code>[CLS]</code>（分类标记）或其他适当的标记上添加任务特定的层，如全连接层（Fully Connected Layer）或 CRF（Conditional Random Field），然后使用任务相关的损失函数（Loss Function）进行优化，如交叉熵损失（Cross-Entropy Loss）用于分类任务。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ RoBERTa 的技术细节？它相比 BERT 做了哪些改进（如动态掩码、移除 NSP 任务）？</strong></summary><div class=markdown-inner><h2><b>RoBERTa 的技术细节？它相比 BERT 做了哪些改进（如动态掩码、移除 NSP 任务）？</b></h2><p><strong>RoBERTa（Robustly Optimized BERT Pretraining Approach）</strong> 在 BERT（Bidirectional Encoder Representations from Transformers）的基础上进行了多项优化，以提高模型的性能和泛化能力。</p><p>首先，RoBERTa 采用了 <strong>动态掩码（Dynamic Masking）</strong> 机制，即在每个训练 epoch 重新随机生成 Masked Language Model（MLM）掩码，而 BERT 仅在数据预处理阶段静态确定掩码。这种动态策略增加了模型学习的多样性，提高了其对不同掩码模式的适应能力。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>训练时使用的动态掩码（Dynamic Masking）与静态掩码有何区别？</strong></p><p>BERT 原始论文使用的是 静态掩码，即在数据预处理阶段，对训练数据进行一次性 Mask 处理，并将其存储起来。在训练过程中，每次使用该数据时，<strong>Mask 位置都是固定的</strong>。假设文本是：</p><pre tabindex=0><code>“The quick brown fox jumps over the lazy dog.”
</code></pre><p><strong>在静态掩码中，预处理时就选好了</strong> <code>fox</code> 和 <code>lazy</code> 被掩码，模型每次都看到:</p><pre tabindex=0><code>“The quick brown [MASK] jumps over the [MASK] dog.”
</code></pre><p>RoBERTa 在 BERT 的基础上采用了 动态掩码，即在每次数据加载时，都会 <strong>随机重新选择 Mask 位置</strong>，确保同一输入文本在不同训练轮次中 Mask 位置不同。这提高了数据多样性，使得模型能够学习更丰富的上下文表示，而 <strong>不会过度拟合某些固定的 Mask 位置</strong>。每次训练时，模型可能看到不同的掩码版本，比如：</p><pre tabindex=0><code>• 第一次训练：The quick brown fox jumps over the [MASK] dog.
• 第二次训练：The quick [MASK] brown fox jumps over the lazy dog.
• 第三次训练：The quick brown [MASK] jumps over the lazy dog.
</code></pre></blockquote><p>其次，RoBERTa <strong>移除了 NSP（Next Sentence Prediction）任务</strong>，BERT 在训练时采用了 NSP 任务以增强模型对句子关系的理解，<strong>但研究发现 NSP 任务并未显著提升下游任务的表现，甚至可能影响模型的学习效率</strong>。因此，RoBERTa 采用了更大规模的 <strong>连续文本（Longer Sequences of Text） 进行预训练，而不再强制区分句子关系</strong>。最后，RoBERTa 通过 增加 batch size 和训练数据量，并 采用更长的训练时间，进一步优化了 BERT 预训练过程，使模型能够更充分地学习语言特征。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ DeBERTa 的“解耦注意力”机制（Disentangled Attention）如何分离内容和位置信息？</strong></summary><div class=markdown-inner><h2><b>DeBERTa 的“解耦注意力”机制（Disentangled Attention）如何分离内容和位置信息？</b></h2><p><strong>DeBERTa（Decoding-enhanced BERT with Disentangled Attention）</strong> 相较于 BERT 主要在 <strong>解耦注意力（Disentangled Attention）</strong> 和 <strong>相对位置编码（Relative Position Encoding）</strong> 方面进行了改进，以提升模型的语言理解能力。BERT 采用标准的 Transformer 注意力机制（Self-Attention），其中查询（Query）、键（Key）、值（Value）向量均是基于相同的嵌入（Embedding），这意味着 内容信息（Content Information） 和 位置信息（Positional Information） 混合在一起，限制了模型对句子结构的建模能力。而 DeBERTa 通过解耦注意力机制，<strong>对内容和位置信息分别编码，使得模型在计算注意力时能够更精确地理解不同词语之间的相对关系</strong>。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：Disentangled Attention 的设计，<strong>本质上是针对 BERT 系列所用的 absolute position embedding（绝对位置编码）问题提出的。</strong> <strong>传统 BERT</strong> 的输入结构是：
$$
h_i = x_i + p_i
$$</p><p>注意力的打分计算是：</p><p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$</p><p>DeBERTa 将词的内容（content） 和 <strong>位置（position）</strong> 分开建模，注意力打分的计算方式改为：</p><p>$$
\alpha_{i,j} = \frac{(q_i^c)^T k_j^c + (q_i^c)^T r_{i-j}^p + (q_i^p)^T k_j^c}{\sqrt{d}}
$$</p></blockquote><p>具体来说，DeBERTa 在计算注意力权重时，不是直接基于词向量（Token Embedding），而是 <strong>分别计算基于内容（Content-based Attention）和基于位置（Position-based Attention）的注意力得分，然后再将两者加权合并</strong>。这种方式使得 DeBERTa 能够在更长的依赖关系建模上表现更好。此外，DeBERTa 还采用了 <strong>增强的相对位置编码（Enhanced Relative Position Encoding）</strong>，相比 BERT 的绝对位置编码（Absolute Positional Encoding），能够更自然地处理长文本结构。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ ALBERT 如何通过参数共享降低模型参数量？</strong></summary><div class=markdown-inner><h2><b>ALBERT 如何通过参数共享降低模型参数量？</b></h2><p><strong>ALBERT（A Lite BERT）是对 BERT 模型的轻量化改进</strong>，旨在通过降低模型参数量而不显著影响模型性能。其主要技术细节涉及两项关键的优化策略：参数分解嵌入（Factorized Embedding） 和 跨层参数共享（Cross-Layer Parameter Sharing）。</p><ol><li><strong>参数分解嵌入（Factorized Embedding）</strong>：BERT 使用了一个巨大的词嵌入矩阵（embedding matrix），其大小通常是词汇表的大小与隐藏层维度（hidden size）的乘积。而 ALBERT 采用了参数分解方法，<strong>将词嵌入矩阵分解为两个低维矩阵</strong>，分别是一个较小的 词汇嵌入矩阵（Word Embedding Matrix） 和一个较小的 隐藏层嵌入矩阵（Hidden Layer Embedding Matrix）。具体来说，词嵌入矩阵被分解为两个矩阵，一个低维的嵌入矩阵和一个较小的输出矩阵，这样就显著减少了参数数量。例如，假设词嵌入矩阵的维度为 V x H（V 是词汇表大小，H 是隐藏层大小），通过分解成两个矩阵 V x E 和 E x H（E 为较小的维度），可以大幅度减少计算复杂度和存储需求。</li><li><strong>跨层参数共享（Cross-Layer Parameter Sharing）</strong>：在标准的 BERT 中，每一层 Transformer 都有一组独立的参数，而 ALBERT 通过跨层共享参数，减少了每一层的独立参数。具体来说，ALBERT 将模型中多个 Transformer 层 的参数进行共享，<strong>所有层都使用相同的权重</strong>。这样，尽管 ALBERT 保留了更多的层数（如 BERT 的 12 层改为 12 层的 ALBERT），但通过共享权重，整体的参数数量大幅度减少。参数共享的核心思想是：<strong>每一层 Transformer 的前向传播和反向传播使用相同的参数，从而减少了每层的权重数量，降低了内存消耗。</strong></li></ol><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>ALBERT提出的方法就是跨层参数共享</strong>，核心思想：<strong>所有Transformer层的权重矩阵都可以复用同一组参数</strong>，居图来说 Attention 模块参数</p><p>$$ W_Q, W_K, W_V, W_O$$ 在模块内部共享。</p><p>FeedForward模块的参数</p><p>$$W_1, W_2$$</p><p>在模块内部共享。因为作者认为原版的 BERT中 每层都独立训练，虽然灵活，但造成了巨大的参数冗余，很多层其实在做非常相似的变换，浪费内存和计算资源。</p></blockquote></div></details><hr><h2 id=encoder-decoder><strong>Encoder-Decoder</strong>
<a class=anchor href=#encoder-decoder>#</a></h2><details><summary><strong class=custom-details-title>⁉️ Encoder-Decoder架构的核心设计目标是什么？适用于哪些任务场景？</strong></summary><div class=markdown-inner><h2><b>Encoder-Decoder架构的核心设计目标是什么？适用于哪些任务场景？</b></h2><p>Encoder-Decoder架构的 <strong>设计初衷</strong> 是解决 <strong>输入输出序列长度不对称、语义空间跨域映射的问题</strong>，确保模型能够有效压缩输入特征并有条件地生成目标序列，广泛应用于机器翻译、文本摘要、图像描述生成、跨模态问答等场景，尤其适合输入与输出的语义空间或结构不同的任务。</p><p>Encoder-Decoder模型同时具备了 <strong>理解（Encoder）和生成（Decoder）</strong> 的能力，因此它能够处理复杂的任务，如机器翻译、文本摘要、图像描述等。这类模型既能够通过 Encoder 理解输入，又能通过 Decoder 生成输出。Encoder–Decoder模型的好处有：</p><ol><li><strong>生成能力</strong>：Encoder–Decoder 架构能够生成任意长度的输出序列，而不是像Encoder-only模型那样只能生成固定长度的表示。它允许通过解码器（Decoder）逐步生成目标序列，非常适合像机器翻译和文本摘要等生成任务。</li><li><strong>灵活的输入和输出</strong>：Encoder-only 模型和 Decoder-only 模型通常输入和输出的长度是固定的，而 Encoder–Decoder 模型能够灵活地处理不同长度的输入和输出。Decoder 可以根据输入序列生成任意长度的目标序列，从而适应更复杂的任务。</li><li><strong>跨任务的预训练能力</strong>：Encoder–Decoder模型可以通过多任务学习提升模型的泛化能力。比如，T5模型通过将不同任务（如文本分类、文本生成等）统一为一个多任务预训练框架，从而增强了模型对不同任务的处理能力。</li></ol></div></details><hr><details><summary><strong class=custom-details-title>⁉️ T5 如何统一不同 NLP 任务的格式？其预训练任务（如 Span Corruption）具体如何实现？</strong></summary><div class=markdown-inner><h2><b>T5 如何统一不同 NLP 任务的格式？其预训练任务（如 Span Corruption）具体如何实现？</b></h2><p>T5（Text-to-Text Transfer Transformer）通过将所有 NLP 任务（如文本分类、机器翻译、问答、摘要生成等）<strong>统一转换为文本到文本（Text-to-Text）的格式</strong>，从而实现了一个通用的 NLP 框架。具体而言，无论是输入句子的分类任务还是填空任务，T5 都会将输入转换为文本序列，并要求模型生成相应的文本输出。例如，情感分析任务的输入可以是 <code>"sentiment: I love this movie"</code>，输出则是 <code>"positive"</code>，而机器翻译任务的输入可能是 <code>"translate English to French: How are you?"</code>，输出为 <code>"Comment ça va?"</code>。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：Span Corruption（Span-Masked Language Modeling, SMLM）与 Masked Language Modeling（MLM）的核心区别在于 Mask 的方式和学习目标的不同。</p><ul><li><strong>MLM（Masked Language Modeling，BERT 采用）</strong>：<ul><li>MLM 主要是随机选择 <strong>单个 token 进行遮蔽</strong>，然后让模型预测被遮蔽的 token。例如：<pre tabindex=0><code>Input: &#34;I love [MASK] learning&#34;
Target: &#34;deep&#34; 
</code></pre></li><li>由于每次仅遮蔽少量 token，BERT 可能 <strong>无法学习到更长跨度的依赖关系</strong>，特别是对完整的子句或短语的理解较弱。</li></ul></li><li><strong>SMLM（Span-Masked Language Modeling，T5 采用）</strong>：<ul><li>SMLM 采用 Span Corruption，即 <strong>一次遮蔽连续的多个 token</strong>，并用特殊标记 <code>&lt;extra_id_0></code> 来表示被遮蔽部分。例如：<pre tabindex=0><code>Input: &#34;I &lt;extra_id_0&gt; deep &lt;extra_id_1&gt;.&#34;
Target: &#34;&lt;extra_id_0&gt; love &lt;extra_id_1&gt; learning&#34;
</code></pre></li><li>能够更好地 <strong>学习长距离的依赖关系</strong>，适用于生成式任务（如摘要、翻译）。训练难度更高。</li></ul></li></ul></blockquote><p>T5 采用的主要预训练任务是 <strong>Span Corruption（Span-Masked Language Modeling, SMLM）</strong>，这是一种变体的掩码语言建模（Masked Language Modeling, MLM）。具体来说，该任务会在输入文本中随机选择若干个 span（即连续的子序列），用特殊的 <code>&lt;extra_id_X></code> 令牌替换它们，并要求模型预测被遮蔽的内容。例如，原始文本 <code>"The quick brown fox jumps over the lazy dog"</code> 可能会被转换为 <code>"The &lt;extra_id_0> fox jumps over the &lt;extra_id_1> dog"</code>，而模型需要输出 <code>"quick brown"</code> <code>&lt;extra_id_0></code> 和 <code>“lazy”</code> <code>&lt;extra_id_1></code>。这种方式比 BERT 的单词级别掩码更灵活，有助于学习更丰富的上下文信息，从而提升生成任务的能力。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：在 Span Corruption 预训练（Span Corruption Pretraining）中，被遮蔽的文本片段（Masked Span）的长度通常遵循 Zipf 分布（Zipf’s Law），<strong>即较短的片段更常见，而较长的片段较少，以模拟自然语言中的信息分布</strong>。具体而言，像 T5 这样的模型使用 <strong>几何分布（Geometric Distribution）</strong> 来采样 span 长度，以确保既有短范围的遮蔽，也有跨多个 token 的长范围遮蔽。</p><p>不同的遮蔽长度会影响模型的学习能力：较短的 span（例如 1-3 个 token）有助于模型学习局部语义填充能力（Local Context Understanding），而较长的 span（如 8-10 个 token 甚至更长）可以增强模型的全局推理能力（Global Reasoning）和段落级理解能力（Document-Level Comprehension）。如果 span 过短，模型可能更倾向于基于表面模式（Surface Patterns）预测，而非真正理解上下文；如果 span 过长，则可能导致学习任务过于困难，使得模型难以有效收敛。</p></blockquote><table><thead><tr><th>任务类型</th><th>示例输入</th><th>示例输出</th></tr></thead><tbody><tr><td><strong>文本分类（Text Classification）</strong></td><td>sst2 sentence: This movie is fantastic!</td><td>positive</td></tr><tr><td><strong>文本生成（Text Generation）</strong></td><td>summarize: The article talks about &mldr;</td><td>The main idea is&mldr;</td></tr><tr><td><strong>机器翻译（Machine Translation）</strong></td><td>translate English to German: How are you?</td><td>Wie geht es dir?</td></tr><tr><td><strong>文本补全（Text Completion）</strong></td><td>fill_mask: I love to [MASK] pizza.</td><td>eat</td></tr><tr><td><strong>问答（Question Answering）</strong></td><td>question: Who wrote Hamlet? context: Shakespeare wrote&mldr;</td><td>Shakespeare</td></tr></tbody></table></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 如何将预训练的 Encoder-Decoder 模型（如 T5）适配到具体下游任务？</strong></summary><div class=markdown-inner><h2><b>如何将预训练的 Encoder-Decoder 模型（如 T5）适配到具体下游任务？</b></h2><p>T5（Text-to-Text Transfer Transformer）模型与BERT相似，也需要通过在特定任务的数据上进行微调（fine-tuning）来完成下游任务。与BERT的微调有所不同，T5的主要特点包括：</p><ol><li><strong>任务描述（Task Prefix）</strong>：T5 的输入不仅包含原始文本，还需要附加一个任务描述。例如，在文本摘要（Text Summarization）任务中，输入可以是 &ldquo;summarize: 原文内容&rdquo;，而在问答（Question Answering）任务中，输入可以是 &ldquo;question: 问题内容 context: 相关文本&rdquo;。这种设计使 T5 能够以统一的 文本到文本（Text-to-Text） 形式处理不同任务。</li><li><strong>端到端序列生成（Sequence-to-Sequence Generation）</strong>：与 BERT 仅能进行分类或填空任务不同，T5 依赖其 Transformer 解码器（Transformer Decoder） 来生成完整的输出序列，使其适用于文本生成任务，如自动摘要、数据到文本转换（Data-to-Text Generation）等。</li><li><strong>无需额外层（No Task-Specific Layers）</strong>：在 BERT 微调时，通常需要在其顶层添加特定的任务头（Task-Specific Head），如分类层（Classification Layer）或 CRF（Conditional Random Field）层，而 T5 直接将任务作为输入提示（Prompt），并使用相同的模型结构进行训练，无需修改额外的网络层。</li></ol></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 什么是BART？BART 的预训练任务与 T5 有何异同？</strong></summary><div class=markdown-inner><h2><b>什么是BART？BART 的预训练任务（如 Text Infilling、Sentence Permutation）与 T5 的 Span Corruption 有何异同？</b></h2><p><strong>BART（Bidirectional and Auto-Regressive Transformers）</strong> 是一种结合了 BERT（Bidirectional Encoder Representations from Transformers）和 GPT（Generative Pretrained Transformer）优点的生成模型。BART 在预训练过程中采用了 <strong>自编码器（Autoencoder）</strong> 结构，其编码器部分像 BERT 一样使用双向编码，能够捕捉上下文信息，而解码器则是像 GPT 一样用于生成任务，通过自回归方式生成文本。</p><table><thead><tr><th>任务类型</th><th>具体示例</th><th>输入 ➡️ 输出特征说明</th></tr></thead><tbody><tr><td><strong>机器翻译（Machine Translation）</strong></td><td>英语 ➡️ 法语</td><td>输入输出语言不同，长度不固定，语义需对应</td></tr><tr><td><strong>文本摘要（Text Summarization）</strong></td><td>新闻文章 ➡️ 简短摘要</td><td>输入长文本，输出短摘要，结构压缩</td></tr><tr><td><strong>图像描述生成（Image Captioning）</strong></td><td>图片特征向量 ➡️ 自然语言描述</td><td>输入非语言特征，输出自然语言，跨模态转换</td></tr><tr><td><strong>文本生成（Text Generation）</strong></td><td>Prompt ➡️ 自动补全文本</td><td>输入提示短语，输出完整文本，顺序自回归建模</td></tr><tr><td><strong>语音转文本（Speech-to-Text）</strong></td><td>语音信号（波形或特征）➡️ 文字</td><td>输入连续音频流，输出离散文本序列，输入输出格式完全不同</td></tr><tr><td><strong>多模态问答（VQA / Multimodal QA）</strong></td><td>图片 + 问题文本 ➡️ 答案文本</td><td>输入多模态，输出单模态文本，结构不对称</td></tr></tbody></table><p>BART 的预训练任务包括 <strong>Text Infilling</strong> 和 <strong>Sentence Permutation</strong>：</p><ol><li><strong>Text Infilling</strong>：在这一任务中，模型需要从一段被掩盖部分的文本中恢复出被移除的词或词组。具体来说，给定一个输入文本，部分单词或短语被替换为掩码（mask），然后模型的任务是预测这些被掩盖的内容。这一任务类似于 BERT 的 Masked Language Model（MLM），但区别在于，BART 不仅仅是根据上下文来填充单一词汇，它的目标是生成整个缺失的文本片段。<pre tabindex=0><code>原始文本：“The quick brown fox jumps over the lazy dog in the park.”

掩盖后的文本：“The quick [MASK] jumps over the lazy dog in the park.”
</code></pre></li></ol><blockquote class="book-hint warning"><p><strong>Note</strong>：在 MLM 中，模型的目标是预测被随机掩盖的单个词（或子词）。虽然 Text Infilling 看起来类似于 MLM，但 BART 的 Text Infilling 中，<strong>掩盖的部分不仅限于单个词，而是可能是一个较大的片段或短语</strong>。通常，模型会随机选择一个较长的文本片段（如一个短语或句子的一部分）并用一个掩码标记替换，然后模型需要预测整个被掩盖的文本片段。</p><p><strong>总结，MLM遮一个词，Text Infilling 遮一个片段，Span Corruption 遮多个片段。</strong></p></blockquote><ol start=2><li><strong>Sentence Permutation</strong>：在这一任务中，输入文本的句子顺序被打乱，模型的任务是根据上下文恢复正确的句子顺序。这个任务是为了帮助模型学习长文本的结构和上下文关系，使其能够生成连贯且符合语法规则的文本。它与 T5 中的 Span Corruption 有一定的相似性，因为两者都涉及对文本进行扰动，并要求模型根据扰动后的文本恢复原始文本。<pre tabindex=0><code>原始文本：
“The dog chased the ball. It was a sunny day.”

打乱顺序后的文本：
“It was a sunny day. The dog chased the ball.”
</code></pre></li></ol></div></details><hr><h2 id=decoder-only><strong>Decoder-Only</strong>
<a class=anchor href=#decoder-only>#</a></h2><details><summary><strong class=custom-details-title>⁉️ Decoder-only 模型与 Encoder-Decoder 模型的核心区别是什么？</strong></summary><div class=markdown-inner><h2><b>Decoder-only 模型与 Encoder-Decoder 模型的核心区别是什么？</b></h2><p>Decoder-only（如GPT）仅保留解码器，通过自回归生成（逐词预测）完成任务。<strong>移除原始Transformer的编码器和交叉注意力层</strong>，保留掩码自注意力（Masked Self-Attention）与前馈网络（FFN）。输入处理时，文本序列添加特殊标记 <code>&lt;bos></code>（序列开始）和 <code>&lt;eos></code>（序列结束），目标序列为输入右移一位。适用任务主要为生成类任务（文本续写、对话、代码生成）。Encoder-Decoder（如原始Transformer、T5）分离编码器（理解输入）与解码器（生成输出），通过交叉注意力传递信息。适用任务主要为需严格分离输入理解与输出生成的任务（翻译、摘要、问答）。</p><div align=center><img src=/images/f6133c18-bfaf-4578-8c5a-e5ac7809f65b_1632x784.png width=600px/></div><p>Decoder-only模型通过 <strong>上下文学习（In-Context Learning）</strong> 将任务隐式编码到输入中（如添加“Translate English to French:”前缀），利用生成能力模拟翻译，完成和 Encoder-Decoder 一样的工作。但是 Encoder-Decoder在以下场景更具优势：</p><ol><li><strong>输入与输出解耦的复杂任务（如翻译）</strong>：Encoder-Decoder 模型的编码器可先提取完整语义，解码器再逐词生成，避免生成过程中的语义偏差，准确性更高。而Decoder-only模型（如GPT-3）需通过Prompt（如“Translate English to French: &mldr;”）隐式对齐输入输出，<strong>易受提示词设计影响</strong>。</li><li><strong>长文本处理效率</strong>：Encoder-Decoder：编码器一次性压缩输入为固定长度表示，解码生成时无需重复处理长输入（节省计算资源）。Decoder-only：<strong>生成每个词时需重新处理整个输入序列</strong>（如输入1000词的文档），导致计算复杂度高。</li></ol><ul><li><p><strong>总结来说</strong></p><ul><li><strong>Decoder-only</strong>：适合开放域生成任务，依赖生成连贯性与上下文学习，但对输入-输出结构复杂的任务效率较低。</li><li><strong>Encoder-Decoder</strong>：在需严格分离理解与生成、处理长输入、多任务统一的场景中更优，尤其适合翻译、摘要等结构化任务。</li><li>类比：Decoder-only像“自由创作的作家”，Encoder-Decoder像“严谨的翻译官”——前者更灵活，后者更精准。</li></ul></div></li></ul></details><hr><details><summary><strong class=custom-details-title>⁉️ Decoder-only 模型的预训练任务通常是什么？</strong></summary><div class=markdown-inner><h2><b>Decoder-only 模型的预训练任务通常是什么？</b></h2><ol><li><strong>自回归语言建模（Autoregressive Language Modeling）</strong>：这个任务的目标是通过给定一部分文本（如前面的词或字符），预测接下来的单词或字符。例如，给定输入 <code>“The cat sat on the”</code>, 模型的任务是预测下一个单词是 <code>“mat”</code>。这个过程是自回归的，因为每次生成新的词都会基于模型已经生成的文本。自回归语言建模任务常见于 GPT（Generative Pre-trained Transformer） 等模型。</li><li><strong>文本填充任务（Cloze Task）</strong>：在这个任务中，模型的目标是根据上下文填充文本中的空白部分。例如，给定句子 <code>“The cat sat on the ____”</code>, 模型需要预测空白处应该填入的词 <code>“mat”</code>。这种填空任务常见于 BERT（Bidirectional Encoder Representations from Transformers） 的变体，如 Masked Language Modeling (MLM)。尽管 BERT 是基于 编码器（Encoder） 架构，但类似的目标也可以应用于 Decoder-only 架构，通过在训练时将部分词语随机遮蔽（mask）并让模型预测被遮蔽的部分。</li></ol></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 为什么 Decoder-only 模型通常采用自回归生成方式？因果掩码的作用及其实现方法。</strong></summary><div class=markdown-inner><h2><b>为什么 Decoder-only 模型通常采用自回归生成方式？因果掩码的作用及其实现方法。</b></h2><p>Decoder-only 模型通常采用自回归（Autoregressive）生成方式，因为这种方式能够通过模型已经生成的输出逐步生成下一个 token，从而形成连贯的序列。自回归生成方式使得每个步骤的生成依赖于前一步的生成结果，这种特性非常适合文本生成任务，如 <strong>语言建模（Language Modeling）</strong> 和 <strong>对话生成（Dialogue Generation）</strong>。通过这种方式，模型能够以逐词的方式生成文本，在每个步骤中利用之前的上下文信息预测下一个 token。即最大化序列联合概率，损失函数为交叉熵（Cross-Entropy）：</p><span>\[
\begin{equation}
\mathcal{L} = -\sum_{t=1}^{T} \log P(w_t | w_{1:t-1 })
\end{equation}
\]</span><p>在 Decoder-only 模型中，<strong>因果掩码（Causal Mask）</strong> 的作用是确保模型在生成时 <strong>仅依赖于已生成的部分</strong>，而不会看到未来的信息。具体来说，在训练时，因果掩码会屏蔽未来 token 的信息，使得模型只能访问当前位置及其之前的 token，这样保证了每个时间步的预测仅受历史信息的影响，而无法窥视未来的输出。实现方法通常是在注意力机制（Attention Mechanism）中，通过对自注意力矩阵应用一个上三角矩阵的掩码，将未来的 token 阻止在计算中。例如，如果在生成第 4 个 token 时，模型不允许访问第 5、6 个 token，<strong>掩码就会在这些位置设置为负无穷</strong>，从而避免信息泄漏。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 解释 Teacher Forcing 在 Decoder-only 模型训练中的作用及其潜在缺陷。</strong></summary><div class=markdown-inner><h2><b>解释 Teacher Forcing 在 Decoder-only 模型训练中的作用及其潜在缺陷。</b></h2><p>Teacher Forcing 是一种在训练序列生成模型时常用的技术，尤其是在 Decoder-only 模型（如 GPT 等自回归语言模型）的训练过程中。在 Teacher Forcing 中，模型在 <strong>每个时间步的输入不依赖于前一步的预测输出，而是直接使用真实的目标词（Ground Truth）作为输入</strong>。这意味着，在训练过程中，Decoder 在每个时间步都接收的是当前时间步的真实标签，而不是模型自己预测的输出。</p><p>这种方法的主要作用是加速模型训练，因为它 <strong>避免了模型在每次预测时犯错后导致的错误传播</strong>。在传统的训练过程中，模型每一次的预测都可能受到前一步错误的影响，这样会使得训练变得更加困难且收敛速度变慢。而 Teacher Forcing 确保每个时间步的输入都是正确的，从而减少了梯度计算中的误差积累，加速了训练过程。</p><p>然而，Teacher Forcing 也存在潜在缺陷，特别是在 推理阶段（Inference）。在训练阶段，模型总是看到真实的目标词作为输入，但在推理时，它必须依赖于自己之前的预测。Teacher Forcing 可能导致 <strong>模型在训练和推理时的分布不匹配（Exposure Bias）</strong>，即训练时的“理想环境”与实际推理时的“真实环境”不一致。若在训练中模型从未经历过自己预测错误的情况，它可能在推理时无法有效地纠正错误，从而影响生成的质量，导致 生成质量下降 或 无法适应真实环境中的错误传播。</p><p>为了缓解这个问题，一些方法如 <strong>Scheduled Sampling</strong> 被提出，它 <strong>逐渐减少训练时的 Teacher Forcing 比例</strong>，让模型在训练阶段逐步适应自己的预测输出，从而提高模型在推理时的稳定性和表现。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 什么是 In-Context Learning？Decoder-only 模型如何实现零样本（Zero-Shot）推理能力？</strong></summary><div class=markdown-inner><h2><b>什么是 In-Context Learning？Decoder-only 模型如何实现零样本（Zero-Shot）推理能力？</b></h2><p><strong>上下文学习（In-Context Learning）</strong> 是指在推理过程中，<strong>模型通过理解并利用输入文本中的上下文信息来做出预测，而无需对任务进行额外的训练或微调（fine-tuning）</strong>。在这种方法中，模型通过直接接收任务的描述和示例输入-输出对，在推理时依赖这些信息来预测结果。与传统的基于训练的学习方式不同，上下文学习使得模型可以灵活应对新任务，而无需重新训练。</p><p>Decoder-only 模型（例如 GPT-3）通过将 <strong>任务的描述、示例以及相关输入文本提供给模型</strong>，使得模型能够在上下文中推理并生成响应。具体而言，GPT-3 和类似的 Transformer 模型基于自回归生成（autoregressive generation）机制，通过逐步生成下一个词，结合前文的上下文信息来进行推理。在这种机制下，模型无需显式的监督学习或微调，只要给定足够的上下文（例如任务描述和输入示例），它就能根据这些信息来做出预测。</p><blockquote class="book-hint warning"><ul><li><strong>Few-shot 示例</strong>：<pre tabindex=0><code>Q: Capital of France? A: Paris  
Q: Capital of Japan? A: Tokyo  
Q: Capital of Brazil? A:  
</code></pre><ul><li>模型通过前两例学习“Q-A”模式，生成“Brasília”。</li></ul></li><li><strong>Zero-shot 指令</strong>：<pre tabindex=0><code>Please answer the following question:  
What is the boiling point of water?  
Answer:  
</code></pre><ul><li>模型根据指令词“Please answer”生成“100°C (212°F) at sea level”。</li></ul></li></ul></blockquote><p>Zero-Shot 推理的实现方式在于 <strong>训练过程中接触了多种任务（如文本生成、问答、翻译、摘要等），使得模型能够在面对新任务时，依靠其通用语言理解能力完成推理，而无需重新训练或微调（Fine-tuning）</strong>。例如，假设我们要求模型完成一个数学问题，尽管模型未曾专门针对该任务训练，但它能依赖于其对语言的广泛理解，推断出合理的解答。大规模的训练数据为模型提供了更广泛的背景知识，使其能够在推理时利用丰富的上下文信息。</p><p>举个例子，当我们给出一个从未见过的任务，比如 “翻译以下文本成法语：‘I have a dream’”，GPT-3 可以准确地根据其训练数据中的语言模式生成翻译：“J’ai un rêve”。这是因为在其训练数据中，它已经接触过大量的文本翻译任务，并学会了如何根据提示进行推理。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 对比 Greedy Search 与 Beam Search 在 Decoder-only 模型中的优缺点。</strong></summary><div class=markdown-inner><h2><b>对比 Greedy Search 与 Beam Search 在 Decoder-only 模型中的优缺点。</b></h2><p>Greedy Search 是一种简单的解码策略，它在 <strong>每个时间步（Time Step）选择概率最大的词作为输出</strong>。具体来说，对于每个生成步骤，模型会选择当前概率分布中 <strong>最大概率的词（Maximum Probability Word）</strong> 作为下一步的输出，并且该词会作为输入传递到下一个时间步。Greedy Search 的优点是 计算效率高（High Computational Efficiency），因为它只进行单一的选择和计算。然而，它的缺点是 <strong>局部最优问题（Local Optima）</strong>，即每次选择最有可能的词，而没有考虑未来可能的其他选择，因此它容易陷入次优解，导致生成的序列质量不高。</p><blockquote class="book-hint warning"><p><strong>Note:</strong> Greedy Search 每一步选择概率最大的一个 token（贪婪策略）。Beam Search <strong>保留 N 个最优的候选序列，步步扩展（每次都在上一步的所有路径扩展出V个可能，再从中挑Top N）</strong>，最终选整体概率最大的句子。</p></blockquote><p>与此不同，Beam Search 是一种更加复杂的解码方法，它通过在每个时间步保留 <strong>多个候选序列（Multiple Candidate Sequences）</strong> 来进行搜索。具体来说，Beam Search 会在每个步骤保留 <strong>k个最优候选（Top-k Candidates）</strong>，而不是仅仅选择概率最大的一个词。通过这种方式，Beam Search 允许模型探索更多的可能性，从而提高生成质量。Beam Search 的优点是能够生成更具多样性的序列，通常能避免 Greedy Search 的局部最优问题，生成的结果更具 <strong>全局最优性（Global Optimality）</strong>。然而，它的缺点是 计算开销较大（Higher Computational Cost），因为需要维护多个候选序列，尤其在长文本生成时，这种计算开销可能会显著增加。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ Top-k 采样 和 Top-p（Nucleus）采样 的核心区别是什么？各适用于什么场景？</strong></summary><div class=markdown-inner><h2><b>Top-k 采样 和 Top-p（Nucleus）采样 的核心区别是什么？各适用于什么场景？</b></h2><p>Top-k 采样、Top-p 采样和 Greedy Search 和 Beam Search 一样都是解码策略（decoding strategies），它们的目标是生成高质量的文本。</p><blockquote class="book-hint warning"><p><strong>Notes：</strong></p><table><thead><tr><th>推荐策略</th><th>适用场景</th><th>策略说明</th></tr></thead><tbody><tr><td>✅ Greedy / Beam Search</td><td>确定性生成（如机器翻译）</td><td>每步选择概率最大的Token（Beam则保留多个路径）</td></tr><tr><td>✅ Top-K / Top-P + Temperature</td><td>开放性生成（如对话、写作、小说）</td><td>限制候选集合后，按概率重新抽样，支持温度调节控制随机性</td></tr></tbody></table></blockquote><p><strong>Top-k 采样</strong> 是一种基于概率分布的截断方法，在每次生成一个单词时，<strong>只从概率分布前 k 个最可能的词中选择一个进行生成，其他词的概率被截断为零</strong>。这种方法通过限制候选词的数量来控制生成文本的多样性，从而避免生成非常低概率的、不太合理的词汇。其公式可以表示为：</p><span>\[
P(w_i) = \begin{cases}
P(w_i), & \text{if } w_i \in \text{Top-}k \\
0, & \text{otherwise}
\end{cases}
\]</span><p>Top-k 表示从前 k 个概率最高的词中进行选择。Top-k 采样适用于生成任务中需要平衡多样性和合理性，如 <strong>对话生成（Dialogue Generation） 和 文本创作（Text Generation）</strong> 等场景。</p><p><strong>Top-p 采样（Nucleus Sampling）</strong> 则是一种基于 <strong>累积概率的采样方法</strong>。在每个时间步，<strong>Top-p 会选择一个最小的词集合，使得这些词的累积概率大于或等于 p</strong>。与 Top-k 采样固定候选词数不同，Top-p 采样动态调整候选词的数量，这使得它在生成过程中更加灵活和多样。其公式为：</p><span>\[
\sum_{i=1}^{n} P(w_i) \geq p
\]</span><p>P(w_i) 是每个候选词的概率，p 是预定的累积概率阈值。Top-p 采样适用于对生成多样性要求较高的任务，如 <strong>创意写作（Creative Writing） 或 开放域问答（Open-Domain QA）</strong>，它能够灵活调整候选词的数量，从而在生成中加入更多的随机性。</p><blockquote class="book-hint warning"><p><strong>Note:</strong> 无论是 Top-K 还是 Top-P，都分两步：</p><ol><li><p><strong>限制候选集合</strong></p><ul><li><strong>Top-K</strong>：截断概率分布，保留概率最高的 K 个 token；</li><li><strong>Top-P</strong>：累计概率到 p，保留使总和 ≥ p 的最小集合。</li></ul></li><li><p><strong>在候选集合中随机采样</strong></p><ul><li>不是选最大概率，而是根据保留下来的子集，重新归一化概率，再随机抽样。</li><li><strong>e.g.</strong> Top-K = 3 → 保留 A, B, C。重新归一化：
$$
P_{\text{new}}(A) = \frac{0.35}{0.35+0.30+0.15} \approx 0.4375
$$</li></ul><p>$$
P_{\text{new}}(B) = \frac{0.30}{0.80} = 0.375
$$</p><p>$$
P_{\text{new}}(C) = \frac{0.15}{0.80} = 0.1875
$$
然后按照这个新概率随机选择。</p></li></ol></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 温度参数（Temperature）如何影响 Decoder-only 模型的生成结果？</strong></summary><div class=markdown-inner><h2><b>温度参数（Temperature）如何影响 Decoder-only 模型的生成结果？</b></h2><p>温度参数（Temperature）在 Decoder-only 模型（如 GPT）中用于 <strong>控制生成文本的随机性或确定性</strong>。它的作用是在模型生成过程中对 <strong>输出概率分布（Output Probability Distribution）进行调整</strong>，从而影响模型的生成结果。温度的公式通常为：</p><span>\[
P(w) = \frac{e^{\frac{log(P(w))}{T}}}{\sum_{w{\prime}} e^{\frac{log(P(w{\prime}))}{T}}}
\]</span><p>其中，P(w) 是生成某个单词 w 的原始概率，T 是温度参数， w&rsquo; 是所有可能的单词。温度参数 T 控制了概率分布的平滑度。当 T = 1 时，模型按照正常的概率分布生成输出；<strong>当 T > 1 时，概率分布变得更加平缓，生成的结果会更加随机</strong>，可能导致较为多样化的输出；<strong>当 T &lt; 1 时，概率分布变得更加陡峭，模型会更加倾向于选择概率较高的词语</strong>，从而生成更加确定性和保守的结果。</p><p><strong>温度的调整作用于 softmax 函数（用于将模型的原始输出转换为概率分布）</strong>。通过改变温度值，模型可以控制生成内容的多样性和创造性。较高的温度通常会增加生成内容的创新性，但可能导致语法错误或不连贯的输出，而较低的温度则会使输出更加连贯和符合预期，但可能缺乏创意或多样性。</p><p>例如，在文本生成任务中，若我们将温度设为 1.0，则生成的文本遵循模型原本的概率分布；若我们将温度设为 0.5，生成的文本将更加趋向于模型最有可能生成的词，文本可能会变得单调和缺乏创意；若温度设为 1.5，则生成的文本可能会表现出更多的创造性，但也可能出现语法错误或不太连贯的部分。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 什么是LLama？它有哪些技术特点？</strong></summary><div class=markdown-inner><h2><b>什么是LLama？它有哪些技术特点？</b></h2><p><strong>LLaMA（Large Language Model Meta AI）</strong> 是由 Meta（前 Facebook）推出的大规模语言模型，旨在提供高效的文本生成和理解能力，同时优化计算资源的利用。它基于 Transformer 架构，并采用了一系列优化技术，使其在计算资源较低的情况下仍能达到或超过 GPT-3 级别的性能。LLaMA 主要基于 <strong>Decoder-Only Transformer</strong>，即 <strong>因果语言模型（Causal Language Model, CLM），用于自回归文本生成</strong>。LLaMA 在多个方面进行了优化，包括数据选择、模型架构调整、训练方法等，主要特点如下：</p><ol><li><strong>更高效的训练</strong><ul><li><strong>数据优化</strong>：LLaMA 训练时使用了更高质量的文本数据，减少了低质量和冗余数据，从而提高了训练效率和泛化能力。</li><li><strong>更少计算资源</strong>：相比 GPT-3（175B 参数），LLaMA 采用了更小规模的参数（如 LLaMA-7B、LLaMA-13B、LLaMA-65B），但在多个基准测试中仍能达到甚至超越 GPT-3 的效果。</li></ul></li><li><strong>Transformer 架构优化</strong></li></ol><ul><li><strong>RoPE（旋转位置编码，Rotary Position Embeddings）</strong>：LLaMA 使用 RoPE 代替传统的 绝对位置编码（Absolute Positional Encoding），使得模型可以更好地处理长文本：</li></ul><span>\[
\text{RoPE}(x, \theta) = x e^{i \theta}
\]</span><ul><li><strong>SwiGLU 激活函数（Swish-Gated Linear Unit）</strong>：LLaMA 采用 SwiGLU 代替 ReLU 或 GELU 作为激活函数，提高了模型的表示能力：</li></ul><span>\[
\text{SwiGLU}(x) = \text{Swish}(x) \cdot W_2 (\text{GELU}(W_1 x))
\]</span><blockquote class="book-hint warning"><p><strong>Note:</strong> LLaMA 选择 SwiGLU (Swish Gated Linear Unit) 作为激活函数，而非标准的 ReLU (Rectified Linear Unit) 或 GELU (Gaussian Error Linear Unit)，主要是因为 SwiGLU 在大规模语言模型训练中的性能优势。首先，SwiGLU 结合了 Swish activation 和 Gated Linear Unit (GLU) 结构，使其能够 <strong>在非线性表达能力和计算效率之间取得平衡</strong>。相比于 ReLU，<strong>SwiGLU 避免了 dying ReLU problem（神经元输出恒为零的问题），并且减少了梯度消失现象</strong>。而相较于 GELU，SwiGLU 在实践中展现出了更优的梯度流动特性，并提高了训练稳定性。此外，SwiGLU 通过门控机制有效地增强了模型的表示能力，同时在 Transformer 结构中带来了更好的 parameter efficiency（参数效率），这对于大规模预训练模型至关重要。因此，LLaMA 采用 SwiGLU 作为激活函数，以提高整体的模型性能和训练效率。</p></blockquote><ul><li><strong>不使用 Position Embedding Table</strong>：LLaMA 放弃了传统的 绝对位置编码，转而使用 RoPE，减少了额外的参数，同时增强了模型的泛化能力。</li></ul><ol start=3><li><strong>Flash Attention 提高推理效率</strong>：LLaMA 可能使用 Flash Attention 进行加速，减少了传统 Transformer 中注意力计算的 时间复杂度：</li></ol><span>\[
O(n^2 d) \rightarrow O(n d)
\]</span></div></details><hr><h2 id=训练优化><strong>训练优化</strong>
<a class=anchor href=#%e8%ae%ad%e7%bb%83%e4%bc%98%e5%8c%96>#</a></h2><details><summary><strong class=custom-details-title>⁉️ 为什么 Decoder-only 模型推理时需要 KV Cache？如何优化其内存占用？</strong></summary><div class=markdown-inner><h2><b>为什么 Decoder-only 模型推理时需要 KV Cache？如何优化其内存占用？</b></h2><blockquote class="book-hint warning"><p><strong>Note</strong>：在推理阶段，所有的 <strong>权重（Weights）已经通过训练学习完毕（不再改变）</strong>。输入的句子会按照预定义的规则转换成 Query (Q)、Key (K) 和 Value (V) 向量。无论是训练阶段还是推理阶段，句子中的 <strong>每个 token（词）都会有一个唯一对应的 Key 和 Value 向量</strong>。在自回归生成过程中，每次输入新的时间步 x_t ，<strong>会计算出当前时刻的 Query 向量 Q_t ，同时，新的 Key 和 Value 向量 K_t, V_t 会与之前的 KV 缓存合并</strong>，形成当前时刻的完整历史信息。新的 Query 用于查询当前输入和历史信息之间的关系，从而生成有意义的上下文信息，用于生成下一个 token。</p></blockquote><p>在 Decoder-only 模型（如 GPT）中，推理时需要 KV Cache（Key-Value Cache）来提高计算效率和减少内存占用。KV Cache 主要用于存储模型在每个时间步生成的 Key 和 Value 向量，这些向量用于自注意力机制（Self-Attention）中计算当前词与之前所有词之间的依赖关系。具体来说，<strong>当模型生成一个新的 token 时，它不仅要计算当前 token 的自注意力，还需要利用已经生成的 tokens 的表示来计算新的 token</strong>，因此必须保存这些 Key 和 Value 向量。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：假设我们正在进行推理，模型已经生成了序列 “I love deep learning” 中的前四个词 “I love deep learning”（也就是说，当前时间步是第5个词）。在传统推理中，如果没有 KV Cache，模型在计算每个新的 token 时都需要重新计算与之前所有已经生成的 tokens（“I love deep learning”）之间的依赖关系。</p><ul><li>对于第5个词 “model”，模型首先计算 “model” 和 “I”、“love”、“deep”、“learning” 之间的自注意力（self-attention）。为了计算这个自注意力，模型需要 <strong>重新计算每个之前词的 Key 和 Value 向量</strong>。</li></ul><p>为了避免这种重复计算，使用 KV Cache 的方法是：当我们生成第5个词时，模型会保存 <strong>前四个词（“I love deep learning”）的 Key 和 Value 向量</strong>。下一次生成新 token 时（比如第6个词），模型只需要利用 缓存中的 Key 和 Value 向量 来计算当前 token 和已经生成的历史 tokens 之间的依赖关系，而无需重新计算历史 tokens 的表示。</p><ul><li>对于第5个词 “model”，模型首先计算 “model” 和缓存中的 “I love deep learning” 之间的自注意力。</li><li>在此过程中，模型使用的是 <strong>已经缓存的 Key 和 Value 向量，而不是重新计算整个输入序列的 Key 和 Value 向量</strong>。</li><li>当模型生成第6个词时，只需将第5个词 “model” 的 Key 和 Value 向量加入缓存，并计算与缓存中所有其他 tokens 之间的关系。</li></ul></blockquote><p>在传统的推理过程中，模型需要重新计算每个时间步的所有 Key 和 Value 向量，导致计算量和内存占用急剧增加。使用 KV Cache 后，模型只需要保存每一层的 Key 和 Value 向量，从而避免了重复计算，极大地提升了推理效率。</p><p><strong>如何优化内存占用：</strong></p><ol><li><strong>动态 KV 缓存大小</strong>：在一些任务中，并不需要保留所有时间步的 Key 和 Value 向量。例如，对于生成式任务，缓存可以按照一定步长进行清理，或者只保留 前 n 步 的缓存。</li><li><strong>分层缓存</strong>：根据模型层数和层间依赖，可以在 不同层 采用不同的缓存策略。例如，可以对较低层进行更频繁的缓存清理，对较高层保留更多的缓存信息。</li><li><strong>量化（Quantization）</strong>：通过降低 Key 和 Value 向量的精度（例如从浮点数精度到低精度存储），减少内存占用，同时尽量保持推理的精度。</li></ol></div></details><hr><details><summary><strong class=custom-details-title>⁉️ Decoder-only 模型如何处理长文本依赖问题？（如稀疏注意力、窗口注意力）</strong></summary><div class=markdown-inner><h2><b>Decoder-only 模型如何处理长文本依赖问题？（如稀疏注意力、窗口注意力）</b></h2><p>Decoder-only 模型（如 GPT 类模型）通过不同的技术来处理长文本中的依赖问题，尤其是在处理长序列时，传统的 全局注意力（Global Attention） 计算会变得非常消耗资源。为了解决这个问题，Decoder-only 模型采用了 稀疏注意力（Sparse Attention） 和 窗口注意力（Windowed Attention） 等方法，从而有效地减小计算复杂度并增强长文本的建模能力。</p><ul><li><strong>窗口注意力（Windowed Attention）</strong> 是一种将输入序列划分为多个固定大小的窗口（或块），每个窗口内的 token 之间通过注意力进行交互，而窗口之间没有直接的依赖关系。窗口大小是一个超参数，通常会选择较小的窗口以限制每次计算的注意力范围，从而减少计算负担。通过这种方式，模型能够在较低的计算成本下捕捉到长序列中的重要信息，同时避免了全局注意力带来的高昂计算开销。</li></ul><blockquote class="book-hint warning"><p><strong>e.g.</strong>：假设我们有一个长度为 6 的序列 <code>[A, B, C, D, E, F]</code>，并且我们选择一个大小为 3 的窗口进行计算。</p><ol><li>在窗口注意力中，我们将输入序列分为若干个滑动窗口。例如，窗口大小为 3 的情况下：<ul><li>第一个窗口：<code>[A, B, C]</code></li><li>第二个窗口：<code>[B, C, D]</code>
每个窗口内部的 token 之间会进行注意力计算，但是不同窗口之间的 token 之间是没有交互的。</li></ul></li><li>对于序列 <code>[A, B, C, D, E, F]</code>，采用窗口大小为 3 的策略，注意力计算矩阵将是：</li></ol><span>\[
\begin{array}{|c|c|c|c|c|c|c|}
\hline
& A & B & C & D & E & F \\
\hline
A & 1 & 1 & 1 & 0 & 0 & 0 \\
B & 1 & 1 & 1 & 1 & 0 & 0 \\
C & 1 & 1 & 1 & 1 & 1 & 0 \\
D & 0 & 1 & 1 & 1 & 1 & 1 \\
E & 0 & 0 & 1 & 1 & 1 & 1 \\
F & 0 & 0 & 0 & 1 & 1 & 1 \\
\hline
\end{array}
\]</span></blockquote><ul><li><strong>稀疏注意力（Sparse Attention）</strong> 的 <strong>核心思想是通过引入局部化注意力机制，使得每个 token 只与部分上下文进行交互，从而减少计算量</strong>。具体而言，稀疏注意力只计算一部分的注意力权重而不是全部，这样可以降低模型计算的复杂度。常见的稀疏注意力结构包括 固定模式（Fixed Patterns） 和 学习模式（Learned Patterns），其中一个代表固定的局部上下文窗口，另一个则依赖于模型在训练过程中自适应学习关注哪些位置的关系。稀疏注意力通常通过 Top-k 注意力（Top-k Attention） 或 Block-sparse 格式 来实现。</li></ul><blockquote class="book-hint warning"><p><strong>Note:</strong> 稀疏注意力和窗口注意力是非常相似的概念，都属于通过减少计算量来优化自注意力机制的方法。<strong>但稀疏注意力不仅仅局限于相邻的 token，还可以是基于某些策略（如全局选择、局部窗口、随机选择等）来选择哪些 token 进行注意力计算</strong>。它可以通过灵活的方式选择注意力的稀疏性，可以是全局性策略（如某些重要的 token）或局部性策略（如基于输入特征选择 token）。</p></blockquote></div></details><hr><ul><li>混合精度训练（FP16、BF16）</li><li>混合精度训练（Mixed Precision）和梯度累积（Gradient Accumulation）的原理是什么？</li><li>什么是 FlashAttention？为什么它比传统 Attention 更高效？</li><li>分布式训练</li><li>训练框架（PyTorch DDP、DeepSpeed）</li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#预训练基础知识><strong>预训练基础知识</strong></a></li><li><a href=#预训练细节><strong>预训练细节</strong></a></li><li><a href=#encoder-only><strong>Encoder-Only</strong></a></li><li><a href=#encoder-decoder><strong>Encoder-Decoder</strong></a></li><li><a href=#decoder-only><strong>Decoder-Only</strong></a></li><li><a href=#训练优化><strong>训练优化</strong></a></li></ul></nav></div></aside></main></body></html>