<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  现代大语言模型（Modern Large Language Models）
  #


  预训练基础知识
  #

⁉️ 什么是预训练？与传统监督学习的区别？
  
什么是预训练？与传统监督学习的区别？
预训练（Pretraining）是一种在大规模无标注数据（Unlabeled Data）上训练深度学习模型的技术，特别常用于自然语言处理（NLP）中的大规模语言模型。在预训练阶段，模型通常采用 自监督学习（Self-Supervised Learning）方法，通过预测被遮蔽的词（如 BERT 的掩码语言模型 Masked Language Model, MLM）或基于上下文预测下一个词（如 GPT 的自回归语言模型 Autoregressive Language Model, AR）来学习文本的统计特性和语义表示（Semantic Representation）。
相比传统的监督学习（Supervised Learning），预训练 不需要大量人工标注数据，而是利用大规模无标签语料，使模型具备广泛的语言理解能力。随后，模型可以通过 微调（Fine-Tuning）在小规模标注数据上进一步优化，以适应具体任务。这种方式相比传统监督学习更加高效，尤其适用于数据标注成本高的任务，同时提升模型的泛化能力（Generalization Ability）和适应性（Adaptability）。

  Note：Pre-training 是在 大规模无监督数据上训练模型，而 Fine-tuning 是在 特定任务或数据集上对模型进行微调。

  


⁉️ LLM 的预训练流程通常涉及到哪些环节？
  
LLM 的预训练流程通常涉及到哪些环节？
在 大规模语言模型（LLM, Large Language Model） 的 预训练（Pretraining） 过程中，整个流程可以从宏观的角度划分为 数据收集与预处理、模型架构设计、训练目标设定、优化与梯度更新、以及分布式训练 五个关键部分，每一部分在模型能力的提升上都起着至关重要的作用。

数据收集与预处理（Data Collection & Preprocessing）：预训练的第一步是收集大规模、高质量的数据集，通常包括 网络文本（web corpus）、书籍（books）、维基百科（Wikipedia）、代码数据（code repositories） 等多种来源。数据经过 去重（deduplication）、格式化（formatting）、分词（tokenization） 等预处理步骤，以确保输入的数据干净、标准化，并能有效地被神经网络处理。此外，还可能应用 文本过滤（text filtering） 和 去毒化（detoxification） 以减少偏见（bias mitigation）和不良内容。
模型架构设计（Model Architecture Design）：现代 LLM 主要采用 Transformer 架构，其核心是 自注意力机制（Self-Attention Mechanism），能够有效建模长距离依赖（long-range dependencies）。具体的设计可能包括 解码器（Decoder-only, 如 GPT 系列） 或 编码器-解码器（Encoder-Decoder, 如 T5） 结构，参数规模从数亿到数千亿不等。此外，还涉及 层数（number of layers）、隐藏维度（hidden size）、多头注意力（multi-head attention）、前馈网络（feedforward network） 等关键超参数的选择。
训练目标设定（Training Objectives）：LLM 的预训练目标通常基于 自监督学习（Self-Supervised Learning），主要分为：

自回归目标（Autoregressive Objective, AR）：如 GPT 系列使用的 因果语言建模（Causal Language Modeling, CLM），即通过已知的上下文预测下一个单词。
自编码目标（Autoencoding Objective, AE）：如 BERT 采用的 掩码语言建模（Masked Language Modeling, MLM），即在输入中随机掩盖（mask）部分单词，并让模型预测原始单词。
对比学习（Contrastive Learning） 和 指令调优（Instruction Tuning） 在部分预训练或微调阶段也会用到，以提升模型的表示能力。


优化与梯度更新（Optimization & Gradient Updates）：预训练的核心是基于 梯度下降（Gradient Descent） 进行参数优化，具体采用 AdamW（带权重衰减的 Adam 优化器）来减少过拟合（overfitting）。此外，由于 LLM 训练涉及超大规模的参数，通常会使用：

混合精度训练（Mixed Precision Training, FP16/BF16） 降低计算成本。
梯度裁剪（Gradient Clipping） 解决梯度爆炸（Gradient Explosion）。
学习率调度（Learning Rate Scheduling, 如 Cosine Decay, Warmup） 提升收敛效率。


分布式训练（Distributed Training）：由于 LLM 规模巨大，单机难以承载完整计算，因此需要分布式训练，包括：

数据并行（Data Parallelism, DP）：多个 GPU 处理相同的模型，但分配不同的数据批次（batches）。
模型并行（Model Parallelism, MP）：模型的不同部分分布在多个 GPU 计算，适用于超大参数模型。
流水线并行（Pipeline Parallelism） 和 张量并行（Tensor Parallelism） 结合使用，以提升大模型的计算效率。



  


⁉️ 解释Encoder-only，Encoder-Decoder，Decoder-only 三种模式的区别以及使用场景？
  
解释Encoder-only，Encoder-Decoder，Decoder-only 三种模式的区别以及使用场景？


Encoder-only 模式：在这种架构中，只有 编码器（Encoder） 部分用于处理输入数据，通常应用于 文本分类（Text Classification）、命名实体识别（Named Entity Recognition, NER） 和 情感分析（Sentiment Analysis） 等任务。该模式的主要任务是从输入序列中提取信息，生成固定长度的表示，适用于 需要理解输入而不生成输出 的任务。例如，BERT 就是一个典型的 Encoder-only 模型，通过 自注意力机制（Self-Attention） 学习输入文本的上下文信息并生成表示。"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/deep-learning/attention-and-transformers/modern-large-language-models/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Modern LLMs and Pre-Training"><meta property="og:description" content="现代大语言模型（Modern Large Language Models） # 预训练基础知识 # ⁉️ 什么是预训练？与传统监督学习的区别？ 什么是预训练？与传统监督学习的区别？ 预训练（Pretraining）是一种在大规模无标注数据（Unlabeled Data）上训练深度学习模型的技术，特别常用于自然语言处理（NLP）中的大规模语言模型。在预训练阶段，模型通常采用 自监督学习（Self-Supervised Learning）方法，通过预测被遮蔽的词（如 BERT 的掩码语言模型 Masked Language Model, MLM）或基于上下文预测下一个词（如 GPT 的自回归语言模型 Autoregressive Language Model, AR）来学习文本的统计特性和语义表示（Semantic Representation）。
相比传统的监督学习（Supervised Learning），预训练 不需要大量人工标注数据，而是利用大规模无标签语料，使模型具备广泛的语言理解能力。随后，模型可以通过 微调（Fine-Tuning）在小规模标注数据上进一步优化，以适应具体任务。这种方式相比传统监督学习更加高效，尤其适用于数据标注成本高的任务，同时提升模型的泛化能力（Generalization Ability）和适应性（Adaptability）。
Note：Pre-training 是在 大规模无监督数据上训练模型，而 Fine-tuning 是在 特定任务或数据集上对模型进行微调。
⁉️ LLM 的预训练流程通常涉及到哪些环节？ LLM 的预训练流程通常涉及到哪些环节？ 在 大规模语言模型（LLM, Large Language Model） 的 预训练（Pretraining） 过程中，整个流程可以从宏观的角度划分为 数据收集与预处理、模型架构设计、训练目标设定、优化与梯度更新、以及分布式训练 五个关键部分，每一部分在模型能力的提升上都起着至关重要的作用。
数据收集与预处理（Data Collection & Preprocessing）：预训练的第一步是收集大规模、高质量的数据集，通常包括 网络文本（web corpus）、书籍（books）、维基百科（Wikipedia）、代码数据（code repositories） 等多种来源。数据经过 去重（deduplication）、格式化（formatting）、分词（tokenization） 等预处理步骤，以确保输入的数据干净、标准化，并能有效地被神经网络处理。此外，还可能应用 文本过滤（text filtering） 和 去毒化（detoxification） 以减少偏见（bias mitigation）和不良内容。 模型架构设计（Model Architecture Design）：现代 LLM 主要采用 Transformer 架构，其核心是 自注意力机制（Self-Attention Mechanism），能够有效建模长距离依赖（long-range dependencies）。具体的设计可能包括 解码器（Decoder-only, 如 GPT 系列） 或 编码器-解码器（Encoder-Decoder, 如 T5） 结构，参数规模从数亿到数千亿不等。此外，还涉及 层数（number of layers）、隐藏维度（hidden size）、多头注意力（multi-head attention）、前馈网络（feedforward network） 等关键超参数的选择。 训练目标设定（Training Objectives）：LLM 的预训练目标通常基于 自监督学习（Self-Supervised Learning），主要分为： 自回归目标（Autoregressive Objective, AR）：如 GPT 系列使用的 因果语言建模（Causal Language Modeling, CLM），即通过已知的上下文预测下一个单词。 自编码目标（Autoencoding Objective, AE）：如 BERT 采用的 掩码语言建模（Masked Language Modeling, MLM），即在输入中随机掩盖（mask）部分单词，并让模型预测原始单词。 对比学习（Contrastive Learning） 和 指令调优（Instruction Tuning） 在部分预训练或微调阶段也会用到，以提升模型的表示能力。 优化与梯度更新（Optimization & Gradient Updates）：预训练的核心是基于 梯度下降（Gradient Descent） 进行参数优化，具体采用 AdamW（带权重衰减的 Adam 优化器）来减少过拟合（overfitting）。此外，由于 LLM 训练涉及超大规模的参数，通常会使用： 混合精度训练（Mixed Precision Training, FP16/BF16） 降低计算成本。 梯度裁剪（Gradient Clipping） 解决梯度爆炸（Gradient Explosion）。 学习率调度（Learning Rate Scheduling, 如 Cosine Decay, Warmup） 提升收敛效率。 分布式训练（Distributed Training）：由于 LLM 规模巨大，单机难以承载完整计算，因此需要分布式训练，包括： 数据并行（Data Parallelism, DP）：多个 GPU 处理相同的模型，但分配不同的数据批次（batches）。 模型并行（Model Parallelism, MP）：模型的不同部分分布在多个 GPU 计算，适用于超大参数模型。 流水线并行（Pipeline Parallelism） 和 张量并行（Tensor Parallelism） 结合使用，以提升大模型的计算效率。 ⁉️ 解释Encoder-only，Encoder-Decoder，Decoder-only 三种模式的区别以及使用场景？ 解释Encoder-only，Encoder-Decoder，Decoder-only 三种模式的区别以及使用场景？ Encoder-only 模式：在这种架构中，只有 编码器（Encoder） 部分用于处理输入数据，通常应用于 文本分类（Text Classification）、命名实体识别（Named Entity Recognition, NER） 和 情感分析（Sentiment Analysis） 等任务。该模式的主要任务是从输入序列中提取信息，生成固定长度的表示，适用于 需要理解输入而不生成输出 的任务。例如，BERT 就是一个典型的 Encoder-only 模型，通过 自注意力机制（Self-Attention） 学习输入文本的上下文信息并生成表示。"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Modern LLMs and Pre-Training | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/deep-learning/attention-and-transformers/modern-large-language-models/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.d88e6e3e0990945d7da04891e0fb668aee8dd117eb68f433a9e3e5cfad9c0fa8.js integrity="sha256-2I5uPgmQlF19oEiR4Ptmiu6N0RfraPQzqePlz62cD6g=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/deep-learning/attention-and-transformers/modern-large-language-models/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li><li><a href=/docs/machine-learning/computational-performance/>Computational Performance</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><input type=checkbox id=section-d0dd931d60033c220ecd4cd60b7c9170 class=toggle>
<label for=section-d0dd931d60033c220ecd4cd60b7c9170 class="flex justify-between"><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a></label><ul><li><a href=/docs/deep-learning/convolutional-neural-networks/modern-convolutional-neural-networks/>Modern Convolutional Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-a3019bfa8037cc33ed6405d1589b6219 class=toggle>
<label for=section-a3019bfa8037cc33ed6405d1589b6219 class="flex justify-between"><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a></label><ul><li><a href=/docs/deep-learning/recurrent-neural-networks/modern-recurrent-neural-networks/>Modern Recurrent Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-0a43584c16258b228ae9aa8d70efc320 class=toggle checked>
<label for=section-0a43584c16258b228ae9aa8d70efc320 class="flex justify-between"><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a></label><ul><li><a href=/docs/deep-learning/attention-and-transformers/tokenization-and-word-embeddings/>Tokenization and Word Embeddings</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/transformer-architecture/>Transformer Architecture</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/modern-large-language-models/ class=active>Modern LLMs and Pre-Training</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/post-training-large-language-models/>Post-training LLMs</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/multimodal-large-language-models/>Multimodal Large Language Models</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92e8358c45c96009753cf4227e9daea8 class=toggle>
<label for=section-92e8358c45c96009753cf4227e9daea8 class="flex justify-between"><a href=/docs/deep-learning/llm-pipelines/>LLM Pipelines</a></label><ul><li><a href=/docs/deep-learning/llm-pipelines/llm-hardware-and-model-size/>LLM Hardware and Model Size</a><ul></ul></li><li><a href=/docs/deep-learning/llm-pipelines/large-scale-pretraining-with-transformers/>Large-Scale Pretraining with Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/llm-pipelines/llm-inference-and-deployment/>LLM Inference and Deployment</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul><li><a href=/docs/others/interview-preparation-guide/>Interview Preparation Guide</a><ul></ul></li></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Modern LLMs and Pre-Training</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#预训练基础知识><strong>预训练基础知识</strong></a></li><li><a href=#预训练细节><strong>预训练细节</strong></a></li><li><a href=#encoder-only><strong>Encoder-Only</strong></a></li><li><a href=#encoder-decoder><strong>Encoder-Decoder</strong></a></li><li><a href=#decoder-only><strong>Decoder-Only</strong></a></li><li><a href=#训练优化><strong>训练优化</strong></a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=现代大语言模型modern-large-language-models><strong>现代大语言模型（Modern Large Language Models）</strong>
<a class=anchor href=#%e7%8e%b0%e4%bb%a3%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8bmodern-large-language-models>#</a></h1><h2 id=预训练基础知识><strong>预训练基础知识</strong>
<a class=anchor href=#%e9%a2%84%e8%ae%ad%e7%bb%83%e5%9f%ba%e7%a1%80%e7%9f%a5%e8%af%86>#</a></h2><details><summary><strong class=custom-details-title>⁉️ 什么是预训练？与传统监督学习的区别？</strong></summary><div class=markdown-inner><h2><b>什么是预训练？与传统监督学习的区别？</b></h2><p>预训练（Pretraining）是一种在大规模无标注数据（Unlabeled Data）上训练深度学习模型的技术，特别常用于自然语言处理（NLP）中的大规模语言模型。在预训练阶段，模型通常采用 <strong>自监督学习（Self-Supervised Learning）方法</strong>，通过预测被遮蔽的词（如 BERT 的掩码语言模型 Masked Language Model, MLM）或基于上下文预测下一个词（如 GPT 的自回归语言模型 Autoregressive Language Model, AR）来学习文本的统计特性和语义表示（Semantic Representation）。</p><p>相比传统的监督学习（Supervised Learning），预训练 <strong>不需要大量人工标注数据，而是利用大规模无标签语料</strong>，使模型具备广泛的语言理解能力。随后，模型可以通过 <strong>微调（Fine-Tuning）在小规模标注数据上进一步优化</strong>，以适应具体任务。这种方式相比传统监督学习更加高效，尤其适用于数据标注成本高的任务，同时提升模型的泛化能力（Generalization Ability）和适应性（Adaptability）。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：Pre-training 是在 <strong>大规模无监督数据上训练模型</strong>，而 Fine-tuning 是在 <strong>特定任务或数据集上对模型进行微调</strong>。</p></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ LLM 的预训练流程通常涉及到哪些环节？</strong></summary><div class=markdown-inner><h2><b>LLM 的预训练流程通常涉及到哪些环节？</b></h2><p>在 大规模语言模型（LLM, Large Language Model） 的 预训练（Pretraining） 过程中，整个流程可以从宏观的角度划分为 数据收集与预处理、模型架构设计、训练目标设定、优化与梯度更新、以及分布式训练 五个关键部分，每一部分在模型能力的提升上都起着至关重要的作用。</p><ol><li><strong>数据收集与预处理（Data Collection & Preprocessing）</strong>：预训练的第一步是收集大规模、高质量的数据集，通常包括 网络文本（web corpus）、书籍（books）、维基百科（Wikipedia）、代码数据（code repositories） 等多种来源。数据经过 去重（deduplication）、格式化（formatting）、分词（tokenization） 等预处理步骤，以确保输入的数据干净、标准化，并能有效地被神经网络处理。此外，还可能应用 文本过滤（text filtering） 和 去毒化（detoxification） 以减少偏见（bias mitigation）和不良内容。</li><li><strong>模型架构设计（Model Architecture Design）</strong>：现代 LLM 主要采用 Transformer 架构，其核心是 自注意力机制（Self-Attention Mechanism），能够有效建模长距离依赖（long-range dependencies）。具体的设计可能包括 解码器（Decoder-only, 如 GPT 系列） 或 编码器-解码器（Encoder-Decoder, 如 T5） 结构，参数规模从数亿到数千亿不等。此外，还涉及 层数（number of layers）、隐藏维度（hidden size）、多头注意力（multi-head attention）、前馈网络（feedforward network） 等关键超参数的选择。</li><li><strong>训练目标设定（Training Objectives）</strong>：LLM 的预训练目标通常基于 自监督学习（Self-Supervised Learning），主要分为：<ul><li>自回归目标（Autoregressive Objective, AR）：如 GPT 系列使用的 因果语言建模（Causal Language Modeling, CLM），即通过已知的上下文预测下一个单词。</li><li>自编码目标（Autoencoding Objective, AE）：如 BERT 采用的 掩码语言建模（Masked Language Modeling, MLM），即在输入中随机掩盖（mask）部分单词，并让模型预测原始单词。</li><li>对比学习（Contrastive Learning） 和 指令调优（Instruction Tuning） 在部分预训练或微调阶段也会用到，以提升模型的表示能力。</li></ul></li><li><strong>优化与梯度更新（Optimization & Gradient Updates）</strong>：预训练的核心是基于 梯度下降（Gradient Descent） 进行参数优化，具体采用 AdamW（带权重衰减的 Adam 优化器）来减少过拟合（overfitting）。此外，由于 LLM 训练涉及超大规模的参数，通常会使用：<ul><li>混合精度训练（Mixed Precision Training, FP16/BF16） 降低计算成本。</li><li>梯度裁剪（Gradient Clipping） 解决梯度爆炸（Gradient Explosion）。</li><li>学习率调度（Learning Rate Scheduling, 如 Cosine Decay, Warmup） 提升收敛效率。</li></ul></li><li><strong>分布式训练（Distributed Training）</strong>：由于 LLM 规模巨大，单机难以承载完整计算，因此需要分布式训练，包括：<ul><li>数据并行（Data Parallelism, DP）：多个 GPU 处理相同的模型，但分配不同的数据批次（batches）。</li><li>模型并行（Model Parallelism, MP）：模型的不同部分分布在多个 GPU 计算，适用于超大参数模型。</li><li>流水线并行（Pipeline Parallelism） 和 张量并行（Tensor Parallelism） 结合使用，以提升大模型的计算效率。</li></ul></li></ol></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 解释Encoder-only，Encoder-Decoder，Decoder-only 三种模式的区别以及使用场景？</strong></summary><div class=markdown-inner><h2><b>解释Encoder-only，Encoder-Decoder，Decoder-only 三种模式的区别以及使用场景？</b></h2><ul><li><p><strong>Encoder-only 模式</strong>：在这种架构中，只有 <strong>编码器（Encoder）</strong> 部分用于处理输入数据，通常应用于 文本分类（Text Classification）、命名实体识别（Named Entity Recognition, NER） 和 情感分析（Sentiment Analysis） 等任务。该模式的主要任务是从输入序列中提取信息，生成固定长度的表示，适用于 <strong>需要理解输入而不生成输出</strong> 的任务。例如，BERT 就是一个典型的 Encoder-only 模型，通过 自注意力机制（Self-Attention） 学习输入文本的上下文信息并生成表示。</p></li><li><p><strong>Encoder-Decoder 模式</strong>：这种架构包含一个 编码器（Encoder） 和一个 解码器（Decoder），用于从输入生成输出。输入通过编码器进行处理，得到一个上下文表示，然后解码器根据这个表示生成最终输出。这种结构非常适合 <strong>序列到序列任务（Sequence-to-Sequence Tasks）</strong>，如 机器翻译（Machine Translation） 和 文本摘要（Text Summarization）。</p></li><li><p><strong>Decoder-only 模式</strong>：这种架构仅包含 <strong>解码器（Decoder）</strong>，通常用于 <strong>自回归生成任务（Autoregressive Generation Tasks）</strong>，例如 文本生成（Text Generation） 和 语言建模（Language Modeling）。在这种模式下，模型根据前面的输入和已经生成的词预测下一个词。一个典型的例子是 GPT 系列模型，它基于 Decoder-only 架构，通过不断预测下一个词来生成连贯的文本。此模式非常适用于 <strong>需要根据上下文生成输出</strong> 的任务。</p></div></li></ul></details><hr><h2 id=预训练细节><strong>预训练细节</strong>
<a class=anchor href=#%e9%a2%84%e8%ae%ad%e7%bb%83%e7%bb%86%e8%8a%82>#</a></h2><details><summary><strong class=custom-details-title>⁉️ 什么是参数初始化？有哪些适合 LLM 训练的参数初始化策略（Parameter Initialization）？</strong></summary><div class=markdown-inner><h2><b>什么是参数初始化？有哪些适合 LLM 训练的参数初始化策略（Parameter Initialization）？</b></h2><p>参数初始化（Parameter Initialization） 是神经网络训练中的一个关键步骤，旨在为 <strong>网络的权重（weights）和偏置（biases）赋予初始值</strong>。这些初始值对模型的训练收敛速度、稳定性及最终性能有着重要影响。<strong>合理的参数初始化策略能够避免梯度消失（Vanishing Gradient）或梯度爆炸（Exploding Gradient）等问题，进而提高训练效率</strong>。</p><p>对于 大规模语言模型（Large Language Models, LLM） 的训练，一些常见且适用的参数初始化策略包括：</p><ol><li><strong>Xavier 初始化（Xavier Initialization）</strong>：也叫做 Glorot 初始化，它通过考虑输入和输出的神经元数量来设置权重的方差。具体来说，权重的方差设置为：</li></ol><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[
W_{i,j} \sim N\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)
\]</span><p>其中 n_in 和 n_out 分别是当前层的输入和输出神经元数量。此策略通常用于 sigmoid 或 tanh 激活函数的网络，能够帮助缓解梯度消失问题。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>Xavier 初始化适用于激活函数是 sigmoid 或 tanh 的网络</strong>的原因是这些激活函数的<strong>导数容易趋于零</strong>，尤其是在输入值落入激活函数的<strong>饱和区（Sigmoid 的两侧平坦区域）</strong>。如果权重初始化过大，输入会快速进入饱和区，导致梯度消失。如果权重初始化过小，输出信号会逐层衰减，最终导致梯度消失。</p><p>Xavier 的初始化方法<strong>将权重分布限定在一个较小的范围内</strong>，使输入值主要分布在 Sigmoid 和 Tanh 的线性区，避免梯度消失。在较深的网络中，信号可能仍会因为层数的累积效应导致衰减或放大。</p></blockquote><ol start=2><li><strong>He 初始化（He Initialization）</strong>：类似于 Xavier 初始化，但特别适用于 ReLU 激活函数。它通过设置权重的方差，有效地解决了 ReLU 激活函数中常见的 dying ReLU 问题，即一些神经元始终不激活。</li></ol><span>\[
W_{i,j} \sim N\left(0, \frac{2}{n_{\text{in}}}\right)
\]</span><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>He 初始化适用于激活函数是ReLU及其变种</strong>的原因是对于 ReLU，当输入为负时，输出恒为 0；当输入为正时，输出为原值。由于一部分神经元输出会被截断为 0，导致<strong>有效的参与计算的神经元数量减少</strong>（称为“稀疏激活”现象）。如果初始化权重过小，信号会迅速减弱，导致梯度消失；而如果权重过大，信号会迅速放大，导致梯度爆炸。</p><p>He 初始化通过<strong>设定较大的方差</strong>，补偿了 ReLU 截断负值导致的信号损失。这样可以让激活值的分布更均衡，<strong>避免信号快速衰减或放大</strong>。He 初始化<strong>根据输入层大小调整权重的方差，使每层的输出方差保持相对稳定</strong>，即使网络层数增加，信号也不会显著衰减或爆炸。</p></blockquote><ol start=3><li><strong>Pretrained Initialization</strong>：对于 LLM，使用在大规模数据集上预训练的权重作为初始化参数（例如 BERT、GPT）是一种常见且有效的做法。通过 <strong>迁移学习（Transfer Learning）</strong>，这种初始化策略能够显著加速训练过程，并提升模型的性能。</li></ol></div></details><hr><ul><li>LLM 预训练中为什么常用 Adam 或 AdamW 优化器？</li><li>什么是 Gradient Clipping？在预训练中它通常怎么设置？</li><li>为什么大模型预训练时通常需要 Warmup 学习率策略？</li><li>如何理解训练中 Batch Size 和模型收敛的关系？</li><li>困惑度（Perplexity）的物理意义是什么？为什么不能完全依赖它评估模型？</li><li>为什么大模型会出现幻觉（Hallucination）？如何检测和减少？</li></ul><h2 id=encoder-only><strong>Encoder-Only</strong>
<a class=anchor href=#encoder-only>#</a></h2><details><summary><strong class=custom-details-title>⁉️ Encoder-Only 在架构上和 transformer 有什么区别？</strong></summary><div class=markdown-inner><h2><b>Encoder-Only 在架构上和 transformer 有什么区别？</b></h2><p>Encoder-Only 架构只保留了 Transformer 的 Encoder 部分，完全去掉了 Decoder，所以它 <strong>只能用于特征提取或上下文建模（Representation Learning），而不是生成任务（Generation Tasks）</strong>。它的训练目标通常是掩码语言模型（Masked Language Modeling, MLM），而不是自回归语言模型（Autoregressive Language Modeling, AR）。换句话说，Encoder-Only模型的输入和输出都是“同时存在的完整句子”，<strong>模型学习的是如何理解和表征输入，而不是如何生成新的输出</strong>。</p><p>和完整的Transformer结构相比，缺少了解码器的部分，因此结构上更简单，适用的任务也偏向于分类（Classification）、特征抽取（Feature Extraction）、检索（Retrieval）等理解类任务（Understanding Tasks），而非文本生成（Text Generation）。</p><div align=center><img src=/images/BERT.png width=200px/></div></div></details><hr><details><summary><strong class=custom-details-title>⁉️ BERT 的预训练目标有哪些？什么是 MLM 和 NSP？具体如何实现？</strong></summary><div class=markdown-inner><h2><b>BERT 的预训练目标有哪些？什么是 MLM 和 NSP？具体如何实现？</b></h2><p>BERT（Bidirectional Encoder Representations from Transformers）的预训练目标主要包括 <strong>Masked Language Modeling（MLM）</strong> 和 <strong>Next Sentence Prediction（NSP）</strong>。</p><ul><li><p><strong>① Masked Language Modeling (MLM)</strong></p><p>Masked Language Modeling 是 BERT 预训练的核心目标，目的是让模型通过理解上下文，预测被遮盖（Masked）的单词。相比 GPT 的自回归预测，MLM 允许模型在训练时同时观察整个输入句子的左右两侧信息，是一种双向语言建模（Bidirectional Language Modeling）。</p><p>在 MLM 任务 中，BERT <strong>随机遮盖（mask）</strong> 输入文本中的部分词汇（通常为 15%），并通过 Transformer Encoder 预测这些被遮盖的词。具体实现时，80% 的被遮盖词替换为 <code>[MASK]</code>，10% 保持不变，另 10% 替换为随机词，以增强模型的泛化能力。<strong>模型接收完整的句子（包括 <code>[MASK]</code>）作为输入，目标是在输出端预测这些被遮盖位置的原始 token。</strong></p><p><strong>损失函数</strong> 通常使用 <strong>Cross-Entropy Loss</strong>，只对被 mask 的位置计算损失：</p><p>$$
L = - \sum_{i \in \text{Mask}} \log P_\theta (x_i | X_{\setminus i})
$$</p></li><li><p>② <strong>Next Sentence Prediction (NSP)</strong></p><p>NSP 任务 旨在学习句子级别的关系，BERT 通过给定的两个句子 <strong>判断它们是否为原始文本中的连续句（IsNext）或随机拼接的无关句（NotNext）</strong>。输入格式为：</p><pre tabindex=0><code>[CLS] Sentence A [SEP] Sentence B [SEP]
</code></pre><p>训练时，BERT 以 50% 的概率选择相邻句子作为正样本，另 50% 选择无关句子作为负样本，并通过 二分类损失函数（Binary Cross-Entropy Loss） 进行优化。最后的输出 <code>[CLS]</code> 位置的向量经过一个二分类器（通常是一个简单的全连接层+Softmax），预测 Sentence B 是否为 Sentence A 的下一个句子。</p><p><strong>损失函数</strong> 通常使用 <strong>二分类交叉熵损失（Binary Cross-Entropy）</strong>：</p><p>$$
L = - [ y \log p + (1 - y) \log (1 - p) ]
$$</p><p><strong>MLM 使 BERT 能够学习上下文双向依赖关系，而 NSP 则有助于建模句子间的全局关系</strong>，这两个目标共同提升了 BERT 在 自然语言理解（Natural Language Understanding, NLU） 任务中的表现。<strong>BERT 原版是通过将这两个目标一起训练的，最终损失是：</strong></p><p>$$
L_{\text{Total}} = L_{\text{MLM}} + L_{\text{NSP}}
$$</p></li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> BERT 的 <strong>Masked Language Modeling</strong> 本质上就是在做“完形填空”：预训练时，先将一部分词随机地盖住，经过模型的拟合，<strong>如果能够很好地预测那些盖住的词，模型就学到了文本的内在逻辑</strong>。这部分的主要作用是让模型 <strong>学到词汇和语法规则，提高语言理解能力</strong>。</p><p>除了“完形填空”，BERT还需要做 <strong>Next Sentence Prediction</strong> 任务：预测句子B是否为句子A的下一句。Next Sentence Prediction有点像英语考试中的“段落排序”题，只不过简化到只考虑两句话。<strong>如果模型无法正确地基于当前句子预测 Next Sentence，而是生硬地把两个不相关的句子拼到一起，两个句子在语义上是毫不相关的，说明模型没有读懂文本背后的意思。</strong> 这部分的主要作用是让模型 <strong>学习句子级别的语义关系</strong>。</p></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 为什么 BERT 的输入需要添加 [CLS] 和 [SEP] 特殊标记？</strong></summary><div class=markdown-inner><h2><b>为什么 BERT 的输入需要添加 [CLS] 和 [SEP] 特殊标记？</b></h2><p>BERT 的输入需要添加 <code>[CLS]</code> 和 <code>[SEP]</code> 特殊标记，主要是用来表示 Next Sentence Prediction（NSP） 任务的输入格式，并增强模型的表示能力。例如：</p><pre tabindex=0><code>[CLS] 句子A [SEP] 句子B [SEP]
</code></pre><ul><li><strong><code>[CLS]</code>（Classification Token）</strong>：BERT 在输入序列的开头始终添加 <code>[CLS]</code>，它的最终隐藏状态（Hidden State）可以作为整个序列的表示，特别 <strong>适用于分类任务</strong>（如情感分析、自然语言推理 NLI）。即使不是分类任务，BERT 仍然会计算 <code>[CLS]</code> 的表示，因此它始终是输入的一部分。</li><li><strong><code>[SEP]</code>（Separator Token）</strong>：BERT 采用双向 Transformer，因此需要区分单句和双句任务。在单句任务（如情感分析）中，输入序列结尾会加 <code>[SEP]</code>，而在双句任务（如问答 QA 或文本匹配），<code>[SEP]</code> 用于分隔两个句子，帮助 BERT 处理跨句子的关系建模。</li></ul><p>在 微调阶段（Fine-Tuning），不同任务对 <code>[CLS]</code> 和 <code>[SEP]</code> 的使用方式略有不同。例如：</p><ul><li><strong>文本分类（如情感分析）</strong>：<code>[CLS]</code> 的最终表示输入到 Softmax 层进行分类。</li><li><strong>问答（QA）</strong>：<code>[SEP]</code> 作为问题和段落的分隔符，BERT 需要预测答案的起始和结束位置。</li><li><strong>命名实体识别（NER）</strong>：<code>[CLS]</code> 不是必须的，而是依赖 Token 级别的输出。</li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>对比 BERT 的 <code>[CLS]</code> 向量和平均池化获取句子表示的优缺点?</strong></p><ul><li><strong><code>[CLS]</code> 向量的优缺点</strong>：<ul><li><strong>简洁性</strong>：只需要一个向量（即 <code>[CLS]</code> 向量）来表示整个句子的语义，非常适用于分类任务，尤其是在输入句子较短时。</li><li><strong>端到端优化</strong>：由于 BERT 在预训练时优化了 <code>[CLS]</code> 向量，使其能够有效地聚合句子的语义信息，且通常与下游任务紧密相关。</li><li><strong>可能信息丢失</strong>：<code>[CLS]</code> 向量是通过加权和整个输入的 token 嵌入得到的，可能导致一些细节信息丢失，特别是当句子较长或复杂时。</li></ul></li><li><strong>平均池化（Mean Pooling）的优缺点</strong>：<ul><li><strong>信息保留</strong>：平均池化将所有 token 的表示进行平均，从而保留了句子中各个部分的信息，相比于 <code>[CLS]</code> 向量，它能保留更多的语义信息。</li><li><strong>缺乏上下文关注</strong>：平均池化忽略了 token 之间的复杂依赖关系，简单的加权平均可能无法捕捉到句子中不同部分的关联性，尤其是在多义词或句子结构复杂时。</li><li><strong>计算开销</strong>：对于长句子，平均池化需要计算所有 token 的平均值，可能增加计算开销，尤其在大规模数据集上。</li></ul></li></ul></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 为什么BERT使用双向自注意力机制？这种设计如何影响下游任务？</strong></summary><div class=markdown-inner><h2><b>为什么BERT使用双向自注意力机制？这种设计如何影响下游任务？</b></h2><p>BERT之所以采用 <strong>双向自注意力机制（Bidirectional Self-Attention）</strong>，核心原因在于它 <strong>希望在预训练阶段同时捕捉上下文的完整信息（full context information）</strong>。在每一个Transformer层中，模型可以在同一时间考虑输入序列中 <strong>当前位置（current position）的左边和右边的所有词汇信息。这种设计通过 Masked Language Model（MLM）</strong> 的训练目标实现：随机遮挡输入中的一部分token，模型需要根据完整的上下文（包括被遮挡位置的左右两侧）去预测被遮挡的token，从而学到双向的信息整合能力。</p><p>这种机制对于下游任务的影响非常显著：</p><ol><li><strong>语义理解增强（Enhanced Semantic Understanding）</strong>：双向结构让模型能更准确捕捉句子中词与词之间的相互关系，尤其在句子语序复杂或有歧义时，能够有效消除偏见并提升上下文理解能力。例如，在情感分析（Sentiment Analysis）任务中，句尾的否定词“but”对整句话的情感倾向至关重要，BERT的双向机制能捕捉到这种句子尾部的反转信息。</li><li><strong>特征表达更加丰富（Richer Feature Representation）</strong>：相较于单向模型，双向自注意力产生的上下文特征向量（contextual embeddings）包含了全局信息，使得微调（Fine-tuning）在分类、序列标注等任务时，模型更容易收敛且表现更优。</li><li><strong>下游任务适配性更广（Better Downstream Adaptability）</strong>：许多自然语言处理任务，比如问答系统（Question Answering）、<strong>文本蕴含识别（Natural Language Inference, NLI）</strong> 等，需要模型理解整段文本的完整含义，而不仅仅是基于局部信息的预测。BERT的双向特性正好契合这类需求，能在不修改架构的前提下，通过不同的微调头（Task-specific Heads）适配各种任务。</li></ol></div></details><hr><details><summary><strong class=custom-details-title>⁉️ BERT 微调的细节？</strong></summary><div class=markdown-inner><h2><b>BERT 微调的细节？</b></h2><p>BERT（Bidirectional Encoder Representations from Transformers）微调（Fine-tuning）通常是在预训练（Pre-training）后的基础上，将整个 BERT 模型与特定任务的分类头（Task-specific Head）一起训练，使其适应 <strong>下游任务（Downstream Task）</strong>。</p><p>在微调过程中，输入文本经过分词（Tokenization）后，会被转换为对应的词嵌入（Token Embeddings）、位置嵌入（Position Embeddings）和分段嵌入（Segment Embeddings），然后输入 BERT 的 Transformer 层。模型通过多层 <strong>双向自注意力（Bidirectional Self-Attention）计算上下文信息</strong>，并在最终的 <code>[CLS]</code>（分类标记）或其他适当的标记上添加任务特定的层，如全连接层（Fully Connected Layer）或 CRF（Conditional Random Field），然后使用任务相关的损失函数（Loss Function）进行优化，如交叉熵损失（Cross-Entropy Loss）用于分类任务。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ RoBERTa 的技术细节？它相比 BERT 做了哪些改进（如动态掩码、移除 NSP 任务）？</strong></summary><div class=markdown-inner><h2><b>RoBERTa 的技术细节？它相比 BERT 做了哪些改进（如动态掩码、移除 NSP 任务）？</b></h2><p><strong>RoBERTa（Robustly Optimized BERT Pretraining Approach）</strong> 在 BERT（Bidirectional Encoder Representations from Transformers）的基础上进行了多项优化，以提高模型的性能和泛化能力。</p><p>首先，RoBERTa 采用了 <strong>动态掩码（Dynamic Masking）</strong> 机制，即在每个训练 epoch 重新随机生成 Masked Language Model（MLM）掩码，而 BERT 仅在数据预处理阶段静态确定掩码。这种动态策略增加了模型学习的多样性，提高了其对不同掩码模式的适应能力。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>训练时使用的动态掩码（Dynamic Masking）与静态掩码有何区别？</strong></p><p>BERT 原始论文使用的是 静态掩码，即在数据预处理阶段，对训练数据进行一次性 Mask 处理，并将其存储起来。在训练过程中，每次使用该数据时，<strong>Mask 位置都是固定的</strong>。假设文本是：</p><pre tabindex=0><code>“The quick brown fox jumps over the lazy dog.”
</code></pre><p><strong>在静态掩码中，预处理时就选好了</strong> <code>fox</code> 和 <code>lazy</code> 被掩码，模型每次都看到:</p><pre tabindex=0><code>“The quick brown [MASK] jumps over the [MASK] dog.”
</code></pre><p>RoBERTa 在 BERT 的基础上采用了 动态掩码，即在每次数据加载时，都会 <strong>随机重新选择 Mask 位置</strong>，确保同一输入文本在不同训练轮次中 Mask 位置不同。这提高了数据多样性，使得模型能够学习更丰富的上下文表示，而 <strong>不会过度拟合某些固定的 Mask 位置</strong>。每次训练时，模型可能看到不同的掩码版本，比如：</p><pre tabindex=0><code>• 第一次训练：The quick brown fox jumps over the [MASK] dog.
• 第二次训练：The quick [MASK] brown fox jumps over the lazy dog.
• 第三次训练：The quick brown [MASK] jumps over the lazy dog.
</code></pre></blockquote><p>其次，RoBERTa <strong>移除了 NSP（Next Sentence Prediction）任务</strong>，BERT 在训练时采用了 NSP 任务以增强模型对句子关系的理解，<strong>但研究发现 NSP 任务并未显著提升下游任务的表现，甚至可能影响模型的学习效率</strong>。因此，RoBERTa 采用了更大规模的 <strong>连续文本（Longer Sequences of Text） 进行预训练，而不再强制区分句子关系</strong>。最后，RoBERTa 通过 增加 batch size 和训练数据量，并 采用更长的训练时间，进一步优化了 BERT 预训练过程，使模型能够更充分地学习语言特征。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ DeBERTa 的“解耦注意力”机制（Disentangled Attention）如何分离内容和位置信息？</strong></summary><div class=markdown-inner><h2><b>DeBERTa 的“解耦注意力”机制（Disentangled Attention）如何分离内容和位置信息？</b></h2><p><strong>DeBERTa（Decoding-enhanced BERT with Disentangled Attention）</strong> 相较于 BERT 主要在 <strong>解耦注意力（Disentangled Attention）</strong> 和 <strong>相对位置编码（Relative Position Encoding）</strong> 方面进行了改进，以提升模型的语言理解能力。BERT 采用标准的 Transformer 注意力机制（Self-Attention），其中查询（Query）、键（Key）、值（Value）向量均是基于相同的嵌入（Embedding），这意味着 内容信息（Content Information） 和 位置信息（Positional Information） 混合在一起，限制了模型对句子结构的建模能力。而 DeBERTa 通过解耦注意力机制，<strong>对内容和位置信息分别编码，使得模型在计算注意力时能够更精确地理解不同词语之间的相对关系</strong>。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：Disentangled Attention 的设计，<strong>本质上是针对 BERT 系列所用的 absolute position embedding（绝对位置编码）问题提出的。</strong> <strong>传统 BERT</strong> 的输入结构是：
$$
h_i = x_i + p_i
$$</p><p>注意力的打分计算是：</p><p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$</p><p>DeBERTa 将词的内容（content） 和 <strong>位置（position）</strong> 分开建模，注意力打分的计算方式改为：</p><p>$$
\alpha_{i,j} = \frac{(q_i^c)^T k_j^c + (q_i^c)^T r_{i-j}^p + (q_i^p)^T k_j^c}{\sqrt{d}}
$$</p></blockquote><p>具体来说，DeBERTa 在计算注意力权重时，不是直接基于词向量（Token Embedding），而是 <strong>分别计算基于内容（Content-based Attention）和基于位置（Position-based Attention）的注意力得分，然后再将两者加权合并</strong>。这种方式使得 DeBERTa 能够在更长的依赖关系建模上表现更好。此外，DeBERTa 还采用了 <strong>增强的相对位置编码（Enhanced Relative Position Encoding）</strong>，相比 BERT 的绝对位置编码（Absolute Positional Encoding），能够更自然地处理长文本结构。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ ALBERT 如何通过参数共享降低模型参数量？</strong></summary><div class=markdown-inner><h2><b>ALBERT 如何通过参数共享降低模型参数量？</b></h2><p><strong>ALBERT（A Lite BERT）是对 BERT 模型的轻量化改进</strong>，旨在通过降低模型参数量而不显著影响模型性能。其主要技术细节涉及两项关键的优化策略：参数分解嵌入（Factorized Embedding） 和 跨层参数共享（Cross-Layer Parameter Sharing）。</p><ol><li><strong>参数分解嵌入（Factorized Embedding）</strong>：BERT 使用了一个巨大的词嵌入矩阵（embedding matrix），其大小通常是词汇表的大小与隐藏层维度（hidden size）的乘积。而 ALBERT 采用了参数分解方法，<strong>将词嵌入矩阵分解为两个低维矩阵</strong>，分别是一个较小的 词汇嵌入矩阵（Word Embedding Matrix） 和一个较小的 隐藏层嵌入矩阵（Hidden Layer Embedding Matrix）。具体来说，词嵌入矩阵被分解为两个矩阵，一个低维的嵌入矩阵和一个较小的输出矩阵，这样就显著减少了参数数量。例如，假设词嵌入矩阵的维度为 V x H（V 是词汇表大小，H 是隐藏层大小），通过分解成两个矩阵 V x E 和 E x H（E 为较小的维度），可以大幅度减少计算复杂度和存储需求。</li><li><strong>跨层参数共享（Cross-Layer Parameter Sharing）</strong>：在标准的 BERT 中，每一层 Transformer 都有一组独立的参数，而 ALBERT 通过跨层共享参数，减少了每一层的独立参数。具体来说，ALBERT 将模型中多个 Transformer 层 的参数进行共享，<strong>所有层都使用相同的权重</strong>。这样，尽管 ALBERT 保留了更多的层数（如 BERT 的 12 层改为 12 层的 ALBERT），但通过共享权重，整体的参数数量大幅度减少。参数共享的核心思想是：<strong>每一层 Transformer 的前向传播和反向传播使用相同的参数，从而减少了每层的权重数量，降低了内存消耗。</strong></li></ol><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>ALBERT提出的方法就是跨层参数共享</strong>，核心思想：<strong>所有Transformer层的权重矩阵都可以复用同一组参数</strong>，居图来说 Attention 模块参数</p><p>$$ W_Q, W_K, W_V, W_O$$ 在模块内部共享。</p><p>FeedForward模块的参数</p><p>$$W_1, W_2$$</p><p>在模块内部共享。因为作者认为原版的 BERT中 每层都独立训练，虽然灵活，但造成了巨大的参数冗余，很多层其实在做非常相似的变换，浪费内存和计算资源。</p></blockquote></div></details><hr><h2 id=encoder-decoder><strong>Encoder-Decoder</strong>
<a class=anchor href=#encoder-decoder>#</a></h2><details><summary><strong class=custom-details-title>⁉️ Encoder-Decoder架构的核心设计目标是什么？适用于哪些任务场景？</strong></summary><div class=markdown-inner><h2><b>Encoder-Decoder架构的核心设计目标是什么？适用于哪些任务场景？</b></h2><p>Encoder-Decoder架构的 <strong>设计初衷</strong> 是解决 <strong>输入输出序列长度不对称、语义空间跨域映射的问题</strong>，确保模型能够有效压缩输入特征并有条件地生成目标序列，广泛应用于机器翻译、文本摘要、图像描述生成、跨模态问答等场景，尤其适合输入与输出的语义空间或结构不同的任务。</p><p>Encoder-Decoder模型同时具备了 <strong>理解（Encoder）和生成（Decoder）</strong> 的能力，因此它能够处理复杂的任务，如机器翻译、文本摘要、图像描述等。这类模型既能够通过 Encoder 理解输入，又能通过 Decoder 生成输出。Encoder–Decoder模型的好处有：</p><ol><li><strong>生成能力</strong>：Encoder–Decoder 架构能够生成任意长度的输出序列，而不是像Encoder-only模型那样只能生成固定长度的表示。它允许通过解码器（Decoder）逐步生成目标序列，非常适合像机器翻译和文本摘要等生成任务。</li><li><strong>灵活的输入和输出</strong>：Encoder-only 模型和 Decoder-only 模型通常输入和输出的长度是固定的，而 Encoder–Decoder 模型能够灵活地处理不同长度的输入和输出。Decoder 可以根据输入序列生成任意长度的目标序列，从而适应更复杂的任务。</li><li><strong>跨任务的预训练能力</strong>：Encoder–Decoder模型可以通过多任务学习提升模型的泛化能力。比如，T5模型通过将不同任务（如文本分类、文本生成等）统一为一个多任务预训练框架，从而增强了模型对不同任务的处理能力。</li></ol></div></details><hr><details><summary><strong class=custom-details-title>⁉️ T5 如何统一不同 NLP 任务的格式？其预训练任务（如 Span Corruption）具体如何实现？</strong></summary><div class=markdown-inner><h2><b>T5 如何统一不同 NLP 任务的格式？其预训练任务（如 Span Corruption）具体如何实现？</b></h2><p>T5（Text-to-Text Transfer Transformer）通过将所有 NLP 任务（如文本分类、机器翻译、问答、摘要生成等）<strong>统一转换为文本到文本（Text-to-Text）的格式</strong>，从而实现了一个通用的 NLP 框架。具体而言，无论是输入句子的分类任务还是填空任务，T5 都会将输入转换为文本序列，并要求模型生成相应的文本输出。例如，情感分析任务的输入可以是 <code>"sentiment: I love this movie"</code>，输出则是 <code>"positive"</code>，而机器翻译任务的输入可能是 <code>"translate English to French: How are you?"</code>，输出为 <code>"Comment ça va?"</code>。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：Span Corruption（Span-Masked Language Modeling, SMLM）与 Masked Language Modeling（MLM）的核心区别在于 Mask 的方式和学习目标的不同。</p><ul><li><strong>MLM（Masked Language Modeling，BERT 采用）</strong>：<ul><li>MLM 主要是随机选择 <strong>单个 token 进行遮蔽</strong>，然后让模型预测被遮蔽的 token。例如：<pre tabindex=0><code>Input: &#34;I love [MASK] learning&#34;
Target: &#34;deep&#34; 
</code></pre></li><li>由于每次仅遮蔽少量 token，BERT 可能 <strong>无法学习到更长跨度的依赖关系</strong>，特别是对完整的子句或短语的理解较弱。</li></ul></li><li><strong>SMLM（Span-Masked Language Modeling，T5 采用）</strong>：<ul><li>SMLM 采用 Span Corruption，即 <strong>一次遮蔽连续的多个 token</strong>，并用特殊标记 <code>&lt;extra_id_0></code> 来表示被遮蔽部分。例如：<pre tabindex=0><code>Input: &#34;I &lt;extra_id_0&gt; deep &lt;extra_id_1&gt;.&#34;
Target: &#34;&lt;extra_id_0&gt; love &lt;extra_id_1&gt; learning&#34;
</code></pre></li><li>能够更好地 <strong>学习长距离的依赖关系</strong>，适用于生成式任务（如摘要、翻译）。训练难度更高。</li></ul></li></ul></blockquote><p>T5 采用的主要预训练任务是 <strong>Span Corruption（Span-Masked Language Modeling, SMLM）</strong>，这是一种变体的掩码语言建模（Masked Language Modeling, MLM）。具体来说，该任务会在输入文本中随机选择若干个 span（即连续的子序列），用特殊的 <code>&lt;extra_id_X></code> 令牌替换它们，并要求模型预测被遮蔽的内容。例如，原始文本 <code>"The quick brown fox jumps over the lazy dog"</code> 可能会被转换为 <code>"The &lt;extra_id_0> fox jumps over the &lt;extra_id_1> dog"</code>，而模型需要输出 <code>"quick brown"</code> <code>&lt;extra_id_0></code> 和 <code>“lazy”</code> <code>&lt;extra_id_1></code>。这种方式比 BERT 的单词级别掩码更灵活，有助于学习更丰富的上下文信息，从而提升生成任务的能力。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：在 Span Corruption 预训练（Span Corruption Pretraining）中，被遮蔽的文本片段（Masked Span）的长度通常遵循 Zipf 分布（Zipf’s Law），<strong>即较短的片段更常见，而较长的片段较少，以模拟自然语言中的信息分布</strong>。具体而言，像 T5 这样的模型使用 <strong>几何分布（Geometric Distribution）</strong> 来采样 span 长度，以确保既有短范围的遮蔽，也有跨多个 token 的长范围遮蔽。</p><p>不同的遮蔽长度会影响模型的学习能力：较短的 span（例如 1-3 个 token）有助于模型学习局部语义填充能力（Local Context Understanding），而较长的 span（如 8-10 个 token 甚至更长）可以增强模型的全局推理能力（Global Reasoning）和段落级理解能力（Document-Level Comprehension）。如果 span 过短，模型可能更倾向于基于表面模式（Surface Patterns）预测，而非真正理解上下文；如果 span 过长，则可能导致学习任务过于困难，使得模型难以有效收敛。</p></blockquote><table><thead><tr><th>任务类型</th><th>示例输入</th><th>示例输出</th></tr></thead><tbody><tr><td><strong>文本分类（Text Classification）</strong></td><td>sst2 sentence: This movie is fantastic!</td><td>positive</td></tr><tr><td><strong>文本生成（Text Generation）</strong></td><td>summarize: The article talks about &mldr;</td><td>The main idea is&mldr;</td></tr><tr><td><strong>机器翻译（Machine Translation）</strong></td><td>translate English to German: How are you?</td><td>Wie geht es dir?</td></tr><tr><td><strong>文本补全（Text Completion）</strong></td><td>fill_mask: I love to [MASK] pizza.</td><td>eat</td></tr><tr><td><strong>问答（Question Answering）</strong></td><td>question: Who wrote Hamlet? context: Shakespeare wrote&mldr;</td><td>Shakespeare</td></tr></tbody></table></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 如何将预训练的 Encoder-Decoder 模型（如 T5）适配到具体下游任务？</strong></summary><div class=markdown-inner><h2><b>如何将预训练的 Encoder-Decoder 模型（如 T5）适配到具体下游任务？</b></h2><p>T5（Text-to-Text Transfer Transformer）模型与BERT相似，也需要通过在特定任务的数据上进行微调（fine-tuning）来完成下游任务。与BERT的微调有所不同，T5的主要特点包括：</p><ol><li><strong>任务描述（Task Prefix）</strong>：T5 的输入不仅包含原始文本，还需要附加一个任务描述。例如，在文本摘要（Text Summarization）任务中，输入可以是 &ldquo;summarize: 原文内容&rdquo;，而在问答（Question Answering）任务中，输入可以是 &ldquo;question: 问题内容 context: 相关文本&rdquo;。这种设计使 T5 能够以统一的 文本到文本（Text-to-Text） 形式处理不同任务。</li><li><strong>端到端序列生成（Sequence-to-Sequence Generation）</strong>：与 BERT 仅能进行分类或填空任务不同，T5 依赖其 Transformer 解码器（Transformer Decoder） 来生成完整的输出序列，使其适用于文本生成任务，如自动摘要、数据到文本转换（Data-to-Text Generation）等。</li><li><strong>无需额外层（No Task-Specific Layers）</strong>：在 BERT 微调时，通常需要在其顶层添加特定的任务头（Task-Specific Head），如分类层（Classification Layer）或 CRF（Conditional Random Field）层，而 T5 直接将任务作为输入提示（Prompt），并使用相同的模型结构进行训练，无需修改额外的网络层。</li></ol></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 什么是BART？BART 的预训练任务与 T5 有何异同？</strong></summary><div class=markdown-inner><h2><b>什么是BART？BART 的预训练任务（如 Text Infilling、Sentence Permutation）与 T5 的 Span Corruption 有何异同？</b></h2><p><strong>BART（Bidirectional and Auto-Regressive Transformers）</strong> 是一种结合了 BERT（Bidirectional Encoder Representations from Transformers）和 GPT（Generative Pretrained Transformer）优点的生成模型。BART 在预训练过程中采用了 <strong>自编码器（Autoencoder）</strong> 结构，其编码器部分像 BERT 一样使用双向编码，能够捕捉上下文信息，而解码器则是像 GPT 一样用于生成任务，通过自回归方式生成文本。</p><table><thead><tr><th>任务类型</th><th>具体示例</th><th>输入 ➡️ 输出特征说明</th></tr></thead><tbody><tr><td><strong>机器翻译（Machine Translation）</strong></td><td>英语 ➡️ 法语</td><td>输入输出语言不同，长度不固定，语义需对应</td></tr><tr><td><strong>文本摘要（Text Summarization）</strong></td><td>新闻文章 ➡️ 简短摘要</td><td>输入长文本，输出短摘要，结构压缩</td></tr><tr><td><strong>图像描述生成（Image Captioning）</strong></td><td>图片特征向量 ➡️ 自然语言描述</td><td>输入非语言特征，输出自然语言，跨模态转换</td></tr><tr><td><strong>文本生成（Text Generation）</strong></td><td>Prompt ➡️ 自动补全文本</td><td>输入提示短语，输出完整文本，顺序自回归建模</td></tr><tr><td><strong>语音转文本（Speech-to-Text）</strong></td><td>语音信号（波形或特征）➡️ 文字</td><td>输入连续音频流，输出离散文本序列，输入输出格式完全不同</td></tr><tr><td><strong>多模态问答（VQA / Multimodal QA）</strong></td><td>图片 + 问题文本 ➡️ 答案文本</td><td>输入多模态，输出单模态文本，结构不对称</td></tr></tbody></table><p>BART 的预训练任务包括 <strong>Text Infilling</strong> 和 <strong>Sentence Permutation</strong>：</p><ol><li><strong>Text Infilling</strong>：在这一任务中，模型需要从一段被掩盖部分的文本中恢复出被移除的词或词组。具体来说，给定一个输入文本，部分单词或短语被替换为掩码（mask），然后模型的任务是预测这些被掩盖的内容。这一任务类似于 BERT 的 Masked Language Model（MLM），但区别在于，BART 不仅仅是根据上下文来填充单一词汇，它的目标是生成整个缺失的文本片段。<pre tabindex=0><code>原始文本：“The quick brown fox jumps over the lazy dog in the park.”

掩盖后的文本：“The quick [MASK] jumps over the lazy dog in the park.”
</code></pre></li></ol><blockquote class="book-hint warning"><p><strong>Note</strong>：在 MLM 中，模型的目标是预测被随机掩盖的单个词（或子词）。虽然 Text Infilling 看起来类似于 MLM，但 BART 的 Text Infilling 中，<strong>掩盖的部分不仅限于单个词，而是可能是一个较大的片段或短语</strong>。通常，模型会随机选择一个较长的文本片段（如一个短语或句子的一部分）并用一个掩码标记替换，然后模型需要预测整个被掩盖的文本片段。</p><p><strong>总结，MLM遮一个词，Text Infilling 遮一个片段，Span Corruption 遮多个片段。</strong></p></blockquote><ol start=2><li><strong>Sentence Permutation</strong>：在这一任务中，输入文本的句子顺序被打乱，模型的任务是根据上下文恢复正确的句子顺序。这个任务是为了帮助模型学习长文本的结构和上下文关系，使其能够生成连贯且符合语法规则的文本。它与 T5 中的 Span Corruption 有一定的相似性，因为两者都涉及对文本进行扰动，并要求模型根据扰动后的文本恢复原始文本。<pre tabindex=0><code>原始文本：
“The dog chased the ball. It was a sunny day.”

打乱顺序后的文本：
“It was a sunny day. The dog chased the ball.”
</code></pre></li></ol></div></details><hr><h2 id=decoder-only><strong>Decoder-Only</strong>
<a class=anchor href=#decoder-only>#</a></h2><details><summary><strong class=custom-details-title>⁉️ Decoder-only 模型与 Encoder-Decoder 模型的核心区别是什么？</strong></summary><div class=markdown-inner><h2><b>Decoder-only 模型与 Encoder-Decoder 模型的核心区别是什么？</b></h2><p>Decoder-only（如GPT）仅保留解码器，通过自回归生成（逐词预测）完成任务。<strong>移除原始Transformer的编码器和交叉注意力层</strong>，保留掩码自注意力（Masked Self-Attention）与前馈网络（FFN）。输入处理时，文本序列添加特殊标记 <code>&lt;bos></code>（序列开始）和 <code>&lt;eos></code>（序列结束），目标序列为输入右移一位。适用任务主要为生成类任务（文本续写、对话、代码生成）。Encoder-Decoder（如原始Transformer、T5）分离编码器（理解输入）与解码器（生成输出），通过交叉注意力传递信息。适用任务主要为需严格分离输入理解与输出生成的任务（翻译、摘要、问答）。</p><div align=center><img src=/images/f6133c18-bfaf-4578-8c5a-e5ac7809f65b_1632x784.png width=600px/></div><p>Decoder-only模型通过 <strong>上下文学习（In-Context Learning）</strong> 将任务隐式编码到输入中（如添加“Translate English to French:”前缀），利用生成能力模拟翻译，完成和 Encoder-Decoder 一样的工作。但是 Encoder-Decoder在以下场景更具优势：</p><ol><li><strong>输入与输出解耦的复杂任务（如翻译）</strong>：Encoder-Decoder 模型的编码器可先提取完整语义，解码器再逐词生成，避免生成过程中的语义偏差，准确性更高。而Decoder-only模型（如GPT-3）需通过Prompt（如“Translate English to French: &mldr;”）隐式对齐输入输出，<strong>易受提示词设计影响</strong>。</li><li><strong>长文本处理效率</strong>：Encoder-Decoder：编码器一次性压缩输入为固定长度表示，解码生成时无需重复处理长输入（节省计算资源）。Decoder-only：<strong>生成每个词时需重新处理整个输入序列</strong>（如输入1000词的文档），导致计算复杂度高。</li></ol><ul><li><p><strong>总结来说</strong></p><ul><li><strong>Decoder-only</strong>：适合开放域生成任务，依赖生成连贯性与上下文学习，但对输入-输出结构复杂的任务效率较低。</li><li><strong>Encoder-Decoder</strong>：在需严格分离理解与生成、处理长输入、多任务统一的场景中更优，尤其适合翻译、摘要等结构化任务。</li><li>类比：Decoder-only像“自由创作的作家”，Encoder-Decoder像“严谨的翻译官”——前者更灵活，后者更精准。</li></ul></div></li></ul></details><hr><details><summary><strong class=custom-details-title>⁉️ Decoder-only 模型的预训练任务通常是什么？</strong></summary><div class=markdown-inner><h2><b>Decoder-only 模型的预训练任务通常是什么？</b></h2><ol><li><strong>自回归语言建模（Autoregressive Language Modeling）</strong>：这个任务的目标是通过给定一部分文本（如前面的词或字符），预测接下来的单词或字符。例如，给定输入 <code>“The cat sat on the”</code>, 模型的任务是预测下一个单词是 <code>“mat”</code>。这个过程是自回归的，因为每次生成新的词都会基于模型已经生成的文本。自回归语言建模任务常见于 GPT（Generative Pre-trained Transformer） 等模型。</li><li><strong>文本填充任务（Cloze Task）</strong>：在这个任务中，模型的目标是根据上下文填充文本中的空白部分。例如，给定句子 <code>“The cat sat on the ____”</code>, 模型需要预测空白处应该填入的词 <code>“mat”</code>。这种填空任务常见于 BERT（Bidirectional Encoder Representations from Transformers） 的变体，如 Masked Language Modeling (MLM)。尽管 BERT 是基于 编码器（Encoder） 架构，但类似的目标也可以应用于 Decoder-only 架构，通过在训练时将部分词语随机遮蔽（mask）并让模型预测被遮蔽的部分。</li></ol></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 为什么 Decoder-only 模型通常采用自回归生成方式？因果掩码的作用及其实现方法。</strong></summary><div class=markdown-inner><h2><b>为什么 Decoder-only 模型通常采用自回归生成方式？因果掩码的作用及其实现方法。</b></h2><p>Decoder-only 模型通常采用自回归（Autoregressive）生成方式，因为这种方式能够通过模型已经生成的输出逐步生成下一个 token，从而形成连贯的序列。自回归生成方式使得每个步骤的生成依赖于前一步的生成结果，这种特性非常适合文本生成任务，如 <strong>语言建模（Language Modeling）</strong> 和 <strong>对话生成（Dialogue Generation）</strong>。通过这种方式，模型能够以逐词的方式生成文本，在每个步骤中利用之前的上下文信息预测下一个 token。即最大化序列联合概率，损失函数为交叉熵（Cross-Entropy）：</p><span>\[
\begin{equation}
\mathcal{L} = -\sum_{t=1}^{T} \log P(w_t | w_{1:t-1 })
\end{equation}
\]</span><p>在 Decoder-only 模型中，<strong>因果掩码（Causal Mask）</strong> 的作用是确保模型在生成时 <strong>仅依赖于已生成的部分</strong>，而不会看到未来的信息。具体来说，在训练时，因果掩码会屏蔽未来 token 的信息，使得模型只能访问当前位置及其之前的 token，这样保证了每个时间步的预测仅受历史信息的影响，而无法窥视未来的输出。实现方法通常是在注意力机制（Attention Mechanism）中，通过对自注意力矩阵应用一个上三角矩阵的掩码，将未来的 token 阻止在计算中。例如，如果在生成第 4 个 token 时，模型不允许访问第 5、6 个 token，<strong>掩码就会在这些位置设置为负无穷</strong>，从而避免信息泄漏。</p></div></details><ul><li>预训练阶段为什么常用数据采样策略（例如 temperature sampling 或 weighted sampling）？</li></ul><hr><h2 id=训练优化><strong>训练优化</strong>
<a class=anchor href=#%e8%ae%ad%e7%bb%83%e4%bc%98%e5%8c%96>#</a></h2><ul><li>混合精度训练（FP16、BF16）</li><li>混合精度训练（Mixed Precision）和梯度累积（Gradient Accumulation）的原理是什么？</li><li>什么是 FlashAttention？为什么它比传统 Attention 更高效？</li><li>分布式训练</li><li>训练框架（PyTorch DDP、DeepSpeed）</li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#预训练基础知识><strong>预训练基础知识</strong></a></li><li><a href=#预训练细节><strong>预训练细节</strong></a></li><li><a href=#encoder-only><strong>Encoder-Only</strong></a></li><li><a href=#encoder-decoder><strong>Encoder-Decoder</strong></a></li><li><a href=#decoder-only><strong>Decoder-Only</strong></a></li><li><a href=#训练优化><strong>训练优化</strong></a></li></ul></nav></div></aside></main></body></html>