<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  大语言模型优化（Large Language Models Optimization）
  #


  训练优化（Training Optimization）
  #

⁉️ 为什么 Decoder-only 模型推理时需要 KV Cache？如何优化其内存占用？
  
为什么 Decoder-only 模型推理时需要 KV Cache？如何优化其内存占用？

  Note：在推理阶段，所有的 权重（Weights）已经通过训练学习完毕（不再改变）。输入的句子会按照预定义的规则转换成 Query (Q)、Key (K) 和 Value (V) 向量。无论是训练阶段还是推理阶段，句子中的 每个 token（词）都会有一个唯一对应的 Key 和 Value 向量。在自回归生成过程中，每次输入新的时间步  x_t ，会计算出当前时刻的 Query 向量  Q_t ，同时，新的 Key 和 Value 向量  K_t, V_t  会与之前的 KV 缓存合并，形成当前时刻的完整历史信息。新的 Query 用于查询当前输入和历史信息之间的关系，从而生成有意义的上下文信息，用于生成下一个 token。

在 Decoder-only 模型（如 GPT）中，推理时需要 KV Cache（Key-Value Cache）来提高计算效率和减少内存占用。KV Cache 主要用于存储模型在每个时间步生成的 Key 和 Value 向量，这些向量用于自注意力机制（Self-Attention）中计算当前词与之前所有词之间的依赖关系。具体来说，当模型生成一个新的 token 时，它不仅要计算当前 token 的自注意力，还需要利用已经生成的 tokens 的表示来计算新的 token，因此必须保存这些 Key 和 Value 向量。"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/deep-learning/attention-and-transformers/large-language-models-optimization/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Large Language Models Optimization"><meta property="og:description" content="大语言模型优化（Large Language Models Optimization） # 训练优化（Training Optimization） # ⁉️ 为什么 Decoder-only 模型推理时需要 KV Cache？如何优化其内存占用？ 为什么 Decoder-only 模型推理时需要 KV Cache？如何优化其内存占用？ Note：在推理阶段，所有的 权重（Weights）已经通过训练学习完毕（不再改变）。输入的句子会按照预定义的规则转换成 Query (Q)、Key (K) 和 Value (V) 向量。无论是训练阶段还是推理阶段，句子中的 每个 token（词）都会有一个唯一对应的 Key 和 Value 向量。在自回归生成过程中，每次输入新的时间步 x_t ，会计算出当前时刻的 Query 向量 Q_t ，同时，新的 Key 和 Value 向量 K_t, V_t 会与之前的 KV 缓存合并，形成当前时刻的完整历史信息。新的 Query 用于查询当前输入和历史信息之间的关系，从而生成有意义的上下文信息，用于生成下一个 token。
在 Decoder-only 模型（如 GPT）中，推理时需要 KV Cache（Key-Value Cache）来提高计算效率和减少内存占用。KV Cache 主要用于存储模型在每个时间步生成的 Key 和 Value 向量，这些向量用于自注意力机制（Self-Attention）中计算当前词与之前所有词之间的依赖关系。具体来说，当模型生成一个新的 token 时，它不仅要计算当前 token 的自注意力，还需要利用已经生成的 tokens 的表示来计算新的 token，因此必须保存这些 Key 和 Value 向量。"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Large Language Models Optimization | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/deep-learning/attention-and-transformers/large-language-models-optimization/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.947574131340145a88d41519def1be16086421292e611565c0d45c05dd78e4db.js integrity="sha256-lHV0ExNAFFqI1BUZ3vG+FghkISkuYRVlwNRcBd145Ns=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/deep-learning/attention-and-transformers/large-language-models-optimization/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li><li><a href=/docs/machine-learning/computational-performance/>Computational Performance</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><input type=checkbox id=section-d0dd931d60033c220ecd4cd60b7c9170 class=toggle>
<label for=section-d0dd931d60033c220ecd4cd60b7c9170 class="flex justify-between"><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a></label><ul><li><a href=/docs/deep-learning/convolutional-neural-networks/modern-convolutional-neural-networks/>Modern Convolutional Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-a3019bfa8037cc33ed6405d1589b6219 class=toggle>
<label for=section-a3019bfa8037cc33ed6405d1589b6219 class="flex justify-between"><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a></label><ul><li><a href=/docs/deep-learning/recurrent-neural-networks/modern-recurrent-neural-networks/>Modern Recurrent Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-0a43584c16258b228ae9aa8d70efc320 class=toggle checked>
<label for=section-0a43584c16258b228ae9aa8d70efc320 class="flex justify-between"><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a></label><ul><li><a href=/docs/deep-learning/attention-and-transformers/tokenization-and-word-embeddings/>Tokenization and Word Embeddings</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/transformer-architecture/>Transformer Architecture</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/modern-large-language-models/>Modern LLMs and Pre-Training</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/post-training-large-language-models/>Post-training LLMs</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/large-language-models-optimization/ class=active>Large Language Models Optimization</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/multimodal-large-language-models/>Multimodal Large Language Models</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92e8358c45c96009753cf4227e9daea8 class=toggle>
<label for=section-92e8358c45c96009753cf4227e9daea8 class="flex justify-between"><a href=/docs/deep-learning/llm-pipelines/>LLM Pipelines</a></label><ul><li><a href=/docs/deep-learning/llm-pipelines/llm-hardware-and-model-size/>LLM Hardware and Model Size</a><ul></ul></li><li><a href=/docs/deep-learning/llm-pipelines/large-scale-pretraining-with-transformers/>Large-Scale Pretraining with Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/llm-pipelines/llm-inference-and-deployment/>LLM Inference and Deployment</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul><li><a href=/docs/others/interview-preparation-guide/>Interview Preparation Guide</a><ul></ul></li></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Large Language Models Optimization</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#训练优化training-optimization><strong>训练优化（Training Optimization）</strong></a></li><li><a href=#推理优化inference-optimization><strong>推理优化（Inference Optimization）</strong></a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=大语言模型优化large-language-models-optimization><strong>大语言模型优化（Large Language Models Optimization）</strong>
<a class=anchor href=#%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e4%bc%98%e5%8c%96large-language-models-optimization>#</a></h1><h2 id=训练优化training-optimization><strong>训练优化（Training Optimization）</strong>
<a class=anchor href=#%e8%ae%ad%e7%bb%83%e4%bc%98%e5%8c%96training-optimization>#</a></h2><details><summary><strong class=custom-details-title>⁉️ 为什么 Decoder-only 模型推理时需要 KV Cache？如何优化其内存占用？</strong></summary><div class=markdown-inner><h2><b>为什么 Decoder-only 模型推理时需要 KV Cache？如何优化其内存占用？</b></h2><blockquote class="book-hint warning"><p><strong>Note</strong>：在推理阶段，所有的 <strong>权重（Weights）已经通过训练学习完毕（不再改变）</strong>。输入的句子会按照预定义的规则转换成 Query (Q)、Key (K) 和 Value (V) 向量。无论是训练阶段还是推理阶段，句子中的 <strong>每个 token（词）都会有一个唯一对应的 Key 和 Value 向量</strong>。在自回归生成过程中，每次输入新的时间步 x_t ，<strong>会计算出当前时刻的 Query 向量 Q_t ，同时，新的 Key 和 Value 向量 K_t, V_t 会与之前的 KV 缓存合并</strong>，形成当前时刻的完整历史信息。新的 Query 用于查询当前输入和历史信息之间的关系，从而生成有意义的上下文信息，用于生成下一个 token。</p></blockquote><p>在 Decoder-only 模型（如 GPT）中，推理时需要 KV Cache（Key-Value Cache）来提高计算效率和减少内存占用。KV Cache 主要用于存储模型在每个时间步生成的 Key 和 Value 向量，这些向量用于自注意力机制（Self-Attention）中计算当前词与之前所有词之间的依赖关系。具体来说，<strong>当模型生成一个新的 token 时，它不仅要计算当前 token 的自注意力，还需要利用已经生成的 tokens 的表示来计算新的 token</strong>，因此必须保存这些 Key 和 Value 向量。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：假设我们正在进行推理，模型已经生成了序列 “I love deep learning” 中的前四个词 “I love deep learning”（也就是说，当前时间步是第5个词）。在传统推理中，如果没有 KV Cache，模型在计算每个新的 token 时都需要重新计算与之前所有已经生成的 tokens（“I love deep learning”）之间的依赖关系。</p><ul><li>对于第5个词 “model”，模型首先计算 “model” 和 “I”、“love”、“deep”、“learning” 之间的自注意力（self-attention）。为了计算这个自注意力，模型需要 <strong>重新计算每个之前词的 Key 和 Value 向量</strong>。</li></ul><p>为了避免这种重复计算，使用 KV Cache 的方法是：当我们生成第5个词时，模型会保存 <strong>前四个词（“I love deep learning”）的 Key 和 Value 向量</strong>。下一次生成新 token 时（比如第6个词），模型只需要利用 缓存中的 Key 和 Value 向量 来计算当前 token 和已经生成的历史 tokens 之间的依赖关系，而无需重新计算历史 tokens 的表示。</p><ul><li>对于第5个词 “model”，模型首先计算 “model” 和缓存中的 “I love deep learning” 之间的自注意力。</li><li>在此过程中，模型使用的是 <strong>已经缓存的 Key 和 Value 向量，而不是重新计算整个输入序列的 Key 和 Value 向量</strong>。</li><li>当模型生成第6个词时，只需将第5个词 “model” 的 Key 和 Value 向量加入缓存，并计算与缓存中所有其他 tokens 之间的关系。</li></ul></blockquote><p>在传统的推理过程中，模型需要重新计算每个时间步的所有 Key 和 Value 向量，导致计算量和内存占用急剧增加。使用 KV Cache 后，模型只需要保存每一层的 Key 和 Value 向量，从而避免了重复计算，极大地提升了推理效率。</p><p><strong>如何优化内存占用：</strong></p><ol><li><strong>动态 KV 缓存大小</strong>：在一些任务中，并不需要保留所有时间步的 Key 和 Value 向量。例如，对于生成式任务，缓存可以按照一定步长进行清理，或者只保留 前 n 步 的缓存。</li><li><strong>分层缓存</strong>：根据模型层数和层间依赖，可以在 不同层 采用不同的缓存策略。例如，可以对较低层进行更频繁的缓存清理，对较高层保留更多的缓存信息。</li><li><strong>量化（Quantization）</strong>：通过降低 Key 和 Value 向量的精度（例如从浮点数精度到低精度存储），减少内存占用，同时尽量保持推理的精度。</li></ol></div></details><hr><details><summary><strong class=custom-details-title>⁉️ Decoder-only 模型如何处理长文本依赖问题？（如稀疏注意力、窗口注意力）</strong></summary><div class=markdown-inner><h2><b>Decoder-only 模型如何处理长文本依赖问题？（如稀疏注意力、窗口注意力）</b></h2><p>Decoder-only 模型（如 GPT 类模型）通过不同的技术来处理长文本中的依赖问题，尤其是在处理长序列时，传统的 全局注意力（Global Attention） 计算会变得非常消耗资源。为了解决这个问题，Decoder-only 模型采用了 稀疏注意力（Sparse Attention） 和 窗口注意力（Windowed Attention） 等方法，从而有效地减小计算复杂度并增强长文本的建模能力。</p><ul><li><strong>窗口注意力（Windowed Attention）</strong> 是一种将输入序列划分为多个固定大小的窗口（或块），每个窗口内的 token 之间通过注意力进行交互，而窗口之间没有直接的依赖关系。窗口大小是一个超参数，通常会选择较小的窗口以限制每次计算的注意力范围，从而减少计算负担。通过这种方式，模型能够在较低的计算成本下捕捉到长序列中的重要信息，同时避免了全局注意力带来的高昂计算开销。</li></ul><blockquote class="book-hint warning"><p><strong>e.g.</strong>：假设我们有一个长度为 6 的序列 <code>[A, B, C, D, E, F]</code>，并且我们选择一个大小为 3 的窗口进行计算。</p><ol><li>在窗口注意力中，我们将输入序列分为若干个滑动窗口。例如，窗口大小为 3 的情况下：<ul><li>第一个窗口：<code>[A, B, C]</code></li><li>第二个窗口：<code>[B, C, D]</code>
每个窗口内部的 token 之间会进行注意力计算，但是不同窗口之间的 token 之间是没有交互的。</li></ul></li><li>对于序列 <code>[A, B, C, D, E, F]</code>，采用窗口大小为 3 的策略，注意力计算矩阵将是：</li></ol><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[
\begin{array}{|c|c|c|c|c|c|c|}
\hline
& A & B & C & D & E & F \\
\hline
A & 1 & 1 & 1 & 0 & 0 & 0 \\
B & 1 & 1 & 1 & 1 & 0 & 0 \\
C & 1 & 1 & 1 & 1 & 1 & 0 \\
D & 0 & 1 & 1 & 1 & 1 & 1 \\
E & 0 & 0 & 1 & 1 & 1 & 1 \\
F & 0 & 0 & 0 & 1 & 1 & 1 \\
\hline
\end{array}
\]</span></blockquote><ul><li><strong>稀疏注意力（Sparse Attention）</strong> 的 <strong>核心思想是通过引入局部化注意力机制，使得每个 token 只与部分上下文进行交互，从而减少计算量</strong>。具体而言，稀疏注意力只计算一部分的注意力权重而不是全部，这样可以降低模型计算的复杂度。常见的稀疏注意力结构包括 固定模式（Fixed Patterns） 和 学习模式（Learned Patterns），其中一个代表固定的局部上下文窗口，另一个则依赖于模型在训练过程中自适应学习关注哪些位置的关系。稀疏注意力通常通过 Top-k 注意力（Top-k Attention） 或 Block-sparse 格式 来实现。</li></ul><blockquote class="book-hint warning"><p><strong>Note:</strong> 稀疏注意力和窗口注意力是非常相似的概念，都属于通过减少计算量来优化自注意力机制的方法。<strong>但稀疏注意力不仅仅局限于相邻的 token，还可以是基于某些策略（如全局选择、局部窗口、随机选择等）来选择哪些 token 进行注意力计算</strong>。它可以通过灵活的方式选择注意力的稀疏性，可以是全局性策略（如某些重要的 token）或局部性策略（如基于输入特征选择 token）。</p></blockquote></div></details><hr><ul><li>混合精度训练（FP16、BF16）</li><li>混合精度训练（Mixed Precision）和梯度累积（Gradient Accumulation）的原理是什么？</li><li>什么是 FlashAttention？为什么它比传统 Attention 更高效？</li><li>分布式训练</li><li>训练框架（PyTorch DDP、DeepSpeed）</li></ul><h2 id=推理优化inference-optimization><strong>推理优化（Inference Optimization）</strong>
<a class=anchor href=#%e6%8e%a8%e7%90%86%e4%bc%98%e5%8c%96inference-optimization>#</a></h2></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#训练优化training-optimization><strong>训练优化（Training Optimization）</strong></a></li><li><a href=#推理优化inference-optimization><strong>推理优化（Inference Optimization）</strong></a></li></ul></nav></div></aside></main></body></html>