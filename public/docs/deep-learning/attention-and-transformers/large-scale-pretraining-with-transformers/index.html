<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Pre-training 是在大规模无监督数据上训练模型，而 Fine-tuning 是在特定任务或数据集上对模型进行微调。

  基于Transformer的大规模预训练（Large-Scale Pretraining with Transformers）
  #

在传统任务（如图像分类、机器翻译）中，模型通常通过特定任务数据集从头训练（trained from scratch），成为专精单一任务的“专家”。例如，用英法双语对训练的Transformer模型仅能完成英译法任务，且对数据分布的微小变化敏感（易受分布偏移影响）。为提高模型泛化能力（generalization）并实现多任务处理（multitasking），大规模预训练（large-scale pretraining）逐渐成为主流。
Transformer的 可扩展性（scalability） 是其核心优势：随着模型参数（parameters）、训练数据量（training tokens）和计算资源（compute）的增加，性能按幂律关系显著提升。这一特性在视觉领域同样成立——更大规模的视觉Transformer（Vision Transformer, ViT）在更多数据训练下表现更优。
根据任务需求，Transformer可配置为三种模式：

仅编码器（Encoder-only）：适用于文本分类、命名实体识别等任务（如BERT）；
编码器-解码器（Encoder-Decoder）：用于序列到序列任务（如机器翻译，原始Transformer设计）；
仅解码器（Decoder-only）：专注于生成任务（如GPT系列），通过自回归（Autoregressive）方式逐个生成token。

预训练阶段常采用自监督学习（Self-Supervised Learning，如掩码语言建模MLM或下一词预测），通过海量数据学习通用表征，再通过微调（Fine-tuning）适配下游任务。


  Encoder-Only（BERT）
  #

仅编码器架构的Transformer（如BERT、Vision Transformer）仅保留编码器层，通过多层 自注意力（Self-Attention）和前馈网络（FFN） 提取输入序列的全局特征。所有输入token（如文本词或图像块）通过自注意力相互关联，最终输出与输入等长的表示向量。

典型应用场景：文本分类（如情感分析）、命名实体识别（NER）、图像分类（ViT）等。
输出处理：通常取序列开头的特殊标记 <cls> 的表示向量作为全局特征，再投影到任务标签（如分类层）。


Note： Encoder-Only 模型的核心特点就是 专注于理解（NLU, Natural Language Understanding），而且它的输出完全基于输入，不会额外生成新的内容。他们具有双向自注意力（Bidirectional Attention），可以同时建模左右上下文信息。因为不包含 Decoder，所以 不能生成文本。Encoder-Only 模型适用于以下任务：

✅ 文本分类（Text Classification）：垃圾邮件检测、情感分析、新闻分类
✅ 文本匹配（Text Matching）：文本相似度计算，如搜索引擎中的相关性排序
✅ 问答系统（QA）：如 SQuAD 任务，提取答案
✅ 信息检索（IR）：如 Google 搜索的 Query-Document 相关性计算




  BERT 的预训练与微调机制
  #

BERT 是 Google 在 2018 年提出的 NLP 预训练模型，全称 Bidirectional Encoder Representations from Transformers，它基于 Transformer 的 Encoder 结构，可以 双向建模上下文信息，用于多种 NLP 任务。BERT 只包含 Transformer 的 Encoder 部分，即："><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/deep-learning/attention-and-transformers/large-scale-pretraining-with-transformers/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Large-Scale Pretraining with Transformers"><meta property="og:description" content="Pre-training 是在大规模无监督数据上训练模型，而 Fine-tuning 是在特定任务或数据集上对模型进行微调。
基于Transformer的大规模预训练（Large-Scale Pretraining with Transformers） # 在传统任务（如图像分类、机器翻译）中，模型通常通过特定任务数据集从头训练（trained from scratch），成为专精单一任务的“专家”。例如，用英法双语对训练的Transformer模型仅能完成英译法任务，且对数据分布的微小变化敏感（易受分布偏移影响）。为提高模型泛化能力（generalization）并实现多任务处理（multitasking），大规模预训练（large-scale pretraining）逐渐成为主流。
Transformer的 可扩展性（scalability） 是其核心优势：随着模型参数（parameters）、训练数据量（training tokens）和计算资源（compute）的增加，性能按幂律关系显著提升。这一特性在视觉领域同样成立——更大规模的视觉Transformer（Vision Transformer, ViT）在更多数据训练下表现更优。
根据任务需求，Transformer可配置为三种模式：
仅编码器（Encoder-only）：适用于文本分类、命名实体识别等任务（如BERT）； 编码器-解码器（Encoder-Decoder）：用于序列到序列任务（如机器翻译，原始Transformer设计）； 仅解码器（Decoder-only）：专注于生成任务（如GPT系列），通过自回归（Autoregressive）方式逐个生成token。 预训练阶段常采用自监督学习（Self-Supervised Learning，如掩码语言建模MLM或下一词预测），通过海量数据学习通用表征，再通过微调（Fine-tuning）适配下游任务。
Encoder-Only（BERT） # 仅编码器架构的Transformer（如BERT、Vision Transformer）仅保留编码器层，通过多层 自注意力（Self-Attention）和前馈网络（FFN） 提取输入序列的全局特征。所有输入token（如文本词或图像块）通过自注意力相互关联，最终输出与输入等长的表示向量。
典型应用场景：文本分类（如情感分析）、命名实体识别（NER）、图像分类（ViT）等。 输出处理：通常取序列开头的特殊标记 <cls> 的表示向量作为全局特征，再投影到任务标签（如分类层）。 Note： Encoder-Only 模型的核心特点就是 专注于理解（NLU, Natural Language Understanding），而且它的输出完全基于输入，不会额外生成新的内容。他们具有双向自注意力（Bidirectional Attention），可以同时建模左右上下文信息。因为不包含 Decoder，所以 不能生成文本。Encoder-Only 模型适用于以下任务：
✅ 文本分类（Text Classification）：垃圾邮件检测、情感分析、新闻分类 ✅ 文本匹配（Text Matching）：文本相似度计算，如搜索引擎中的相关性排序 ✅ 问答系统（QA）：如 SQuAD 任务，提取答案 ✅ 信息检索（IR）：如 Google 搜索的 Query-Document 相关性计算 BERT 的预训练与微调机制 # BERT 是 Google 在 2018 年提出的 NLP 预训练模型，全称 Bidirectional Encoder Representations from Transformers，它基于 Transformer 的 Encoder 结构，可以 双向建模上下文信息，用于多种 NLP 任务。BERT 只包含 Transformer 的 Encoder 部分，即："><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Large-Scale Pretraining with Transformers | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/deep-learning/attention-and-transformers/large-scale-pretraining-with-transformers/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.19c687bfdb47c973d92046444581b3486f0ed1b311302a8bb55450dd59e49806.js integrity="sha256-GcaHv9tHyXPZIEZERYGzSG8O0bMRMCqLtVRQ3VnkmAY=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/deep-learning/attention-and-transformers/large-scale-pretraining-with-transformers/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><input type=checkbox id=section-d0dd931d60033c220ecd4cd60b7c9170 class=toggle>
<label for=section-d0dd931d60033c220ecd4cd60b7c9170 class="flex justify-between"><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a></label><ul><li><a href=/docs/deep-learning/convolutional-neural-networks/modern-convolutional-neural-networks/>Modern Convolutional Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-a3019bfa8037cc33ed6405d1589b6219 class=toggle>
<label for=section-a3019bfa8037cc33ed6405d1589b6219 class="flex justify-between"><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a></label><ul><li><a href=/docs/deep-learning/recurrent-neural-networks/modern-recurrent-neural-networks/>Modern Recurrent Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-0a43584c16258b228ae9aa8d70efc320 class=toggle checked>
<label for=section-0a43584c16258b228ae9aa8d70efc320 class="flex justify-between"><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a></label><ul><li><a href=/docs/deep-learning/attention-and-transformers/large-scale-pretraining-with-transformers/ class=active>Large-Scale Pretraining with Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/modern-large-language-models-copy/>Modern Large Language Models</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92e8358c45c96009753cf4227e9daea8 class=toggle>
<label for=section-92e8358c45c96009753cf4227e9daea8 class="flex justify-between"><a href=/docs/deep-learning/llm-pipelines/>Large Language Model Pipelines</a></label><ul><li><a href=/docs/deep-learning/llm-pipelines/llm-inference-and-deployment/>LLM Inference and Deployment</a><ul></ul></li></ul></li><li><a href=/docs/deep-learning/computer-vision/>Computer Vision</a><ul></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Large-Scale Pretraining with Transformers</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#encoder-onlybert><strong>Encoder-Only（BERT）</strong></a><ul><li><a href=#bert-的预训练与微调机制><strong>BERT 的预训练与微调机制</strong></a></li></ul></li><li><a href=#encoder-decodert5><strong>Encoder-Decoder（T5）</strong></a><ul><li><a href=#t5-的预训练与微调机制><strong>T5 的预训练与微调机制</strong></a></li></ul></li><li><a href=#decoder-onlygpt><strong>Decoder-Only（GPT）</strong></a><ul><li><a href=#gpt如何区分不同任务如问答-vs-文本生成><strong>GPT如何区分不同任务（如问答 vs 文本生成）？</strong></a></li></ul></li><li><a href=#基础预训练模型><strong>基础预训练模型</strong></a><ul><li><a href=#bertmasked-language-model-next-sentence-prediction><strong>BERT（Masked Language Model, Next Sentence Prediction）</strong></a></li><li><a href=#gpt-2--gpt-3autoregressive-language-model><strong>GPT-2 / GPT-3（Autoregressive Language Model）</strong></a></li><li><a href=#t5bartseq2seq-预训练模型><strong>T5、BART（Seq2Seq 预训练模型）</strong></a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><p>Pre-training 是在大规模无监督数据上训练模型，而 Fine-tuning 是在特定任务或数据集上对模型进行微调。</p><h1 id=基于transformer的大规模预训练large-scale-pretraining-with-transformers><strong>基于Transformer的大规模预训练（Large-Scale Pretraining with Transformers）</strong>
<a class=anchor href=#%e5%9f%ba%e4%ba%8etransformer%e7%9a%84%e5%a4%a7%e8%a7%84%e6%a8%a1%e9%a2%84%e8%ae%ad%e7%bb%83large-scale-pretraining-with-transformers>#</a></h1><p>在传统任务（如图像分类、机器翻译）中，模型通常通过特定任务数据集从头训练（trained from scratch），成为专精单一任务的“专家”。例如，用英法双语对训练的Transformer模型仅能完成英译法任务，且对数据分布的微小变化敏感（易受分布偏移影响）。为提高模型泛化能力（generalization）并实现多任务处理（multitasking），大规模预训练（large-scale pretraining）逐渐成为主流。</p><p>Transformer的 <strong>可扩展性（scalability）</strong> 是其核心优势：<strong>随着模型参数（parameters）、训练数据量（training tokens）和计算资源（compute）的增加，性能按幂律关系显著提升</strong>。这一特性在视觉领域同样成立——更大规模的视觉Transformer（Vision Transformer, ViT）在更多数据训练下表现更优。</p><p>根据任务需求，Transformer可配置为三种模式：</p><ul><li><strong>仅编码器（Encoder-only）</strong>：适用于文本分类、命名实体识别等任务（如BERT）；</li><li><strong>编码器-解码器（Encoder-Decoder）</strong>：用于序列到序列任务（如机器翻译，原始Transformer设计）；</li><li><strong>仅解码器（Decoder-only）</strong>：专注于生成任务（如GPT系列），通过自回归（Autoregressive）方式逐个生成token。</li></ul><p>预训练阶段常采用自监督学习（Self-Supervised Learning，如掩码语言建模MLM或下一词预测），通过海量数据学习通用表征，再通过微调（Fine-tuning）适配下游任务。</p><hr><h2 id=encoder-onlybert><strong>Encoder-Only（BERT）</strong>
<a class=anchor href=#encoder-onlybert>#</a></h2><p>仅编码器架构的Transformer（如BERT、Vision Transformer）仅保留编码器层，通过多层 <strong>自注意力（Self-Attention）和前馈网络（FFN）</strong> 提取输入序列的全局特征。所有输入token（如文本词或图像块）通过自注意力相互关联，最终输出与输入等长的表示向量。</p><ul><li><strong>典型应用场景</strong>：文本分类（如情感分析）、命名实体识别（NER）、图像分类（ViT）等。</li><li><strong>输出处理</strong>：通常取序列开头的特殊标记 <code>&lt;cls></code> 的表示向量作为全局特征，再投影到任务标签（如分类层）。</li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> Encoder-Only 模型的核心特点就是 <strong>专注于理解</strong>（NLU, Natural Language Understanding），<strong>而且它的输出完全基于输入，不会额外生成新的内容</strong>。他们具有双向自注意力（Bidirectional Attention），可以同时建模左右上下文信息。因为不包含 Decoder，所以 <strong>不能生成文本</strong>。Encoder-Only 模型适用于以下任务：</p><ul><li>✅ <strong>文本分类（Text Classification）</strong>：垃圾邮件检测、情感分析、新闻分类</li><li>✅ <strong>文本匹配（Text Matching）</strong>：文本相似度计算，如搜索引擎中的相关性排序</li><li>✅ <strong>问答系统（QA）</strong>：如 SQuAD 任务，提取答案</li><li>✅ <strong>信息检索（IR）</strong>：如 Google 搜索的 Query-Document 相关性计算</li></ul></blockquote><hr><h3 id=bert-的预训练与微调机制><strong>BERT 的预训练与微调机制</strong>
<a class=anchor href=#bert-%e7%9a%84%e9%a2%84%e8%ae%ad%e7%bb%83%e4%b8%8e%e5%be%ae%e8%b0%83%e6%9c%ba%e5%88%b6>#</a></h3><p>BERT 是 Google 在 2018 年提出的 NLP 预训练模型，全称 <strong>Bidirectional Encoder Representations from Transformers</strong>，它基于 Transformer 的 Encoder 结构，可以 双向建模上下文信息，用于多种 NLP 任务。BERT 只包含 Transformer 的 Encoder 部分，即：</p><ul><li>输入是整个句子（token embedding + positional encoding）</li><li>多个 Encoder 层进行双向自注意力计算</li><li><strong>输出是整个句子的上下文表示</strong></li></ul><p>它没有 Transformer Decoder，BERT 主要用于理解任务，而非生成任务，所以是 Encoder-only 结构。</p><ol><li><strong>预训练（Pretraining）</strong></li></ol><div align=center><img src=/images/bert-encoder-only.svg width=500px/></div><ul><li><strong>任务设计</strong>：采用<strong>掩码语言建模（Masked Language Modeling, MLM）</strong>，随机遮盖输入文本中的部分token（如将“I love this red car”中的“love”替换为 <code>&lt;mask></code>）。在训练时，BERT 随机掩盖（Mask）输入文本的 15% token，然后让模型预测被遮挡的 token。</li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> BERT 的 <strong>Masked Language Modeling</strong> 本质上就是在做“完形填空”：预训练时，先将一部分词随机地盖住，经过模型的拟合，<strong>如果能够很好地预测那些盖住的词，模型就学到了文本的内在逻辑</strong>。这部分的主要作用是让模型 <strong>学到词汇和语法规则，提高语言理解能力</strong>。</p><p>除了“完形填空”，BERT还需要做 <strong>Next Sentence Prediction</strong> 任务：预测句子B是否为句子A的下一句。Next Sentence Prediction有点像英语考试中的“段落排序”题，只不过简化到只考虑两句话。<strong>如果模型无法正确地基于当前句子预测 Next Sentence，而是生硬地把两个不相关的句子拼到一起，两个句子在语义上是毫不相关的，说明模型没有读懂文本背后的意思。</strong> 这部分的主要作用是让模型 <strong>学习句子级别的语义关系</strong>。</p></blockquote><ul><li><strong>双向上下文建模</strong>：因编码器自注意力无方向限制，预测遮盖token时可利用前后文信息（如“red car”帮助预测“love”）。不像传统的 LSTM 只能从左到右或右到左。</li><li><strong>数据优势</strong>：无需人工标注，可基于书籍、维基百科等大规模文本自监督学习。</li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>自监督学习（Self-Supervised Learning, SSL）</strong> 是一种机器学习范式，它不依赖人工标注的数据，而是让模型自己从数据的结构或属性中创造监督信号。他的核心在于 <strong>“数据自己监督自己”，也就是说，模型用数据的一部分去预测另一部分</strong>，从而学到有意义的表示。</p></blockquote><ol start=2><li><p><strong>微调（Fine-Tuning）</strong></p><p>预训练的 BERT 模型可以通过微调（Fine-Tuning）适应下游的编码任务，包括单文本或文本对的处理。在微调过程中，<strong>可以在 BERT 之上添加参数随机初始化的额外层，这些新层和 BERT 的预训练参数将共同更新，以适应下游任务的训练数据</strong>。</p><p>以情感分析为例，微调过程如下图所示：预训练的 BERT 作为 Transformer 编码器，输入一个文本序列，并将其 <code>[CLS]</code> 表示（即输入的全局表示）传递给一个额外的全连接层，以预测情感倾向。在微调期间，通过基于梯度的算法最小化预测结果与情感分析数据标签之间的交叉熵损失。此时，额外的全连接层从头开始训练，而 BERT 的预训练参数也会更新。</p><div align=center><img src=/images/bert-finetune-classification.svg width=300px/></div><p>BERT 不仅适用于情感分析。通过对 2,500 亿个训练标记（tokens）进行预训练，拥有 3.5 亿参数的 BERT 学习到了通用的语言表示，这使其在单文本分类、文本对分类或回归、文本标注以及问答等自然语言处理任务上达到了新的水平。</p></li></ol><hr><h2 id=encoder-decodert5><strong>Encoder-Decoder（T5）</strong>
<a class=anchor href=#encoder-decodert5>#</a></h2><p>Transformer 中的 Encoder-Decoder 架构最早是为机器翻译（Machine Translation）提出的。在这种架构中，Encoder 将输入序列转换成相应数量的输出表示，而 Decoder 根据 Encoder 的输出和先前的 Decoder 输出，逐步自回归地生成目标序列（token-by-token）。</p><p>为了在机器翻译数据之外进行预训练，BART（Lewis et al., 2019）和T5（Raffel et al., 2020）是两个被提出的编码器-解码器Transformer模型，这两个模型在大规模文本语料库上进行了预训练。BART强调通过对输入进行加噪处理（如masking、删除、排列和旋转）来预训练，而T5则通过多任务学习的方式统一目标，并通过全面的消融研究（ablation studies）来进行验证。这些方法使得<strong>Transformer 的编码器-解码器架构不仅限于机器翻译，还能够扩展到其他文本生成任务</strong>。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> Encoder-Decoder模型同时具备了 <strong>理解（Encoder）和生成（Decoder）</strong> 的能力，因此它能够处理复杂的任务，如机器翻译、文本摘要、图像描述等。这类模型既能够通过 Encoder 理解输入，又能通过 Decoder 生成输出。Encoder–Decoder模型的好处有：</p><ol><li><strong>生成能力</strong>：Encoder–Decoder 架构能够生成任意长度的输出序列，而不是像Encoder-only模型那样只能生成固定长度的表示。它允许通过解码器（Decoder）逐步生成目标序列，非常适合像机器翻译和文本摘要等生成任务。</li><li><strong>灵活的输入和输出</strong>：Encoder-only 模型和 Decoder-only 模型通常输入和输出的长度是固定的，而 Encoder–Decoder 模型能够灵活地处理不同长度的输入和输出。Decoder 可以根据输入序列生成任意长度的目标序列，从而适应更复杂的任务。</li><li><strong>跨任务的预训练能力</strong>：Encoder–Decoder模型可以通过多任务学习提升模型的泛化能力。比如，T5模型通过将不同任务（如文本分类、文本生成等）统一为一个多任务预训练框架，从而增强了模型对不同任务的处理能力。</li></ol></blockquote><hr><h3 id=t5-的预训练与微调机制><strong>T5 的预训练与微调机制</strong>
<a class=anchor href=#t5-%e7%9a%84%e9%a2%84%e8%ae%ad%e7%bb%83%e4%b8%8e%e5%be%ae%e8%b0%83%e6%9c%ba%e5%88%b6>#</a></h3><p><strong>T5（Text-to-Text Transfer Transformer）</strong> 是一个预训练的 Transformer 编码器-解码器模型，它将许多任务统一为同一个文本到文本的问题。在T5的任务设置中，编码器的输入包括一个任务描述（例如：“Summarize”表示总结任务），后跟任务的输入（例如文章的标记序列）。解码器则预测任务的输出（例如输入文章的摘要）。T5的训练目标是基于输入文本生成目标文本。</p><ol><li><p><strong>预训练（Pretraining）</strong>
为了进行预训练，T5通过预测连续的标记范围来进行训练。具体来说，<strong>文本中的一些标记会被随机替换成特殊标记</strong>，每一组连续的标记被替换成相同的特殊标记。例如，在一个句子中，“I”, “love”, “this”, “red”, “car”中，“love”被替换为一个特殊标记“”，“red”和“car”也被替换为另一个特殊标记“”。这样，输入序列就变成了“I”, “<code>&lt;X></code>”, “this”, “<code>&lt;Y></code>”，而目标序列是“<code>&lt;X></code>”, “love”, “<code>&lt;Y></code>”, “red”, “car”, “<code>&lt;Z></code>”，其中“<code>&lt;Z></code>”是表示结束的特殊标记。此任务的目标是通过这种方式 <strong>恢复被替换的文本，从而在预训练中学习到文本的结构和语言模式</strong>。</p><div align=center><img src=/images/t5-encoder-decoder.svg width=500px/></div><p>在T5的Transformer结构中，编码器的自注意力（Self-Attention）机制使得所有输入标记相互注意，而编码器-解码器之间的交叉注意力（Cross-Attention）使得每个目标标记能够关注所有输入标记。解码器的自注意力则具有因果性（Causal Attention）模式，以确保在预测时不会关注到未来的标记。</p><p>T5的预训练使用了1000亿个来自C4（Colossal Clean Crawled Corpus）数据集的标记，该数据集包含来自网络的清洁英语文本。T5的预训练目标是通过这种方式预测连续的标记范围（也称为 <strong>重建损坏的文本</strong>），帮助模型学习如何生成与输入文本相关的输出文本。</p><p>这种结构的核心优势在于，它为各种自然语言处理任务提供了一个统一的框架，无论是文本分类、摘要生成，还是问答任务，都可以通过这种“文本到文本”的方式进行处理。</p></li><li><p><strong>微调（Fine-Tuning）</strong></p><p>T5（Text-to-Text Transfer Transformer）模型与BERT相似，也需要通过在特定任务的数据上进行微调（fine-tuning）来完成下游任务。与BERT的微调有所不同，T5的主要特点包括：<strong>（1）T5的输入包含任务描述；（2）T5通过其 Transformer 解码器可以生成任意长度的序列；（3）T5不需要额外的层来进行微调。</strong></p><p>以文本摘要任务为例，T5的微调过程如下图所示。具体来说，任务描述标记（如“Summarize”）与文章的标记一起输入到Transformer编码器，用以预测摘要。这样，T5能够理解不同任务，并通过任务描述来指导模型进行相应的生成任务。</p><div align=center><img src=/images/t5-finetune-summarization.svg width=400px/></div></li></ol><hr><h2 id=decoder-onlygpt><strong>Decoder-Only（GPT）</strong>
<a class=anchor href=#decoder-onlygpt>#</a></h2><p>在 decoder-only 结构中，模型只包含解码器部分，这使得它主要专注于 <strong>生成任务</strong>。与 encoder-decoder 结构不同，decoder-only Transformer 可以直接根据输入的文本生成输出，因此在许多自然语言处理任务中表现出色，尤其是在大规模的预训练任务中。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>Encoder-Decoder 和 Decoder-only 的区别</strong>：Decoder-only（如GPT）仅保留解码器，通过自回归生成（逐词预测）完成任务。适用任务主要为生成类任务（文本续写、对话、代码生成）。Encoder-Decoder（如原始Transformer、T5）分离编码器（理解输入）与解码器（生成输出），通过交叉注意力传递信息。适用任务主要为需严格分离输入理解与输出生成的任务（翻译、摘要、问答）。</p><p>Decoder-only模型通过 <strong>上下文学习（In-Context Learning）</strong> 将任务隐式编码到输入中（如添加“Translate English to French:”前缀），利用生成能力模拟翻译，完成和 Encoder-Decoder 一样的工作。但是 Encoder-Decoder在以下场景更具优势：</p><ol><li><strong>输入与输出解耦的复杂任务（如翻译）</strong>：Encoder-Decoder 模型的编码器可先提取完整语义，解码器再逐词生成，避免生成过程中的语义偏差，准确性更高。而Decoder-only模型（如GPT-3）需通过Prompt（如“Translate English to French: &mldr;”）隐式对齐输入输出，<strong>易受提示词设计影响</strong>。</li><li><strong>长文本处理效率</strong>：Encoder-Decoder：编码器一次性压缩输入为固定长度表示，解码生成时无需重复处理长输入（节省计算资源）。Decoder-only：<strong>生成每个词时需重新处理整个输入序列</strong>（如输入1000词的文档），导致计算复杂度高。</li></ol><ul><li><strong>总结来说</strong><ul><li><strong>Decoder-only</strong>：适合开放域生成任务，依赖生成连贯性与上下文学习，但对输入-输出结构复杂的任务效率较低。</li><li><strong>Encoder-Decoder</strong>：在需严格分离理解与生成、处理长输入、多任务统一的场景中更优，尤其适合翻译、摘要等结构化任务。</li><li>类比：Decoder-only像“自由创作的作家”，Encoder-Decoder像“严谨的翻译官”——前者更灵活，后者更精准。</li></ul></li></ul></blockquote><div align=center><img src=/images/gpt-decoder-only.svg width=500px/></div><p>GPT（Generative Pre-trained Transformer）基于仅解码器架构（Decoder-only），移除原始Transformer的编码器和交叉注意力层，保留掩码自注意力（Masked Self-Attention）与前馈网络（FFN）。输入处理时，文本序列添加特殊标记 <code>&lt;bos></code>（序列开始）和 <code>&lt;eos></code>（序列结束），目标序列为输入右移一位。注意力限制方面，通过 <strong>因果掩码（Causal Mask）</strong> 强制每个token仅关注其左侧上下文。</p><p>Decoder-Only 模型移除了Encoder和交叉注意力层，仅保留自注意力层和前馈网络（FFN）。<strong>所有注意力均为自注意力</strong>：
<link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\(Q、K、V\)
</span>均来自同一输入序列（例如输入文本的前缀部分）。例如，输入“I love deep”，模型通过自注意力计算每个词与所有已输入词的关系，生成下一个词“learning”。</p><p>预训练任务是 <strong>自回归语言建模（Autoregressive LM）</strong>：最大化序列联合概率，损失函数为交叉熵（Cross-Entropy）：
<span>\[
\begin{equation}
\mathcal{L} = -\sum_{t=1}^{T} \log P(w_t | w_{1:t-1 })
\end{equation}
\]</span></p><ul><li><strong>输入输出</strong>：共享同一序列（如输入为"Translate English to French: &lsquo;hello&rsquo; →"，输出生成"bonjour"）。</li><li><strong>核心机制</strong>：GPT通过统计建模（而非“记忆”）学习语言规律。预训练时，模型并非记住所有可能的输入与输出组合，而是通过概率分布捕捉词与词之间的关联性。<ul><li>例如：输入“The capital of France is”，模型根据统计规律高概率生成“Paris”（而非“London”）。</li><li>对于罕见组合（如“The capital of France is made of”），模型可能生成符合语法但语义荒谬的结果（如“cheese”），反映其依赖训练数据的分布。</li></ul></li><li><strong>生成能力</strong>：模型通过自回归生成（逐词预测）产生连贯文本，但无法保证事实准确性（可能产生“幻觉”）。</li></ul><hr><h3 id=gpt如何区分不同任务如问答-vs-文本生成><strong>GPT如何区分不同任务（如问答 vs 文本生成）？</strong>
<a class=anchor href=#gpt%e5%a6%82%e4%bd%95%e5%8c%ba%e5%88%86%e4%b8%8d%e5%90%8c%e4%bb%bb%e5%8a%a1%e5%a6%82%e9%97%ae%e7%ad%94-vs-%e6%96%87%e6%9c%ac%e7%94%9f%e6%88%90>#</a></h3><p>GPT本身不具备显式任务识别模块，而是 <strong>通过输入格式（Prompt）的上下文模式隐式引导生成结果</strong>。所有任务均<strong>被转化为文本生成任务</strong>，其核心原理基于预训练阶段对海量文本模式的学习。</p><ol><li><p><strong>预训练数据的模式学习</strong>：在预训练阶段，GPT接触了包含多种任务格式的文本（如问答对、翻译示例、代码片段），通过自回归目标学习这些模式：</p><ul><li><p><strong>示例</strong>：</p><pre tabindex=0><code>## 问答类
Q: What is photosynthesis?  
A: Photosynthesis is the process by which plants convert sunlight into energy.  
</code></pre><pre tabindex=0><code>## 翻译类
Translate English to French: &#34;hello&#34; → &#34;bonjour&#34;  
</code></pre></li><li><p><strong>学习结果</strong>：模型统计性掌握不同任务对应的输入-输出格式规律（如“Q:”后通常接答案，“Translate”后接目标语言）。</p></li></ul></li><li><p><strong>训练数据的多样性是关键：</strong> GPT的预训练数据包含海量互联网文本（书籍、网页、代码等），天然涵盖多种任务模式：</p><ul><li><strong>问答对</strong>：论坛讨论、维基百科（如“Q: What is photosynthesis? A: &mldr;”）。</li><li><strong>翻译示例</strong>：多语言网页对照、教材例句（如“Hello → Bonjour”）。</li><li><strong>代码注释</strong>：GitHub代码库中的函数与注释（如“# 计算阶乘 → def factorial(n): &mldr;”）。</li><li><strong>对话记录</strong>：社交媒体对话（如“User: How are you? Bot: I’m fine.”）。</li></ul><p>模型通过自回归目标（预测下一词）<strong>隐式学习这些模式，而非显式标注任务类型</strong>。即模型在训练时并不是根据任务去显式的分类学习的，而是隐式的学习规律。</p></li><li><p><strong>生成过程的隐式任务引导</strong>：生成时，模型基于Prompt的上下文模式，激活预训练中学习到的对应任务生成策略</p></li></ol><div align=center><img src=/images/gpt-3-xshot.svg width=650px/></div><ol start=4><li><strong>上下文学习（In-Context Learning）</strong>：GPT通过<strong>少量示例（Few-shot）或纯指令（Zero-shot）</strong> 显式定义任务类型：<ul><li><strong>Few-shot示例：</strong><pre tabindex=0><code>Q: Capital of France? A: Paris  
Q: Capital of Japan? A: Tokyo  
Q: Capital of Brazil? A:  
</code></pre><ul><li>模型通过前两例学习“Q-A”模式，生成“Brasília”。</li></ul></li><li><strong>Zero-shot指令：</strong><pre tabindex=0><code>Please answer the following question:  
What is the boiling point of water?  
Answer:  
</code></pre><ul><li>模型根据指令词“Please answer”生成“100°C (212°F) at sea level”。</li></ul></li></ul></li><li><strong>若Prompt设计模糊，模型可能生成不符合预期的结果。</strong></li></ol><hr><h2 id=基础预训练模型><strong>基础预训练模型</strong>
<a class=anchor href=#%e5%9f%ba%e7%a1%80%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b>#</a></h2><h3 id=bertmasked-language-model-next-sentence-prediction><strong>BERT（Masked Language Model, Next Sentence Prediction）</strong>
<a class=anchor href=#bertmasked-language-model-next-sentence-prediction>#</a></h3><h3 id=gpt-2--gpt-3autoregressive-language-model><strong>GPT-2 / GPT-3（Autoregressive Language Model）</strong>
<a class=anchor href=#gpt-2--gpt-3autoregressive-language-model>#</a></h3><h3 id=t5bartseq2seq-预训练模型><strong>T5、BART（Seq2Seq 预训练模型）</strong>
<a class=anchor href=#t5bartseq2seq-%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b>#</a></h3></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#encoder-onlybert><strong>Encoder-Only（BERT）</strong></a><ul><li><a href=#bert-的预训练与微调机制><strong>BERT 的预训练与微调机制</strong></a></li></ul></li><li><a href=#encoder-decodert5><strong>Encoder-Decoder（T5）</strong></a><ul><li><a href=#t5-的预训练与微调机制><strong>T5 的预训练与微调机制</strong></a></li></ul></li><li><a href=#decoder-onlygpt><strong>Decoder-Only（GPT）</strong></a><ul><li><a href=#gpt如何区分不同任务如问答-vs-文本生成><strong>GPT如何区分不同任务（如问答 vs 文本生成）？</strong></a></li></ul></li><li><a href=#基础预训练模型><strong>基础预训练模型</strong></a><ul><li><a href=#bertmasked-language-model-next-sentence-prediction><strong>BERT（Masked Language Model, Next Sentence Prediction）</strong></a></li><li><a href=#gpt-2--gpt-3autoregressive-language-model><strong>GPT-2 / GPT-3（Autoregressive Language Model）</strong></a></li><li><a href=#t5bartseq2seq-预训练模型><strong>T5、BART（Seq2Seq 预训练模型）</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>