<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  注意力机制（Attention and Transformers）
  #

Transformer架构已成为几乎所有NLP任务的核心模型。处理NLP任务的默认方法是选择一个预训练的Transformer模型（如BERT、ELECTRA、RoBERTa或Longformer），根据下游任务调整输出层并进行微调。此外，Transformer架构也已成为视觉任务（如图像识别、目标检测、语义分割和超分辨率）的默认选择，并在语音识别、强化学习以及图神经网络中表现出竞争力。
Transformer模型的 核心是Attention机制，最初被设计为增强编码器-解码器RNN在序列到序列（seq2seq）任务中的表现（如机器翻译）。在传统的seq2seq模型中，编码器将输入压缩为固定长度的向量再传递给解码器。Attention的直觉是，解码器在每个时间步可以动态关注输入序列的不同部分，而不是使用单一的固定表示。
Bahdanau等（2014）提出了一种简单的Attention机制，允许解码器在每个时间步选择性地关注输入序列的某些部分。具体而言，编码器生成与输入序列长度相等的表示；解码器通过一个控制机制，从这些表示中计算加权和（context vector）作为输入。这些权重表示了 解码器在某一时间步对输入序列每个元素的“关注程度”，并通过可微分的方式学习。
Attention最初作为增强RNN的机制，在机器翻译任务中表现优异，并展示了对跨语言词义对齐的解释性。然而，Vaswani等（2017）提出了完全基于Attention的Transformer架构，彻底摒弃了循环结构。Transformer通过Attention机制捕获输入和输出序列中所有元素之间的关系，在性能上超过了传统架构。到2018年，Transformer在大多数NLP任务中成为主流。
此外，NLP领域逐渐采用大规模预训练的模式：在庞大的通用语料上进行自监督预训练，然后使用下游任务数据进行微调。这种预训练-微调范式进一步拉大了Transformer与传统架构的性能差距。如今，基于Transformer的预训练模型（如GPT系列）被称为 基础模型（Foundation Models），其广泛应用标志着Transformer的全面崛起。


  注意力机制中的查询 (Query)、键 (Key) 和值 (Value)
  #

目前接触过的网络模型大多依赖于 固定大小的输入。例如，ImageNet中的图像尺寸固定为特定像素大小，而卷积神经网络（CNNs）针对这种固定尺寸进行了优化。在自然语言处理中，循环神经网络（RNNs）的输入通常也是固定的，若输入大小可变，则通过逐步处理每个token或设计专用卷积核来解决。然而，当输入序列长度不定且信息含量变化时（如文本生成任务中），这些方法会带来显著问题，尤其在长序列中，网络难以跟踪已经生成或处理过的内容。
这种问题可以与数据库的运作方式类比。数据库通常是由 键-值 (key-value) 对组成的集合，例如：



  \(\{(“Zhang”, “Aston”), (“Lipton”, “Zachary”), (“Li”, “Mu”), (“Smola”, “Alex”)\}\)

。键是姓氏，值是名字。查询可以通过提供键来找到对应的值（如查询“Li”返回“Mu”）。这一简单示例表明：

查询可以在不同大小的数据库上有效运行。
相同的查询根据数据库内容可能 返回不同结果。
数据库操作的“代码”（如精确匹配、近似匹配）可以非常简洁，无需压缩或简化数据库即可有效运行。

这种理念引出了深度学习中的重要概念：注意力机制（Attention Mechanism）。它的核心思想是将 输入看作键-值对的数据库，并基于查询计算注意力权重 (attention weights)。假设数据库包含 
  \(\mathcal{D} \stackrel{\textrm{def}}{=} \{(\mathbf{k}_1, \mathbf{v}_1), \ldots (\mathbf{k}_m, \mathbf{v}_m)\}\)

 个键值对，键和值分别记为 
  \(k_m\)

 和 
  \(v_m\)

，查询记为 
  \(q\)

。关于的 
  \(\mathcal{D}\)

 的注意力机制定义如下：

  \[
\textrm{Attention}(\mathbf{q}, \mathcal{D}) \stackrel{\textrm{def}}{=} \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i,
\]


其中，
  \(\alpha(\mathbf{q}, \mathbf{k}_i) \in \mathbb{R}（i = 1, \ldots）\)

。这一操作称为 注意力池化（Attention Pooling），其关键点如下："><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/deep-learning/attention-and-transformers/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Attention and Transformers"><meta property="og:description" content="注意力机制（Attention and Transformers） # Transformer架构已成为几乎所有NLP任务的核心模型。处理NLP任务的默认方法是选择一个预训练的Transformer模型（如BERT、ELECTRA、RoBERTa或Longformer），根据下游任务调整输出层并进行微调。此外，Transformer架构也已成为视觉任务（如图像识别、目标检测、语义分割和超分辨率）的默认选择，并在语音识别、强化学习以及图神经网络中表现出竞争力。
Transformer模型的 核心是Attention机制，最初被设计为增强编码器-解码器RNN在序列到序列（seq2seq）任务中的表现（如机器翻译）。在传统的seq2seq模型中，编码器将输入压缩为固定长度的向量再传递给解码器。Attention的直觉是，解码器在每个时间步可以动态关注输入序列的不同部分，而不是使用单一的固定表示。
Bahdanau等（2014）提出了一种简单的Attention机制，允许解码器在每个时间步选择性地关注输入序列的某些部分。具体而言，编码器生成与输入序列长度相等的表示；解码器通过一个控制机制，从这些表示中计算加权和（context vector）作为输入。这些权重表示了 解码器在某一时间步对输入序列每个元素的“关注程度”，并通过可微分的方式学习。
Attention最初作为增强RNN的机制，在机器翻译任务中表现优异，并展示了对跨语言词义对齐的解释性。然而，Vaswani等（2017）提出了完全基于Attention的Transformer架构，彻底摒弃了循环结构。Transformer通过Attention机制捕获输入和输出序列中所有元素之间的关系，在性能上超过了传统架构。到2018年，Transformer在大多数NLP任务中成为主流。
此外，NLP领域逐渐采用大规模预训练的模式：在庞大的通用语料上进行自监督预训练，然后使用下游任务数据进行微调。这种预训练-微调范式进一步拉大了Transformer与传统架构的性能差距。如今，基于Transformer的预训练模型（如GPT系列）被称为 基础模型（Foundation Models），其广泛应用标志着Transformer的全面崛起。
注意力机制中的查询 (Query)、键 (Key) 和值 (Value) # 目前接触过的网络模型大多依赖于 固定大小的输入。例如，ImageNet中的图像尺寸固定为特定像素大小，而卷积神经网络（CNNs）针对这种固定尺寸进行了优化。在自然语言处理中，循环神经网络（RNNs）的输入通常也是固定的，若输入大小可变，则通过逐步处理每个token或设计专用卷积核来解决。然而，当输入序列长度不定且信息含量变化时（如文本生成任务中），这些方法会带来显著问题，尤其在长序列中，网络难以跟踪已经生成或处理过的内容。
这种问题可以与数据库的运作方式类比。数据库通常是由 键-值 (key-value) 对组成的集合，例如： \(\{(“Zhang”, “Aston”), (“Lipton”, “Zachary”), (“Li”, “Mu”), (“Smola”, “Alex”)\}\) 。键是姓氏，值是名字。查询可以通过提供键来找到对应的值（如查询“Li”返回“Mu”）。这一简单示例表明：
查询可以在不同大小的数据库上有效运行。 相同的查询根据数据库内容可能 返回不同结果。 数据库操作的“代码”（如精确匹配、近似匹配）可以非常简洁，无需压缩或简化数据库即可有效运行。 这种理念引出了深度学习中的重要概念：注意力机制（Attention Mechanism）。它的核心思想是将 输入看作键-值对的数据库，并基于查询计算注意力权重 (attention weights)。假设数据库包含 \(\mathcal{D} \stackrel{\textrm{def}}{=} \{(\mathbf{k}_1, \mathbf{v}_1), \ldots (\mathbf{k}_m, \mathbf{v}_m)\}\) 个键值对，键和值分别记为 \(k_m\) 和 \(v_m\) ，查询记为 \(q\) 。关于的 \(\mathcal{D}\) 的注意力机制定义如下：
\[ \textrm{Attention}(\mathbf{q}, \mathcal{D}) \stackrel{\textrm{def}}{=} \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i, \] 其中， \(\alpha(\mathbf{q}, \mathbf{k}_i) \in \mathbb{R}（i = 1, \ldots）\) 。这一操作称为 注意力池化（Attention Pooling），其关键点如下："><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Attention and Transformers | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/deep-learning/attention-and-transformers/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.5524b17f930ecc82d640e17bcb18a9bddbe34e667f313e69da4a815f2d47369f.js integrity="sha256-VSSxf5MOzILWQOF7yxipvdvjTmZ/MT5p2kqBXy1HNp8=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/deep-learning/attention-and-transformers/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a><ul></ul></li><li><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/ class=active>Attention and Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/computer-vision/>Computer Vision</a><ul></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Attention and Transformers</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#注意力机制中的查询-query键-key-和值-value><strong>注意力机制中的查询 (Query)、键 (Key) 和值 (Value)</strong></a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=注意力机制attention-and-transformers><strong>注意力机制（Attention and Transformers）</strong>
<a class=anchor href=#%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6attention-and-transformers>#</a></h1><p>Transformer架构已成为几乎所有NLP任务的核心模型。处理NLP任务的默认方法是选择一个预训练的Transformer模型（如BERT、ELECTRA、RoBERTa或Longformer），根据下游任务调整输出层并进行微调。此外，Transformer架构也已成为视觉任务（如图像识别、目标检测、语义分割和超分辨率）的默认选择，并在语音识别、强化学习以及图神经网络中表现出竞争力。</p><p>Transformer模型的 <strong>核心是Attention机制</strong>，最初被设计为增强编码器-解码器RNN在序列到序列（seq2seq）任务中的表现（如机器翻译）。在传统的seq2seq模型中，编码器将输入压缩为固定长度的向量再传递给解码器。<strong>Attention的直觉是，解码器在每个时间步可以动态关注输入序列的不同部分，而不是使用单一的固定表示。</strong></p><p>Bahdanau等（2014）提出了一种简单的Attention机制，允许解码器在每个时间步选择性地关注输入序列的某些部分。具体而言，编码器生成与输入序列长度相等的表示；解码器通过一个控制机制，从这些表示中计算加权和（context vector）作为输入。这些权重表示了 <strong>解码器在某一时间步对输入序列每个元素的“关注程度”</strong>，并通过可微分的方式学习。</p><p>Attention最初作为增强RNN的机制，在机器翻译任务中表现优异，并展示了对跨语言词义对齐的解释性。然而，Vaswani等（2017）提出了完全基于Attention的Transformer架构，彻底摒弃了循环结构。Transformer通过Attention机制捕获输入和输出序列中所有元素之间的关系，在性能上超过了传统架构。到2018年，Transformer在大多数NLP任务中成为主流。</p><p>此外，NLP领域逐渐采用大规模预训练的模式：在庞大的通用语料上进行自监督预训练，然后使用下游任务数据进行微调。这种预训练-微调范式进一步拉大了Transformer与传统架构的性能差距。如今，基于Transformer的预训练模型（如GPT系列）被称为 <strong>基础模型（Foundation Models）</strong>，其广泛应用标志着Transformer的全面崛起。</p><hr><h2 id=注意力机制中的查询-query键-key-和值-value><strong>注意力机制中的查询 (Query)、键 (Key) 和值 (Value)</strong>
<a class=anchor href=#%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e4%b8%ad%e7%9a%84%e6%9f%a5%e8%af%a2-query%e9%94%ae-key-%e5%92%8c%e5%80%bc-value>#</a></h2><p>目前接触过的网络模型大多依赖于 <strong>固定大小的输入</strong>。例如，ImageNet中的图像尺寸固定为特定像素大小，而卷积神经网络（CNNs）针对这种固定尺寸进行了优化。在自然语言处理中，循环神经网络（RNNs）的输入通常也是固定的，若输入大小可变，则通过逐步处理每个token或设计专用卷积核来解决。然而，当输入序列长度不定且信息含量变化时（如文本生成任务中），这些方法会带来显著问题，尤其在长序列中，<strong>网络难以跟踪已经生成或处理过的内容</strong>。</p><p>这种问题可以与数据库的运作方式类比。数据库通常是由 <strong>键-值 (key-value)</strong> 对组成的集合，例如：
<link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\(\{(“Zhang”, “Aston”), (“Lipton”, “Zachary”), (“Li”, “Mu”), (“Smola”, “Alex”)\}\)
</span>。键是姓氏，值是名字。查询可以通过提供键来找到对应的值（如查询“Li”返回“Mu”）。这一简单示例表明：</p><ol><li>查询可以在不同大小的数据库上有效运行。</li><li>相同的查询根据数据库内容可能 <strong>返回不同结果</strong>。</li><li>数据库操作的“代码”（如精确匹配、近似匹配）可以非常简洁，无需压缩或简化数据库即可有效运行。</li></ol><p>这种理念引出了深度学习中的重要概念：<strong>注意力机制（Attention Mechanism）</strong>。它的核心思想是将 <strong>输入看作键-值对的数据库</strong>，并<strong>基于查询计算注意力权重 (attention weights)</strong>。假设数据库包含 <span>\(\mathcal{D} \stackrel{\textrm{def}}{=} \{(\mathbf{k}_1, \mathbf{v}_1), \ldots (\mathbf{k}_m, \mathbf{v}_m)\}\)
</span>个键值对，键和值分别记为 <span>\(k_m\)
</span>和 <span>\(v_m\)
</span>，查询记为 <span>\(q\)
</span>。关于的 <span>\(\mathcal{D}\)
</span>的注意力机制定义如下：</p><span>\[
\textrm{Attention}(\mathbf{q}, \mathcal{D}) \stackrel{\textrm{def}}{=} \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i,
\]</span><p>其中，<span>
\(\alpha(\mathbf{q}, \mathbf{k}_i) \in \mathbb{R}（i = 1, \ldots）\)
</span>。这一操作称为 <strong>注意力池化（Attention Pooling）</strong>，其关键点如下：</p><ul><li><p>当权重 <span>\(\alpha\)
</span>较大时，机制对对应的值 <span>\(v_i\)
</span>赋予更多关注。</p></li><li><p>结果 <span>\(\textrm{Attention}(\mathbf{q}, \mathcal{D})\)
</span>是数据库中值的线性组合。</p><p><strong>权重 <span>\(\alpha(\mathbf{q}, \mathbf{k}_i)\)
</span>的特殊情况</strong>：</p><ol><li><p><strong>非负权重</strong>：若所有 <span>\(\alpha(\mathbf{q}, \mathbf{k}_i) \geq 0\)
</span>，输出在值的凸锥中。</p></li><li><p><strong>权重归一化</strong>：若 <span>\(\sum_{i=1}^n \alpha(\mathbf{q}, \mathbf{k}_i) = 1\)
</span>，且 <span>\(\alpha(\mathbf{q}, \mathbf{k}_i) \geq 0\)
</span>，输出为值的凸组合。</p><p>确保权重总和为 1 的常见策略是通过以下方式使它们 Normalize：<span>
\(\alpha(\mathbf{q}, \mathbf{k}_i) = \frac{\alpha(\mathbf{q}, \mathbf{k}_i)}{{\sum_j} \alpha(\mathbf{q}, \mathbf{k}_j)}\)
</span>。
为了确保权重也是非负的，我们可以添加指数化：<span>
\(\alpha(\mathbf{q}, \mathbf{k}_i) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_j \exp(a(\mathbf{q}, \mathbf{k}_j))}\)
</span>。</p></li><li><p><strong>单一选择</strong>：若 <span>\(\alpha(\mathbf{q}, \mathbf{k}_i) = 1\)
</span>（其余为0），等同于传统的数据库精确查询。</p></li><li><p><strong>平均池化</strong>：若 <span>\(\alpha(\mathbf{q}, \mathbf{k}_i)\)
</span>均相等，即 <span>\(\alpha(\mathbf{q}, \mathbf{k}_i) = 1/n\)
</span>，等同于对所有值进行平均。</p></li></ol></li></ul><div align=center><img src=/images/qkv.svg width=400px/></div><p>虽然上述注意力机制可微分且适用于深度学习，但也有非可微的注意力模型（例如使用强化学习训练）。现代研究多集中于这种可微机制的变体。</p><p>通过注意力机制，<strong>网络可以高效操作任意大小的键值对集合</strong>，而无需改变操作方式。这种灵活性和参数高效性使其成为深度学习中重要的工具。</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#注意力机制中的查询-query键-key-和值-value><strong>注意力机制中的查询 (Query)、键 (Key) 和值 (Value)</strong></a></li></ul></nav></div></aside></main></body></html>