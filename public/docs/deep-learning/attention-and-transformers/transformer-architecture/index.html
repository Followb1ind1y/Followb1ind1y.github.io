<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Transformer 模型架构（Transformer Architecture）
  #


  注意力机制（Attention Mechanism）
  #

⁉️ 什么是注意力机制（Attention Mechanism）？其中的Q，K，V都代表什么？
  
什么是注意力机制（Attention Mechanism）？其中的Q，K，V都代表什么？
注意力机制（Attention Mechanism） 的核心思想是将 输入看作键-值对的数据库，并 基于查询计算注意力权重 (attention weights)，可以动态地选择哪些输入部分（例如词语或特征）最为重要。注意力机制定义如下：



  \[
\textrm{Attention}(\mathbf{q}, \mathcal{D}) \stackrel{\textrm{def}}{=} \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i,
\]


这里的 q，查询（Query） 是当前模型试图“寻找”或“关注”的目标。它是一个向量，代表了你想匹配的信息。
公式中的 k，键（key） 是所有潜在匹配目标的特征表示。每个键对应于输入中的一个元素，表示这个元素的特性或身份。
其中的 v，值（value） 是和键一起存储的信息，也是最终被提取的信息。注意力机制的目标是通过查询和键找到最相关的值。


  注意力机制的 一般步骤 为：

对查询和每个键计算相似度。（⚠️注意：Transformer在比较相似性时不是像 Embedding 后一样通过手动定义相似性方法（如余弦相似度，kernel）的比较 vector，而是使用自注意力（Self-Attention），它不是直接定义相似度，而是 让神经网络自己学习“什么是相似”。在提升模型表现，调整 weight 的过程中，模型利用自注意力（Self-Attention）最终学会了：哪些 Query 和哪些 Key 需要匹配。）
对这些相似度进行归一化（通常使用 Softmax 函数）。归一化后的结果称为注意力权重（Attention Weights）。
将注意力权重与对应的值相乘，得到一个加权求和结果。这个结果就是当前查询的输出。


在 transformer 中，Query (Q)、Key (K) 和 Value (V) 都是从输入嵌入（embedding）中线性变换得到的。它们的计算方式如下："><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/deep-learning/attention-and-transformers/transformer-architecture/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Transformer Architecture"><meta property="og:description" content="Transformer 模型架构（Transformer Architecture） # 注意力机制（Attention Mechanism） # ⁉️ 什么是注意力机制（Attention Mechanism）？其中的Q，K，V都代表什么？ 什么是注意力机制（Attention Mechanism）？其中的Q，K，V都代表什么？ 注意力机制（Attention Mechanism） 的核心思想是将 输入看作键-值对的数据库，并 基于查询计算注意力权重 (attention weights)，可以动态地选择哪些输入部分（例如词语或特征）最为重要。注意力机制定义如下：
\[ \textrm{Attention}(\mathbf{q}, \mathcal{D}) \stackrel{\textrm{def}}{=} \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i, \] 这里的 q，查询（Query） 是当前模型试图“寻找”或“关注”的目标。它是一个向量，代表了你想匹配的信息。 公式中的 k，键（key） 是所有潜在匹配目标的特征表示。每个键对应于输入中的一个元素，表示这个元素的特性或身份。 其中的 v，值（value） 是和键一起存储的信息，也是最终被提取的信息。注意力机制的目标是通过查询和键找到最相关的值。 注意力机制的 一般步骤 为：
对查询和每个键计算相似度。（⚠️注意：Transformer在比较相似性时不是像 Embedding 后一样通过手动定义相似性方法（如余弦相似度，kernel）的比较 vector，而是使用自注意力（Self-Attention），它不是直接定义相似度，而是 让神经网络自己学习“什么是相似”。在提升模型表现，调整 weight 的过程中，模型利用自注意力（Self-Attention）最终学会了：哪些 Query 和哪些 Key 需要匹配。） 对这些相似度进行归一化（通常使用 Softmax 函数）。归一化后的结果称为注意力权重（Attention Weights）。 将注意力权重与对应的值相乘，得到一个加权求和结果。这个结果就是当前查询的输出。 在 transformer 中，Query (Q)、Key (K) 和 Value (V) 都是从输入嵌入（embedding）中线性变换得到的。它们的计算方式如下："><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Transformer Architecture | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/deep-learning/attention-and-transformers/transformer-architecture/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.bf0be8157834ec1dce460c6f25f5007a73ccb26eeb036678ba4caeeff445f6c8.js integrity="sha256-vwvoFXg07B3ORgxvJfUAenPMsm7rA2Z4ukyu7/RF9sg=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/deep-learning/attention-and-transformers/transformer-architecture/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li><li><a href=/docs/machine-learning/computational-performance/>Computational Performance</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><input type=checkbox id=section-d0dd931d60033c220ecd4cd60b7c9170 class=toggle>
<label for=section-d0dd931d60033c220ecd4cd60b7c9170 class="flex justify-between"><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a></label><ul><li><a href=/docs/deep-learning/convolutional-neural-networks/modern-convolutional-neural-networks/>Modern Convolutional Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-a3019bfa8037cc33ed6405d1589b6219 class=toggle>
<label for=section-a3019bfa8037cc33ed6405d1589b6219 class="flex justify-between"><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a></label><ul><li><a href=/docs/deep-learning/recurrent-neural-networks/modern-recurrent-neural-networks/>Modern Recurrent Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-0a43584c16258b228ae9aa8d70efc320 class=toggle checked>
<label for=section-0a43584c16258b228ae9aa8d70efc320 class="flex justify-between"><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a></label><ul><li><a href=/docs/deep-learning/attention-and-transformers/tokenization-and-word-embeddings/>Tokenization and Word Embeddings</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/transformer-architecture/ class=active>Transformer Architecture</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/large-scale-pretraining-with-transformers/>Large-Scale Pretraining with Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/modern-large-language-models/>Modern Large Language Models</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/post-training-large-language-models/>Post-training Large Language Models</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/multimodal-large-language-models/>Multimodal Large Language Models</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92e8358c45c96009753cf4227e9daea8 class=toggle>
<label for=section-92e8358c45c96009753cf4227e9daea8 class="flex justify-between"><a href=/docs/deep-learning/llm-pipelines/>LLM Pipelines</a></label><ul><li><a href=/docs/deep-learning/llm-pipelines/llm-hardware-and-model-size/>LLM Hardware and Model Size</a><ul></ul></li><li><a href=/docs/deep-learning/llm-pipelines/llm-inference-and-deployment/>LLM Inference and Deployment</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul><li><a href=/docs/others/interview-preparation-guide/>Interview Preparation Guide</a><ul></ul></li></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Transformer Architecture</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#注意力机制attention-mechanism><strong>注意力机制（Attention Mechanism）</strong></a></li><li><a href=#位置编码positional-encoding><strong>位置编码（Positional Encoding）</strong></a></li><li><a href=#transformer-模型架构细节><strong>Transformer 模型架构细节</strong></a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=transformer-模型架构transformer-architecture><strong>Transformer 模型架构（Transformer Architecture）</strong>
<a class=anchor href=#transformer-%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84transformer-architecture>#</a></h1><h2 id=注意力机制attention-mechanism><strong>注意力机制（Attention Mechanism）</strong>
<a class=anchor href=#%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6attention-mechanism>#</a></h2><details><summary><strong class=custom-details-title>⁉️ 什么是注意力机制（Attention Mechanism）？其中的Q，K，V都代表什么？</strong></summary><div class=markdown-inner><h2><b>什么是注意力机制（Attention Mechanism）？其中的Q，K，V都代表什么？</b></h2><p><strong>注意力机制（Attention Mechanism）</strong> 的核心思想是将 <strong>输入看作键-值对的数据库</strong>，并 <strong>基于查询计算注意力权重 (attention weights)</strong>，可以动态地选择哪些输入部分（例如词语或特征）最为重要。注意力机制定义如下：</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[
\textrm{Attention}(\mathbf{q}, \mathcal{D}) \stackrel{\textrm{def}}{=} \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i,
\]</span><ul><li>这里的 q，<strong>查询（Query）</strong> 是当前模型试图“寻找”或“关注”的目标。它是一个向量，代表了你想匹配的信息。</li><li>公式中的 k，<strong>键（key）</strong> 是所有潜在匹配目标的特征表示。每个键对应于输入中的一个元素，表示这个元素的特性或身份。</li><li>其中的 v，<strong>值（value）</strong> 是和键一起存储的信息，也是最终被提取的信息。注意力机制的目标是通过查询和键找到最相关的值。</li></ul><blockquote class="book-hint warning"><p>注意力机制的 <strong>一般步骤</strong> 为：</p><ol><li><strong>对查询和每个键计算相似度</strong>。（<strong>⚠️注意</strong>：Transformer在比较相似性时不是像 Embedding 后一样通过手动定义相似性方法（如余弦相似度，kernel）的比较 vector，而是使用自注意力（Self-Attention），它不是直接定义相似度，而是 <strong>让神经网络自己学习“什么是相似”</strong>。在提升模型表现，调整 weight 的过程中，模型利用自注意力（Self-Attention）最终学会了：哪些 Query 和哪些 Key 需要匹配。）</li><li><strong>对这些相似度进行归一化（通常使用 Softmax 函数）</strong>。归一化后的结果称为注意力权重（Attention Weights）。</li><li><strong>将注意力权重与对应的值相乘</strong>，得到一个加权求和结果。这个结果就是当前查询的输出。</li></ol></blockquote><p>在 transformer 中，Query (Q)、Key (K) 和 Value (V) 都是从输入嵌入（embedding）中线性变换得到的。它们的计算方式如下：</p><span>\[
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
\]</span><p><strong>得到 Q，K，V 的过程 相当于经历了一次线性变换</strong>。Attention不直接使用 X 而是使用经过矩阵乘法生成的这三个矩阵，因为使用三个可训练的参数矩阵，可增强模型的拟合能力。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> 这里的</p><p>$$
X ∈ R^{B \times L \times d_{model}}
$$</p><p>其中 <code>B: Batch_size</code>，<code>L: Context_Window_size</code>，<code>d_{model}: Embedding size</code>。通过和</p><p>$$
W_Q ∈ R^{d_{model} \times d_q}, W_K ∈ R^{d_{model} \times d_k}, W_V ∈ R^{d_{model} \times d_v}
$$</p><p>三个 weight matrix，将原本的 embedded 信息变换到新的</p><p>$$
Q ∈ R^{B \times L \times d_q}, K ∈ R^{B \times L \times d_k}, V ∈ R^{B \times L \times d_v}
$$
中。在标准实现中，通常令 <code>d_q = d_k = d_v = d_{model} / h</code>，其中 <code>h</code> 是 head 数（multi-head attention）。</p></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 关于点积的理解？</strong></summary><div class=markdown-inner><h2><b>关于点积的理解？</b></h2><p>在 Self-Attention 机制中，相似性本质上是由 点积（Dot Product） 计算得出的，它用于衡量词向量（embedding）之间的关系。</p><p>$$
\mathbf{x} \cdot \mathbf{y} = x_0 y_0 + x_1 y_1 + \dots + x_n y_n
$$</p><p>$$
如 \mathbf{a} \cdot \mathbf{b} = (1)(4) + (3)(2) = 4 + 6 = 10
$$</p><blockquote class="book-hint warning"><p><strong>Note：</strong> 点乘的几何意义是：<code>x</code> 在 <code>y</code> 方向上的投影再与 <code>y</code> 相乘，反映了两个向量的相似度。<strong>点乘结果越大，表示两个向量越相似</strong>。</p></blockquote><p>一个矩阵 <code>X</code> 由 <code>n</code> 行向量组成。比如，我们可以将某一行向量 <strong><code>V_i</code> 理解成一个词的词向量</strong>，共有 <code>n</code> 个行向量组成 <code>n×n</code> 的方形矩阵：</p><p>$$
V_0 =
\begin{bmatrix}
v_{00}, v_{01}, \dots, v_{0d_{model}}
\end{bmatrix}
$$</p><span>\[
V =
\begin{bmatrix}
V_0 \\
V_1 \\
\vdots \\
V_n
\end{bmatrix},
V^\top =
\begin{bmatrix}
V_0^\top & V_1^\top & \dots & V_n^\top
\end{bmatrix}
\]</span><p>矩阵相乘计算如下：</p><span>\[
VV^\top =
\begin{bmatrix}
V_0 \cdot V_0 & V_0 \cdot V_1 & \dots & V_0 \cdot V_n \\
V_1 \cdot V_0 & V_1 \cdot V_1 & \dots & V_1 \cdot V_n \\
\vdots & \vdots & \ddots & \vdots \\
V_n \cdot V_0 & V_n \cdot V_1 & \dots & V_n \cdot V_n
\end{bmatrix}
\]</span><p>以 <code>VV^T</code> 中的第一行第一列元素为例，其实是向量 <code>V_0</code> 与 <code>V_0</code> 自身做点乘，其实就是 <code>V_0</code> 自身与自身的相似度，那第一行第二列元素就是 <code>V_0</code> 与 <code>V_1</code> 之间的相似度。</p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 什么是缩放点积注意力（Scaled Dot-Product Attention）？其中的过程是什么？</strong></summary><div class=markdown-inner><h2><b>什么是缩放点积注意力（Scaled Dot-Product Attention）？其中的过程是什么？</b></h2><p>缩放点积注意力（Scaled Dot-Product Attention） 是 Transformer 结构中的核心机制之一，它用于计算查询（Query）、键（Key）和值（Value）之间的注意力分数，以捕捉序列中不同位置的关联性。其数学公式为：</p><p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$</p><ol><li>在计算过程中，首先对查询矩阵 Q 和键矩阵 K 进行点积（Dot Product），得到注意力得分（Attention Scores）。<strong>这个点积运算的本质是衡量 查询向量（Query） 和 键向量（Key） 之间的相似度。</strong></li></ol><p>$$
S = QK^T \in \mathbb{R}^{L \times L}
$$</p><blockquote class="book-hint warning"><p><strong>Note：</strong> 查询矩阵 Q 和键矩阵 K 进行点积的结果 - LxL 的矩阵，可以理解为在 Context Window 为 L 的文本中，<strong>每一个位置的 token i，去“问”整句话里的每个 token（包括它自己），打分每个 token 的重要性</strong>。</p><p>在 Softmax 后就变成每行是一个 <strong>关注分布</strong>。</p></blockquote><ol start=2><li>之后，<strong>Softmax 作用于Q，K计算出的相似度得分，以将其转换为概率分布</strong>，使其满足：</li></ol><ul><li><strong>归一化（Normalization）</strong>：确保所有注意力权重总和为 1，便于解释。</li><li>放大差异（Sharpening）：通过指数运算增强高相关性词的权重，抑制低相关性词。</li></ul><p>$$
A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)
$$</p><ol start=3><li>最后将 Softmax 的结果（矩阵 A）和矩阵 V 加权求和，每一行输出是当前 token 从整句话中“拉取”的语义信息。</li></ol><ul><li><strong>Q 与 K 的相似度</strong> → 决定应该「从谁那获取信息」</li><li><strong>V 储存的是语义信息</strong>（比如词性、上下文、含义等）</li><li><strong>最后加权融合</strong> → 得到的是上下文感知的语义表示</li></ul><p>这一步是 Transformer 的“信息流动”核心—— <strong>你不是只看自己，而是看整句话对你有意义的部分，然后合成一个更丰富的表示。</strong></p><p>$$
\text{Output} = A \cdot V \in \mathbb{R}^{L \times d_v}
$$</p><ul><li><p><code>L</code> 表示输入序列的 token 数量，即 <strong>每个 token 都有一个上下文增强的表示</strong>。</p></li><li><p><code>d_v</code> 是值向量的维度，表示每个 token 的信息表示的维度。</p></div></li></ul></details><hr><details><summary><strong class=custom-details-title>⁉️ 为什么缩放点积注意力（Scaled Dot-Product Attention）要除以 √d？</strong></summary><div class=markdown-inner><h2><b>为什么缩放点积注意力（Scaled Dot-Product Attention）要除以 √d？</b></h2><p>缩放点积注意力的计算过程如下：</p><p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$</p><p>设想 Query 和 Key 是随机向量，维度为 <code>d_k</code>。假设它们的每个元素都是 0 均值，单位方差的正态分布。那么：</p><p>$$
Q \cdot K = \sum_{i=1}^{d_k} q_i k_i
$$</p><p>这个点积的期望为 0，但方差是：</p><p>$$
\text{Var}(Q \cdot K) = d_k
$$</p><p>也就是说，当维度越大，点积的结果可能会随着 <strong>d_k（Key 维度的大小）增加而变大</strong>。将点积作为输入传递给 Softmax 函数时，Softmax 对大数值特别敏感，因为它会根据输入的相对大小来计算概率值。如果点积值变得非常大，<strong>Softmax 会让其中一些值的输出接近 1，而其他值接近 0</strong>，这会 导致计算不稳定或梯度消失等问题。</p><p>因此，在应用 Softmax 之前，需要对注意力得分进行缩放，即除以 √d_k，这样可以防止梯度消失或梯度爆炸问题，提高训练稳定性。除以 √d_k 相当于把方差从 d_k 降为 1，使得输入的数值规模保持稳定：</p><p>$$
\text{Var}\left( \frac{Q \cdot K}{\sqrt{d_k}} \right) = 1
$$</p><p><strong>这个缩放的主要目的是 将点积的方差控制在一个合理的范围，使其不随向量维度的增加而变得过大。</strong></p></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 什么是自注意力（Self-Attention）机制？它与传统注意力有何区别？</strong></summary><div class=markdown-inner><h2><b>什么是自注意力（Self-Attention）机制？它与传统注意力有何区别？</b></h2><p>Self-Attention（自注意力） 和 一般 Attention（注意力机制） 的 <strong>核心计算原理是相同的</strong>，都是通过 Query（Q） 和 Key（K） 计算相似度分数，再对 Value（V） 进行加权求和。但它们的区别在于作用目标不同：</p><ul><li><p><strong>Self-Attention（自注意力）</strong></p><ul><li><strong>Q、K、V 都来自同一个输入序列 X ，即 自身内部计算注意力</strong>，挖掘序列中不同位置之间的关系。例如，在 Transformer 的 Encoder 里，每个单词都和句子中的所有单词计算注意力。</li></ul></li><li><p><strong>General Attention（通用注意力，通常用于 Seq2Seq 结构）</strong></p><ul><li><strong>Q 和 K、V 来自不同的地方</strong>，通常是 Q 来自 Decoder，而 K、V 来自 Encoder，用于建立 Encoder 和 Decoder 之间的联系。例如，在机器翻译中，Decoder 生成当前词时，会对 Encoder 编码的所有词计算注意力，从而获取最相关的信息。</li></ul></div></li></ul></details><hr><details><summary><strong class=custom-details-title>⁉️ 计算自注意力机制的时间和空间复杂度，分析其瓶颈。</strong></summary><div class=markdown-inner><h2><b>计算自注意力机制的时间和空间复杂度，分析其瓶颈。</b></h2><p>自注意力机制（Self-Attention Mechanism）的时间复杂度（Time Complexity）和空间复杂度（Space Complexity）主要受输入序列长度 n 影响。在标准的 Transformer 结构中，每个 Self-Attention Layer 计算 注意力权重（Attention Weights） 需要进行矩阵乘法，计算 Query Q 和 Key K 之间的点积并进行 Softmax 归一化。</p><p>其中， Q 和 K 的维度均为 (n x d_k) ，计算 QK^T 需要 O(n^2 d_k) 次乘法运算，而应用 Softmax 需要 O(n^2) 的额外计算，因此 <strong>整体时间复杂度为</strong>：</p><span>\[
O(n^2 d_k)
\]</span><p>Self-Attention 计算过程中，需要存储 <strong>注意力权重矩阵（ n x n ），此外还需要存储 中间结果（如 Softmax 输出、梯度）</strong>，使得 <strong>空间复杂度达到</strong>：</p><span>\[
O(n^2 + n d_k)
\]</span><ul><li><p><strong>瓶颈分析（Bottleneck Analysis）</strong></p><ol><li><strong>计算瓶颈（Computational Bottleneck）</strong>：由于 Self-Attention 需要 O(n^2 d_k) 的计算量，因此在超长文本（如 10K 以上 Token）上，计算成本极高，推理速度变慢。</li><li><strong>内存瓶颈（Memory Bottleneck）</strong>：存储 O(n^2) 的注意力权重矩阵会 占用大量显存（VRAM），限制了可处理的最大序列长度。</li><li><strong>长序列扩展性差（Scalability for Long Sequences）</strong>：当 n 增大时，Transformer 计算复杂度随 n^2 级增长，难以应用于长文本建模。</li></ol></div></li></ul></details><hr><details><summary><strong class=custom-details-title>⁉️ 为什么需要多头注意力（Multi-Head Attention）？多头设计如何提升模型表达能力？</strong></summary><div class=markdown-inner><h2><b>为什么需要多头注意力（Multi-Head Attention）？多头设计如何提升模型表达能力？</b></h2><p>多头注意力（Multi-Head Attention）是 Transformer 结构中的关键组件，它通过多个独立的注意力头来提升模型的表达能力。其核心思想是 <strong>让模型在不同的子空间（Subspaces）中独立学习不同的特征表示，而不是仅依赖单一注意力机制</strong>。例如可以关注不同类型的关系（如语法关系、语义关系、长距离依赖等）。</p><div align=center><img src=/images/multi-head-attention.svg width=450px/></div><p>在计算过程中，输入序列的特征矩阵首先经过线性变换，生成查询（Query, Q）、键（Key, K）、和值（Value, V）。然后，<strong>每个注意力头都会独立地对 Q、K、V 进行投影</strong>，将其拆分成多个低维子空间，即：</p><span>\[
\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v},
\]</span><p>其中 W_i^q, W_i^k, W_i^v 是可训练的投影矩阵，每个头都对应一组独立的参数。随后，每个头分别执行 Scaled Dot-Product Attention（缩放点积注意力）。计算完成后，各个头的注意力输出会被拼接（Concatenation），然后通过一个最终的线性变换矩阵 W^o 进行映射：</p><span>\[
\begin{split}\mathbf W_o \begin{bmatrix}\mathbf h_1\\\vdots\\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}.\end{split}
\]</span><p>这样，多头注意力的最终输出仍然保持与输入相同的维度，同时融合了来自多个注意力头的信息，提高了模型对不同层次语义的建模能力。</p></div></details><hr><h2 id=位置编码positional-encoding><strong>位置编码（Positional Encoding）</strong>
<a class=anchor href=#%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81positional-encoding>#</a></h2><details><summary><strong class=custom-details-title>⁉️ 为什么 Transformer 需要位置编码？纯 Self-Attention 为何无法感知位置信息？</strong></summary><div class=markdown-inner><h2><b>为什么 Transformer 需要位置编码？纯 Self-Attention 为何无法感知位置信息？</b></h2><p>在 Transformer 模型中，位置编码（Position Encoding）是用于注入位置信息的关键机制，因为模型本身的 Self-Attention 机制无法感知输入序列中元素的顺序或位置。Transformer 通过 Self-Attention 计算序列中各元素之间的关系，每个元素的表示（representation）由其与其他所有元素的相互作用决定。然而， <strong>Self-Attention 本身是位置无关的（position-independent）</strong>，即它并不考虑元素在序列中的相对或绝对位置。因此，如果不显式地引入位置编码，<strong>模型就无法了解输入序列的顺序信息</strong>。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> 我们在使用 PE 时关注的是词语之间的 <strong>相对关系，而不是绝对位置</strong>。因为Transformer 结构没有像 RNN 那样的 <strong>顺序处理能力</strong>，所以我们必须显式告诉它词语的位置信息。</p></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 绝对位置编码（Absolute PE）和相对位置编码（Relative PE）的核心区别是什么？</strong></summary><div class=markdown-inner><h2><b>绝对位置编码（Absolute PE）和相对位置编码（Relative PE）的核心区别是什么？</b></h2><p>绝对位置编码（Absolute Position Encoding, Absolute PE）和相对位置编码（Relative Position Encoding, Relative PE）的核心区别在于它们对序列中单词位置的表示方式。<strong>绝对位置编码是基于序列中单词的固定位置来定义每个单词的位置编码</strong>，这些编码是通过对每个位置进行显式编码（例如使用正弦和余弦函数）来获得的。这意味着 <strong>每个位置的编码是固定的，与其他词汇之间的相对关系无关</strong>。简单来说，<strong>绝对位置编码的设计是通过为每个位置分配唯一的标识符来捕捉顺序信息</strong>。绝对位置编码被广泛用于 Transformer 模型中，如原始的 Transformer 和 BERT，这些模型通过对输入的词汇序列和其位置编码的加和来保留词汇的顺序信息。</p><p>相对位置编码则是通过 <strong>考虑单词之间的相对位置来计算每个单词的编码，而不是单纯地依赖于其绝对位置</strong>。在这种方法中，<strong>位置编码的更新基于词语之间的相对距离，因此它能捕捉到不同词之间的相对关系</strong>，而不仅仅是它们在序列中的固定位置。相对位置编码的一个例子是 Transformer-XL 模型，它通过引入相对位置编码来克服标准 Transformer 在处理长序列时存在的记忆限制问题，从而提升了对长距离依赖的建模能力。</p><p>尽管在某些情况下，相对位置编码可以通过绝对位置得到（例如，简单地计算位置差），但这种方法仍然有限。<strong>相对位置编码有以下优势</strong>：</p><ol><li><strong>灵活性和泛化性</strong>：相对位置编码使得模型能够处理不同长度的输入，而绝对位置编码依赖于固定的输入长度。这意味着在不同任务或不同数据集上，使用相对位置编码的模型能够更好地进行泛化，尤其是在处理较长序列时。</li><li><strong>更好的长距离依赖建模</strong>：相对位置编码能够更有效地捕捉长距离的依赖关系，因为它直接反映了词汇间的相对关系，而绝对位置编码则对远距离的依赖建模较弱，尤其是在长序列的上下文中。</li><li><strong>减少位置编码的冗余</strong>：在传统的绝对位置编码中，序列中的每个位置都有唯一的编码，且这些编码是全局固定的，而相对位置编码只关心词汇间的相对位置，从而避免了位置编码的冗余，尤其是在处理非常长的序列时。</li></ol></div></details><hr><details><summary><strong class=custom-details-title>⁉️ 为什么不直接用位置索引（如idx=1,2,3,...）作为位置编码？</strong></summary><div class=markdown-inner><h2><b>为什么不直接用位置索引（如idx=1,2,3,...）作为位置编码？</b></h2><p>假设用位置索引直接作为编码，例如：</p><pre tabindex=0><code>位置1 → 编码为[1]
位置2 → 编码为[2]
位置3 → 编码为[3]
</code></pre><p>这种方式存在以下问题：</p><ol><li><strong>数值范围不受控</strong><ul><li>索引值会随着序列长度的增加无限增长（例如，序列长度100时位置编码为100），导致数值过大。</li><li>深度学习模型（尤其是基于梯度的优化）对输入的范围敏感，过大的值可能破坏训练的稳定性。</li></ul></li><li><strong>无法泛化到未见过的序列长度</strong><ul><li>如果模型在训练时只见过长度为512的序列，而测试时遇到长度为1024的序列，直接用idx会导致位置编码超出训练时的范围，模型无法处理。</li></ul></li><li><strong>无法表示相对位置</strong><ul><li>绝对位置索引（如1和2）无法直接表达相对距离（如“相邻”或“间隔3”）。</li><li>例如，位置 <code>2-1=1</code> 和位置 <code>100-99=1</code> 的差值相同，但它们的语义关联可能完全不同。</li></ul></li></ol><blockquote class="book-hint warning"><p><strong>Note：</strong> 位置编码的本质作用，就是在 <strong>点积（dot-product）这个步骤里，将位置信息“掺和进”模型的注意力计算，并不是直接告诉模型“这是第5个词”</strong>，而是通过数学结构，引导模型在点积阶段感知相对位置的关系。<strong>如果直接用位置索引，在点积后会丢失位置信息。</strong></p></blockquote></div></details><hr><details><summary><strong class=custom-details-title>⁉️ Sinusoidal 位置编码的公式是什么？为什么选择正弦/余弦函数组合？</strong></summary><div class=markdown-inner><h2><b>Sinusoidal 位置编码的公式是什么？为什么选择正弦/余弦函数组合？</b></h2><p>Sinusoidal 位置编码（Sinusoidal Positional Encoding） 是 Transformer 模型中用于捕捉序列中单词位置的一种方法，是常见的绝对位置编码（Absolute Position Encoding）方法。Sinusoidal 位置编码通过正弦和余弦函数的组合来生成每个位置的唯一向量，这些向量与输入的词嵌入（Word Embedding）相加，从而使模型能够学习到每个单词在序列中的位置。Sinusoidal 位置编码使用相同形状的位置嵌入矩阵 P 输出 X+P，其元素按以下公式生成：</p><span>\[
\begin{split}\begin{aligned} p_{i, 2j} &= \sin\left(\frac{i}{10000^{2j/d}}\right),\\p_{i, 2j+1} &= \cos\left(\frac{i}{10000^{2j/d}}\right).\end{aligned}\end{split}
\]</span><ul><li><p><strong>为什么选择正弦/余弦函数组合？</strong></p><ol><li><strong>不同频率的周期性</strong>：正弦和余弦函数有不同的频率，使得每个位置的编码在不同维度上具有不同的周期。这种周期性使得模型可以通过不同频率的变化来学习相对位置关系。通过正弦和余弦函数的组合，位置编码能够覆盖较长序列的不同范围，模型可以捕捉到全局和局部的位置信息。</li><li><strong>无重复的唯一表示</strong>：正弦和余弦函数的组合能够确保每个位置有一个独特的编码，这些编码在向量空间中是可区分的，能够提供丰富的位置信息。而且由于这两种函数的周期性和无穷制性质，不同位置的编码不会重复。</li><li><strong>容易计算和扩展</strong>：正弦和余弦函数的计算非常简单且高效。它们无需额外的学习参数，且可以通过简单的公式根据位置直接计算得出。这样的位置编码方式能够在大规模数据中有效应用，同时支持较长序列的处理。</li><li><strong>支持相对位置关系</strong>：这种编码方法能够通过比较不同位置的编码来推测它们之间的相对距离和顺序，尤其是在模型学习到的位置编码与实际任务（如机器翻译、文本生成）相关时，正弦/余弦函数的变化有助于保持序列的结构和信息流动。</li></ol></div></li></ul></details><hr><details><summary><strong class=custom-details-title>⁉️ 为什么位置编码可以直接与词向量逐元素相加？位置编码会破坏词向量的语义空间吗？</strong></summary><div class=markdown-inner><h2><b>为什么位置编码可以直接与word embedding逐元素相加？位置编码会破坏词向量的语义空间吗？</b></h2><p>Word embedding 主要表示单词的语义信息。Positional Encoding 提供额外的位置信息，以弥补 Transformer 结构中缺少序列顺序感的问题。相加的效果 是让同一个词（如 “apple”）在不同的位置有略微不同的表示，但仍保留其主要的语义信息。</p><p>虽然位置嵌入矩阵 P 与词向量 X 直接相加，但在 transformer 获得 Query (Q)、Key (K) 和 Value (V)的线形变化过程中（i.e. Q = XW_q），在学习 Weight 的过程中会将语义和位置信息分别投射在不同的维度上。Positional Encoding 并不需要通过训练来学习，<strong>它是固定的、基于位置的函数，因此不干扰原本的语义信息</strong>。</p><p>例如输入 token 的向量是：</p><p>$$
X_i = \text{WordEmbedding}(i) + \text{PositionalEncoding}(i)
$$</p><blockquote class="book-hint warning"><p><strong>Note：</strong> 位置编码与词向量的相加（在这个点，词向量和位置编码已经不可分辨地融合在了一起，对于模型来说，这就是一个“带位置信息的词语表示”），<strong>本质上就是把“位置信息”混入词的表达空间，形成一个新的、更丰富的特征空间</strong>。模型并不需要“知道”两者的来源，而是会 <strong>自动学习如何从这个混合后的向量中解读出信息</strong>。</p><p>Transformer的注意力机制和FeedForward层，本质上是线性变换 + 非线性激活的堆叠。只要输入的特征是连续的、可学习的，<strong>模型可以自己从统计规律中拆解出“这是位置的贡献”还是“这是语义的贡献”</strong>。很多论文都验证过这个现象：</p><ul><li>如果用不同方式编码位置（比如加、拼接、乘法），最终模型的表现差别很小。</li><li>因为Transformer可以通过权重学习，把位置特征和词义特征解耦，或者做融合。</li></ul><p><strong>e.g.</strong> 好比听音乐，左声道和右声道的声音混在一起，你不用知道左右声道分别是什么，依然能听出音乐的立体感。模型也是一样，输入的 <code>X_i = E_i + PE_i</code> 就像混音，学习的过程就是在拆解、重构信号。</p></blockquote><p>其中 <code>PositionalEncoding</code> 对应 <code>sinusoidal</code> 编码：</p><p>$$
PE_{pos, 2i} = \sin \left( \frac{pos}{10000^{2i/d}} \right),
PE_{pos, 2i+1} = \cos \left( \frac{pos}{10000^{2i/d}} \right)
$$</p><p>到注意力打分阶段，每个 token 被映射为：</p><p>$$
Q_i = W_Q X_i,
K_j = W_K X_j
$$</p><p>它们的点积：</p><p>$$
Q_i \cdot K_j = (W_Q (E_i + PE_i)) \cdot (W_K (E_j + PE_j))
$$</p><p>展开：</p><p>$$
= (W_Q E_i) \cdot (W_K E_j) + (W_Q E_i) \cdot (W_K PE_j) + (W_Q PE_i) \cdot (W_K E_j) + (W_Q PE_i) \cdot (W_K PE_j)
$$</p><table><thead><tr><th>项目</th><th>含义</th></tr></thead><tbody><tr><td>$$(W_Q E_i) \cdot (W_K E_j)$$</td><td>💡词义相似度项：纯靠词向量计算的语义相关性。如果 <code>E_i</code> 和 <code>E_j</code> 意义接近，这一项会大。</td></tr><tr><td>$$(W_Q E_i) \cdot (W_K PE_j)$$</td><td>⚡词对位置敏感项：查询是词，键是位置。代表“词 <code>i</code> ”是否偏好关注某种位置的 <code>j</code>。</td></tr><tr><td>$$(W_Q PE_i) \cdot (W_K E_j)$$</td><td>⚡位置对词的敏感项：查询是位置，键是词。代表“我在这个位置”是否想关注语义为 <code>E_j</code> 的词。</td></tr><tr><td>$$(W_Q PE_i) \cdot (W_K PE_j)$$</td><td>📏相对位置信息项：纯位置信息，捕捉 token 之间的空间关系，决定远近感知。</td></tr></tbody></table><p>位置编码部分：</p><ul><li><code>PE_i</code> 和 <code>PE_j</code> 会直接进入点积结果。</li><li>因为 <code>PE_i</code> 本身带有顺序的波动特征，两个位置的 <code>PE</code> 相似度决定了最终的 QK 值。</li></ul><p>也就是说：</p><ol><li>如果两个 token 相邻， <code>PE_i</code> 和 <code>PE_j</code> 非常接近。</li><li>这样它们的点积值更高，<code>Softmax</code> 权重更大。</li><li>反之，距离越远，点积下降，权重越小。</li></ol></div></details><hr><h2 id=transformer-模型架构细节><strong>Transformer 模型架构细节</strong>
<a class=anchor href=#transformer-%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84%e7%bb%86%e8%8a%82>#</a></h2></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#注意力机制attention-mechanism><strong>注意力机制（Attention Mechanism）</strong></a></li><li><a href=#位置编码positional-encoding><strong>位置编码（Positional Encoding）</strong></a></li><li><a href=#transformer-模型架构细节><strong>Transformer 模型架构细节</strong></a></li></ul></nav></div></aside></main></body></html>