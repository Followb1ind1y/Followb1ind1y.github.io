<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  卷积神经网络（Convolutional Neural Networks）
  #

卷积神经网络（CNN）的必要性在于它能够高效地处理具有高维结构的数据，特别是图像数据。在处理如图像这样的高维感知数据时，传统的多层感知机（MLP）存在局限性，因为它没有考虑到数据中的空间结构，导致在图像分类等任务中，参数量和计算开销巨大，训练起来非常不实用。举例来说，在猫狗图片分类任务中，使用百万像素的图像作为输入时，全连接层将产生数量庞大的参数，这不仅需要大量的计算资源，还可能导致过拟合。因此，卷积神经网络通过局部连接和权重共享的方式有效减少了参数数量，利用图像本身的空间结构进行特征提取，使得网络能够在较少的参数下仍能有效学习图像中的模式和特征。这种方式不仅极大地减少了计算复杂度，还能在图像处理任务中取得显著的性能提升。


  从全连接层到卷积（From FC Layer to Convolutional）
  #

想象一下，假设我们想从一张图片中找到某个物体。 合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。卷积神经网络正是将 空间不变性（spatial invariance） 的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。


什么是不变性？ 不变性是指模型对某些输入变化（如平移、旋转、缩放等）保持输出的一致性。例如，图像中的物体移动、缩放不应影响分类结果。
为什么不变性重要？

现实场景中的数据变化： 如图像中的摄像机角度、光线、物体位置变化。
提升泛化能力： 通过不变性，模型能够适应更多样化的输入数据。




卷积（Convolution）引入了三个重要的概念，这些概念极大地提升了机器学习系统的效率和性能：稀疏交互（sparse interactions）、参数共享（parameter sharing）以及等变表示（equivariant representations）。此外，卷积还为处理可变大小的输入提供了有效的方法。


稀疏交互（Sparse Interactions）：
在传统的全连接神经网络中，每个输入单元都与每个输出单元直接相连，这种完全连接的结构会导致参数数量巨大且计算量庞大。而卷积通过使用卷积核（kernel）仅与局部区域交互，大大减少了连接的数量。这种稀疏交互能够显著降低计算复杂度，同时保留局部特征的关键信息。例如，
  \(3 \times 3\)

 的卷积核只需与输入图像的一小部分进行操作，而不是整个图像。


参数共享（Parameter Sharing）：
在卷积操作中，卷积核的权重在整个输入中是共享的。这意味着，同一个卷积核在输入图像的不同位置应用相同的权重，从而大幅减少了参数数量。这种共享机制不仅降低了模型复杂度，还提高了模型的泛化能力，因为共享的卷积核能够捕获输入中的通用模式（如边缘、角点等）。


等变表示（Equivariant Representations）：
卷积具有平移等变性（translation equivariance），这意味着如果输入发生平移，卷积操作的输出会以相应方式平移。这种特性非常适合处理图像等感知数据，因为目标的空间位置可能会有所变化，但其本质特征保持不变。等变性使得模型能够在不同位置识别相同的模式，从而增强了模型的鲁棒性。


支持可变大小输入（Variable Size Inputs）：
卷积操作可以灵活地处理不同尺寸的输入。这是因为卷积核是基于局部区域滑动的，不依赖输入的绝对尺寸。例如，无论输入图像是 
  \(128 \times 128\)

 还是 
  \(256 \times 256\)

，卷积核都可以逐步滑动并提取特征。这种能力使卷积神经网络非常适合处理变长或变形的数据，如图片、音频或序列信号。




  多层感知机（MLP）的限制
  #


平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。例如，图像中的一只猫无论位于左上角还是右下角，模型都应能够正确识别。

MLP的局限性：

完全连接的结构：在MLP中，每个神经元与所有输入特征相连，因此模型对输入的每个像素赋予独立的权重。换句话说，特定像素的特征无法被直接用于识别全局的物体特征。如果输入图像中的物体位置发生平移，输入像素值会重新映射到不同的神经元，导致特征表示的变化。
缺乏空间归纳偏置：MLP无法内嵌有关输入空间结构的先验知识（如相邻像素间的关系），因此需要依赖大量数据学习这些模式。


影响：

MLP对图像特征的空间位置高度敏感，难以通过有限数据泛化到不同的物体位置。
模型需要对每种可能的位置分别学习特征表示，这导致数据需求和计算开销成倍增加。




局部性（locality）：局部性是指数据中的局部区域通常具有更强的相关性，例如图像中相邻像素往往属于同一物体或边缘。

MLP的局限性：

忽略局部相关性：MLP的每个神经元对整个输入空间具有感知能力，但无法重点关注局部区域的特征交互。例如，在图像中，MLP无法直接识别相邻像素构成的边缘或纹理。这种全局感知的特性导致模型在提取局部模式（如边缘或角点）时效率低下。
参数冗余：MLP为每个输入特征分配独立的权重，因此即使相邻像素之间存在高度相关性，模型仍需单独学习这些特征，造成参数冗余和过拟合风险。


影响：

MLP难以捕捉高维数据中的局部模式，尤其是在输入维度较高时（如图像或语音）。
局部特征无法有效提取，导致模型在特征表达能力上不足。







  图像卷积（Convolutions for Images）
  #

卷积（Convolution）是卷积神经网络（CNN）的核心操作，用于 从输入数据（如图像）中提取特征模式。"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/deep-learning/convolutional-neural-networks/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Convolutional Neural Networks"><meta property="og:description" content="卷积神经网络（Convolutional Neural Networks） # 卷积神经网络（CNN）的必要性在于它能够高效地处理具有高维结构的数据，特别是图像数据。在处理如图像这样的高维感知数据时，传统的多层感知机（MLP）存在局限性，因为它没有考虑到数据中的空间结构，导致在图像分类等任务中，参数量和计算开销巨大，训练起来非常不实用。举例来说，在猫狗图片分类任务中，使用百万像素的图像作为输入时，全连接层将产生数量庞大的参数，这不仅需要大量的计算资源，还可能导致过拟合。因此，卷积神经网络通过局部连接和权重共享的方式有效减少了参数数量，利用图像本身的空间结构进行特征提取，使得网络能够在较少的参数下仍能有效学习图像中的模式和特征。这种方式不仅极大地减少了计算复杂度，还能在图像处理任务中取得显著的性能提升。
从全连接层到卷积（From FC Layer to Convolutional） # 想象一下，假设我们想从一张图片中找到某个物体。 合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。卷积神经网络正是将 空间不变性（spatial invariance） 的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。
什么是不变性？ 不变性是指模型对某些输入变化（如平移、旋转、缩放等）保持输出的一致性。例如，图像中的物体移动、缩放不应影响分类结果。 为什么不变性重要？ 现实场景中的数据变化： 如图像中的摄像机角度、光线、物体位置变化。 提升泛化能力： 通过不变性，模型能够适应更多样化的输入数据。 卷积（Convolution）引入了三个重要的概念，这些概念极大地提升了机器学习系统的效率和性能：稀疏交互（sparse interactions）、参数共享（parameter sharing）以及等变表示（equivariant representations）。此外，卷积还为处理可变大小的输入提供了有效的方法。
稀疏交互（Sparse Interactions）：
在传统的全连接神经网络中，每个输入单元都与每个输出单元直接相连，这种完全连接的结构会导致参数数量巨大且计算量庞大。而卷积通过使用卷积核（kernel）仅与局部区域交互，大大减少了连接的数量。这种稀疏交互能够显著降低计算复杂度，同时保留局部特征的关键信息。例如， \(3 \times 3\) 的卷积核只需与输入图像的一小部分进行操作，而不是整个图像。
参数共享（Parameter Sharing）：
在卷积操作中，卷积核的权重在整个输入中是共享的。这意味着，同一个卷积核在输入图像的不同位置应用相同的权重，从而大幅减少了参数数量。这种共享机制不仅降低了模型复杂度，还提高了模型的泛化能力，因为共享的卷积核能够捕获输入中的通用模式（如边缘、角点等）。
等变表示（Equivariant Representations）：
卷积具有平移等变性（translation equivariance），这意味着如果输入发生平移，卷积操作的输出会以相应方式平移。这种特性非常适合处理图像等感知数据，因为目标的空间位置可能会有所变化，但其本质特征保持不变。等变性使得模型能够在不同位置识别相同的模式，从而增强了模型的鲁棒性。
支持可变大小输入（Variable Size Inputs）：
卷积操作可以灵活地处理不同尺寸的输入。这是因为卷积核是基于局部区域滑动的，不依赖输入的绝对尺寸。例如，无论输入图像是 \(128 \times 128\) 还是 \(256 \times 256\) ，卷积核都可以逐步滑动并提取特征。这种能力使卷积神经网络非常适合处理变长或变形的数据，如图片、音频或序列信号。
多层感知机（MLP）的限制 # 平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。例如，图像中的一只猫无论位于左上角还是右下角，模型都应能够正确识别。 MLP的局限性： 完全连接的结构：在MLP中，每个神经元与所有输入特征相连，因此模型对输入的每个像素赋予独立的权重。换句话说，特定像素的特征无法被直接用于识别全局的物体特征。如果输入图像中的物体位置发生平移，输入像素值会重新映射到不同的神经元，导致特征表示的变化。 缺乏空间归纳偏置：MLP无法内嵌有关输入空间结构的先验知识（如相邻像素间的关系），因此需要依赖大量数据学习这些模式。 影响： MLP对图像特征的空间位置高度敏感，难以通过有限数据泛化到不同的物体位置。 模型需要对每种可能的位置分别学习特征表示，这导致数据需求和计算开销成倍增加。 局部性（locality）：局部性是指数据中的局部区域通常具有更强的相关性，例如图像中相邻像素往往属于同一物体或边缘。 MLP的局限性： 忽略局部相关性：MLP的每个神经元对整个输入空间具有感知能力，但无法重点关注局部区域的特征交互。例如，在图像中，MLP无法直接识别相邻像素构成的边缘或纹理。这种全局感知的特性导致模型在提取局部模式（如边缘或角点）时效率低下。 参数冗余：MLP为每个输入特征分配独立的权重，因此即使相邻像素之间存在高度相关性，模型仍需单独学习这些特征，造成参数冗余和过拟合风险。 影响： MLP难以捕捉高维数据中的局部模式，尤其是在输入维度较高时（如图像或语音）。 局部特征无法有效提取，导致模型在特征表达能力上不足。 图像卷积（Convolutions for Images） # 卷积（Convolution）是卷积神经网络（CNN）的核心操作，用于 从输入数据（如图像）中提取特征模式。"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Convolutional Neural Networks | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/deep-learning/convolutional-neural-networks/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.8d7dedb748c28c645cbe56c4476a1fa9e07bbcd840fafe24d07b5e0e4b134a1b.js integrity="sha256-jX3tt0jCjGRcvlbER2ofqeB7vNhA+v4k0HteDksTShs=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/deep-learning/convolutional-neural-networks/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><span>Common Libraries</span><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><a href=/docs/deep-learning/convolutional-neural-networks/ class=active>Convolutional Neural Networks</a><ul></ul></li><li><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a><ul></ul></li><li><a href=/docs/deep-learning/computer-vision/>Computer Vision</a><ul></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Convolutional Neural Networks</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#从全连接层到卷积from-fc-layer-to-convolutional><strong>从全连接层到卷积（From FC Layer to Convolutional）</strong></a><ul><li><a href=#多层感知机mlp的限制><strong>多层感知机（MLP）的限制</strong></a></li></ul></li><li><a href=#图像卷积convolutions-for-images><strong>图像卷积（Convolutions for Images）</strong></a><ul><li><a href=#互相关操作the-cross-correlation-operation><strong>互相关操作（The Cross-Correlation Operation）</strong></a></li><li><a href=#卷积层convolutional-layers><strong>卷积层（Convolutional Layers）</strong></a></li><li><a href=#利用学习卷积核实现图像边缘检测><strong>利用学习卷积核实现图像边缘检测</strong></a></li><li><a href=#特征映射和感受野feature-map-and-receptive-field><strong>特征映射和感受野（Feature Map and Receptive Field）</strong></a></li></ul></li><li><a href=#填充padding><strong>填充（Padding）</strong></a></li><li><a href=#步幅stride><strong>步幅（Stride）</strong></a></li><li><a href=#多输入通道和多输出通道multiple-input-and-multiple-output-channels><strong>多输入通道和多输出通道（Multiple Input and Multiple Output Channels）</strong></a><ul><li><a href=#多输入通道multiple-input-channels><strong>多输入通道（Multiple Input Channels）</strong></a></li><li><a href=#多输出通道multiple-output-channels><strong>多输出通道（Multiple Output Channels）</strong></a></li><li><a href=#结合多输入和多输出通道><strong>结合多输入和多输出通道</strong></a></li><li><a href=#11-卷积层><strong>1×1 卷积层</strong></a></li></ul></li><li><a href=#池化层pooling><strong>池化层（Pooling）</strong></a><ul><li><a href=#最大池化层和平均池化层maximum-pooling-and-average-pooling><strong>最大池化层和平均池化层（Maximum Pooling and Average Pooling）</strong></a></li><li><a href=#填充步幅以及多通道padding-stride-and-multiple-channels><strong>填充，步幅以及多通道（Padding, Stride, and Multiple Channels）</strong></a></li><li><a href=#池化的主要作用><strong>池化的主要作用</strong></a></li></ul></li><li><a href=#经典卷积神经网络-modern-convolutional-neural-networks><strong>经典卷积神经网络 （Modern Convolutional Neural Networks）</strong></a><ul><li><a href=#lenet><strong>LeNet</strong></a></li><li><a href=#alexnet><strong>AlexNet</strong></a></li><li><a href=#vgg><strong>VGG</strong></a></li><li><a href=#network-in-network-nin><strong>Network in Network (NiN)</strong></a></li><li><a href=#resnet><strong>ResNet</strong></a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=卷积神经网络convolutional-neural-networks><strong>卷积神经网络（Convolutional Neural Networks）</strong>
<a class=anchor href=#%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9cconvolutional-neural-networks>#</a></h1><p>卷积神经网络（CNN）的必要性在于它能够<strong>高效地处理具有高维结构的数据</strong>，特别是图像数据。在处理如图像这样的高维感知数据时，传统的多层感知机（MLP）存在局限性，因为它没有考虑到数据中的空间结构，导致在图像分类等任务中，<strong>参数量和计算开销巨大</strong>，训练起来非常不实用。举例来说，在猫狗图片分类任务中，使用百万像素的图像作为输入时，<strong>全连接层将产生数量庞大的参数</strong>，这不仅需要大量的计算资源，还可能导致过拟合。因此，卷积神经网络通过局部连接和权重共享的方式有效减少了参数数量，<strong>利用图像本身的空间结构进行特征提取</strong>，使得网络能够在<strong>较少的参数下仍能有效学习图像中的模式和特征</strong>。这种方式不仅极大地减少了计算复杂度，还能在图像处理任务中取得显著的性能提升。</p><hr><h2 id=从全连接层到卷积from-fc-layer-to-convolutional><strong>从全连接层到卷积（From FC Layer to Convolutional）</strong>
<a class=anchor href=#%e4%bb%8e%e5%85%a8%e8%bf%9e%e6%8e%a5%e5%b1%82%e5%88%b0%e5%8d%b7%e7%a7%affrom-fc-layer-to-convolutional>#</a></h2><p>想象一下，假设我们想从一张图片中找到某个物体。 合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。卷积神经网络正是将 <strong>空间不变性（spatial invariance）</strong> 的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。</p><blockquote class="book-hint warning"><ul><li><strong>什么是不变性？</strong> 不变性是指模型对某些输入变化（如平移、旋转、缩放等）<strong>保持输出的一致性</strong>。例如，图像中的物体移动、缩放不应影响分类结果。</li><li><strong>为什么不变性重要？</strong><ul><li><strong>现实场景中的数据变化：</strong> 如图像中的摄像机角度、光线、物体位置变化。</li><li><strong>提升泛化能力：</strong> 通过不变性，模型能够适应更多样化的输入数据。</li></ul></li></ul></blockquote><p>卷积（Convolution）引入了三个<strong>重要的概念</strong>，这些概念极大地提升了机器学习系统的效率和性能：稀疏交互（sparse interactions）、参数共享（parameter sharing）以及等变表示（equivariant representations）。此外，卷积还为处理可变大小的输入提供了有效的方法。</p><ol><li><p><strong>稀疏交互（Sparse Interactions）：</strong></p><p>在传统的全连接神经网络中，每个输入单元都与每个输出单元直接相连，这种<strong>完全连接的结构会导致参数数量巨大且计算量庞大</strong>。而卷积通过使用卷积核（kernel）仅与局部区域交互，大大减少了连接的数量。这种稀疏交互能够显著<strong>降低计算复杂度</strong>，同时保留局部特征的关键信息。例如，<span>
\(3 \times 3\)
</span>的卷积核只需与输入图像的一小部分进行操作，而不是整个图像。</p></li><li><p><strong>参数共享（Parameter Sharing）：</strong></p><p>在卷积操作中，卷积核的<strong>权重在整个输入中是共享的</strong>。这意味着，<strong>同一个卷积核在输入图像的不同位置应用相同的权重</strong>，从而大幅减少了参数数量。这种共享机制不仅降低了模型复杂度，还提高了模型的泛化能力，因为共享的卷积核能够捕获<strong>输入中的通用模式</strong>（如边缘、角点等）。</p></li><li><p><strong>等变表示（Equivariant Representations）：</strong></p><p>卷积具有平移等变性（translation equivariance），这意味着如果<strong>输入发生平移，卷积操作的输出会以相应方式平移</strong>。这种特性非常适合处理图像等感知数据，因为目标的空间位置可能会有所变化，但其本质特征保持不变。等变性使得模型能够在不同位置识别相同的模式，从而增强了模型的鲁棒性。</p></li><li><p><strong>支持可变大小输入（Variable Size Inputs）：</strong></p><p>卷积操作可以<strong>灵活地处理不同尺寸的输入</strong>。这是因为卷积核是基于局部区域滑动的，不依赖输入的绝对尺寸。例如，无论输入图像是 <span>\(128 \times 128\)
</span>还是 <span>\(256 \times 256\)
</span>，卷积核都可以逐步滑动并提取特征。这种能力使卷积神经网络非常适合处理变长或变形的数据，如图片、音频或序列信号。</p></li></ol><hr><h3 id=多层感知机mlp的限制><strong>多层感知机（MLP）的限制</strong>
<a class=anchor href=#%e5%a4%9a%e5%b1%82%e6%84%9f%e7%9f%a5%e6%9c%bamlp%e7%9a%84%e9%99%90%e5%88%b6>#</a></h3><ol><li><strong>平移不变性（translation invariance）</strong>：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有<strong>相似的反应</strong>，即为“平移不变性”。例如，图像中的一只猫无论位于左上角还是右下角，模型都应能够正确识别。<ul><li><strong>MLP的局限性</strong>：<ol><li><strong>完全连接的结构</strong>：在MLP中，<strong>每个神经元与所有输入特征相连</strong>，因此模型对输入的每个像素赋予独立的权重。换句话说，<strong>特定像素的特征无法被直接用于识别全局的物体特征</strong>。如果输入图像中的物体位置发生平移，输入像素值会重新映射到不同的神经元，导致特征表示的变化。</li><li><strong>缺乏空间归纳偏置</strong>：MLP无法内嵌有关输入<strong>空间结构的先验知识（如相邻像素间的关系）</strong>，因此需要依赖大量数据学习这些模式。</li></ol></li><li><strong>影响</strong>：<ul><li>MLP对图像特征的<strong>空间位置高度敏感</strong>，难以通过有限数据泛化到不同的物体位置。</li><li>模型需要<strong>对每种可能的位置分别学习特征表示</strong>，这导致数据需求和计算开销成倍增加。</li></ul></li></ul></li><li><strong>局部性（locality）</strong>：局部性是指数据中的<strong>局部区域通常具有更强的相关性</strong>，例如图像中相邻像素往往属于同一物体或边缘。<ul><li><strong>MLP的局限性</strong>：<ol><li><strong>忽略局部相关性</strong>：MLP的每个神经元对整个输入空间具有感知能力，但<strong>无法重点关注局部区域的特征交互</strong>。例如，在图像中，MLP无法直接识别相邻像素构成的边缘或纹理。这种全局感知的特性导致模型在<strong>提取局部模式（如边缘或角点）时效率低下</strong>。</li><li><strong>参数冗余</strong>：MLP为<strong>每个输入特征分配独立的权重</strong>，因此即使相邻像素之间存在高度相关性，模型仍需单独学习这些特征，造成参数冗余和过拟合风险。</li></ol></li><li><strong>影响</strong>：<ul><li>MLP难以捕捉<strong>高维数据中的局部模式</strong>，尤其是在输入维度较高时（如图像或语音）。</li><li>局部特征无法有效提取，导致模型在特征表达能力上不足。</li></ul></li></ul></li></ol><hr><h2 id=图像卷积convolutions-for-images><strong>图像卷积（Convolutions for Images）</strong>
<a class=anchor href=#%e5%9b%be%e5%83%8f%e5%8d%b7%e7%a7%afconvolutions-for-images>#</a></h2><p>卷积（Convolution）是卷积神经网络（CNN）的核心操作，用于 <strong>从输入数据（如图像）中提取特征模式</strong>。</p><hr><h3 id=互相关操作the-cross-correlation-operation><strong>互相关操作（The Cross-Correlation Operation）</strong>
<a class=anchor href=#%e4%ba%92%e7%9b%b8%e5%85%b3%e6%93%8d%e4%bd%9cthe-cross-correlation-operation>#</a></h3><p>虽然卷积层得名于 <strong>卷积（Convolution）</strong> 运算，但我们通常在卷积层中使用更加直观的 <strong>互相关（Cross-correlation）</strong> 运算。互相关是一种滑动窗口操作，它通过 <strong>核（kernel，或称 filter）</strong> 在输入上移动并计算点积来提取局部特征。</p><p>在二维卷积层中，一个二维输入数组和一个二维 <strong>核（<em>kernel</em>）</strong> 数组通过互相关运算输出一个二维数组。如下图所示，输入是一个高和宽均为 3 的二维数组。我们将该数组的形状记为 <span>\(3\times 3\)
</span>。核数组的高和宽分别为 2。该数组在卷积计算中又称 <strong>卷积核</strong> 或 <strong>过滤器（<em>filter</em>）</strong>。卷积核窗口（又称卷积窗口）的形状取决于卷积核的高和宽，即 <span>\(2 \times 2\)
</span>。图中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：<span>
\(0×0+1×1+3×2+4×3=19\)
</span>。</p><div align=center><img src="/images/The Cross-Correlation Operation.png" width=400px/></div><p>在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。图中的输出数组高和宽分别为 2，其中的 4 个元素由二维互相关运算得出：</p><span>\[
0\times0+1\times1+3\times2+4\times3=19,\\
1\times0+2\times1+4\times2+5\times3=25,\\
3\times0+4\times1+6\times2+7\times3=37,\\
4\times0+5\times1+7\times2+8\times3=43.
\]</span><ul><li><strong>互相关操作代码实现</strong><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 自定义二维互相关操作</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>corr2d</span>(X, K):  
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;二维互相关操作&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>## 获取卷积核的高度和宽度</span>
</span></span><span style=display:flex><span>    h, w <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>    <span style=color:#75715e>## 初始化输出特征图，大小与输入和核的尺寸相关</span>
</span></span><span style=display:flex><span>    Y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros((X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>-</span> h <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>-</span> w <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(Y<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):  
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(Y<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]):  
</span></span><span style=display:flex><span>            <span style=color:#75715e># 计算局部区域与卷积核的点积</span>
</span></span><span style=display:flex><span>            Y[i, j] <span style=color:#f92672>=</span> (X[i:i <span style=color:#f92672>+</span> h, j:j <span style=color:#f92672>+</span> w] <span style=color:#f92672>*</span> K)<span style=color:#f92672>.</span>sum()  
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> Y  <span style=color:#75715e>## 返回卷积结果</span>
</span></span></code></pre></div></li></ul><hr><h3 id=卷积层convolutional-layers><strong>卷积层（Convolutional Layers）</strong>
<a class=anchor href=#%e5%8d%b7%e7%a7%af%e5%b1%82convolutional-layers>#</a></h3><div align=center><img src=/images/Working-of-a-convolutional-layer-CNNs-force-kernel-weights-to-become-network-parameters.ppm.png width=650px/></div>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。卷积窗口形状为 <span>\(p×q\)
</span>的卷积层称为 <span>\(p×q\)
</span>卷积层。同样，<span>
\(p×q\)
</span>卷积或 <span>\(p×q\)
</span>卷积核说明卷积核的高和宽分别为 <span>\(p\)
</span>和 <span>\(q\)
</span>。卷积的数学表达式可以表达为：
<span>\[
Z[i,j] = \sum_{m=0}^{k-1}\sum_{n=0}^{k-1} X[i+m, j+n] \cdot W[m, n] + b
\]</span><blockquote class="book-hint warning"><p>在卷积神经网络（CNN）中，<strong>卷积核（kernel）中的权重（weight）是我们训练过程中需要优化的参数</strong>。实际中，我们并不是手动设计固定的卷积核，而是通过训练优化这些权重，使得卷积核能够提取最适合任务的特征。</p></blockquote><ul><li><strong>卷积层（Convolutional Layers）代码实现</strong><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 自定义2D卷积层</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Conv2D</span>(nn<span style=color:#f92672>.</span>Module):  
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, kernel_size):  
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()  
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义卷积核的权重，并将其作为可学习参数</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>## 卷积核参数，随机初始化，形状由 kernel_size 决定  </span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>weight <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>rand(kernel_size)) 
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义偏置项，并将其作为可学习参数</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>bias <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>zeros(<span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):  
</span></span><span style=display:flex><span>        <span style=color:#75715e># 将输入与权重进行互相关计算，并加上偏置项</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> corr2d(x, self<span style=color:#f92672>.</span>weight) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>bias  <span style=color:#75715e>## 调用自定义的二维互相关操作函数</span>
</span></span></code></pre></div></li></ul><hr><h3 id=利用学习卷积核实现图像边缘检测><strong>利用学习卷积核实现图像边缘检测</strong>
<a class=anchor href=#%e5%88%a9%e7%94%a8%e5%ad%a6%e4%b9%a0%e5%8d%b7%e7%a7%af%e6%a0%b8%e5%ae%9e%e7%8e%b0%e5%9b%be%e5%83%8f%e8%be%b9%e7%bc%98%e6%a3%80%e6%b5%8b>#</a></h3><p>在图像处理中，边缘检测是提取对象轮廓和结构信息的关键操作。通过卷积运算，卷积核（也称为滤波器）能够<strong>捕捉图像的局部变化</strong>，例如<strong>亮度或颜色的快速变化，这通常对应于边缘</strong>。手工设计的卷积核可以实现简单的边缘检测，例如通过水平或垂直边缘检测器来突出特定方向的边缘。然而，更灵活的方法是通过学习卷积核，使其能够适应复杂的数据分布和任务需求。</p><p>在深度学习中，卷积神经网络（CNN）通过反向传播算法自动优化卷积核的参数，使其能够<strong>提取最有利于目标任务的特征</strong>。以边缘检测为例，通过初始化卷积核并利用标注数据训练网络，模型可以逐渐<strong>学习到如何识别边缘</strong>，并适配不同的图像特性。这种学习过程不仅提高了检测的准确性，还可以扩展到更高级的特征提取任务，如纹理、形状和物体识别。</p><ol><li><strong>初始化</strong>：核的参数通常随机初始化。</li><li><strong>前向传播</strong>：使用当前的核参数计算输出特征图。</li><li><strong>反向传播</strong>：根据损失对核参数计算梯度。</li><li><strong>参数更新</strong>：使用优化算法（如 SGD 或 Adam）调整核的权重。</li></ol><blockquote class="book-hint warning"><p>当图像输入到CNN中时，<strong>图像被分成多个小的局部区域</strong>（通常是由卷积核的大小决定的），这些局部区域与卷积核进行互相关操作。通过这种方式，卷积核可以在图像中滑动，并生成一张<strong>特征图（feature map）</strong>，该特征图包含了图像中<strong>不同位置的特征响应</strong>。通过反向传播优化权重，卷积核能够逐步学习到如何从图像中提取有意义的特征，如<strong>边缘、角点、纹理</strong>等。</p></blockquote><hr><h3 id=特征映射和感受野feature-map-and-receptive-field><strong>特征映射和感受野（Feature Map and Receptive Field）</strong>
<a class=anchor href=#%e7%89%b9%e5%be%81%e6%98%a0%e5%b0%84%e5%92%8c%e6%84%9f%e5%8f%97%e9%87%8efeature-map-and-receptive-field>#</a></h3><p><strong>Feature Map（特征图）：</strong></p><ul><li>特征图是<strong>卷积操作的输出</strong>，每个特征图表示输入数据中一个特定的模式或 <strong>特征（如边缘、纹理）</strong>。</li><li><strong>多个卷积核生成多个特征图</strong>，以捕获输入中的多样性信息。</li></ul><p><strong>Receptive Field（感受野）：</strong></p><ul><li>感受野是指特定神经元在输入空间中“看到”的区域。</li><li>初始卷积核的感受野大小等于其尺寸，每层操作都会<strong>扩大感受野</strong>，具体由核大小、步幅和填充共同决定。多层网络中，<strong>靠后的神经元感受野更大</strong>，能够捕获更全局的模式。</li><li>感受野的大小决定了网络<strong>捕获全局模式的能力</strong>。在分类任务中，充分大的感受野可以确保模型关注到整个输入图像的关键信息。</li></ul><div align=center><img src="/images/Feature Map and Receptive Field.png" width=450px/></div><p>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫 <strong>特征图（feature map）</strong>。影响元素 <span>\(x\)
</span>的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做 <span>\(x\)
</span>的 <strong>感受野（receptive field）</strong>。以下图为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将形状为 <span>\(2×2\)
</span>的输出记为 <span>\(Y\)
</span>，并考虑一个更深的卷积神经网络：将 <span>\(Y\)
</span>与另一个形状为 <span>\(2×2\)
</span>的核数组做互相关运算，输出单个元素 <span>\(z\)
</span>。那么，<span>
\(z\)
</span>在 <span>\(Y\)
</span>上的感受野包括 <span>\(Y\)
</span>的全部四个元素，在输入上的感受野包括其中全部 9 个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的 <strong>感受野变得更加广阔</strong>，从而捕捉输入上更大尺寸的特征。</p><div align=center><img src="/images/The Cross-Correlation Operation.png" width=400px/></div><hr><h2 id=填充padding><strong>填充（Padding）</strong>
<a class=anchor href=#%e5%a1%ab%e5%85%85padding>#</a></h2><p>在卷积神经网络中，填充（padding）是<strong>处理输入图像边缘像素丢失问题</strong>的重要技巧，尤其是当连续应用多个卷积层时，图像尺寸的逐渐减小会影响特征的提取。通过在输入的边界添加<strong>额外像素（通常为零），填充可以增加图像的有效尺寸</strong>，从而保留边缘信息，改善模型性能。</p><p>下图我们在原输入高和宽的两侧分别添加了值为 0 的元素，使得输入高和宽从 3 变成了 5，并导致输出高和宽由 2 增加到 4。图中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：<span>
\(0×0+0×1+0×2+0×3=0\)
</span>。</p><div align=center><img src=/images/padding.png width=450px/></div><p>一般来说，假设输入形状是 <span>\(n_{h}×n_{w}\)
</span>，卷积核窗口形状是 <span>\(k_{h}×k_{w}\)
</span>，那么输出形状将会是: <span>\((n_{h}-k_{h}+1)×(n_{w}-k_{w}+1)\)
</span>。如果在高的两侧一共填充 <span>\(p_{h}\)
</span>行，在宽的两侧一共填充 <span>\(p_{w}\)
</span>列，那么输出形状将会是</p><span>\[
(n_{h}−k_{h}+p_{h}+1)×(n_{w}−k_{w}+p_{w}+1)
\]</span><p>也就是说，输出的高和宽会分别增加<span>
\(p_{h}\)
</span>和 <span>\(p_{w}\)
</span>。</p><p>在很多情况下，我们会<strong>设置 <span>\(p_{h}=k_{h}−1\)
</span>和 <span>\(p_{w}=k_{w}−1\)
</span>来使输入和输出具有相同的高和宽</strong>。这样会方便在构造网络时推测每个层的输出形状。假设这里 <span>\(k_{h}\)
</span>是奇数，我们会在高的两侧分别填充 <span>\(p_{h}/2\)
</span>行。如果 <span>\(k_{h}\)
</span>是偶数，一种可能是在输入的顶端一侧填充 <span>\(⌈p_{h}/2⌉\)
</span>行，而在底端一侧填充 <span>\(⌊p_{h}/2⌋\)
</span>行。在宽的两侧填充同理。</p><hr><h2 id=步幅stride><strong>步幅（Stride）</strong>
<a class=anchor href=#%e6%ad%a5%e5%b9%85stride>#</a></h2><p>在之前提到的二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为 <strong>步幅（Stride）</strong>。<strong>跨步卷积往往能够帮助大幅降低维数</strong>。</p><p>目前我们看到的例子里，在高和宽两个方向上步幅均为 1。我们也可以使用更大步幅。下图展示了在高上步幅为 3、在宽上步幅为 2 的二维互相关运算。可以看到，输出第一列第二个元素时，卷积窗口向下滑动了 3 行，而在输出第一行第二个元素时卷积窗口向右滑动了 2 列。当卷积窗口在输入上再向右滑动 2 列时，由于输入元素无法填满窗口，无结果输出。图中的阴影部分为输出元素及其计算所使用的输入和核数组元素：<span>
\(0×0+0×1+1×2+2×3=8\)
</span>、<span>
\(0×0+6×1+0×2+0×3=6\)
</span>。</p><div align=center><img src=/images/stride.png width=450px/></div><p>一般来说，当高上步幅为 <span>\(s_{h}\)
</span>，宽上步幅为 <span>\(s_{w}\)
</span>时，输出形状为:</p><span>\[
⌊(n_{h}−k_{h}+p_{h}+s_{h})/s_{h}⌋×⌊(n_{w}−k_{w}+p_{w}+s_{w})/s_{w}⌋
\]</span><p>如果设置 <span>\(p_{h}=k_{h}−1\)
</span>和 <span>\(p_{w}=k_{w}−1\)
</span>，那么输出形状将简化为 <span>\(⌊(n_{h}+s_{h}−1)/s_{h}⌋×⌊(n_{w}+s_{w}−1)/s_{w}⌋\)
</span>。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是 <span>\((n_{h}/s_{h})×(n_{w}/s_{w})\)
</span>。</p><hr><h2 id=多输入通道和多输出通道multiple-input-and-multiple-output-channels><strong>多输入通道和多输出通道（Multiple Input and Multiple Output Channels）</strong>
<a class=anchor href=#%e5%a4%9a%e8%be%93%e5%85%a5%e9%80%9a%e9%81%93%e5%92%8c%e5%a4%9a%e8%be%93%e5%87%ba%e9%80%9a%e9%81%93multiple-input-and-multiple-output-channels>#</a></h2><h3 id=多输入通道multiple-input-channels><strong>多输入通道（Multiple Input Channels）</strong>
<a class=anchor href=#%e5%a4%9a%e8%be%93%e5%85%a5%e9%80%9a%e9%81%93multiple-input-channels>#</a></h3><p>多输入通道通常用于处理具有多个特征维度的数据，例如彩色图像的红色、绿色和蓝色（RGB）通道。输入图像的每个像素不仅用二维的空间位置（高度和宽度）表示，还<strong>需要包含多个颜色通道的信息</strong>，这一维称为 <strong>通道（<em>channel</em>）</strong>。。因此，输入是一个三维张量，其形状为 <span>\((C_{\text{in}}, H, W)\)
</span>，其中：</p><ul><li><span>\(C_{\text{in}}\)
</span>：输入的通道数。</li><li><span>\(H, W\)
</span>：输入的高度和宽度。</li></ul><p>对于<strong>每个输入通道</strong>，卷积核具有一个<strong>独立的权重矩阵</strong>。这些权重矩阵与对应的输入通道进行逐像素的加权求和，最后将所有输入通道的<strong>结果相加</strong>，形成<strong>单个二维特征图</strong>。对于输入张量 <span>\(\mathbf{X} \in \mathbb{R}^{C_{\text{in}} \times H \times W}\)
</span>，卷积核权重为 <span>\(\mathbf{W} \in \mathbb{R}^{C_{\text{in}} \times k_h \times k_w}\)
</span>，偏置为 <span>\(b \in \mathbb{R}\)
</span>，输出特征图的某个位置 <span>\((i, j)\)
</span>由以下公式计算：
<span>\[
Y[i, j] = \sum_{c=1}^{C_{\text{in}}} \sum_{p=1}^{k_h} \sum_{q=1}^{k_w} X[c, i+p, j+q] \cdot W[c, p, q] + b
\]</span></p><p>下图展示了含 2 个输入通道的二维互相关计算的例子。在每个通道上，二维输入数组与二维核数组做互相关运算，再按通道相加即得到输出。图中阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：
<span>\[
(1×1+2×2+4×3+5×4)+(0×0+1×1+3×2+4×3)=56
\]</span></p><div align=center><img src=/images/Cin.png width=450px/></div><blockquote class="book-hint warning"><p>当输入通道有多个时，因为我们对各个通道的结果做了<strong>累加</strong>，所以不论输入通道数是多少，<strong>输出通道数总是为 1</strong>。</p></blockquote><hr><h3 id=多输出通道multiple-output-channels><strong>多输出通道（Multiple Output Channels）</strong>
<a class=anchor href=#%e5%a4%9a%e8%be%93%e5%87%ba%e9%80%9a%e9%81%93multiple-output-channels>#</a></h3><p>多输出通道用于在隐藏层中<strong>生成多个特征图（feature maps）</strong>，每个特征图捕捉不同的特征模式，例如边缘、纹理或特定对象形状。输出特征图也以三维张量表示，其形状为 <span>\((C_{\text{out}}, H_{\text{out}}, W_{\text{out}})\)
</span>，其中：</p><ul><li><span>\(C_{\text{out}}\)
</span>：输出通道数。</li><li><span>\(H_{\text{out}}, W_{\text{out}}\)
</span>：输出特征图的高度和宽度。</li></ul><p><strong>每个输出通道对应一个独立的卷积核组</strong>，每组卷积核与所有输入通道进行卷积并求和。因此，卷积层通过学习不同的卷积核来生成多个输出通道，<strong>每个输出通道表示不同的特征</strong>。对于输出通道 <span>\(o\)
</span>，卷积结果为：
<span>\[
Y_o[i, j] = \sum_{c=1}^{C_{\text{in}}} \sum_{p=1}^{k_h} \sum_{q=1}^{k_w} X[c, i+p, j+q] \cdot W[o, c, p, q] + b[o]
\]</span></p><p>其中，<span>
\(W[o, c, p, q]\)
</span>是第 <span>\(o\)
</span>个输出通道和第 <span>\(c\)
</span>个输入通道对应的权重，<span>
\(b[o]\)
</span>是第 <span>\(o\)
</span>个输出通道的偏置。</p><blockquote class="book-hint warning"><p>在卷积过程中，单个卷积核会处理所有的输入通道，并将其加权求和，生成一个单独的输出特征图。所以不论输入通道多少，对于单个卷积核累加输出通道数总是为 1。为了<strong>增加输出通道数，可以通过增加卷积核组的数量来实现</strong>。如果我们使用 N 个卷积核，就可以生成 N 个输出通道。</p></blockquote><hr><h3 id=结合多输入和多输出通道><strong>结合多输入和多输出通道</strong>
<a class=anchor href=#%e7%bb%93%e5%90%88%e5%a4%9a%e8%be%93%e5%85%a5%e5%92%8c%e5%a4%9a%e8%be%93%e5%87%ba%e9%80%9a%e9%81%93>#</a></h3><p>如果输入有 <span>\(C_{\text{in}}\)
</span>个通道，输出有 <span>\(C_{\text{out}}\)
</span>个通道，那么卷积核的权重形状为：
<span>\[
\mathbf{W} \in \mathbb{R}^{C_{\text{out}} \times C_{\text{in}} \times k_h \times k_w}
\]
</span>这意味着<strong>每个输出通道对应一组卷积核</strong>，这组卷积核与所有输入通道进行卷积，并将结果求和生成该输出通道的特征图。这种设计允许网络在不同输出通道中<strong>捕获多种特征</strong>，从而更好地描述输入数据。</p><ul><li>假设输入有 <span>\(C_{\text{in}}\)
</span>个通道，我们可以定义 <span>\(C_{\text{out}}\)
</span>个卷积核。</li><li>每个卷积核的权重形状为 <span>\((C_{\text{in}}, k_h, k_w)\)
</span>。</li><li>卷积操作会将输入张量形状从 <span>\((C_{\text{in}}, H, W)\)
</span>转换为输出张量形状 <span>\((C_{\text{out}}, H_{\text{out}}, W_{\text{out}})\)
</span>。</li></ul><blockquote class="book-hint warning"><p><strong>Note:</strong> <strong>虽然卷积层通常被叫做 N x N 卷积，但是实际他是一个四维的权重矩阵，连接输入和输出的通道数（C_in x N x N x C_out）。</strong></p></blockquote><hr><h3 id=11-卷积层><strong>1×1 卷积层</strong>
<a class=anchor href=#11-%e5%8d%b7%e7%a7%af%e5%b1%82>#</a></h3><p><span>\(1×1\)
</span>卷积层是一种特殊的卷积层，其中卷积核的大小为 <span>\(1×1\)
</span>。虽然核的尺寸小，它在卷积神经网络中具有广泛的应用和重要的作用，能够高效地处理输入通道的特征并优化模型结构。<span>
\(1×1\)
</span>卷积核<strong>操作的核心</strong>是：</p><ul><li><strong>对输入的每个像素位置</strong>，仅对<strong>通道维度进行线性组合</strong>，而不会涉及空间范围。</li><li>输入的<strong>每个通道的值会被乘以对应卷积核权重后求和，并添加偏置</strong>。</li></ul><p>因为使用了最小窗口，<span>
\(1×1\)
</span>卷积<strong>失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能</strong>。实际上，<strong><span>
\(1×1\)
</span>卷积的主要计算发生在通道维上</strong>。下图展示了使用输入通道数为 3、输出通道数为 2 的 <span>\(1×1\)
</span>卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，<strong>那么 <span>\(1×1\)
</span>卷积层的作用与全连接层等价</strong>。</p><blockquote class="book-hint warning"><p><strong>Note:</strong> 1 x 1 卷积层的权重矩阵为 <strong>C_in x 1 x 1 x C_out</strong>。对于图像的单个像素位置，<strong>每个像素有 C_in 个通道，对应每一个通道上的 weight</strong>。空间维度（高和宽）上的每个元素相当于样本，通道相当于特征，因此这相当于一个线形的组合（w1x1+w2x2+&mldr;）。根据输出通道的需求可以调整卷积核的组数来实现。**</p></blockquote><div align=center><img src=/images/1x1.png width=450px/></div><h4 id=11-卷积层的主要作用><strong>1×1 卷积层的主要作用</strong>
<a class=anchor href=#11-%e5%8d%b7%e7%a7%af%e5%b1%82%e7%9a%84%e4%b8%bb%e8%a6%81%e4%bd%9c%e7%94%a8>#</a></h4><ol><li><strong>通道维度的特征变换</strong><ul><li><span>\(1×1\)
</span>卷积核可以在不改变输入空间分辨率（即高和宽）的情况下，通过<strong>加权求和实现输入通道的重新组合，提取新的特征</strong>。</li><li>它本质上是一个<strong>逐像素的全连接层</strong>，作用于每个像素位置上的所有输入通道。</li></ul></li><li><strong>降维与升维</strong><ul><li>通过减少输出通道数，<span>
\(1×1\)
</span>卷积可以实现特征图的降维，从而减少计算量和参数数量。</li><li>通过增加输出通道数，它可以进行特征扩展，为后续卷积层提供更丰富的特征。</li></ul></li><li><strong>跨通道交互</strong><ul><li>常规的卷积核会处理空间和通道上的信息，但无法高效地在<strong>通道维度间进行交互</strong>。<span>
\(1×1\)
</span>卷积则专注于通道间的关系建模，有助于提取新的特征表示。</li></ul></li></ol><blockquote class="book-hint warning"><p>1x1 卷积的核心在于对<strong>单个像素位置的所有通道元素进行线性组合</strong>。它不涉及空间范围的操作，只在每个像素格的通道维度上进行计算。通过<strong>控制1x1卷积核组的数量</strong>，我们可以达到降维与升维的效果，并在通道维度间进行交互（如红色、绿色和蓝色三个通道之间）</p></blockquote><hr><h2 id=池化层pooling><strong>池化层（Pooling）</strong>
<a class=anchor href=#%e6%b1%a0%e5%8c%96%e5%b1%82pooling>#</a></h2><p>池化层的作用是逐渐降低隐藏表示的空间分辨率并聚合信息，这样随着神经网络层级的增加，每个神经元对更大范围的输入（感受野）变得敏感。池化层帮助神经网络<strong>在最后一层聚焦于全局特征</strong>（如“图像是否包含猫”）。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。</p><p>池化层还通过减少位置敏感性，提升网络对物体平移等变换的鲁棒性。例如，<strong>图像的小范围平移</strong>（如向右移动一个像素）可能<strong>不会影响网络的输出</strong>。这样，池化不仅帮助提取底层特征，还增强了模型对小变动（如图像移动或拍摄角度变化）的不变性。</p><hr><h3 id=最大池化层和平均池化层maximum-pooling-and-average-pooling><strong>最大池化层和平均池化层（Maximum Pooling and Average Pooling）</strong>
<a class=anchor href=#%e6%9c%80%e5%a4%a7%e6%b1%a0%e5%8c%96%e5%b1%82%e5%92%8c%e5%b9%b3%e5%9d%87%e6%b1%a0%e5%8c%96%e5%b1%82maximum-pooling-and-average-pooling>#</a></h3><p>同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称 <strong>池化窗口（pooling window）</strong>）中的元素计算输出。不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做 <strong>最大池化（maximum pooling）</strong> 或 <strong>平均池化（average pooling）</strong>。在二维最大池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当池化窗口滑动到某一位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。</p><div align=center><img src=/images/pooling.png width=400px/></div><p>上图展示了池化窗口形状为 <span>\(2×2\)
</span>的最大池化，阴影部分为第一个输出元素及其计算所使用的输入元素。输出数组的高和宽分别为 2，其中的 4 个元素由取最大值运算 <code>max</code> 得出：</p><span>\[
\max(0,1,3,4)=4 \\
\max(1,2,4,5)=5 \\
\max(3,4,6,7)=7 \\
\max(4,5,7,8)=8 \\
\]</span><p>平均池化的工作原理与最大池化类似，但将最大运算符替换成平均运算符。由于我们正在组合来自多个相邻像素的信息，因此我们可以对相邻像素进行平均以获得具有更好信噪比的图像。池化窗口形状为 <span>\(p×q\)
</span>的池化层称为 <span>\(p×q\)
</span>池化层，其中的池化运算叫作 <span>\(p×q\)
</span>池化。</p><p>在物体边缘检测的例子中，我们现在将卷积层的输出作为 <span>\(2×2\)
</span>最大池化的输入。设该卷积层输入是 <span>\(X\)
</span>、池化层输出为 <span>\(Y\)
</span>。无论是 <span>\(X[i, j]\)
</span>和 <span>\(X[i, j+1]\)
</span>值不同，还是 <span>\(X[i, j+1]\)
</span>和 <span>\(X[i, j+2]\)
</span>不同，池化层输出均有 <span>\(Y[i, j]=1\)
</span>。也就是说，使用 <span>\(2×2\)
</span><strong>最大池化层时，只要卷积层识别的模式在高和宽上移动不超过一个元素，我们依然可以将它检测出来</strong>。</p><hr><h3 id=填充步幅以及多通道padding-stride-and-multiple-channels><strong>填充，步幅以及多通道（Padding, Stride, and Multiple Channels）</strong>
<a class=anchor href=#%e5%a1%ab%e5%85%85%e6%ad%a5%e5%b9%85%e4%bb%a5%e5%8f%8a%e5%a4%9a%e9%80%9a%e9%81%93padding-stride-and-multiple-channels>#</a></h3><p>同卷积层一样，池化层也可以在输入的高和宽两侧的填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。窗口的高度和宽度决定了局部区域的范围（如 <span>\(2 \times 2\)
</span>、<span>
\(3 \times 3\)
</span>）。步幅较小，则池化后的输出较大。步幅较大，则池化后的输出较小。填充决定了是否在输入边界填充像素。</p><p>在处理多通道输入数据时，池化层对 <strong>每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加</strong>。这意味着<strong>池化层的输出通道数与输入通道数相等</strong>。</p><blockquote class="book-hint warning"><p>池化操作<strong>不会改变通道数</strong>，但会减小空间维度，因此池化操作有效地减少了特征图的总数据量。</p></blockquote><hr><h3 id=池化的主要作用><strong>池化的主要作用</strong>
<a class=anchor href=#%e6%b1%a0%e5%8c%96%e7%9a%84%e4%b8%bb%e8%a6%81%e4%bd%9c%e7%94%a8>#</a></h3><p>池化层的主要作用是通过下采样（downsampling）来减少输入数据的空间维度，降低计算复杂度，并增强特征的平移不变性。具体来说，池化的作用体现在以下几个方面：</p><ol><li><p><strong>降低计算量和内存消耗</strong>：</p><p>通过减少特征图的空间维度（高度和宽度），池化层<strong>减小了后续卷积层需要处理的输入数据量</strong>。这不仅降低了计算复杂度，还减少了模型所需的内存和存储空间。池化通常使用较小的窗口（如2x2或3x3），并且使用步长（stride）跳跃地遍历输入图像，减小特征图的尺寸。</p></li><li><p><strong>增加平移不变性</strong>：</p><p>池化层通过<strong>聚合局部区域的信息</strong>，使得<strong>小的平移、旋转或者尺度变化对输出结果的影响减小</strong>。例如，如果一个物体从图像的一个区域移动到另一个区域，池化操作会使得相邻区域的特征聚合，而不太依赖于精确的空间位置。</p></li><li><p><strong>突出显著特征</strong>：</p><p>池化操作通过选择区域中的最大值（最大池化）或平均值（平均池化），能够<strong>保留图像中最显著的特征</strong>。例如，在最大池化中，池化窗口中最大的像素值代表该区域最显著的特征，而<strong>小的细节（如噪声）则被抑制</strong>。这有助于模型聚焦于重要的高层次特征，而忽略低层次的细节。</p></li><li><p><strong>减少过拟合</strong>：</p><p>通过池化，网络的<strong>参数量减少</strong>了，从而<strong>降低了模型的复杂度</strong>。较少的参数意味着网络更不容易过拟合。池化通过降低特征图的分辨率，减少了网络对训练数据的过度依赖，使得模型能够更好地泛化到未见过的数据。</p></li><li><p><strong>促进层次特征学习</strong>：</p><p>由于池化使得网络更关注<strong>局部区域的汇总信息</strong>，它有助于学习更具层次性的特征。在卷积神经网络的不同层中，<strong>较底层的卷积层通常会学习到简单的特征</strong>（如边缘和纹理），而较高层的卷积层则会<strong>聚焦于更复杂的特征</strong>（如物体部分和整体形状）。池化通过逐层下采样，帮助网络逐步汇总和聚焦更抽象的层次特征。</p></li></ol><hr><h2 id=经典卷积神经网络-modern-convolutional-neural-networks><strong>经典卷积神经网络 （Modern Convolutional Neural Networks）</strong>
<a class=anchor href=#%e7%bb%8f%e5%85%b8%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c-modern-convolutional-neural-networks>#</a></h2><hr><h3 id=lenet><strong>LeNet</strong>
<a class=anchor href=#lenet>#</a></h3><p>LeNet 分为<strong>卷积层块</strong>和<strong>全连接层块</strong>两个部分。卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用 <span>\(5×5\)
</span>的窗口，并在输出上使用 <code>sigmoid</code> 激活函数。第一个卷积层输出通道数为 6，第二个卷积层输出通道数则增加到 16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为 <span>\(2×2\)
</span>，且步幅为 2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。</p><p>卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本 <strong>变平（<em>flatten</em>）</strong>。也就是说，<strong>全连接层的输入形状将变成二维</strong>，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含 3 个全连接层。它们的输出个数分别是120、84和10，其中 10 为输出的类别个数。</p><div align=center><img src=/images/lenet.png width=200px/></div><ul><li><strong>LeNet 代码实现</strong><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>LeNet</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        <span style=color:#75715e># input image size 28x28 -&gt; output size = 28-5+1=24x24</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>conv1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(in_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># input size 24x24 -&gt; output size = (24-2+2)/2=12x12</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>pool <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>AvgPool2d(kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># input size 12x12 -&gt; output size = 12-5+1=8x8</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>conv2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(in_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># after pooling 8x8 -&gt; 4x4</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>16</span><span style=color:#f92672>*</span><span style=color:#ae81ff>4</span><span style=color:#f92672>*</span><span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>120</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>120</span>, <span style=color:#ae81ff>84</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc3 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>84</span>, <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>sigmoid <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sigmoid()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>sigmoid(self<span style=color:#f92672>.</span>conv1(x))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>pool(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>sigmoid(self<span style=color:#f92672>.</span>conv2(x))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>pool(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>16</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span>)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>sigmoid(self<span style=color:#f92672>.</span>fc1(x))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>sigmoid(self<span style=color:#f92672>.</span>fc2(x))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc3(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div></li></ul><hr><h3 id=alexnet><strong>AlexNet</strong>
<a class=anchor href=#alexnet>#</a></h3><p>2012年，AlexNet 横空出世。这个模型的名字来源于论文第一作者的姓名 Alex Krizhevsky。AlexNet 使用了 8 层卷积神经网络，并以很大的优势赢得了 ImageNet 2012 图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。</p><div align=center><img src=/images/alexnet.png width=300px/></div><ul><li><p><strong>AlexNet 设计框架（Architecture）</strong>:</p><ol><li><p><strong>第一，与相对较小的 LeNet 相比，AlexNet 包含 8 层变换，其中有 5 层卷积和 2 层全连接隐藏层，以及 1 个全连接输出层</strong>。AlexNet 第一层中的卷积窗口形状是 <span>\(11×11\)
</span>。因为 ImageNet 中绝大多数图像的高和宽均比 MNIST 图像的高和宽大 10 倍以上，ImageNet 图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。第二层中的卷积窗口形状减小到 <span>\(5×5\)
</span>，之后全采用 <span>\(3×3\)
</span>。此外，第一、第二和第五个卷积层之后都使用了窗口形状为 <span>\(3×3\)
</span>、步幅为 2 的最大池化层。而且，AlexNet 使用的卷积通道数也大于 LeNet 中的卷积通道数数十倍。</p><p>紧接着最后一个卷积层的是两个输出个数为 4096 的全连接层。这两个巨大的全连接层带来将近 1 GB的模型参数。由于早期显存的限制，最早的 AlexNet 使用双数据流的设计使一个 GPU 只需要处理一半模型。幸运的是，显存在过去几年得到了长足的发展，因此通常我们不再需要这样的特别设计了。</p></li><li><p><strong>第二，AlexNet 将 sigmoid 激活函数改成了更加简单的 ReLU 激活函数</strong>。一方面，ReLU 激活函数的计算更简单，例如它并没有 sigmoid 激活函数中的求幂运算。另一方面，ReLU 激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当 sigmoid 激活函数输出极接近 0 或 1 时，这些区域的梯度几乎为 0，从而造成反向传播无法继续更新部分模型参数；而 ReLU 激活函数在正区间的梯度恒为 1。因此，若模型参数初始化不当，sigmoid 函数可能在正区间得到几乎为 0 的梯度，从而令模型无法得到有效训练。</p></li><li><p><strong>第三，AlexNet 通过 丢弃法（<em>dropout</em>）控制全连接层的模型复杂度，而 LeNet 只使用权重衰减</strong>。为了进一步增强数据，AlexNet 的训练循环添加了大量图像增强，例如翻转、裁剪和颜色变化。这使得模型更加健壮，更大的样本量有效地减少了过拟合。</p></li></ol></li><li><p><strong>LeNet 代码实现</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>AlexNet</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>features <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>96</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>11</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>MaxPool2d(kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>96</span>, <span style=color:#ae81ff>256</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>MaxPool2d(kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>384</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>384</span>, <span style=color:#ae81ff>384</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>384</span>, <span style=color:#ae81ff>256</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>MaxPool2d(kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>classifier <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>256</span><span style=color:#f92672>*</span><span style=color:#ae81ff>6</span><span style=color:#f92672>*</span><span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>4096</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Dropout(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>4096</span>, <span style=color:#ae81ff>4096</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Dropout(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>4096</span>, <span style=color:#ae81ff>10</span>),
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>features(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>view(x<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>0</span>), <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>classifier(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div></li></ul><hr><h3 id=vgg><strong>VGG</strong>
<a class=anchor href=#vgg>#</a></h3><p><strong>VGG Network</strong> (Simonyan and Zisserman, 2014)的名字来源于论文作者所在的实验室Visual Geometry Group。VGG（Networks Using Blocks） 提出了可以通过<strong>重复使用简单的基础块来构建深度模型</strong>的思路。VGG 的主要特点是<strong>通过堆叠小卷积核和池化层来增加网络深度，从而提升模型的表达能力</strong>。</p><p>CNN 的基本构件是以下的序列。(i) 带有填充的<strong>卷积层</strong>，以保持分辨率；(ii) <strong>非线性层</strong>，如ReLU；(iii) <strong>池化层</strong>，如最大池化，以降低分辨率。这种方法的问题之一是，<strong>空间分辨率下降得相当快</strong>。Simonyan 和 Zisserman 的关键想法是在下采样过程中以 <strong>块（<em>block</em>）</strong> 的形式<strong>在最大池化前使用多个卷积</strong>。他们最初的主要关注点是深层网络还是宽层网络表现更好。例如，连续应用两个 <span>\(3×3\)
</span>卷积和单个 <span>\(5×5\)
</span>卷积触及相同的像素哪个效果更好。在一个相当详细的分析中，他们表明<strong>深层和窄层网络的表现明显优于浅层的同类网络</strong>。这使深度学习走上了追求更深的网络的道路，在典型的应用中，网络层数超过100层。堆叠 <span>\(3×3\)
</span>卷积已经成为后期深度网络的一个黄金标准。</p><p>VGG 块的组成规律是：<strong>连续使用数个相同的填充为 1、窗口形状为 <span>\(3×3\)
</span>的卷积层后接上一个步幅为 2、窗口形状为 <span>\(2×2\)
</span>的最大池化层</strong>。卷积层保持输入的高和宽不变，而池化层则对其减半。</p><p>对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以<strong>增加网络深度来保证学习更复杂的模式</strong>，而且代价还比较小（参数更少）。例如，在 VGG 中，使用了 3 个 <span>\(3×3\)
</span>卷积核来代替 <span>\(7×7\)
</span>卷积核，使用了 2 个 <span>\(3×3\)
</span>卷积核来代替 <span>\(5×5\)
</span>卷积核，这样做的主要目的是在保证具有<strong>相同感知野的条件下</strong>，提升了<strong>网络的深度</strong>，在一定程度上提升了神经网络的效果。</p><div align=center><img src=/images/vgg.svg width=500px/></div><ul><li><strong>VGG 代码实现</strong><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 定义生成 VGG 卷积块的函数</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>make_vgg_block</span>(input_channels, output_channels, num_convs):
</span></span><span style=display:flex><span>    layers <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(num_convs):
</span></span><span style=display:flex><span>        layers<span style=color:#f92672>.</span>append(nn<span style=color:#f92672>.</span>Conv2d(input_channels, output_channels, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>        layers<span style=color:#f92672>.</span>append(nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>))
</span></span><span style=display:flex><span>        input_channels <span style=color:#f92672>=</span> output_channels  <span style=color:#75715e># 更新输入通道为当前输出通道</span>
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>append(nn<span style=color:#f92672>.</span>MaxPool2d(kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>))  <span style=color:#75715e># 添加池化层</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> nn<span style=color:#f92672>.</span>Sequential(<span style=color:#f92672>*</span>layers)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 动态定义 VGG 模型</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>VGG</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, architecture, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>features <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_make_features(architecture)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>classifier <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>512</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>7</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>4096</span>), nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>), nn<span style=color:#f92672>.</span>Dropout(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>4096</span>, <span style=color:#ae81ff>4096</span>), nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>), nn<span style=color:#f92672>.</span>Dropout(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>4096</span>, num_classes),
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_make_features</span>(self, architecture):
</span></span><span style=display:flex><span>        layers <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> input_channels, output_channels, num_convs <span style=color:#f92672>in</span> architecture:
</span></span><span style=display:flex><span>            layers<span style=color:#f92672>.</span>append(make_vgg_block(input_channels, output_channels, num_convs))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> nn<span style=color:#f92672>.</span>Sequential(<span style=color:#f92672>*</span>layers)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>features(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>flatten(x, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>classifier(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 定义 VGG 架构 (以 VGG-16 为例)</span>
</span></span><span style=display:flex><span>vgg16_architecture <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    (<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>2</span>),     <span style=color:#75715e># Block 1: 3-&gt;64 通道, 2 个卷积层</span>
</span></span><span style=display:flex><span>    (<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>2</span>),   <span style=color:#75715e># Block 2: 64-&gt;128 通道, 2 个卷积层</span>
</span></span><span style=display:flex><span>    (<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>3</span>),  <span style=color:#75715e># Block 3: 128-&gt;256 通道, 3 个卷积层</span>
</span></span><span style=display:flex><span>    (<span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>512</span>, <span style=color:#ae81ff>3</span>),  <span style=color:#75715e># Block 4: 256-&gt;512 通道, 3 个卷积层</span>
</span></span><span style=display:flex><span>    (<span style=color:#ae81ff>512</span>, <span style=color:#ae81ff>512</span>, <span style=color:#ae81ff>3</span>),  <span style=color:#75715e># Block 5: 512-&gt;512 通道, 3 个卷积层</span>
</span></span><span style=display:flex><span>]
</span></span></code></pre></div></li></ul><hr><h3 id=network-in-network-nin><strong>Network in Network (NiN)</strong>
<a class=anchor href=#network-in-network-nin>#</a></h3><p>如同 LeNet、AlexNet 和 VGG 的设计中所体现的，传统的卷积神经网络通过一系列卷积层和池化层利用 <strong>空间结构（spatial structure）</strong> 提取特征，并通过全连接层对表征进行后处理。然而，随着网络深度的增加，特别是在 AlexNet 和 VGG 中，网络末端的全连接层带来了两个主要问题：</p><ol><li><strong>参数数量庞大</strong>：<ul><li>在网络的最后阶段，通常通过全连接层将特征映射到分类结果。这些全连接层通常含有巨量的参数，容易造成训练过程中的过拟合，并且会大大增加计算资源的消耗。</li></ul></li><li><strong>无法增加非线性</strong>：<ul><li>在传统的卷积神经网络中，卷积层负责从图像中提取局部特征，而全连接层则将这些特征组合成最终的输出。然而，<strong>增加非线性通常依赖于全连接层的引入</strong>，然而<strong>过早地加入全连接层可能会破坏卷积层捕捉到的空间结构信息</strong>，从而影响模型的性能。</li></ul></li></ol><p><strong>网络中的网络（network in network (NiN)）区块</strong> 提供了一个替代方案，能够在一个简单的策略中解决这两个问题。它们是基于一个非常简单的洞察力提出的:</p><ol><li><strong>使用 <span>\(1×1\)
</span>卷积来增加通道激活的局部非线性</strong>：<ul><li>卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。</li><li><span>\(1×1\)
</span>卷积层。它可以看成全连接层，其中<strong>空间维度（高和宽）上的每个元素相当于样本</strong>，<strong>通道相当于特征</strong>。NiN 背后的想法是<strong>在每个像素位置应用一个全连接层</strong>（对于每个高度和宽度）。由此产生的 <span>\(1×1\)
</span>卷积可以被认为是一个<strong>独立作用于每个像素点的全连接层</strong>。</li><li>通过将多个 <span>\(1×1\)
</span>卷积层堆叠，可以<strong>有效地将每个局部区域的激活映射变得更加复杂</strong>，类似于全连接层的作用，但它不依赖于传统的全连接结构，因此避免了参数量爆炸的问题。</li></ul></li></ol><blockquote class="book-hint warning"><p><strong>Note:</strong> 在传统的卷积层中，每个卷积核只会在 <strong>同一通道内</strong> 提取信息。这意味着每个卷积核仅能操作输入特征图的一个通道，<strong>并不会直接与其他通道的特征进行交互</strong>，不同通道间很难实现结合。为了将不同通道的信息结合，<strong>我们通常需要依赖网络中的 全连接层</strong>，尤其是在网络的最后几层。</p><p><strong>1x1卷积改变了这个局限性，它允许在 通道维度 上进行信息融合。</strong> 除此之外 1x1 卷积可以通过增加通道数来提高网络的表达能力，同时也能在较低的计算开销下实现更复杂的特征学习。它通过引入非线性（通常通过ReLU激活）来增加模型的表现力。</p></blockquote><ol start=2><li><strong>使用全局平均池化 (Global Average Pooling, GAP) 来整合特征</strong>：<ul><li>NiN去除了容易造成过拟合的全连接层，将它们<strong>替换为全局平均池化层（即在所有位置上进行求和）</strong>。该池化层通道数量为所需的输出数量。</li><li>全局平均池化是对每个通道的输出进行空间上的平均，得到一个标量值，<strong>最终生成的输出被用来进行分类或其他任务</strong>。</li><li>与传统的最大池化或平均池化不同，GAP 被用作替代全连接层的方案。它不仅有效减少了参数量，还可以在不丢失空间结构信息的情况下将整个图像的空间特征映射整合成最终的类别输出。</li></ul></li></ol><blockquote class="book-hint warning"><p><strong>Note:</strong> 最终的全局平均池化之前，通道数通常会被设置为与分类任务中的类别数量相匹配。每个通道对应一个特征。对于每一个特征图（即每个通道），全局平均池化（GAP）会对该通道中的所有空间位置（即每个像素）进行平均操作。换句话说，<strong>GAP将每个通道的所有像素值压缩成一个数字</strong>。如果网络有 C 个通道，那么输出就是一个 <strong>C-维的向量</strong>。这个向量包含了所有通道的全局信息。这些数值作为网络的输出特征，来进行<strong>类别预测</strong>。</p></blockquote><div align=center><img src=/images/nin.svg width=550px/></div><p>NiN是在AlexNet问世不久后提出的。它们的卷积层设定有类似之处。NiN使用卷积窗口形状分别为 <span>\(11×11\)
</span>、<span>
\(5×5\)
</span>和 <span>\(3×3\)
</span>的卷积层，相应的输出通道数也与 AlexNet 中的一致。每个 NiN 块后接一个步幅为 2、窗口形状为 <span>\(3×3\)
</span>的最大池化层。</p><ul><li><strong>NiN 代码实现</strong><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># NiN Block</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>NiNBlock</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, in_channels, out_channels, kernel_size, stride, padding):
</span></span><span style=display:flex><span>        super(NiNBlock, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>conv1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(in_channels, out_channels, kernel_size, stride, padding)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>conv2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(out_channels, out_channels, kernel_size, stride, padding)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>conv3 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(out_channels, out_channels, kernel_size, stride, padding)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>conv1(x))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>conv2(x))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>conv3(x))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># NiN Model</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>NiN</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>net <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            NiNBlock(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>96</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>11</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>MaxPool2d(<span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            NiNBlock(<span style=color:#ae81ff>96</span>, <span style=color:#ae81ff>256</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>MaxPool2d(<span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            NiNBlock(<span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>384</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>MaxPool2d(<span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.5</span>),  <span style=color:#75715e># Dropout for regularization</span>
</span></span><span style=display:flex><span>            NiNBlock(<span style=color:#ae81ff>384</span>, num_classes, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>AdaptiveAvgPool2d((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)),  <span style=color:#75715e># Global Average Pooling</span>
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Flatten()  <span style=color:#75715e># Flatten the output for final classification</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>net(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div></li></ul><hr><h3 id=resnet><strong>ResNet</strong>
<a class=anchor href=#resnet>#</a></h3><p>ResNet (Residual Network) 是由微软研究团队提出的深度神经网络架构，其核心思想是通过 <strong>残差模块 (Residual Block)</strong> 解决深度网络中常见的 <strong>梯度消失问题</strong> 和 <strong>退化问题</strong>。ResNet 在 2015 年的 ImageNet 大赛中取得了冠军，是深度学习领域的里程碑模型。</p><hr><h4 id=residual-block-的结构><strong>Residual Block 的结构</strong>
<a class=anchor href=#residual-block-%e7%9a%84%e7%bb%93%e6%9e%84>#</a></h4><ol><li><p><strong>直观理解：</strong></p><ul><li>一个标准的网络层尝试学习一个复杂的映射 <span>\(H(x)\)
</span>。</li><li>Residual Block 则将目标分解为 <span>\(F(x) + x\)
</span>，其中：<ul><li><span>\(F(x) = H(x) - x\)
</span>：<strong>残差，即网络学习的部分</strong>。</li><li><span>\(x\)
</span>：输入，通过跳跃连接直接传递到输出。</li></ul></li></ul><p>这种结构鼓励网络<strong>专注于学习残差 <span>\(F(x)\)
</span></strong>，而非直接学习 <span>\(H(x)\)
</span>。</p></li><li><p><strong>公式表示：</strong> 假设输入为 <span>\(x\)
</span>，Residual Block 的输出为：
<span>\[
y = F(x, \{W_i\}) + x
\]</span></p><ul><li><span>\(F(x, \{W_i\})\)
</span>：通过卷积、Batch Normalization、ReLU 等操作后得到的输出。</li><li><span>\(x\)
</span>：输入，通过跳跃连接直接添加到 <span>\(F(x)\)
</span>。</li><li><span>\(\{W_i\}\)
</span>：Residual Block 中的可学习参数。</li></ul></li></ol><blockquote><p><strong>举个具体例子：</strong> 假设目标映射 <span>\(H(x)\)
</span>是恒等映射（即 <span>\(H(x) = x\)
</span>），如果网络直接学习 <span>\(H(x)\)
</span>，需要每一层的参数精确调整才能接近这个目标；但如果采用残差学习，网络只需学习 <span>\(F(x) = 0\)
</span>，这对优化过程来说非常简单。</p></blockquote><p>如下图所示，设输入为 <span>\(x\)
</span>。假设我们希望学出的理想映射为 <span>\(f(x)\)
</span>，从而作为图中上方激活函数的输入。左图虚线框中的部分需要直接拟合出该映射 <span>\(f(x)\)
</span>，而右图虚线框中的部分则需要拟合出有关恒等映射的残差映射 <span>\(f(x)−x\)
</span>。残差映射在实际中往往更容易优化。当理想映射 <span>\(f(x)\)
</span>极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。图中右图也是 ResNet 的基础块，即<strong>残差块（<em>residual block</em>）</strong>。其中实线承载层输入 <span>\(x\)
</span>加法运算符称为<strong>残差连接（<em>residual connection</em>）</strong>。在残差块中，输入可通过跨层的数据线路更快地向前传播。</p><div align=center><img src=/images/residual-block.svg width=400px/></div><p>ResNet 沿用了 VGG 全 <span>\(3×3\)
</span>卷积层的设计。残差块里首先有 2 个有相同输出通道数的 <span>\(3×3\)
</span>卷积层。每个卷积层后接一个批量归一化层和 ReLU 激活函数。然后我们将输入跳过这两个卷积运算后直接加在最后的 ReLU 激活函数前。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的 <span>\(1×1\)
</span>卷积层来将输入变换成需要的形状后再做相加运算。</p><div align=center><img src=/images/resnet-block.svg width=500px/></div><hr><h4 id=residual-block-的意义><strong>Residual Block 的意义</strong>
<a class=anchor href=#residual-block-%e7%9a%84%e6%84%8f%e4%b9%89>#</a></h4><ol><li><strong>降低学习难度</strong>：<ul><li><strong>直接学习 <span>\(H(x)\)
</span>（输入到输出的完整映射）可能是一个高度复杂的问题</strong>，而学习残差 <span>\(F(x) = H(x) - x\)
</span>相对简单得多。</li><li>在许多实际任务中，输入 <span>\(x\)
</span>与目标 <span>\(H(x)\)
</span><strong>通常是接近的</strong>（例如图像分类任务中，特征提取后的信息不会发生剧烈变化）。</li><li>通过学习残差 <span>\(F(x)\)
</span>，<strong>网络只需关注输入与输出之间的细微差异</strong>，而不必重新建模整个映射。</li></ul></li><li><strong>缓解梯度消失问题：</strong><ul><li>在深度神经网络中，随着层数增加，梯度会因为<strong>多次链式求导而逐渐减小</strong>，最终导致<strong>梯度消失</strong>问题。</li><li>残差连接提供了一条直接路径，使得<strong>梯度能够不经过中间层直接流回前面的层</strong>。这种 “shortcut” 避免了梯度的逐层衰减，缓解了梯度消失问题，<strong>允许更深层的网络训练</strong>。</li><li>数学上，梯度通过残差连接流回时只需求导 <span>\(\frac{\partial y}{\partial x} = 1 + \frac{\partial F(x)}{\partial x}\)
</span>，<strong>即便 <span>\(\frac{\partial F(x)}{\partial x}\)
</span>较小，梯度仍然保留一个恒定值 1</strong>。</li></ul></li><li><strong>解决退化问题：</strong><ul><li>深度网络常面临 <strong>退化问题</strong>：随着网络<strong>层数增加，模型性能可能不升反降</strong>，即便出现过拟合的风险，这种现象依然存在。</li><li>残差连接的设计允许某些层 “跳过”，从而实现恒等映射（identity mapping），如果某些层对任务无用，网络会自动倾向于<strong>学习恒等映射，使这些层的输出等于输入</strong>。这<strong>确保了增加网络深度不会降低性能</strong>。</li></ul></li><li><strong>简化优化问题：</strong><ul><li>从优化角度看，直接拟合复杂的 <span>\(H(x)\)
</span>映射可能存在多个局部极小值，导致优化困难。而拟合残差 <span>\(F(x) = H(x) - x\)
</span>则相当于让网络优化从输入的一个初始解 <span>\(x\)
</span>开始，再逐步修正偏差。</li><li>这将复杂映射分解为多个简单问题，简化了优化过程，使训练更快、更稳定。</li></ul></li><li><strong>提高非线性能力：</strong><ul><li>在传统的网络结构中，每层都直接学习 <span>\(H(x)\)
</span>，如果层数较深，层之间的信息传递可能会导致过平滑的问题（即<strong>较浅层的细节信息在深层被逐渐削弱</strong>）。</li><li>残差学习通过直接连接输入 <span>\(x\)
</span>和输出 <span>\(F(x)\)
</span>，<strong>让浅层信息直接参与深层的输出，避免了过度平滑</strong>，从而提高了网络的表达能力。</li></ul></li></ol><div align=center><img src=/images/resnet18-90.svg width=600px/></div><ul><li><strong>ResNet 代码实现</strong><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torchvision <span style=color:#f92672>import</span> models
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ResidualBlock</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, in_channels, out_channels, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, downsample<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        super(ResidualBlock, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>conv1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(in_channels, out_channels, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span>stride, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>bn1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>BatchNorm2d(out_channels)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>relu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>conv2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(out_channels, out_channels, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>bn2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>BatchNorm2d(out_channels)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>downsample <span style=color:#f92672>=</span> downsample
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        identity <span style=color:#f92672>=</span> x
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>downsample <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            identity <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>downsample(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>conv1(x)
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>bn1(out)
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>relu(out)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>conv2(out)
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>bn2(out)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        out <span style=color:#f92672>+=</span> identity
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>relu(out)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> out
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ResNet18</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>):
</span></span><span style=display:flex><span>        super(ResNet18, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>in_channels <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>conv1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>64</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>bn1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>BatchNorm2d(<span style=color:#ae81ff>64</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>relu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>maxpool <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MaxPool2d(kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer1 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_make_layer(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer2 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_make_layer(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>2</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer3 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_make_layer(<span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>2</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer4 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_make_layer(<span style=color:#ae81ff>512</span>, <span style=color:#ae81ff>2</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>avgpool <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>AdaptiveAvgPool2d((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>512</span>, num_classes)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_make_layer</span>(self, out_channels, blocks, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>        downsample <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> stride <span style=color:#f92672>!=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>or</span> self<span style=color:#f92672>.</span>in_channels <span style=color:#f92672>!=</span> out_channels:
</span></span><span style=display:flex><span>            downsample <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>                nn<span style=color:#f92672>.</span>Conv2d(self<span style=color:#f92672>.</span>in_channels, out_channels, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span>stride, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>),
</span></span><span style=display:flex><span>                nn<span style=color:#f92672>.</span>BatchNorm2d(out_channels),
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        layers <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        layers<span style=color:#f92672>.</span>append(ResidualBlock(self<span style=color:#f92672>.</span>in_channels, out_channels, stride, downsample))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>in_channels <span style=color:#f92672>=</span> out_channels
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, blocks):
</span></span><span style=display:flex><span>            layers<span style=color:#f92672>.</span>append(ResidualBlock(out_channels, out_channels))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> nn<span style=color:#f92672>.</span>Sequential(<span style=color:#f92672>*</span>layers)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>conv1(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>bn1(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>relu(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>maxpool(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer1(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer2(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer3(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer4(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>avgpool(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>flatten(x, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#从全连接层到卷积from-fc-layer-to-convolutional><strong>从全连接层到卷积（From FC Layer to Convolutional）</strong></a><ul><li><a href=#多层感知机mlp的限制><strong>多层感知机（MLP）的限制</strong></a></li></ul></li><li><a href=#图像卷积convolutions-for-images><strong>图像卷积（Convolutions for Images）</strong></a><ul><li><a href=#互相关操作the-cross-correlation-operation><strong>互相关操作（The Cross-Correlation Operation）</strong></a></li><li><a href=#卷积层convolutional-layers><strong>卷积层（Convolutional Layers）</strong></a></li><li><a href=#利用学习卷积核实现图像边缘检测><strong>利用学习卷积核实现图像边缘检测</strong></a></li><li><a href=#特征映射和感受野feature-map-and-receptive-field><strong>特征映射和感受野（Feature Map and Receptive Field）</strong></a></li></ul></li><li><a href=#填充padding><strong>填充（Padding）</strong></a></li><li><a href=#步幅stride><strong>步幅（Stride）</strong></a></li><li><a href=#多输入通道和多输出通道multiple-input-and-multiple-output-channels><strong>多输入通道和多输出通道（Multiple Input and Multiple Output Channels）</strong></a><ul><li><a href=#多输入通道multiple-input-channels><strong>多输入通道（Multiple Input Channels）</strong></a></li><li><a href=#多输出通道multiple-output-channels><strong>多输出通道（Multiple Output Channels）</strong></a></li><li><a href=#结合多输入和多输出通道><strong>结合多输入和多输出通道</strong></a></li><li><a href=#11-卷积层><strong>1×1 卷积层</strong></a></li></ul></li><li><a href=#池化层pooling><strong>池化层（Pooling）</strong></a><ul><li><a href=#最大池化层和平均池化层maximum-pooling-and-average-pooling><strong>最大池化层和平均池化层（Maximum Pooling and Average Pooling）</strong></a></li><li><a href=#填充步幅以及多通道padding-stride-and-multiple-channels><strong>填充，步幅以及多通道（Padding, Stride, and Multiple Channels）</strong></a></li><li><a href=#池化的主要作用><strong>池化的主要作用</strong></a></li></ul></li><li><a href=#经典卷积神经网络-modern-convolutional-neural-networks><strong>经典卷积神经网络 （Modern Convolutional Neural Networks）</strong></a><ul><li><a href=#lenet><strong>LeNet</strong></a></li><li><a href=#alexnet><strong>AlexNet</strong></a></li><li><a href=#vgg><strong>VGG</strong></a></li><li><a href=#network-in-network-nin><strong>Network in Network (NiN)</strong></a></li><li><a href=#resnet><strong>ResNet</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>