<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  循环神经网络（Recurrent Neural Networks）
  #

传统的机器学习模型（如线性回归、逻辑回归和多层感知机）主要针对固定长度的数据，比如表格数据，这些数据通常以行和列的形式组织，其中每一行是一个样本，每一列是一个特征。对于这些模型，数据的结构并不重要，只需确保每个样本的特征数量固定即可。
在处理图像数据时，由于图像包含固定长度的像素信息，我们可以引入卷积神经网络（CNN）来捕获数据的层次结构和空间不变性。然而，这类数据仍然是固定长度的，例如Fashion-MNIST数据集中每张图像是固定大小的 



  \(28 \times 28\)

 的像素网格。
许多学习任务需要处理序列数据（Sequential Data）。例如，图像字幕生成、语音合成和音乐生成需要模型输出序列化数据；而时间序列预测、视频分析等任务需要模型从序列化数据中学习。在自然语言处理、对话系统设计、机器人控制等领域，这些输入和输出的序列特性通常同时存在。
循环神经网络（RNN） 是一种深度学习模型，专门设计用来捕获序列数据的动态特性。通过引入循环连接（可看作网络中的循环边），RNN能够跨时间步共享参数并传递信息。在展开视图中（即将循环连接按时间步展开），RNN可以看作一种特殊的前馈神经网络，其参数在各时间步间共享。循环连接动态地跨时间步传播信息，而常规连接则在同一时间步内传播信息。
RNN的核心思想是：许多输入和目标无法轻易表示为固定长度的向量，但可以表示为固定长度向量的变长序列。例如，文档可以表示为单词序列，医疗记录可以表示为事件序列，视频可以表示为静态图像序列。


  处理序列（Working with Sequences）
  #

在传统模型中，输入通常是单一的特征向量（feature vector）。而在处理序列数据时，输入变为一个有序的特征向量列表，每个特征向量按照时间步（time step）进行索引。这样的序列数据在许多场景中广泛存在，例如长时间的传感器数据流、文本序列或病人住院记录。
对于序列数据，有两种主要形式：

单个超长序列（如气候科学中的传感器数据流），可以通过随机采样固定长度的子序列生成训练数据集。
独立的多个序列集合（如文档集合，每个文档由不同长度的单词序列表示；或住院记录，每次住院由不同长度的事件序列组成）。

在传统的独立样本假设下，我们认为每个输入样本是从相同分布中独立采样的。而在序列数据中，尽管整个序列可以被认为是独立的，但序列内部的时间步之间往往具有强相关性。例如，文档后续单词的出现通常依赖于前面的单词，病人第10天的用药往往取决于前9天的病情记录。这种依赖性正是序列模型存在的意义：如果序列中的元素互不相关，就没有必要将其建模为序列。序列模型的核心在于捕捉这些依赖性。
根据任务目标的不同，序列建模可以分为以下几种类型：

固定目标预测：给定一个序列输入，预测一个固定目标，例如基于电影评论进行情感分类。

输入： 一个文本序列，例如电影评论。 dim = [batch_size, seq_length]，其中 batch_size 是批次大小，seq_length 是每个文本序列的长度。
输出：对应的情感类别（通常是一个标量），例如“正面”或“负面”。dim = [batch_size, 1] 或 [batch_size]，这是一个标量值表示类别。


序列目标预测：给定固定输入，预测一个序列目标，例如图像描述生成。

输入： 一个图像输入，通常是一个张量表示的图像或图像的特征（例如从CNN提取的特征向量）。 [batch_size, feature_size]，其中 feature_size 是图像特征的维度。
输出： 一个文本序列（描述）。[batch_size, seq_length]，其中 seq_length 是生成的描述的词数。


序列到序列预测：同时处理序列输入和序列输出。例如：

对齐的序列到序列任务：输入和输出在时间步上逐一对应，如词性标注（part-of-speech tagging）。

输入： 一个输入文本序列，例如源语言句子（例如英文句子）。[batch_size, seq_length]，其中 seq_length 是源语言句子的长度。
输出：一个输出文本序列，目标语言的翻译（例如中文句子）。 [batch_size, seq_length]，其中 seq_length 是目标语言句子的长度。


非对齐的序列到序列任务：输入与输出没有严格的时间步对应关系，如机器翻译或视频字幕生成。

输入：一个视频帧序列（或者是视频帧的特征表示）。[batch_size, num_frames, feature_size]，其中 num_frames 是视频帧数，feature_size 是每个帧的特征维度。
输出： 一个文本描述序列，描述视频内容。[batch_size, seq_length]，这是生成的字幕。







  自回归模型（Autoregressive Models）
  #

假设一位交易员希望进行短期交易，根据对下一时间步指数价格涨跌的预测来决定买入或卖出。如果没有其他辅助特征（如新闻或财报数据），交易员只能依据截至当前的价格历史数据预测下一时刻的价格。这时需要估计的就是价格在下一时间步可能取值的概率分布 
  \(P(x_{t+1} | x_1, \dots, x_t)\)

。由于直接估计连续变量的整个概率分布较为困难，交易员通常更关注分布的一些关键统计量，例如期望值（expected value）和方差（variance）。"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/deep-learning/recurrent-neural-networks/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Recurrent Neural Networks"><meta property="og:description" content="循环神经网络（Recurrent Neural Networks） # 传统的机器学习模型（如线性回归、逻辑回归和多层感知机）主要针对固定长度的数据，比如表格数据，这些数据通常以行和列的形式组织，其中每一行是一个样本，每一列是一个特征。对于这些模型，数据的结构并不重要，只需确保每个样本的特征数量固定即可。
在处理图像数据时，由于图像包含固定长度的像素信息，我们可以引入卷积神经网络（CNN）来捕获数据的层次结构和空间不变性。然而，这类数据仍然是固定长度的，例如Fashion-MNIST数据集中每张图像是固定大小的 \(28 \times 28\) 的像素网格。
许多学习任务需要处理序列数据（Sequential Data）。例如，图像字幕生成、语音合成和音乐生成需要模型输出序列化数据；而时间序列预测、视频分析等任务需要模型从序列化数据中学习。在自然语言处理、对话系统设计、机器人控制等领域，这些输入和输出的序列特性通常同时存在。
循环神经网络（RNN） 是一种深度学习模型，专门设计用来捕获序列数据的动态特性。通过引入循环连接（可看作网络中的循环边），RNN能够跨时间步共享参数并传递信息。在展开视图中（即将循环连接按时间步展开），RNN可以看作一种特殊的前馈神经网络，其参数在各时间步间共享。循环连接动态地跨时间步传播信息，而常规连接则在同一时间步内传播信息。
RNN的核心思想是：许多输入和目标无法轻易表示为固定长度的向量，但可以表示为固定长度向量的变长序列。例如，文档可以表示为单词序列，医疗记录可以表示为事件序列，视频可以表示为静态图像序列。
处理序列（Working with Sequences） # 在传统模型中，输入通常是单一的特征向量（feature vector）。而在处理序列数据时，输入变为一个有序的特征向量列表，每个特征向量按照时间步（time step）进行索引。这样的序列数据在许多场景中广泛存在，例如长时间的传感器数据流、文本序列或病人住院记录。
对于序列数据，有两种主要形式：
单个超长序列（如气候科学中的传感器数据流），可以通过随机采样固定长度的子序列生成训练数据集。 独立的多个序列集合（如文档集合，每个文档由不同长度的单词序列表示；或住院记录，每次住院由不同长度的事件序列组成）。 在传统的独立样本假设下，我们认为每个输入样本是从相同分布中独立采样的。而在序列数据中，尽管整个序列可以被认为是独立的，但序列内部的时间步之间往往具有强相关性。例如，文档后续单词的出现通常依赖于前面的单词，病人第10天的用药往往取决于前9天的病情记录。这种依赖性正是序列模型存在的意义：如果序列中的元素互不相关，就没有必要将其建模为序列。序列模型的核心在于捕捉这些依赖性。
根据任务目标的不同，序列建模可以分为以下几种类型：
固定目标预测：给定一个序列输入，预测一个固定目标，例如基于电影评论进行情感分类。 输入： 一个文本序列，例如电影评论。 dim = [batch_size, seq_length]，其中 batch_size 是批次大小，seq_length 是每个文本序列的长度。 输出：对应的情感类别（通常是一个标量），例如“正面”或“负面”。dim = [batch_size, 1] 或 [batch_size]，这是一个标量值表示类别。 序列目标预测：给定固定输入，预测一个序列目标，例如图像描述生成。 输入： 一个图像输入，通常是一个张量表示的图像或图像的特征（例如从CNN提取的特征向量）。 [batch_size, feature_size]，其中 feature_size 是图像特征的维度。 输出： 一个文本序列（描述）。[batch_size, seq_length]，其中 seq_length 是生成的描述的词数。 序列到序列预测：同时处理序列输入和序列输出。例如： 对齐的序列到序列任务：输入和输出在时间步上逐一对应，如词性标注（part-of-speech tagging）。 输入： 一个输入文本序列，例如源语言句子（例如英文句子）。[batch_size, seq_length]，其中 seq_length 是源语言句子的长度。 输出：一个输出文本序列，目标语言的翻译（例如中文句子）。 [batch_size, seq_length]，其中 seq_length 是目标语言句子的长度。 非对齐的序列到序列任务：输入与输出没有严格的时间步对应关系，如机器翻译或视频字幕生成。 输入：一个视频帧序列（或者是视频帧的特征表示）。[batch_size, num_frames, feature_size]，其中 num_frames 是视频帧数，feature_size 是每个帧的特征维度。 输出： 一个文本描述序列，描述视频内容。[batch_size, seq_length]，这是生成的字幕。 自回归模型（Autoregressive Models） # 假设一位交易员希望进行短期交易，根据对下一时间步指数价格涨跌的预测来决定买入或卖出。如果没有其他辅助特征（如新闻或财报数据），交易员只能依据截至当前的价格历史数据预测下一时刻的价格。这时需要估计的就是价格在下一时间步可能取值的概率分布 \(P(x_{t+1} | x_1, \dots, x_t)\) 。由于直接估计连续变量的整个概率分布较为困难，交易员通常更关注分布的一些关键统计量，例如期望值（expected value）和方差（variance）。"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Recurrent Neural Networks | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/deep-learning/recurrent-neural-networks/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.d8049fa7eed2603b55910f8399054d94d9a219d3b8ef9968130d31788c22de20.js integrity="sha256-2ASfp+7SYDtVkQ+DmQVNlNmiGdO475loEw0xeIwi3iA=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/deep-learning/recurrent-neural-networks/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a><ul></ul></li><li><a href=/docs/deep-learning/recurrent-neural-networks/ class=active>Recurrent Neural Networks</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/natural-language-processing/>Natural Language Processing</a><ul></ul></li><li><a href=/docs/deep-learning/computer-vision/>Computer Vision</a><ul></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Recurrent Neural Networks</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#处理序列working-with-sequences><strong>处理序列（Working with Sequences）</strong></a><ul><li><a href=#自回归模型autoregressive-models><strong>自回归模型（Autoregressive Models）</strong></a></li><li><a href=#序列模型sequence-models><strong>序列模型（Sequence Models）</strong></a></li></ul></li><li><a href=#文本预处理converting-raw-text-into-sequence-data><strong>文本预处理（Converting Raw Text into Sequence Data）</strong></a></li><li><a href=#语言模型和数据集language-models><strong>语言模型和数据集（Language Models）</strong></a><ul><li></li><li><a href=#困惑度perplexity><strong>困惑度（Perplexity）</strong></a></li><li><a href=#读取长序列数据partitioning-sequences><strong>读取长序列数据（Partitioning Sequences）</strong></a></li></ul></li><li><a href=#循环神经网络rnn概述><strong>循环神经网络（RNN）概述</strong></a><ul><li><a href=#rnn的结构与计算><strong>RNN的结构与计算</strong></a></li><li><a href=#基于循环神经网络的字符级语言模型><strong>基于循环神经网络的字符级语言模型</strong></a></li><li><a href=#one-hot-encoding-独热编码><strong>One-Hot Encoding 独热编码</strong></a></li><li><a href=#梯度裁剪gradient-clipping><strong>梯度裁剪（Gradient Clipping）</strong></a></li><li><a href=#解码-decoding><strong>解码 (Decoding)</strong></a></li></ul></li><li><a href=#rnn中的反向传播算法><strong>RNN中的反向传播算法</strong></a><ul><li><a href=#反向传播的细节><strong>反向传播的细节</strong></a></li></ul></li><li><a href=#简单循环神经网络实现><strong>简单循环神经网络实现</strong></a></li><li><a href=#经典循环神经网络modern-recurrent-neural-networks><strong>经典循环神经网络（Modern Recurrent Neural Networks）</strong></a><ul><li><a href=#长短时记忆网络long-short-term-memory-lstm><strong>长短时记忆网络（Long Short-Term Memory, LSTM）</strong></a></li><li><a href=#门控循环单元gated-recurrent-units-gru><strong>门控循环单元（Gated Recurrent Units, GRU）</strong></a></li><li><a href=#深层循环神经网络deep-recurrent-neural-networks-drnn><strong>深层循环神经网络（Deep Recurrent Neural Networks, DRNN）</strong></a></li><li><a href=#双向循环神经网络bidirectional-recurrent-neural-networks><strong>双向循环神经网络（Bidirectional Recurrent Neural Networks）</strong></a></li><li><a href=#编码器-解码器encoder-decoder架构><strong>编码器-解码器（Encoder-Decoder）架构</strong></a></li><li><a href=#序列到序列学习seq2seq><strong>序列到序列学习（seq2seq）</strong></a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=循环神经网络recurrent-neural-networks><strong>循环神经网络（Recurrent Neural Networks）</strong>
<a class=anchor href=#%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9crecurrent-neural-networks>#</a></h1><p>传统的机器学习模型（如线性回归、逻辑回归和多层感知机）主要针对固定长度的数据，比如表格数据，这些数据通常以行和列的形式组织，其中每一行是一个样本，每一列是一个特征。对于这些模型，<strong>数据的结构并不重要</strong>，只需确保每个样本的<strong>特征数量固定</strong>即可。</p><p>在处理图像数据时，由于图像包含固定长度的像素信息，我们可以引入卷积神经网络（CNN）来捕获数据的层次结构和空间不变性。然而，<strong>这类数据仍然是固定长度的</strong>，例如Fashion-MNIST数据集中每张图像是固定大小的
<link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\(28 \times 28\)
</span>的像素网格。</p><p>许多学习任务需要<strong>处理序列数据（Sequential Data）</strong>。例如，图像字幕生成、语音合成和音乐生成需要模型输出序列化数据；而时间序列预测、视频分析等任务需要模型从序列化数据中学习。在自然语言处理、对话系统设计、机器人控制等领域，这些输入和输出的序列特性通常同时存在。</p><p>循环神经网络（RNN） 是一种深度学习模型，专门设计用来捕获序列数据的动态特性。通过引入循环连接（可看作网络中的循环边），RNN能够跨时间步共享参数并传递信息。在展开视图中（即将循环连接按时间步展开），RNN可以看作一种特殊的前馈神经网络，其参数在各时间步间共享。循环连接动态地跨时间步传播信息，而常规连接则在同一时间步内传播信息。</p><p><strong>RNN的核心思想</strong>是：许多输入和目标<strong>无法轻易表示为固定长度的向量</strong>，但<strong>可以表示为固定长度向量的变长序列</strong>。例如，文档可以表示为单词序列，医疗记录可以表示为事件序列，视频可以表示为静态图像序列。</p><hr><h2 id=处理序列working-with-sequences><strong>处理序列（Working with Sequences）</strong>
<a class=anchor href=#%e5%a4%84%e7%90%86%e5%ba%8f%e5%88%97working-with-sequences>#</a></h2><p>在传统模型中，输入通常是<strong>单一的特征向量（feature vector）</strong>。而在处理序列数据时，输入变为一个<strong>有序的特征向量列表</strong>，每个特征向量按照时间步（time step）进行索引。这样的序列数据在许多场景中广泛存在，例如长时间的传感器数据流、文本序列或病人住院记录。</p><p>对于序列数据，有两种主要形式：</p><ol><li><strong>单个超长序列</strong>（如气候科学中的传感器数据流），可以通过随机<strong>采样固定长度的子序列生成训练数据集</strong>。</li><li><strong>独立的多个序列集合</strong>（如文档集合，每个文档由不同长度的单词序列表示；或住院记录，每次住院由不同长度的事件序列组成）。</li></ol><p>在传统的独立样本假设下，我们认为每个输入<strong>样本是从相同分布中独立采样的</strong>。而在序列数据中，尽管整个序列可以被认为是独立的，但<strong>序列内部的时间步之间往往具有强相关性</strong>。例如，文档后续单词的出现通常依赖于前面的单词，病人第10天的用药往往取决于前9天的病情记录。这种依赖性正是序列模型存在的意义：<strong>如果序列中的元素互不相关，就没有必要将其建模为序列</strong>。序列模型的核心在于捕捉这些依赖性。</p><p>根据任务目标的不同，序列建模可以分为以下几种类型：</p><ol><li><strong>固定目标预测</strong>：给定一个序列输入，预测一个固定目标，例如基于电影评论进行情感分类。<ul><li><strong>输入</strong>： 一个文本序列，例如电影评论。 <code>dim = [batch_size, seq_length]</code>，其中 <code>batch_size</code> 是批次大小，<code>seq_length</code> 是每个文本序列的长度。</li><li><strong>输出</strong>：对应的情感类别（通常是一个标量），例如“正面”或“负面”。<code>dim = [batch_size, 1]</code> 或 <code>[batch_size]</code>，这是一个标量值表示类别。</li></ul></li><li><strong>序列目标预测</strong>：给定固定输入，预测一个序列目标，例如图像描述生成。<ul><li><strong>输入</strong>： 一个图像输入，通常是一个张量表示的图像或图像的特征（例如从CNN提取的特征向量）。 <code>[batch_size, feature_size]</code>，其中 <code>feature_size</code> 是图像特征的维度。</li><li><strong>输出</strong>： 一个文本序列（描述）。<code>[batch_size, seq_length]</code>，其中 <code>seq_length</code> 是生成的描述的词数。</li></ul></li><li><strong>序列到序列预测</strong>：同时处理序列输入和序列输出。例如：<ul><li>对齐的序列到序列任务：输入和输出在时间步上逐一对应，如词性标注（part-of-speech tagging）。<ul><li><strong>输入</strong>： 一个输入文本序列，例如源语言句子（例如英文句子）。<code>[batch_size, seq_length]</code>，其中 <code>seq_length</code> 是源语言句子的长度。</li><li><strong>输出</strong>：一个输出文本序列，目标语言的翻译（例如中文句子）。 <code>[batch_size, seq_length]</code>，其中 <code>seq_length</code> 是目标语言句子的长度。</li></ul></li><li>非对齐的序列到序列任务：输入与输出没有严格的时间步对应关系，如机器翻译或视频字幕生成。<ul><li><strong>输入</strong>：一个视频帧序列（或者是视频帧的特征表示）。<code>[batch_size, num_frames, feature_size]</code>，其中 <code>num_frames</code> 是视频帧数，<code>feature_size</code> 是每个帧的特征维度。</li><li><strong>输出</strong>： 一个文本描述序列，描述视频内容。<code>[batch_size, seq_length]</code>，这是生成的字幕。</li></ul></li></ul></li></ol><hr><h3 id=自回归模型autoregressive-models><strong>自回归模型（Autoregressive Models）</strong>
<a class=anchor href=#%e8%87%aa%e5%9b%9e%e5%bd%92%e6%a8%a1%e5%9e%8bautoregressive-models>#</a></h3><p>假设一位交易员希望进行短期交易，根据对下一时间步指数价格涨跌的预测来决定买入或卖出。如果没有其他辅助特征（如新闻或财报数据），交易员只能依据截至当前的价格历史数据预测下一时刻的价格。这时需要估计的就是价格在下一时间步可能取值的概率分布 <span>\(P(x_{t+1} | x_1, \dots, x_t)\)
</span>。由于直接估计连续变量的整个概率分布较为困难，交易员通常更关注分布的一些关键统计量，例如期望值（expected value）和方差（variance）。</p><p>一种简单的估计条件期望的方法是应用线性回归模型（linear regression model），即<strong>根据信号的历史值预测其未来值</strong>。这类模型被称为<strong>自回归模型（autoregressive models）</strong>。自回归模型假设当前时间步的观测值 <span>\(x_t\)
</span>是之前时间步观测值 <span>\(x_{t-1}, x_{t-2}, \dots\)
</span>的线性组合加上一个随机噪声项 <span>\(\epsilon_t\)
</span>。表达式如下：
<span>\[
x_t = c + \phi_1 x_{t-1} + \phi_2 x_{t-2} + \dots + \phi_p x_{t-p} + \epsilon_t
\]</span></p><blockquote class="book-hint warning"><p>自回归模型特指那些<strong>仅依赖于序列本身的历史观测值进行预测</strong>的统计模型。如果模型涉及到外部因素、非线性关系或更复杂的结构（如RNN、LSTM等），则它们不再是严格意义上的自回归模型。</p></blockquote><p>然而，自回归模型面临一个主要问题：输入数量 <span>\(t\)
</span><strong>随着时间步数增长而变化</strong>，导致每个样本的<strong>特征数量不一致</strong>。这给训练过程带来了挑战，因为许多模型（例如线性回归或深度网络）都<strong>要求固定长度的输入向量</strong>。克服这一挑战的常用策略有：</p><ol><li><strong>窗口化（Windowing）</strong>：为了简化模型的输入维度，可以假设在预测短期未来时，仅需要观察最近的 <span>\(\tau\)
</span>个时间步数据，而无需回溯到整个历史。这种情况下，只需使用一个长度为 <span>\(\tau\)
</span>的<strong>滑动窗口 <span>\((x_{t-\tau+1}, \dots, x_t)\)
</span>作为输入</strong>。这种方式<strong>确保了输入特征的数量固定</strong>，适用于许多要求固定输入长度的模型。</li><li><strong>隐变量模型（Latent Autoregressive Models）</strong>：构建一种模型，该模型通过<strong>维护一个过去观测值的总结 <span>\(h_t\)
</span>来压缩历史信息</strong>。在每个时间步，该模型<strong>不仅预测 <span>\(x_{t+1}\)
</span>，还更新摘要 <span>\(h_{t+1} = g(h_t, x_t)\)
</span></strong>。由于 <span>\(h_t\)
</span>是未观测到的<strong>隐变量（latent variable）</strong>，这样的模型也被称为<strong>隐变量自回归模型</strong>。这种方法可以捕捉更复杂的历史依赖关系。</li></ol><div align=center><img src=/images/sequence-model.svg width=300px/></div><hr><h3 id=序列模型sequence-models><strong>序列模型（Sequence Models）</strong>
<a class=anchor href=#%e5%ba%8f%e5%88%97%e6%a8%a1%e5%9e%8bsequence-models>#</a></h3><p>在处理序列数据，尤其是语言时，我们常常希望估计整个序列的联合概率。这种任务通常被称为<strong>序列建模（sequence modeling）</strong>，在自然语言处理中，序列建模常被称为语言模型（language model）。语言模型不仅可以用来评估句子的可能性，还能生成新序列或优化生成的序列。在语言建模中，我们可以通过链式法则将序列的联合概率分解成条件概率的乘积，从而将问题转化为自回归预测（autoregressive prediction）。如果序列数据是离散信号（如单词），<strong>自回归模型通常是一个概率分类器，输出词汇表中下一个词的概率分布</strong>。
<span>\[
P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1)
\]</span></p><p>有时我们会希望在建模时 <strong>仅依赖于前几个时间步的历史数据</strong>，而不是整个序列的历史。此时，如果我们丢弃超过前几个时间步的历史而不损失预测能力，我们称该序列满足 <strong>马尔可夫条件（Markov condition）</strong>，即 <strong>未来仅依赖于最近的历史</strong>，而与更早的历史无关。当我们仅依赖于前一个时间步时，数据符合一阶马尔可夫模型；如果依赖于前两个时间步，则符合二阶马尔可夫模型。在实际应用中，我们通常会选择近似满足马尔可夫条件的模型，尽管真实的文本数据会随着更多历史信息的加入逐渐改善预测效果，但增益是有限的。因此，有时我们会选择使用高阶马尔可夫模型，以减少计算和统计上的困难。
<span>\[
P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1})
\]</span></p><p>在文本序列解码时，通常选择按照<strong>从左到右的顺序来分解条件概率</strong>。这种顺序更符合我们日常阅读习惯（如大多数语言是从左到右读的），而且我们也能更直观地预测下一个可能出现的词。通过这种方式，我们可以为任意长的序列分配概率，只需要将新的词的条件概率乘以前面已计算的概率。此外，<strong>预测相邻词的模型通常比预测其他位置的词更加精准</strong>，这也是选择从左到右解码的一个原因。对于许多数据类型来说，这种顺序的预测比其他顺序更易于建模。例如，在因果结构数据中，未来的事件不能影响过去的事件，这使得从当前时刻预测未来比反向预测更容易。</p><blockquote class="book-hint warning"><p>基于 n-阶马尔可夫条件（Markov condition），即只依赖于前 n 个数据点来做预测。当我们用这种方式进行 <strong>一步预测（one-step-ahead prediction）时，模型效果良好，因为它依赖于已知的历史数据</strong>。（e.g. 基于时间点604 预测时间点 605）</p><p>然而，当我们进行 <strong>多步预测（multi-step-ahead prediction）</strong> 时，问题变得复杂。（e.g. 基于时间点604 预测时间点 609）我们无法直接通过已知数据计算预测值，因此我们需要利用先前的预测值作为输入来进行后续预测（e.g. 因为我们没有时间点 605-608 的数据，所以需要根据 604 先预测 605，再依据 605 预测 606，以此类推）。<strong>这种逐步递推的方式会导致预测的误差在每一步都积累。这些误差会随着时间步的推进而累积，导致预测结果逐渐偏离真实值</strong>。就像天气预报一样，短期预测较为准确，但长期预测误差逐渐增大。</p></blockquote><hr><h2 id=文本预处理converting-raw-text-into-sequence-data><strong>文本预处理（Converting Raw Text into Sequence Data）</strong>
<a class=anchor href=#%e6%96%87%e6%9c%ac%e9%a2%84%e5%a4%84%e7%90%86converting-raw-text-into-sequence-data>#</a></h2><p>在处理文本数据时，我们通常需要将原始文本转换为适合模型使用的数值形式。这一过程包含以下几个步骤：</p><ol><li><strong>读取文本数据</strong>：将原始文本加载为字符串，并预处理以去掉标点和大小写。</li><li><strong>分词（Tokenization）</strong>：将文本分割为基本的语义单元（Token）。Token可以是单词、字符，或更小的词片（Word Piece）。例如，句子“Baby needs a new pair of shoes”可以被表示为包含7个单词的序列或30个字符的序列。选择哪种形式取决于具体应用。</li><li><strong>构建词汇表（Vocabulary）</strong>：将Token映射到唯一的数值索引。首先确定训练数据中所有唯一Token的集合，并为每个Token分配索引。构建好的词汇表可以将字符串转换为数值序列，同时保留原始信息，支持将数值序列还原为字符串。例如：<pre tabindex=0><code>文本: [&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;, &#39;by&#39;, &#39;h&#39;, &#39;g&#39;, &#39;wells&#39;]
索引: [1, 19, 50, 40, 2183, 2184, 400]
文本: [&#39;twinkled&#39;, &#39;and&#39;, &#39;his&#39;, &#39;usually&#39;, &#39;pale&#39;, &#39;face&#39;]
索引: [2186, 3, 25, 1044, 362, 113]
</code></pre></li></ol><ul><li><strong>文本预处理代码示例：</strong></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 原始文本</span>
</span></span><span style=display:flex><span>text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Baby needs a new pair of shoes. Baby likes shoes too!&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 转小写并去掉标点符号</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> re
</span></span><span style=display:flex><span>processed_text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#34;[^\w\s]&#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>, text<span style=color:#f92672>.</span>lower())
</span></span><span style=display:flex><span>print(processed_text)
</span></span><span style=display:flex><span><span style=color:#75715e># 输出: &#34;baby needs a new pair of shoes baby likes shoes too&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 按单词分词</span>
</span></span><span style=display:flex><span>tokens <span style=color:#f92672>=</span> processed_text<span style=color:#f92672>.</span>split()
</span></span><span style=display:flex><span>print(tokens)
</span></span><span style=display:flex><span><span style=color:#75715e># 输出: [&#39;baby&#39;, &#39;needs&#39;, &#39;a&#39;, &#39;new&#39;, &#39;pair&#39;, &#39;of&#39;, </span>
</span></span><span style=display:flex><span><span style=color:#75715e>#       &#39;shoes&#39;, &#39;baby&#39;, &#39;likes&#39;, &#39;shoes&#39;, &#39;too&#39;]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 构建词汇表</span>
</span></span><span style=display:flex><span>vocab <span style=color:#f92672>=</span> {token: idx <span style=color:#66d9ef>for</span> idx, token <span style=color:#f92672>in</span> enumerate(sorted(set(tokens)))}
</span></span><span style=display:flex><span>print(vocab)
</span></span><span style=display:flex><span><span style=color:#75715e># 输出: {&#39;a&#39;: 0, &#39;baby&#39;: 1, &#39;likes&#39;: 2, &#39;needs&#39;: 3, </span>
</span></span><span style=display:flex><span><span style=color:#75715e>#       &#39;new&#39;: 4, &#39;of&#39;: 5, &#39;pair&#39;: 6, &#39;shoes&#39;: 7, &#39;too&#39;: 8}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 将单词序列转换为数值索引序列</span>
</span></span><span style=display:flex><span>numerical_sequence <span style=color:#f92672>=</span> [vocab[token] <span style=color:#66d9ef>for</span> token <span style=color:#f92672>in</span> tokens]
</span></span><span style=display:flex><span>print(numerical_sequence)
</span></span><span style=display:flex><span><span style=color:#75715e># 输出: [1, 3, 0, 4, 6, 5, 7, 1, 2, 7, 8]</span>
</span></span></code></pre></div><hr><h2 id=语言模型和数据集language-models><strong>语言模型和数据集（Language Models）</strong>
<a class=anchor href=#%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%92%8c%e6%95%b0%e6%8d%ae%e9%9b%86language-models>#</a></h2><p>语言模型通过估计整个文本序列的联合概率 <span>\(P(x_1, x_2, \dots, x_T)\)
</span>来建模语言，其中 <span>\(T\)
</span>是文本序列的长度，<span>
\(x_t\)
</span>表示序列中的第 <span>\(t\)
</span>个 token。这种联合概率可以分解为条件概率的连乘形式：
<span>\[
P(x_1, x_2, \dots, x_T) = \prod_{t=1}^{T} P(x_t \mid x_1, x_2, \dots, x_{t-1})
\]</span></p><blockquote><p>比如:
<span>\[
\begin{split}\begin{aligned}&P(\textrm{deep}, \textrm{learning}, \textrm{is}, \textrm{fun}) \\
=&P(\textrm{deep}) P(\textrm{learning} \mid \textrm{deep}) P(\textrm{is} \mid \textrm{deep}, \textrm{learning}) P(\textrm{fun} \mid \textrm{deep}, \textrm{learning}, \textrm{is}).\end{aligned}\end{split}
\]</span></p></blockquote><p>理想的语言模型不仅可以生成自然语言文本，还可以通过上下文信息生成合理的对话内容。然而，设计这样一个能够真正理解文本含义的系统仍然非常困难。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>语言模型解决的问题</strong>：语言模型和根据给定的序列预测下一个词。它们的<strong>核心任务是</strong>：通过对前面已经出现的词（或符号）进行建模，来预测下一个最可能出现的词，或者生成后续的词序列。</p></blockquote><hr><h4 id=概率规则与马尔可夫模型markov-models><strong>概率规则与马尔可夫模型（Markov Models）</strong>
<a class=anchor href=#%e6%a6%82%e7%8e%87%e8%a7%84%e5%88%99%e4%b8%8e%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e6%a8%a1%e5%9e%8bmarkov-models>#</a></h4><p>在马尔可夫模型中，序列满足一阶马尔可夫性质，即当前状态只依赖于前一个状态。根据依赖长度，模型可分为单词（Unigram）、双词（Bigram）和三词（Trigram）模型。模型参数包括单词概率和条件概率。
<span>\[
\begin{split}\begin{aligned}
P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2) P(x_3) P(x_4),\\
P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3 \mid x_2) P(x_4 \mid x_3),\\
P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3 \mid x_1, x_2) P(x_4 \mid x_2, x_3).
\end{aligned}\end{split}
\]</span></p><hr><h4 id=词频估计word-frequency><strong>词频估计（Word Frequency）</strong>
<a class=anchor href=#%e8%af%8d%e9%a2%91%e4%bc%b0%e8%ae%a1word-frequency>#</a></h4><p>假设训练数据集是一个大规模的文本语料库，例如维基百科条目或网络上发布的所有文本。<strong>单词的概率可以通过该单词在训练数据中的相对词频来计算</strong>。例如，可以通过统计“deep”作为句子开头出现的次数，来估计概率。另一种稍微不准确的方法是统计“deep”出现的总次数，并除以语料库中的总单词数。这种方法对于频繁出现的单词效果较好。进一步地，可以尝试估计二元组（如“deep learning”）的概率：
<span>\[
\hat{P}(\textrm{learning} \mid \textrm{deep}) = \frac{\text{Count}(\text{deep, learning})}{\text{Count}(\text{deep})}
\]
</span>其中，分子是二元组的出现次数，分母是单个单词的出现次数。然而，估计二元组的概率更加困难，因为“deep learning”这样的二元组在语料库中出现的频率通常较低。对于一些不常见的词组合，可能很难找到足够的出现次数来进行准确的估计。</p><p>对于三元组及以上的组合情况（如“deep learning models”），问题变得更严重。<strong>许多可能的三词组合在语料库中可能完全不存在</strong>。如果不给这些词组合分配一个非零的计数，就无法在语言模型中使用它们。当数据集较小或单词本身极为罕见时，甚至可能找不到这些组合的任何一个实例。</p><p>因此，基于词频的简单统计方法虽然可以处理常见单词和短语，但在应对长序列或罕见组合时存在明显局限性，需要其他方法进行改进。</p><hr><h4 id=拉普拉斯平滑laplace-smoothing><strong>拉普拉斯平滑（Laplace Smoothing）</strong>
<a class=anchor href=#%e6%8b%89%e6%99%ae%e6%8b%89%e6%96%af%e5%b9%b3%e6%bb%91laplace-smoothing>#</a></h4><p>为此，我们使用拉普拉斯平滑（Laplace Smoothing）来改善上述问题。具体方法是在所有计数中添加一个小常量。 用<span>
\(n\)
</span>表示训练集中的单词总数，用<span>
\(m\)
</span>表示唯一单词的数量。
<span>\[
\begin{split}\begin{aligned}
\hat{P}(x) & = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}, \\
\hat{P}(x' \mid x) & = \frac{n(x, x') + \epsilon_2 \hat{P}(x')}{n(x) + \epsilon_2}, \\
\hat{P}(x'' \mid x,x') & = \frac{n(x, x',x'') + \epsilon_3 \hat{P}(x'')}{n(x, x') + \epsilon_3}.
\end{aligned}\end{split}
\]
</span>其中 <span>\(\epsilon_1,\epsilon_2\)
</span>，和 <span>\(\epsilon_3\)
</span>是超参数。以 <span>\(\epsilon_1\)
</span>为例：当 <span>\(\epsilon_1=0\)
</span>时，不应用平滑；当 <span>\(\epsilon_1\)
</span>接近正无穷大时，<span>
\(\hat{P}(x)\)
</span>接近均匀概率分布 <span>\(1/m\)
</span>。</p><p>然而，这样的模型很容易变得无效，原因如下： 首先，我们需要存储所有的计数； 其次，这完全忽略了单词的意思。 例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中， 但是想根据上下文调整这类模型其实是相当困难的。 最后，长单词序列大部分是没出现过的， 因此一个模型如果只是简单地统计先前“看到”的单词序列频率， 那么模型面对这种问题肯定是表现不佳的。</p><hr><h3 id=困惑度perplexity><strong>困惑度（Perplexity）</strong>
<a class=anchor href=#%e5%9b%b0%e6%83%91%e5%ba%a6perplexity>#</a></h3><p>衡量语言模型质量的一种方法是评估其对文本的预测能力。一个好的语言模型能够<strong>以较高的准确性预测下一个词（token）</strong>。例如，对于短语“It is raining”，不同模型可能生成以下扩展：</p><ol><li>“It is raining outside”（合理且逻辑通顺）</li><li>“It is raining banana tree”（语法正确但意义不通）</li><li>“It is raining piouw;kcj pwepoiut”（完全无意义且不合规范）</li></ol><p>显然，第一个扩展质量最好，模型能够捕获合理的词序和上下文语义。第二个扩展较差，但至少模型学会了单词拼写和部分词语之间的关联性。而第三个扩展表明模型训练不足，无法正确拟合数据。</p><p>为了量化模型质量，可以通过计算序列的似然（likelihood）。然而，直接比较似然值并不直观，因为较短的序列通常有更高的似然值。因此，需要一种标准化的方法使得不同长度的文档结果具有可比性。在信息论中，我们用交叉熵（cross-entropy）衡量模型对序列的预测能力。具体地，对于给定序列 <span>\(x_1, x_2, \ldots, x_n\)
</span>，交叉熵损失的公式为：
<span>\[
\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1)
\]
</span>这里， <span>\(P(x_t \mid x_{t-1}, \ldots, x_1)\)
</span>是模型预测的概率，<span>
\(x_t\)
</span>是序列中实际的词。这种方法将不同长度文档的性能变得可比。</p><p>自然语言处理领域通常使用<strong>困惑度（Perplexity</strong> 作为评价标准，它是交叉熵损失的指数形式：
<span>\[
\text{Perplexity} = \exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right)
\]</span></p><p>困惑度可以理解为我们在选择下一个词时<strong>平均可用的真实选项数的倒数</strong>，具体如下：</p><ol><li><strong>最佳情况</strong>：模型对目标词的概率预测为 1，此时困惑度为 1，表明模型预测完全准确。</li><li><strong>最差情况</strong>：模型对目标词的概率预测为 0，此时困惑度为正无穷。</li><li><strong>基线情况</strong>：模型对所有词分布均匀预测，此时困惑度等于词汇表大小 <span>\(V\)
</span>。这提供了一个非平凡的上界，任何有用的模型都应超越这一基线。</li></ol><p><strong>困惑度越低，模型质量越高</strong>，表明其对文本序列的预测能力越强。</p><hr><h3 id=读取长序列数据partitioning-sequences><strong>读取长序列数据（Partitioning Sequences）</strong>
<a class=anchor href=#%e8%af%bb%e5%8f%96%e9%95%bf%e5%ba%8f%e5%88%97%e6%95%b0%e6%8d%aepartitioning-sequences>#</a></h3><p>由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。为了高效处理数据，模型通常以<strong>固定长度的序列小批量（minibatch）为单位</strong>进行训练。一个关键问题是如何从数据集中随机读取输入序列和目标序列的小批量。</p><p>假设数据集是一个表示语料库中词索引的序列。我们将其划分为固定长度 <span>\(n\)
</span>的子序列（subsequence）。为了在每个训练周期（epoch）中覆盖几乎所有的词，同时保证随机性，我们在每轮训练开始时丢弃前 <span>\(r\)
</span>个词，其中 <span>\(r\)
</span>是从均匀分布随机采样的整数。接下来，将剩余的序列划分为 <span>\(n\)
</span>长度的子序列。每个子序列从时间步 <span>\(i\)
</span>的第 <span>\(n\)
</span>个词开始，可以记为：
<span>\[
x_i = \{x_i, x_{i+1}, \ldots, x_{i+n-1}\}
\]
</span>假设网络一次只处理具有<span>
\(n\)
</span>个时间步的子序列。 下图画出了 从原始文本序列获得子序列的所有不同的方式， 其中<span>
\(n=5\)
</span>，并且每个时间步的词元对应于一个字符。</p><div align=center><img src=/images/timemachine-5gram.svg width=300px/></div><p>对于语言建模，目标是基于当前观察到的标记预测下一个标记。因此，<strong>对于任何输入序列 <span>\(x_i\)
</span>，目标序列（标签）是原始序列右移一个时间步，记为 <span>\(y_i\)
</span>，其长度与 <span>\(x_i\)
</span>相同</strong>。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> 在语言模型中，<strong>截断长度 n 并不意味着只根据前 n 个词预测下一个词</strong>，而是定义了一种固定长度的输入序列，从中学习预测下一个词的能力。模型不仅仅预测输入序列的最后一个标记（如 D 的目标是 E），而是同时<strong>预测输入序列中所有时间步的下一个标记</strong>。</p></blockquote><ul><li><strong>准备输入数据和标签</strong></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TextDataset</span>(Dataset):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, data, vocab_size, seq_length):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> data  <span style=color:#75715e># 数据是一个索引序列</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>vocab_size <span style=color:#f92672>=</span> vocab_size
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>seq_length <span style=color:#f92672>=</span> seq_length
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __len__(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> len(self<span style=color:#f92672>.</span>data) <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>seq_length
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __getitem__(self, idx):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 获取一个长度为seq_length的输入序列和一个目标值</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>data[idx:idx <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>seq_length]  <span style=color:#75715e># [seq_length]</span>
</span></span><span style=display:flex><span>        y <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>data[idx <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>:idx <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>seq_length <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>]  <span style=color:#75715e># [seq_length]，目标是下一个时间步的序列</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Inputs: tensor([[2, 3, 4],</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>#                 [6, 7, 8]])</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Targets: tensor([[3, 4, 5],</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>#                 [7, 8, 9]])</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>tensor(x), torch<span style=color:#f92672>.</span>tensor(y)
</span></span></code></pre></div><hr><h2 id=循环神经网络rnn概述><strong>循环神经网络（RNN）概述</strong>
<a class=anchor href=#%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9crnn%e6%a6%82%e8%bf%b0>#</a></h2><p>在用马尔可夫模型和n-gram模型用于语言建模时，这些模型中某一时刻的词（token）只依赖于前 <span>\(n\)
</span>个词。如果我们希望将更早时间步的词对当前词的影响考虑进来，就需要增加 <span>\(n\)
</span>的值。然而，随着 <span>\(n\)
</span>增加，模型参数的数量也会呈指数级增长，因为需要为词汇表中的每个词存储对应的参数。因此， 因此与其将 <span>\(P(x_t \mid x_{t-1}, \ldots, x_{t-n+1})\)
</span>模型化，不如使用<strong>隐变量模型（latent variable model）</strong>：
<span>\[
P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid h_{t-1})
\]
</span>潜在变量模型的核心思想是通过<strong>引入一个隐藏状态（hidden state）</strong>，它保存了直到当前时间步的序列信息。具体来说，隐藏状态在任意时间步 <span>\(t\)
</span>可以通过<strong>当前输入 <span>\(x_t\)
</span></strong>和<strong>上一个隐藏状态 <span>\(h_{t-1}\)
</span></strong>来计算：
<span>\[
h_t = f(x_{t}, h_{t-1})
\]
</span>这里，<span>
\(f\)
</span>是一个强大的函数，通过它可以计算隐藏状态。在这个模型中，隐藏状态不仅存储了之前所有的观测数据，而且通过适当设计，可以避免直接增加模型参数的数量。尽管如此，计算和存储的成本仍然可能较高。<strong>循环神经网络（RNN） 就是通过隐藏状态来建模序列数据的神经网络。</strong></p><blockquote class="book-hint warning"><p><strong>Note：</strong> “隐藏层”（hidden layers）和“隐藏状态”（hidden states）是两个完全不同的概念。隐藏层指的是从输入到输出路径中不可见的层，而<strong>隐藏状态是当前步骤的输入，可以通过查看之前的时间步的数据来计算</strong>。</p></blockquote><hr><h3 id=rnn的结构与计算><strong>RNN的结构与计算</strong>
<a class=anchor href=#rnn%e7%9a%84%e7%bb%93%e6%9e%84%e4%b8%8e%e8%ae%a1%e7%ae%97>#</a></h3><p>在没有隐藏状态的神经网络中（例如多层感知机，MLP），输入通过隐藏层的激活函数进行处理，得到隐藏层的输出。对于每个小批量 <span>\(\mathbf{X} \in \mathbb{R}^{n \times d}\)
</span>（minibatch）的输入，通过加权求和（包括偏置项），然后应用激活函数来计算隐藏层的输出 <span>\(\mathbf{H} \in \mathbb{R}^{n \times h}\)
</span>。
<span>\[
\mathbf{H} = \phi(\mathbf{X} \mathbf{W}_{xh} + \mathbf{b}_h)
\]
</span>这些输出将作为下一层（通常是输出层）的输入。输出层的计算则通过类似于回归问题的方法得到。如果是分类问题，输出层会通过激活函数（如softmax）生成概率分布，来预测输出类别。通过自动微分和随机梯度下降（SGD），可以优化网络的参数。
<span>\[
\mathbf{O} = \mathbf{H} \mathbf{W}_{hq} + \mathbf{b}_q,
\]</span></p><p>然而，当神经网络引入了 <strong>隐藏状态（hidden states）</strong> 时，情况就有所不同。假设在时间步 <span>\(t\)
</span>时，我们有一个小批量的输入 <span>\(x_t\)
</span>。与MLP不同，RNN在每个时间步 <span>\(t\)
</span>都会保存前一个时间步的隐藏状态 <span>\(h_{t-1}\)
</span>，并使用一个新的权重矩阵 <span>\(W_{hh}\)
</span>来结合当前输入和前一个时间步的隐藏状态进行计算。具体地，当前时间步的隐藏状态 <span>\(h_t\)
</span>由<strong>当前输入 <span>\(x_t\)
</span>和前一时间步的隐藏状态 <span>\(h_{t-1}\)
</span>共同决定</strong>：
<span>\[
\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh} + \mathbf{b}_h)
\]</span></p><p><span>\(\mathbf{X}_t\)
</span>是当前时间步的输入向量，其维度为 <span>\((\text{batch size}, \text{num inputs})\)
</span>，其中 <span>\(\text{batch size}\)
</span>是批量大小，<span>
\(\text{num inputs}\)
</span>是每个输入样本的特征数；<span>
\(\mathbf{H}_{t-1}\)
</span>是上一时间步的隐藏状态，维度为 <span>\((\text{batch size}, \text{num hiddens})\)
</span>，<span>
\(\text{num hiddens}\)
</span>表示隐藏单元的数量。权重矩阵 <span>\(\mathbf{W}_{xh}\)
</span>的维度为 <span>\((\text{num inputs}, \text{num hiddens})\)
</span>，用于将输入特征映射到隐藏单元；<span>
\(\mathbf{W}_{hh}\)
</span>的维度为 <span>\((\text{num hiddens}, \text{num hiddens})\)
</span>，用于描述隐藏单元之间的递归关系；<span>
\(\mathbf{b}_h\)
</span>是偏置向量，维度为 <span>\((\text{num hiddens},)\)
</span>。隐藏状态 <span>\(\mathbf{H}_t\)
</span>的维度为 <span>\((\text{batch size}, \text{num hiddens})\)
</span>，是激活函数 <span>\(\phi\)
</span>作用后的结果，捕捉当前时间步的特征和动态信息。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> RNN 的隐藏单元与普通神经网络（NN）的隐藏单元<strong>有本质的不同</strong>。在传统的前馈神经网络（如全连接层，CNN）中，<strong>隐藏单元主要负责提取固定的特征表示</strong>。每个隐藏单元通常捕捉输入数据中的某种模式（feature），如图像的边缘、纹理或数据的非线性关系。</p><p>RNN 的隐藏单元不再是独立学习某个静态的特征，而是通过递归公式动态更新，捕捉输入序列中的时间依赖性规律，主要用于<strong>总结从时间步 1 到 t 的所有历史信息</strong>。例如，在自然语言处理中，RNN 的隐藏状态可以表示句子中已经看到的词的语义和语法结构。</p><p><strong>RNN 的隐藏状态仅对当前时间有效</strong>。RNN 的设计目标是逐步传播信息。当前时间步的隐藏状态 h_t 是基于前 t 个输入计算的「总结」，只对当前任务有直接的意义。它<strong>并不显式保留每个时间步的特征，而是对这些特征进行压缩和提炼</strong>。随着时间步的增加，隐藏状态会逐渐遗忘较早的输入信息。</p></blockquote><p>其中 <span>\(f\)
</span>表示激活函数，<span>
\(W_{xh}\)
</span>是输入到隐藏层的权重，<span>
\(W_{hh}\)
</span>是前一时间步隐藏状态到当前时间步的权重，<span>
\(b_h\)
</span>是偏置项。通过这种方式，RNN的隐藏层不仅仅依赖于当前输入，还考虑了历史信息，因此具有”记忆”的功能，<strong>隐藏状态就是网络当前时刻的”记忆”</strong>。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>「num_inputs」 x 表示输入特征的维度</strong>。每个时间步的输入向量长度。如果输入是一个独热编码（one-hot encoding）表示的词汇，num_inputs 就等于词汇表的大小（vocabulary size）。例如，一个词汇表有 10000 个单词，使用独热编码表示每个单词，那么 num_inputs = 10000。</p><p><strong>「num_hiddens」h 表示 RNN 隐藏层中隐藏状态的维度</strong>。隐藏层状态向量的长度，<strong>控制网络的记忆能力</strong>（越大代表记忆能力越强）。每个时间步 RNN 的隐藏状态维度是固定的。如果 num_hiddens = 128，说明每个时间步的隐藏状态是一个 128 维的向量。</p></blockquote><p>在RNN中，隐藏状态的计算是递归的，意味着它依赖于前一个时间步的状态。这种递归计算的特点使得RNN能够处理序列数据，并捕捉数据中的时序依赖关系。RNN的每一层都执行这一递归计算，被称为<strong>递归层（recurrent layer）</strong>。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> 在标准的RNN模型中，隐藏单元（hidden unit）的权重是共享的，<strong>即相同的权重矩阵 W_xh（从输入到隐藏状态的权重）和 W_hh（从上一个时间步的隐藏状态到当前隐藏状态的权重）在每个时间步都会被复用</strong>。这意味着，RNN通过不断更新隐藏状态，并依赖同一组权重来捕捉序列中的时序信息。尽管RNN在每个时间步都会计算新的隐藏状态，但在基础模型中<strong>没有涉及多层结构的概念</strong>。模型的核心是通过递归地传递隐藏状态来逐步更新信息。因此，RNN的“深度”通常指的是层数，在基础RNN中，只需要学习这些基本的权重。</p></blockquote><p>与MLP类似，RNN的输出层的计算也类似，只是它的输入是当前时间步的隐藏状态 <span>\(h_t\)
</span>。RNN的目标是根据当前的隐藏状态，输出当前时间步的预测结果：
<span>\[
\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q
\]</span></p><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>W_hq 是语言模型的输出层权重矩阵</strong>，主要负责将 RNN 隐藏状态（hidden state）映射到词汇表的概率分布。它将隐藏状态 H_{t} 转换为一个向量，其中<strong>每个维度对应词汇表中的一个词的得分</strong>。得分经过 softmax 转换为概率分布，用于预测下一个词的概率。</p><p>W_{xh} 负责将当前输入 X_{t} 映射到隐藏状态空间。 W_{hh} 负责将前一时间步隐藏状态 H_{t-1} 更新到当前时间步隐藏状态 H_{t} 。</p></blockquote><div align=center><img src=/images/rnn.svg width=550px/></div><p>上图展示了循环神经网络在三个相邻时间步的计算逻辑。 在任意时间步<span>
\(t\)
</span>，隐状态的计算可以被视为：</p><ol><li><strong>拼接当前时间步 <span>\(t\)
</span>的输入 <span>\(X_t\)
</span>和前一时间步 <span>\(t-1\)
</span>的隐状态 <span>\(H_{t-1}\)
</span></strong>；</li><li>将拼接的结果送入带有激活函数 <span>\(\phi\)
</span>的全连接层。 全连接层的输出是当前时间步的隐状态<span>
\(H_{t}\)
</span>。</li></ol><ul><li><strong>RNN 代码实现</strong><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>RNNScratch</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, num_inputs, num_hiddens, sigma<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>num_inputs <span style=color:#f92672>=</span> num_inputs
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>num_hiddens <span style=color:#f92672>=</span> num_hiddens
</span></span><span style=display:flex><span>        <span style=color:#75715e># 初始化权重参数</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>W_xh <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Parameter(
</span></span><span style=display:flex><span>            torch<span style=color:#f92672>.</span>randn(num_inputs, num_hiddens) <span style=color:#f92672>*</span> sigma)  <span style=color:#75715e># 输入到隐藏层权重</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>W_hh <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Parameter(
</span></span><span style=display:flex><span>            torch<span style=color:#f92672>.</span>randn(num_hiddens, num_hiddens) <span style=color:#f92672>*</span> sigma)  <span style=color:#75715e># 隐藏层到隐藏层权重</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b_h <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>zeros(num_hiddens))  <span style=color:#75715e># 隐藏层偏置</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, inputs, state<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> state <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 初始化隐藏状态为零</span>
</span></span><span style=display:flex><span>            state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros((inputs<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>], self<span style=color:#f92672>.</span>num_hiddens))
</span></span><span style=display:flex><span>        outputs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> X <span style=color:#f92672>in</span> inputs:  <span style=color:#75715e># X 的形状为 (batch_size, num_inputs)</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 隐藏状态更新公式</span>
</span></span><span style=display:flex><span>            state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(torch<span style=color:#f92672>.</span>matmul(X, self<span style=color:#f92672>.</span>W_xh) <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>                            torch<span style=color:#f92672>.</span>matmul(state, self<span style=color:#f92672>.</span>W_hh) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b_h)
</span></span><span style=display:flex><span>            outputs<span style=color:#f92672>.</span>append(state)  <span style=color:#75715e># 保存每个时间步的输出</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> outputs, state
</span></span></code></pre></div></li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>为什么以列表保存state</strong>？</p><p>在RNN的过程中，我们通常需要保存每个时间步的隐藏状态（state）。在序列输入-序列输出（Sequence-to-Sequence）任务中，比如机器翻译、时间序列预测、语音生成等，<strong>模型需要对每一个时间步生成对应的输出</strong>。在这些情况下，每个时间步的隐藏状态都需要被保存。</p></blockquote><hr><h3 id=基于循环神经网络的字符级语言模型><strong>基于循环神经网络的字符级语言模型</strong>
<a class=anchor href=#%e5%9f%ba%e4%ba%8e%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e5%ad%97%e7%ac%a6%e7%ba%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b>#</a></h3><p>假设我们将文本“machine”作为输入序列，并且为了简化训练过程，我们将文本划分为字符，而不是词汇。在训练过程中，对于每个时间步，我们对输出层的结果进行 softmax 操作，然后使用交叉熵损失函数计算模型输出和目标之间的误差。在 RNN 中，由于隐层的隐藏状态是递归计算的，因此第 3 个时间步的输出是由“m”、“a”和“c”三个字符决定的。由于训练数据中序列的下一个字符是“h”，因此第 3 个时间步的损失将依赖于基于特征序列“m”、“a”、“c”生成的下一个字符的概率分布，并与目标“h”进行比较。</p><div align=center><img src=/images/rnn-train.svg width=550px/></div><p>在实际应用中，每个字符通常表示为一个维度为 <span>\(d\)
</span>的向量，而我们使用批量大小 <span>\(B\)
</span>。因此，在时间步 <span>\(t\)
</span>时的输入将是一个 <span>\(B \times d\)
</span>的矩阵。</p><hr><h3 id=one-hot-encoding-独热编码><strong>One-Hot Encoding 独热编码</strong>
<a class=anchor href=#one-hot-encoding-%e7%8b%ac%e7%83%ad%e7%bc%96%e7%a0%81>#</a></h3><p>在处理类别型数据（如词汇中的单词或字符）时，常用 <strong>独热编码（One-Hot Encoding）</strong> 表示。独热<strong>向量的长度等于词汇表大小</strong>，<strong>只有一个位置的值为1，其余为0</strong>。例如，词汇表长度为5时，索引0和2的独热向量分别为：<span>
\([1, 0, 0, 0, 0] \quad \text{and} \quad [0, 0, 1, 0, 0]\)
</span>。</p><p>对于每个输入的时间步，独热编码生成的输入张量形状为 (<span>
\(\text{batch size}, \text{time steps}, \text{vocab size}\)
</span>)。模型通常会转置输入，以方便循环逐时间步更新隐状态。</p><ul><li><strong>RNN One-Hot Encoding运用代码实现</strong><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>RNNLMScratch</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;从零实现的基于 RNN 的语言模型&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, rnn, vocab_size, sigma<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>rnn <span style=color:#f92672>=</span> rnn  <span style=color:#75715e># RNN 模型</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>vocab_size <span style=color:#f92672>=</span> vocab_size
</span></span><span style=display:flex><span>        <span style=color:#75715e># 输出层参数</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>W_hq <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>randn(rnn<span style=color:#f92672>.</span>num_hiddens, vocab_size) <span style=color:#f92672>*</span> sigma)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b_q <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>zeros(vocab_size))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>one_hot</span>(self, X):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;将输入转换为独热编码&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> F<span style=color:#f92672>.</span>one_hot(X<span style=color:#f92672>.</span>T, self<span style=color:#f92672>.</span>vocab_size)<span style=color:#f92672>.</span>type(torch<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>output_layer</span>(self, rnn_outputs):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;应用输出层将隐藏状态映射到词表&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        outputs <span style=color:#f92672>=</span> [torch<span style=color:#f92672>.</span>matmul(H, self<span style=color:#f92672>.</span>W_hq) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b_q <span style=color:#66d9ef>for</span> H <span style=color:#f92672>in</span> rnn_outputs]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>stack(outputs, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)  <span style=color:#75715e># 形状: (batch_size, num_steps, vocab_size)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, X, state<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;前向传播&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        embs <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>one_hot(X)  <span style=color:#75715e># 独热编码: (num_steps, batch_size, vocab_size)</span>
</span></span><span style=display:flex><span>        rnn_outputs, state <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>rnn(embs, state)  <span style=color:#75715e># 经过 RNN</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>output_layer(rnn_outputs), state  <span style=color:#75715e># 返回输出和最终隐藏状态</span>
</span></span></code></pre></div></li></ul><hr><h3 id=梯度裁剪gradient-clipping><strong>梯度裁剪（Gradient Clipping）</strong>
<a class=anchor href=#%e6%a2%af%e5%ba%a6%e8%a3%81%e5%89%aagradient-clipping>#</a></h3><p>在RNN中，序列长度引入了新的深度概念：输入不仅在单个时间步内通过网络从输入传播到输出，还需要沿着时间步形成一个深度为 <span>\(T\)
</span>的层链。反向传播时，<strong>梯度需要通过这条时间链传递，从而形成长度为 <span>\(T\)
</span>的矩阵乘积链</strong>。由于权重矩阵的特性，梯度可能会出现数值不稳定的情况，导致<strong>梯度爆炸（exploding gradients）或消失（vanishing gradients）</strong>。</p><p>当梯度过大时，可能在一次梯度更新中对模型造成严重破坏，甚至导致训练发散或损失函数不稳定。最直接的方法是：<strong>减小学习率</strong>，但这会降低所有训练步骤的优化效率，即使大梯度事件是少数。<strong>梯度裁剪</strong> 是一种更常见的替代方法是将<strong>梯度投影到一个以半径 <span>\(\theta\)
</span>为界的球中，限制梯度范数不超过 <span>\(\theta\)
</span></strong>，公式为：
<span>\[
\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}
\]</span></p><p>这样做确保了梯度大小受到控制，并且更新后的梯度方向与原始梯度一致。这种方法还可以限制单个小批量数据对模型参数的影响，提高模型的鲁棒性。</p><ul><li><strong>梯度裁剪（Gradient Clipping）代码实现</strong>：<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>clip_gradients</span>(model, grad_clip_val):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 筛选需要梯度更新的参数</span>
</span></span><span style=display:flex><span>    params <span style=color:#f92672>=</span> [p <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> model<span style=color:#f92672>.</span>parameters() <span style=color:#66d9ef>if</span> p<span style=color:#f92672>.</span>requires_grad]
</span></span><span style=display:flex><span>    <span style=color:#75715e># 计算梯度的 L2 范数</span>
</span></span><span style=display:flex><span>    total_norm <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sqrt(sum(torch<span style=color:#f92672>.</span>sum(p<span style=color:#f92672>.</span>grad <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>) <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> params))
</span></span><span style=display:flex><span>    <span style=color:#75715e># 如果梯度范数超过阈值，则按比例缩小</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> total_norm <span style=color:#f92672>&gt;</span> grad_clip_val:
</span></span><span style=display:flex><span>        scaling_factor <span style=color:#f92672>=</span> grad_clip_val <span style=color:#f92672>/</span> total_norm
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> param <span style=color:#f92672>in</span> params:
</span></span><span style=display:flex><span>            param<span style=color:#f92672>.</span>grad[:] <span style=color:#f92672>*=</span> scaling_factor
</span></span></code></pre></div>梯度裁剪通常在<strong>训练过程的反向传播（backward pass）之后和参数更新（optimizer.step()）之前</strong>执行。这是因为梯度裁剪的目的是直接对反向传播计算得到的梯度进行操作，在它们被优化器用来更新模型参数之前对其进行规范化或限制。<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> batch_inputs, batch_targets <span style=color:#f92672>in</span> dataloader:
</span></span><span style=display:flex><span>        <span style=color:#75715e># 前向传播</span>
</span></span><span style=display:flex><span>        logits, hidden_state <span style=color:#f92672>=</span> model(batch_inputs, hidden_state)
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> criterion(logits<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, vocab_size), batch_targets<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 反向传播</span>
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>        loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 自定义梯度裁剪</span>
</span></span><span style=display:flex><span>        clip_gradients(model, grad_clip_val)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 参数更新</span>
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>step()
</span></span></code></pre></div></li></ul><hr><h3 id=解码-decoding><strong>解码 (Decoding)</strong>
<a class=anchor href=#%e8%a7%a3%e7%a0%81-decoding>#</a></h3><p>在训练好语言模型后，模型不仅可以预测下一个 token，还可以通过将前一个预测的 token 作为输入，连续地预测后续的 token。这种解码过程既可以从一个空白文档开始生成文本，也可以基于用户提供的前缀 (prefix) 来生成。比如，在搜索引擎的自动补全功能或邮件撰写助手中，可以将用户已经输入的内容作为前缀，生成可能的续写。解码过程的步骤可以总结为：</p><ol><li><strong>Warm-up阶段</strong>：<ul><li>解码开始时，将前缀输入到模型中，不输出任何结果。</li><li>目的是通过传递隐藏状态 <span>\(\text{hidden state}\)
</span>，初始化模型内部状态以适应上下文。</li></ul></li><li><strong>续写生成</strong>：<ul><li>在输入完前缀后，模型开始生成后续字符。</li><li>每次生成一个字符，将其作为下一个时间步的输入，依次循环生成目标长度的文本。</li></ul></li><li><strong>输入和输出映射</strong>：<ul><li>使用独热编码（one-hot embedding）处理输入。</li><li>通过输出层预测字符分布，并选择概率最大的字符作为结果。</li></ul></li></ol><ul><li><strong>RNN 预测部分代码实现</strong><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(model, prefix, num_preds, vocab, device<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    基于前缀生成文本
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Parameters:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        model: nn.Module
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            用于生成文本的语言模型，通常包含 RNN 和输出层。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        prefix: list[str]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            文本生成的前缀，即用于初始化上下文信息的一段文本序列。例如 [&#34;I&#34;, &#34;love&#34;]。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        num_preds: int
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            需要生成的后续单词数（预测的时间步数）。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        vocab: object
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            词汇表对象，通常具有以下属性：
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                - vocab[token]: 将 token 转换为其对应的索引。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                - vocab.idx_to_token: 索引到 token 的映射，用于将生成的索引还原为文本。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        str
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            拼接后的生成文本，包括前缀和预测的后续内容。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    state <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>  <span style=color:#75715e># 初始化RNN的状态</span>
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> [vocab[prefix[<span style=color:#ae81ff>0</span>]]]  <span style=color:#75715e># 将前缀的第一个字符的索引加入输出序列</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(prefix) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>):  <span style=color:#75715e># Warm-up阶段</span>
</span></span><span style=display:flex><span>        X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[outputs[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]]], device<span style=color:#f92672>=</span>device)  <span style=color:#75715e># 当前输入</span>
</span></span><span style=display:flex><span>        embs <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>one_hot(X)  <span style=color:#75715e># 获取当前输入的独热编码</span>
</span></span><span style=display:flex><span>        rnn_outputs, state <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>rnn(embs, state)  <span style=color:#75715e># 经过RNN更新状态</span>
</span></span><span style=display:flex><span>        outputs<span style=color:#f92672>.</span>append(vocab[prefix[i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>]])  <span style=color:#75715e># 继续在输出序列中加入前缀的下一个字符</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 预测阶段，生成后续字符</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(num_preds):  <span style=color:#75715e># 预测num_preds步</span>
</span></span><span style=display:flex><span>        X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[outputs[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]]], device<span style=color:#f92672>=</span>device)  <span style=color:#75715e># 当前输入</span>
</span></span><span style=display:flex><span>        embs <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>one_hot(X)  <span style=color:#75715e># 获取当前输入的独热编码</span>
</span></span><span style=display:flex><span>        rnn_outputs, state <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>rnn(embs, state)  <span style=color:#75715e># 经过RNN更新状态</span>
</span></span><span style=display:flex><span>        Y <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>output_layer(rnn_outputs)  <span style=color:#75715e># 通过输出层映射到词汇表</span>
</span></span><span style=display:flex><span>        next_token <span style=color:#f92672>=</span> int(Y<span style=color:#f92672>.</span>argmax(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>1</span>))  <span style=color:#75715e># 选择最大概率的字符</span>
</span></span><span style=display:flex><span>        outputs<span style=color:#f92672>.</span>append(next_token)  <span style=color:#75715e># 将预测结果添加到输出序列</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 将索引转为对应的字符并拼接为生成的文本</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join([vocab<span style=color:#f92672>.</span>idx_to_token[i] <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> outputs])
</span></span></code></pre></div></li></ul><hr><h2 id=rnn中的反向传播算法><strong>RNN中的反向传播算法</strong>
<a class=anchor href=#rnn%e4%b8%ad%e7%9a%84%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95>#</a></h2><p>时间反向传播是将递归神经网络（RNN）展开为一个时间步长的计算图，并通过链式法则对参数进行梯度反向传播。它的主要挑战在于处理长序列时可能出现的数值不稳定问题，比如梯度爆炸和梯度消失。</p><p>在简化模型中，我们将时间步 <span>\(t\)
</span>的隐状态表示为 <span>\(h_t\)
</span>，输入表示为 <span>\(x_t\)
</span>，输出表示为 <span>\(o_t\)
</span>。 输入和隐状态可以拼接后与隐藏层中的一个权重变量相乘。 因此，我们分别使用 <span>\(w_h\)
</span>和 <span>\(w_o\)
</span>来表示隐藏层和输出层的权重。 每个时间步的隐状态和输出可以写为：</p><span>\[
\begin{split}\begin{aligned}h_t &= f(x_t, h_{t-1}, w_h),\\
o_t &= g(h_t, w_o),\end{aligned}\end{split}
\]</span><p>其中 <span>\(f\)
</span>和 <span>\(g\)
</span>分别是隐藏层和输出层的变换。 因此，我们有一个链 <span>\(\{\ldots, (x_{t-1}, h_{t-1}, o_{t-1}), (x_{t}, h_{t}, o_t), \ldots\}\)
</span>，它们通过循环计算彼此依赖。前向传播相当简单，一次一个时间步的遍历三元组 <span>\((x_t, h_t, o_t)\)
</span>，然后通过一个目标函数在所有个时间步 <span>\(T\)
</span>内 评估输出 <span>\(o_t\)
</span>和对应的标签 <span>\(y_t\)
</span>之间的差异：</p><span>\[
L(x_1, \ldots, x_T, y_1, \ldots, y_T, w_h, w_o) = \frac{1}{T}\sum_{t=1}^T l(y_t, o_t)
\]</span><p>对于反向传播，按照链式法则：</p><span>\[
\begin{split}\begin{aligned}\frac{\partial L}{\partial w_h} & = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial w_h} \\& = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial o_t} \frac{\partial g(h_t, w_o)}{\partial h_t} \frac{\partial h_t}{\partial w_h}.\end{aligned}\end{split}
\]</span><p>乘积的第一项和第二项很容易计算，而第三项 <span>\(\partial h_t/\partial w_h\)
</span>是使事情变得棘手的地方，因为我们需要循环地计算参数对的影响。使用链式法则得到：</p><span>\[
\begin{split}\begin{aligned}
\frac{\partial h_t}{\partial w_h} &= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h} +\frac{\partial f(x_{t},h_{t-1},w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h} \\
& =\frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h}+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t} \frac{\partial f(x_{j},h_{j-1},w_h)}{\partial h_{j-1}} \right) \frac{\partial f(x_{i},h_{i-1},w_h)}{\partial w_h}. \end{aligned}\end{split}
\]</span><p>虽然我们可以使用链式法则递归地计算 <span>\(\partial h_t/\partial w_h\)
</span>， 但当 <span>\(t\)
</span>很大时这个链就会变得很长。<strong>常用解决策略</strong>可以总结为：</p><ol><li><strong>全量计算：</strong> 对所有时间步长进行完整的梯度反向传播，然而，这样的计算非常缓慢，并且可能会发生梯度爆炸，因为初始条件的微小变化就可能会对结果产生巨大的影响。</li><li><strong>时间步截断（Truncated BPTT）：</strong> 截断时间步长，仅计算过去 <span>\(\tau\)
</span>步的梯度。这样做导致该模型主要侧重于短期影响，而不是长期影响。 这在现实中是可取的，因为它会将估计值偏向更简单和更稳定的模型。</li><li><strong>随机截断（Randomized Truncation）：</strong> 用一个随机变量替换 <span>\(\partial h_t/\partial w_h\)
</span>，该随机变量在预期中是正确的，但是会截断序列。虽然随机截断在理论上具有吸引力， 但很可能是由于多种因素在实践中并不比常规截断更好。</li></ol><hr><h3 id=反向传播的细节><strong>反向传播的细节</strong>
<a class=anchor href=#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%9a%84%e7%bb%86%e8%8a%82>#</a></h3><p>在时间反向传播（BPTT）中，我们需要计算目标函数对模型参数的梯度。考虑一个没有偏置参数的循环神经网络， 其在隐藏层中的激活函数使用恒等映射（<span>
\(\phi(x)=x\)
</span>）。对于时间步 <span>\(t\)
</span>，设单个样本的输入及其对应的标签分别为 <span>\(\mathbf{x}_t \in \mathbb{R}^d\)
</span>和 <span>\(y_{t}\)
</span>。 计算隐状态 <span>\(\mathbf{h}_t \in \mathbb{R}^h\)
</span>和输出 <span>\(\mathbf{o}_t \in \mathbb{R}^q\)
</span>的方式为：</p><span>\[
\begin{split}\begin{aligned}\mathbf{h}_t &= \mathbf{W}_{hx} \mathbf{x}_t + \mathbf{W}_{hh} \mathbf{h}_{t-1},\\
\mathbf{o}_t &= \mathbf{W}_{qh} \mathbf{h}_{t},\end{aligned}\end{split}
\]</span><p>其中权重参数为 <span>\(\mathbf{W}_{hx} \in \mathbb{R}^{h \times d}\)
</span>、<span>
\(\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}\)
</span>和 <span>\(\mathbf{W}_{qh} \in \mathbb{R}^{q \times h}\)
</span>。用 <span>\(l(\mathbf{o}_t, y_t)\)
</span>表示时间步处（即从序列开始起的超过 <span>\(T\)
</span>个时间步）的损失函数， 则我们的目标函数的总体损失是：</p><span>\[
L = \frac{1}{T} \sum_{t=1}^T l(\mathbf{o}_t, y_t).
\]</span><div align=center><img src=/images/01_biological_neuron.svg width=450px/></div><p>损失函数对输出 <span>\(\mathbf{o}_t\)
</span>的梯度计算如下：
<span>\[
\frac{\partial L}{\partial \mathbf{o}_t} = \frac{\partial l (\mathbf{o}_t, y_t)}{T \cdot \partial \mathbf{o}_t} \in \mathbb{R}^q.
\]</span></p><p>对于输出层参数 <span>\(\mathbf{W}_{qh}\)
</span>，使用链式法则得：</p><span>\[
\frac{\partial L}{\partial \mathbf{W}_{qh}}
= \sum_{t=1}^T \text{prod}\left(\frac{\partial L}{\partial \mathbf{o}_t}, \frac{\partial \mathbf{o}_t}{\partial \mathbf{W}_{qh}}\right)
= \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{o}_t} \mathbf{h}_t^\top,
\]</span><p>对于最终时间步 <span>\(T\)
</span>，目标函数只通过 <span>\(\mathbf{o}_T\)
</span>依赖于隐藏状态 <span>\(\mathbf{h}_T\)
</span>，因此：
<span>\[
\frac{\partial L}{\partial \mathbf{h}_T} = \text{prod}\left(\frac{\partial L}{\partial \mathbf{o}_T}, \frac{\partial \mathbf{o}_T}{\partial \mathbf{h}_T} \right) = \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_T}.
\]</span></p><p>对于中间时间步 <span>\(t\)
</span>，隐藏状态 <span>\(\mathbf{h}_t\)
</span>同时通过 <span>\(\mathbf{o}_t\)
</span>和 <span>\(\mathbf{h}_{t+1}\)
</span>影响目标函数。利用递归公式计算：</p><span>\[
\frac{\partial L}{\partial \mathbf{h}_t} = \text{prod}\left(\frac{\partial L}{\partial \mathbf{h}_{t+1}}, \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_t} \right) + \text{prod}\left(\frac{\partial L}{\partial \mathbf{o}_t}, \frac{\partial \mathbf{o}_t}{\partial \mathbf{h}_t} \right) = \mathbf{W}_{hh}^\top \frac{\partial L}{\partial \mathbf{h}_{t+1}} + \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_t}.
\]</span><p>展开递归公式可得：</p><span>\[
\frac{\partial L}{\partial \mathbf{h}_t}= \sum_{i=t}^T {\left(\mathbf{W}_{hh}^\top\right)}^{T-i} \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_{T+t-i}}.
\]</span><p><strong>在长序列中，由于 <span>\(\mathbf{W}_{hh}\)
</span>的特征值可能远小于或大于 1，导致梯度逐步消失或爆炸。</strong></p><p>最终隐藏层参数 <span>\(\mathbf{W}_{xh}\)
</span>和 <span>\(\mathbf{W}_{hh}\)
</span>的梯度计算如下：
<span>\[
\begin{split}\begin{aligned}
\frac{\partial L}{\partial \mathbf{W}_{hx}}
&= \sum_{t=1}^T \text{prod}\left(\frac{\partial L}{\partial \mathbf{h}_t}, \frac{\partial \mathbf{h}_t}{\partial \mathbf{W}_{hx}}\right)
= \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{h}_t} \mathbf{x}_t^\top,\\
\frac{\partial L}{\partial \mathbf{W}_{hh}}
&= \sum_{t=1}^T \text{prod}\left(\frac{\partial L}{\partial \mathbf{h}_t}, \frac{\partial \mathbf{h}_t}{\partial \mathbf{W}_{hh}}\right)
= \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{h}_t} \mathbf{h}_{t-1}^\top,
\end{aligned}\end{split}
\]</span></p><hr><h2 id=简单循环神经网络实现><strong>简单循环神经网络实现</strong>
<a class=anchor href=#%e7%ae%80%e5%8d%95%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%ae%9e%e7%8e%b0>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.optim <span style=color:#66d9ef>as</span> optim
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># RNN模型定义</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>RNN</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, num_inputs, num_hiddens):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>rnn <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>RNN(num_inputs, num_hiddens, batch_first<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, inputs, hidden_state<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>rnn(inputs, hidden_state)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># RNN语言模型定义</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>RNNLM</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;基于RNN的语言模型&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, rnn, vocab_size, num_hiddens, lr<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-2</span>):
</span></span><span style=display:flex><span>        super(RNNLM, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>rnn <span style=color:#f92672>=</span> rnn
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>vocab_size <span style=color:#f92672>=</span> vocab_size
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>num_hiddens <span style=color:#f92672>=</span> num_hiddens
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>linear <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(num_hiddens, vocab_size)  <span style=color:#75715e># 输出层</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>softmax <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Softmax(dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> lr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, inputs, hidden_state<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 前向传播，输入通过RNN，接着通过线性层</span>
</span></span><span style=display:flex><span>        rnn_output, hidden_state <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>rnn(inputs, hidden_state)
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>linear(rnn_output)  <span style=color:#75715e># 获取输出</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> output, hidden_state
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>output_layer</span>(self, hiddens):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>linear(hiddens)<span style=color:#f92672>.</span>swapaxes(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_params</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;初始化模型参数&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> name, param <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>named_parameters():
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#39;weight&#39;</span> <span style=color:#f92672>in</span> name:
</span></span><span style=display:flex><span>                nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>normal_(param, mean<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, std<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>zeros_(param)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(self, prefix, num_preds, vocab, device<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;基于前缀生成文本&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        state <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        outputs <span style=color:#f92672>=</span> [vocab[prefix[<span style=color:#ae81ff>0</span>]]]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(prefix) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>):  <span style=color:#75715e># Warm-up 阶段</span>
</span></span><span style=display:flex><span>            X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[outputs[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]]], device<span style=color:#f92672>=</span>device)  <span style=color:#75715e># 当前输入</span>
</span></span><span style=display:flex><span>            embs <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>one_hot(X)  <span style=color:#75715e># 获取当前输入的独热编码</span>
</span></span><span style=display:flex><span>            rnn_outputs, state <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>rnn(embs, state)  <span style=color:#75715e># 经过RNN更新状态</span>
</span></span><span style=display:flex><span>            outputs<span style=color:#f92672>.</span>append(vocab[prefix[i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>]])  <span style=color:#75715e># 继续在输出序列中加入前缀的下一个字符</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 预测阶段，生成后续字符</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(num_preds):  <span style=color:#75715e># 预测num_preds步</span>
</span></span><span style=display:flex><span>            X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[outputs[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]]], device<span style=color:#f92672>=</span>device)  <span style=color:#75715e># 当前输入</span>
</span></span><span style=display:flex><span>            embs <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>one_hot(X)  <span style=color:#75715e># 获取当前输入的独热编码</span>
</span></span><span style=display:flex><span>            rnn_outputs, state <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>rnn(embs, state)  <span style=color:#75715e># 经过RNN更新状态</span>
</span></span><span style=display:flex><span>            Y <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>output_layer(rnn_outputs)  <span style=color:#75715e># 通过输出层映射到词汇表</span>
</span></span><span style=display:flex><span>            next_token <span style=color:#f92672>=</span> int(Y<span style=color:#f92672>.</span>argmax(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>1</span>))  <span style=color:#75715e># 选择最大概率的字符</span>
</span></span><span style=display:flex><span>            outputs<span style=color:#f92672>.</span>append(next_token)  <span style=color:#75715e># 将预测结果添加到输出序列中</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 将索引转为对应的字符并拼接为生成的文本</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join([vocab<span style=color:#f92672>.</span>idx_to_token[i] <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> outputs])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>one_hot</span>(self, X):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;将输入转换为独热编码&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>one_hot(X<span style=color:#f92672>.</span>T, self<span style=color:#f92672>.</span>vocab_size)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 实例化模型</span>
</span></span><span style=display:flex><span>rnn <span style=color:#f92672>=</span> RNN(num_inputs<span style=color:#f92672>=</span>len(vocab), num_hiddens<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> RNNLM(rnn, vocab_size<span style=color:#f92672>=</span>len(vocab), num_hiddens<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>, lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)
</span></span></code></pre></div><h2 id=经典循环神经网络modern-recurrent-neural-networks><strong>经典循环神经网络（Modern Recurrent Neural Networks）</strong>
<a class=anchor href=#%e7%bb%8f%e5%85%b8%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9cmodern-recurrent-neural-networks>#</a></h2><hr><h3 id=长短时记忆网络long-short-term-memory-lstm><strong>长短时记忆网络（Long Short-Term Memory, LSTM）</strong>
<a class=anchor href=#%e9%95%bf%e7%9f%ad%e6%97%b6%e8%ae%b0%e5%bf%86%e7%bd%91%e7%bb%9clong-short-term-memory-lstm>#</a></h3><p>循环神经网络（RNN）通过反向传播进行训练之后，人们发现了学习长期依赖问题的显著难点，这主要是由于<strong>梯度消失和梯度爆炸问题</strong>。虽然梯度裁剪（gradient clipping）可以部分缓解梯度爆炸，但处理梯度消失需要更复杂的解决方案。Hochreiter和Schmidhuber于1997年提出的长短时记忆网络（LSTM）是解决梯度消失问题的早期且成功的技术之一。</p><p>LSTM与标准的RNN类似，但在LSTM中，每个普通的循环节点被替换为一个<strong>记忆单元（memory cell）</strong>。记忆单元内部包含一个<strong>内部状态（internal state）</strong>，这是一个具有固定权重为1的自连接回边的节点。这种设计确保了梯度能够在多个时间步内传播，而不会因梯度消失或梯度爆炸而中断。</p><hr><h4 id=门控记忆单元gated-memory-cell><strong>门控记忆单元（Gated Memory Cell）</strong>
<a class=anchor href=#%e9%97%a8%e6%8e%a7%e8%ae%b0%e5%bf%86%e5%8d%95%e5%85%83gated-memory-cell>#</a></h4><p>门控记忆单元通过内部状态（internal state）和多个乘性门控机制（multiplicative gates）管理信息流动。具体包括以下三种门控：</p><ol><li><strong>输入门（Input Gate）</strong>：控制是否允许当前输入影响记忆单元的内部状态。它决定了<strong>多少输入值应该加入当前的记忆单元状态</strong>。</li><li><strong>遗忘门（Forget Gate）</strong>：决定<strong>是否清除</strong>部分或全部内部状态。</li><li><strong>输出门（Output Gate）</strong>：确定内部状态<strong>是否可以影响单元的输出</strong>。</li></ol><p>在LSTM中，输入门（Input Gate）、遗忘门（Forget Gate）和输出门（Output Gate）的计算依赖于当前<strong>时间步的输入数据</strong>和<strong>前一时间步的隐藏状态</strong>，如下图所示。三个全连接层通过sigmoid激活函数计算输入门、遗忘门和输出门的值，因此这三个门的值都被限制在0到1的区间内。此外，我们还需要一个输入节点，通常使用tanh激活函数进行计算。</p><div align=center><img src=/images/lstm-1.svg width=450px/></div><p>数学上，假设有 <span>\(h\)
</span>个隐藏单元，批次大小为 <span>\(n\)
</span>，输入的维度为 <span>\(d\)
</span>，则输入为 <span>\(\mathbf{X}_t \in \mathbb{R}^{n \times d}\)
</span>，前一时间步的隐藏状态为 <span>\(\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}\)
</span>。在此基础上，输入门、遗忘门和输出门的定义如下：输入门为 <span>\(\mathbf{I}_t \in \mathbb{R}^{n \times h}\)
</span>，遗忘门为 <span>\(\mathbf{F}_t \in \mathbb{R}^{n \times h}\)
</span>，输出门为 <span>\(\mathbf{O}_t \in \mathbb{R}^{n \times h}\)
</span>。它们的计算公式为：</p><span>\[
\begin{split}\begin{aligned}
\mathbf{I}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xi} + \mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i),\\
\mathbf{F}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xf} + \mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f),\\
\mathbf{O}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xo} + \mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o),
\end{aligned}\end{split}
\]</span><blockquote class="book-hint warning"><p><strong>Note：</strong> 尽管公式形式类似，LSTM 中的门和一般 RNN 中的 Hidden state 的<strong>核心作用完全不同</strong>：</p><ol><li><strong>门的作用是“控制流动”</strong>：<ul><li>每个门的输出 [0, 1] 表示“通过”信息的比例。</li><li>它们主要用于<strong>调节信息的流动</strong>，而不是直接参与信息存储。</li></ul></li><li><strong>Hidden State 的作用是“存储和传递信息”</strong>：<ul><li>Hidden state 是序列模型的核心状态，用于传递时间步之间的主要信息。</li><li>它不仅受门控机制影响，还通过非线性变换从记忆单元中提取信息。</li></ul></li></ol></blockquote><p>我们需要设计了一个输入节点（Input node）。输入节点的计算方式类似于前面提到的门控单元（gate），但它使用一个具有特定值范围的激活函数（tanh），函数的值范围为 <span>\((-1, 1)\)
</span>。具体来说，输入节点的计算公式为：
<span>\[
\tilde{\mathbf{C}}_t = \textrm{tanh}(\mathbf{X}_t \mathbf{W}_{\textrm{xc}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hc}} + \mathbf{b}_\textrm{c}),
\]</span></p><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>为什么门的公式需要 Sigmoid 激活</strong>：</p><ul><li>LSTM 的门本质上是一种控制开关，它的输出必须在 [0, 1] 之间，这样才能<strong>直观地表示“通过的信息比例”</strong>。</li><li>而 Hidden state 的激活函数使用的是 tanh，其范围为 [-1, 1]，<strong>更适合表示信息本身的动态特征</strong>。</li></ul></blockquote><p>输入门 <span>\(I_t\)
</span>（input gate）控制我们在多大程度上考虑新输入数据 <span>\(\tilde{\mathbf{C}}_t\)
</span>，而遗忘门 <span>\(F_{t}\)
</span>（forget gate）则决定了我们保留多少旧的记忆单元内部状态 <span>\(\mathbf{C}_{t-1} \in \mathbb{R}^{n \times h}\)
</span>。通过使用Hadamard积 <span>\(\odot\)
</span>（逐元素相乘）运算符，LSTM的记忆单元内部状态的更新方程为：
<span>\[
\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t
\]</span></p><div align=center><img src=/images/lstm-2.svg width=550px/></div><blockquote><p><strong>关于 Hadamard积 <span>\(\odot\)
</span>：</strong> Hadamard 积是逐元素相乘的操作，对于两个相同形状的矩阵或向量 <span>\(\mathbf{a}\)
</span>和 <span>\(\mathbf{b}\)
</span>，定义如下：<span>
\(\mathbf{c} = \mathbf{a} \odot \mathbf{b}, \quad c_i = a_i \cdot b_i\)
</span>。这里的 <span>\(\odot\)
</span>表示逐元素相乘，而不是普通的矩阵乘法。</p></blockquote><blockquote><p><strong>例如：</strong>
<span>\[
\mathbf{a} =
\begin{bmatrix}
1 \\ 2 \\ 3
\end{bmatrix}, \quad
\mathbf{b} =
\begin{bmatrix}
0.1 \\ 0.5 \\ 0.9
\end{bmatrix}, \quad
\mathbf{a} \odot \mathbf{b} =
\begin{bmatrix}
1 \cdot 0.1 \\ 2 \cdot 0.5 \\ 3 \cdot 0.9
\end{bmatrix} =
\begin{bmatrix}
0.1 \\ 1 \\ 2.7
\end{bmatrix}
\]</span></p><p>如果遗忘门 <span>\(\mathbf{F}_t = [1, 0.5, 0]\)
</span>，则：</p><ul><li>第一维的旧信息完全保留（*1）。</li><li>第二维的旧信息减半（*0.5）。</li><li>第三维的旧信息完全丢弃（*0）。</li></ul></blockquote><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>Cell State（Internal State）</strong> 是 LSTM 相比经典 RNN 的核心创新之一，用于长期存储信息。它是贯穿整个时间序列的一条“主线”，可以通过加法和遗忘门机制实现对信息的选择性记忆或删除。它<strong>负责长时依赖信息的存储</strong>。可以<strong>直接从一个时间步传播到下一个时间步</strong>（通过“直通”机制），不会像hidden state那样受到非线性变换的影响。因为cell state的“直通性”，梯度不会像传统RNN中那样容易消失或爆炸，从而使LSTM能够更好地捕捉长距离依赖关系。</p><p>而Hidden state 是LSTM在每个时间步的输出。它是对<strong>当前时间步下所有输入信息和记忆信息的非线性处理结果</strong>，是一种“短时记忆”。提供即时输出信息，作为<strong>当前时间步的表征（representation）</strong>。在序列的每个时间步中，hidden state通过非线性变换与cell state交互，提取即时特征。Hidden state通常被用于后续任务（例如，分类或生成）中。</p></blockquote><p>如果遗忘门始终为1且输入门始终为0，则记忆单元的内部状态将保持不变，传递到每个后续时间步。然而，输入门和遗忘门赋予模型灵活性，使其能够学习何时保持值不变，以及何时根据后续输入调整这一值。这一设计有效<strong>缓解了梯度消失问题</strong>，尤其在处理长序列数据集时，使得模型训练变得更加容易。</p><div align=center><img src=/images/lstm-3.svg width=550px/></div><p>最后，隐藏状态 <span>\(I_t\)
</span>（Hidden State）定义了记忆单元的输出方式，它由输出门 <span>\(O_t\)
</span>（Output Gate）控制。具体计算步骤如下：首先，对记忆单元的内部状态 <span>\(\mathbf{C}_t\)
</span>应用 <span>\(\tanh\)
</span>函数，使其值被规范化到 <span>\((-1, 1)\)
</span>区间内。然后，将这一结果与输出门的值 <span>\(\mathbf{O}_t\)
</span>逐元素相乘，计算得到隐藏状态 <span>\(\mathbf{H}_t\)
</span>：
<span>\[
\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t)
\]</span></p><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>输出门（Output Gate） 的主要作用</strong>是控制 <strong>当前时刻的隐藏状态的输出</strong>内容，从而决定 LSTM 的对外信息传递。Cell State 是 LSTM 的内部长期记忆，但它并不直接对外输出。输出门通过<strong>选择性地提取 Cell State 中的信息</strong>，并结合门控机制生成 <strong>新的Hidden State（短期记忆的表达）</strong>，作为当前时间步的隐藏状态对外输出。这种机制确保 LSTM 在对外传递信息时，<strong>不会将 Cell State 中的所有内容暴露出去</strong>，避免噪声干扰，同时保留最相关的信息。</p></blockquote><p>这一机制确保了隐藏状态 <span>\(\mathbf{H}_t\)
</span>的值始终在 <span>\((-1, 1)\)
</span>区间内。</p><ul><li>当输出门 <span>\(\mathbf{O}_t\)
</span>的值接近 1 时，记忆单元的内部状态 <span>\(\mathbf{C}_t\)
</span>会直接影响后续网络层；</li><li>当输出门 <span>\(\mathbf{O}_t\)
</span>的值接近 0 时，记忆单元当前的状态对其他层没有影响。</li></ul><p>这种设计使得记忆单元可以在多个时间步内积累信息，而不会对网络的其他部分造成干扰（只要输出门保持接近 0）。当输出门的值突然从接近 0 变为接近 1 时，记忆单元会迅速对网络的其他部分产生显著影响。这种特性允许 LSTM 高效地处理长时间的依赖关系。</p><blockquote class="book-hint warning"><p>LSTM 和RNN 一样 Hidden state 是对 <strong>当前时刻输入和历史信息的总结</strong>，而 <strong>非直接表示最终的概率输出</strong>。<strong>最终输出需要通过额外的线性变换和可能的激活函数处理</strong>，来生成模型的最终预测值或概率分布。</p></blockquote><hr><h4 id=lstm解决的问题和原因><strong>LSTM解决的问题和原因</strong>
<a class=anchor href=#lstm%e8%a7%a3%e5%86%b3%e7%9a%84%e9%97%ae%e9%a2%98%e5%92%8c%e5%8e%9f%e5%9b%a0>#</a></h4><p>LSTM（Long Short-Term Memory）主要解决了标准RNN在处理长序列时的 <strong>梯度消失（vanishing gradients）和梯度爆炸（exploding gradients）</strong> 问题。 这些问题导致普通RNN在处理长期依赖（long-term dependencies）时性能较差，无法有效捕获长时间跨度的信息。</p><ol><li><strong>细胞状态（Cell State）作为长期记忆的载体</strong><ul><li>LSTM引入了一个额外的细胞状态 <span>\(\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t\)
</span>，它可以通过直通路径（&ldquo;constant error carousel&rdquo;）跨时间步传播信息，<strong>几乎不受梯度消失或梯度爆炸的影响</strong>。</li><li><span>\(\mathbf{F}_t \odot \mathbf{C}_{t-1}\)
</span>这一项将上一时间步的细胞状态 <span>\(\mathbf{C}_{t-1}\)
</span>直接传递到当前时间步，乘以遗忘门 <span>\(\mathbf{F}_t\)
</span>的值来控制保留的比例。由于 <strong>这部分没有激活函数的非线性变换，梯度可以在反向传播中稳定地通过时间步传播</strong>。</li><li><span>\(\mathbf{I}_t \odot \tilde{\mathbf{C}}_t\)
</span>这一项将当前时间步的候选记忆 <span>\(\tilde{\mathbf{C}}_t\)
</span>加入到细胞状态中，比例由输入门 <span>\(\mathbf{I}_t\)
</span>控制。这使得 LSTM 能够<strong>灵活地选择哪些新的信息需要加入长期记忆</strong>。</li></ul></li><li><strong>梯度传播更稳定</strong><ul><li>普通RNN的梯度通过时间步传播时，会被<strong>反复乘以隐状态的权重矩阵 <span>\(\mathbf{W}\)
</span></strong>。当权重矩阵的特征值远离 1 时，梯度会出现指数增长（梯度爆炸）或指数衰减（梯度消失）。</li><li>在LSTM中，<strong>细胞状态通过线性加权方式更新</strong>（不直接经过激活函数的非线性变换），从而避免了梯度的剧烈变化。门机制确保了信息流动的可控性，进一步减轻了梯度不稳定的问题。</li></ul></li><li><strong>更强的记忆能力</strong><ul><li>LSTM能<strong>同时捕获短期依赖（通过隐藏状态 <span>\(\mathbf{H}_t\)
</span>）和长期依赖（通过细胞状态 <span>\(\mathbf{C}_t\)
</span>）</strong>。</li><li>在长时间序列中，它可以动态调整对不同时间跨度的信息的关注程度，使其既能够记住长期信息，又不会因记忆过多导致模型过载。</li></ul></li></ol><ul><li><strong>LSTM 代码实现</strong></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>LSTMScratch</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, num_inputs, num_hiddens, sigma<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>):
</span></span><span style=display:flex><span>        super(LSTMScratch, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>num_hiddens <span style=color:#f92672>=</span> num_hiddens
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义初始化函数</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_weight</span>(<span style=color:#f92672>*</span>shape):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>randn(<span style=color:#f92672>*</span>shape) <span style=color:#f92672>*</span> sigma)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义门所需的权重和偏置初始化</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>triple</span>():
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> (init_weight(num_inputs, num_hiddens),  <span style=color:#75715e># 输入到隐藏层</span>
</span></span><span style=display:flex><span>                    init_weight(num_hiddens, num_hiddens), <span style=color:#75715e># 隐藏到隐藏层</span>
</span></span><span style=display:flex><span>                    nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>zeros(num_hiddens))) <span style=color:#75715e># 偏置</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 初始化各门的权重和偏置</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>W_xi, self<span style=color:#f92672>.</span>W_hi, self<span style=color:#f92672>.</span>b_i <span style=color:#f92672>=</span> triple()  <span style=color:#75715e># 输入门</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>W_xf, self<span style=color:#f92672>.</span>W_hf, self<span style=color:#f92672>.</span>b_f <span style=color:#f92672>=</span> triple()  <span style=color:#75715e># 遗忘门</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>W_xo, self<span style=color:#f92672>.</span>W_ho, self<span style=color:#f92672>.</span>b_o <span style=color:#f92672>=</span> triple()  <span style=color:#75715e># 输出门</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>W_xc, self<span style=color:#f92672>.</span>W_hc, self<span style=color:#f92672>.</span>b_c <span style=color:#f92672>=</span> triple()  <span style=color:#75715e># 候选细胞状态</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, inputs, H_C<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        前向传播
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        inputs: [seq_length, batch_size, num_inputs]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        H_C: 一个元组 (H, C)，分别为初始化的隐藏状态和细胞状态
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        seq_length, batch_size, _ <span style=color:#f92672>=</span> inputs<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> H_C <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            H <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros((batch_size, self<span style=color:#f92672>.</span>num_hiddens), device<span style=color:#f92672>=</span>inputs<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>            C <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros((batch_size, self<span style=color:#f92672>.</span>num_hiddens), device<span style=color:#f92672>=</span>inputs<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            H, C <span style=color:#f92672>=</span> H_C
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        outputs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> range(seq_length):
</span></span><span style=display:flex><span>            X_t <span style=color:#f92672>=</span> inputs[t]  <span style=color:#75715e># 当前时间步输入: [batch_size, num_inputs]</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 输入门</span>
</span></span><span style=display:flex><span>            I <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sigmoid(torch<span style=color:#f92672>.</span>matmul(X_t, self<span style=color:#f92672>.</span>W_xi) <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>                              torch<span style=color:#f92672>.</span>matmul(H, self<span style=color:#f92672>.</span>W_hi) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b_i)
</span></span><span style=display:flex><span>            <span style=color:#75715e># 遗忘门</span>
</span></span><span style=display:flex><span>            F <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sigmoid(torch<span style=color:#f92672>.</span>matmul(X_t, self<span style=color:#f92672>.</span>W_xf) <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>                              torch<span style=color:#f92672>.</span>matmul(H, self<span style=color:#f92672>.</span>W_hf) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b_f)
</span></span><span style=display:flex><span>            <span style=color:#75715e># 输出门</span>
</span></span><span style=display:flex><span>            O <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sigmoid(torch<span style=color:#f92672>.</span>matmul(X_t, self<span style=color:#f92672>.</span>W_xo) <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>                              torch<span style=color:#f92672>.</span>matmul(H, self<span style=color:#f92672>.</span>W_ho) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b_o)
</span></span><span style=display:flex><span>            <span style=color:#75715e># 候选细胞状态</span>
</span></span><span style=display:flex><span>            C_tilde <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(torch<span style=color:#f92672>.</span>matmul(X_t, self<span style=color:#f92672>.</span>W_xc) <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>                                 torch<span style=color:#f92672>.</span>matmul(H, self<span style=color:#f92672>.</span>W_hc) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b_c)
</span></span><span style=display:flex><span>            <span style=color:#75715e># 更新细胞状态</span>
</span></span><span style=display:flex><span>            C <span style=color:#f92672>=</span> F <span style=color:#f92672>*</span> C <span style=color:#f92672>+</span> I <span style=color:#f92672>*</span> C_tilde
</span></span><span style=display:flex><span>            <span style=color:#75715e># 更新隐藏状态</span>
</span></span><span style=display:flex><span>            H <span style=color:#f92672>=</span> O <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>tanh(C)
</span></span><span style=display:flex><span>            outputs<span style=color:#f92672>.</span>append(H)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 将所有时间步的隐藏状态拼接成张量</span>
</span></span><span style=display:flex><span>        outputs <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>stack(outputs, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)  <span style=color:#75715e># [seq_length, batch_size, num_hiddens]</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> outputs, (H, C)
</span></span></code></pre></div><hr><h3 id=门控循环单元gated-recurrent-units-gru><strong>门控循环单元（Gated Recurrent Units, GRU）</strong>
<a class=anchor href=#%e9%97%a8%e6%8e%a7%e5%be%aa%e7%8e%af%e5%8d%95%e5%85%83gated-recurrent-units-gru>#</a></h3><p>GRU（门控循环单元）是<strong>LSTM记忆单元的简化版本</strong>并保留内部状态和乘法门控机制（multiplicative gating mechanisms）的核心思想。相比LSTM，GRU在很多任务中可以达到相似的性能，但由于结构更加简单，计算速度更快。</p><p>在GRU（Gated Recurrent Unit）中，LSTM的三个门被替换为两个门：<strong>重置门（Reset Gate）<strong>和</strong>更新门（Update Gate）</strong>。这两个门使用了Sigmoid激活函数，输出值限制在区间 [0, 1] 内。</p><ul><li><strong>重置门</strong>：决定了当前状态需要记住多少之前隐藏状态的信息。</li><li><strong>更新门</strong>：控制新状态有多少是继承自旧状态的。</li></ul><p>在GRU中，给定当前时间步的输入 <span>\(\mathbf{X}_t\)
</span>和上一时间步的隐藏状态 <span>\(\mathbf{H}_{t-1}\)
</span>，重置门和更新门通过两个全连接层计算，激活函数为 Sigmoid。</p><div align=center><img src=/images/gru-1.svg width=400px/></div><p>对于给定的时间步 <span>\(t\)
</span>，假设输入是一个小批量 <span>\(\mathbf{X}_t \in \mathbb{R}^{n \times d}\)
</span>（样本个数 <span>\(n\)
</span>，输入个数 <span>\(d\)
</span>），上一个时间步的隐状态是 <span>\(\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}\)
</span>（隐藏单元个数 <span>\(t\)
</span>）。那么，重置门 <span>\(\mathbf{R}_t \in \mathbb{R}^{n \times h}\)
</span>和更新门 <span>\(\mathbf{Z}_t \in \mathbb{R}^{n \times h}\)
</span>的计算如下所示：
<span>\[
\begin{split}\begin{aligned}
\mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r),\\
\mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} + \mathbf{b}_z),
\end{aligned}\end{split}
\]</span></p><p>重置门（reset gate）<span>
\(\mathbf{R}_t\)
</span>与标准更新机制相结合，生成时间步 <span>\(t\)
</span>的<strong>候选隐藏状态（candidate hidden state） <span>\(\tilde{\mathbf{H}}_t \in \mathbb{R}^{n \times h}\)
</span></strong>，公式如下：
<span>\[
\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} + \mathbf{b}_h),
\]</span></p><p><strong>重置门 <span>\(\mathbf{R}t\)
</span>控制前一时间步隐藏状态</strong> <span>\(\mathbf{H}_{t-1}\)
</span>对候选隐藏状态的影响：</p><ul><li>当 <span>\(\mathbf{R}_t\)
</span>的某些元素接近 1，公式退化为标准RNN。</li><li>当 <span>\(\mathbf{R}_t\)
</span>的某些元素接近 0，前一时间步隐藏状态被忽略，候选隐藏状态仅依赖于当前输入 <span>\(\mathbf{x}_t\)
</span>。</li></ul><p>候选隐藏状态中通过 <span>\(\left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh}\)
</span>限制了 <span>\(\mathbf{H}_{t-1}\)
</span>的影响，增强了模型的灵活性，使其能够在必要时“重置”某些隐藏状态。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> 重置门主要是对长期记忆进行筛选，在计算候选隐藏状态时，抑制过去隐藏状态中的某些部分，使得模型更多地依赖当前输入。这种机制让模型能够<strong>专注于短期依赖</strong>，通过结合当前输入<strong>产生一个更符合短期记忆的候选状态</strong>。</p></blockquote><div align=center><img src=/images/gru-2.svg width=550px/></div><p><strong>更新门（update gate, <span>\(\mathbf{Z}_t\)
</span>）决定了新隐藏状态</strong> <span>\(\mathbf{H}_t \in \mathbb{R}^{n \times h}\)
</span>在多大程度上保留旧状态 <span>\(\mathbf{H}_{t-1}\)
</span>与新候选状态 <span>\(\tilde{\mathbf{H}}_t\)
</span>的信息。具体而言，<span>
\(\mathbf{Z}_t\)
</span>控制了二者的加权组合，公式如下：
<span>\[
\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1} + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t.
\]</span></p><ul><li>当 <span>\(\mathbf{Z}_t\)
</span>接近 1 时，旧状态 <span>\(\mathbf{H}_{t-1}\)
</span>被主要保留，忽略了新候选状态 <span>\(\tilde{\mathbf{H}}_t\)
</span>，从而在依赖链中跳过了当前时间步 <span>\(t\)
</span>。</li><li>当 <span>\(\mathbf{Z}_t\)
</span>接近 0 时，隐藏状态 <span>\(\mathbf{H}_t\)
</span>接近于新候选状态 <span>\(\tilde{\mathbf{H}}_t\)
</span>，将当前时间步的信息更强地融入到模型中。</li></ul><p>这些设计可以帮助我们处理循环神经网络中的梯度消失问题，并更好地捕获时间步距离很长的序列的依赖关系。例如，如果整个子序列的所有时间步的更新门都接近于 1，则无论序列的长度如何，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> 更新门通过在过去隐藏状态（长期记忆）和候选隐藏状态（短期记忆）之间进行加权平均，决定当前隐藏状态的更新方式。当更新门更倾向于长期记忆时，模型保留更多的历史信息；当更倾向于短期记忆时，模型更关注当前输入。这样，更新门实现了<strong>长期和短期记忆的动态平衡</strong>。</p></blockquote><div align=center><img src=/images/gru-3.svg width=550px/></div><p>总结来说，门控循环单元具有以下两个显著特征：</p><ol><li>重置门（reset gate）帮助捕捉序列中的 <strong>短期依赖关系</strong>。</li><li>更新门（update gate）帮助捕捉序列中的 <strong>长期依赖关系</strong>。</li></ol><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>重置门（reset gate）</strong> 通过控制<strong>当前时刻的隐藏状态与之前隐藏状态的结合程度</strong>，帮助捕捉短期依赖关系。当重置门的值接近0时，模型几乎完全忽略过去的信息，只依赖当前输入来计算候选隐藏状态，这使得模型能够专注于当前时刻的短期信息，从而适应短期依赖。</p><p><strong>更新门（update gate）</strong> 则决定<strong>当前时刻的隐藏状态是由之前的隐藏状态（长期记忆）和当前输入（短期信息）以何种比例组合而成</strong>。更新门的值接近1时，模型保留大部分的长期记忆，接近0时则依赖更多的当前输入。这种机制帮助GRU捕捉长期依赖关系，因为更新门可以调节模型在序列中如何传递和利用过去的记忆。</p></blockquote><ul><li><strong>GRU 代码实现</strong></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>GRUScratch</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, num_inputs, num_hiddens, sigma<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>):
</span></span><span style=display:flex><span>        super(GRUScratch, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>num_hiddens <span style=color:#f92672>=</span> num_hiddens
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义初始化函数</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_weight</span>(<span style=color:#f92672>*</span>shape):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>randn(<span style=color:#f92672>*</span>shape) <span style=color:#f92672>*</span> sigma)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义权重和偏置初始化</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>double</span>():
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> (init_weight(num_inputs, num_hiddens),  <span style=color:#75715e># 输入到隐藏层</span>
</span></span><span style=display:flex><span>                    init_weight(num_hiddens, num_hiddens), <span style=color:#75715e># 隐藏到隐藏层</span>
</span></span><span style=display:flex><span>                    nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>zeros(num_hiddens))) <span style=color:#75715e># 偏置</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 更新门权重和偏置</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>W_xz, self<span style=color:#f92672>.</span>W_hz, self<span style=color:#f92672>.</span>b_z <span style=color:#f92672>=</span> double()  <span style=color:#75715e># 更新门</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 重置门权重和偏置</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>W_xr, self<span style=color:#f92672>.</span>W_hr, self<span style=color:#f92672>.</span>b_r <span style=color:#f92672>=</span> double()  <span style=color:#75715e># 重置门</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 候选隐藏状态权重和偏置</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>W_xh, self<span style=color:#f92672>.</span>W_hh, self<span style=color:#f92672>.</span>b_h <span style=color:#f92672>=</span> double()  <span style=color:#75715e># 候选隐藏状态</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, inputs, H<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        前向传播
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        inputs: [seq_length, batch_size, num_inputs]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        H: 初始化的隐藏状态 [batch_size, num_hiddens]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        seq_length, batch_size, _ <span style=color:#f92672>=</span> inputs<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> H <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            H <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros((batch_size, self<span style=color:#f92672>.</span>num_hiddens), device<span style=color:#f92672>=</span>inputs<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        outputs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> range(seq_length):
</span></span><span style=display:flex><span>            X_t <span style=color:#f92672>=</span> inputs[t]  <span style=color:#75715e># 当前时间步输入: [batch_size, num_inputs]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 更新门</span>
</span></span><span style=display:flex><span>            Z <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sigmoid(torch<span style=color:#f92672>.</span>matmul(X_t, self<span style=color:#f92672>.</span>W_xz) <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>                              torch<span style=color:#f92672>.</span>matmul(H, self<span style=color:#f92672>.</span>W_hz) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b_z)
</span></span><span style=display:flex><span>            <span style=color:#75715e># 重置门</span>
</span></span><span style=display:flex><span>            R <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sigmoid(torch<span style=color:#f92672>.</span>matmul(X_t, self<span style=color:#f92672>.</span>W_xr) <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>                              torch<span style=color:#f92672>.</span>matmul(H, self<span style=color:#f92672>.</span>W_hr) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b_r)
</span></span><span style=display:flex><span>            <span style=color:#75715e># 候选隐藏状态</span>
</span></span><span style=display:flex><span>            H_tilde <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(torch<span style=color:#f92672>.</span>matmul(X_t, self<span style=color:#f92672>.</span>W_xh) <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>                                 torch<span style=color:#f92672>.</span>matmul(R <span style=color:#f92672>*</span> H, self<span style=color:#f92672>.</span>W_hh) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b_h)
</span></span><span style=display:flex><span>            <span style=color:#75715e># 新的隐藏状态</span>
</span></span><span style=display:flex><span>            H <span style=color:#f92672>=</span> Z <span style=color:#f92672>*</span> H <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> Z) <span style=color:#f92672>*</span> H_tilde
</span></span><span style=display:flex><span>            outputs<span style=color:#f92672>.</span>append(H)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 将所有时间步的隐藏状态拼接成张量</span>
</span></span><span style=display:flex><span>        outputs <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>stack(outputs, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)  <span style=color:#75715e># [seq_length, batch_size, num_hiddens]</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> outputs, H
</span></span></code></pre></div><hr><h3 id=深层循环神经网络deep-recurrent-neural-networks-drnn><strong>深层循环神经网络（Deep Recurrent Neural Networks, DRNN）</strong>
<a class=anchor href=#%e6%b7%b1%e5%b1%82%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9cdeep-recurrent-neural-networks-drnn>#</a></h3><p>深层循环神经网络（Deep Recurrent Neural Networks, DRNN）是通过<strong>堆叠多个RNN层</strong>实现的。单隐藏层的RNN网络结构，由一个序列输入、一层隐藏层（hidden layer），以及一个输出层组成。尽管这样的网络在时间方向上只有一层隐藏层，但输入在初始时间步的影响可以通过隐藏层在时间上的递归传播。</p><p>然而，这种单层结构在捕捉时间步内部输入与输出之间复杂关系时存在局限。因此，为了同时增强 <strong>模型对时间依赖（temporal dependency）</strong> 和 <strong>时间步内部输入与输出关系</strong> 的建模能力，常会构造在时间方向和输入输出方向上都更深的RNN网络。这种深度的概念类似于在多层感知机（MLP）和深度卷积神经网络（CNN）中见到的层级加深方法。</p><p>在深层RNN中，每一时间步的隐藏单元 <strong>不仅依赖于同层前一个时间步的隐藏状态</strong>，还依赖于 <strong>前一层相同时间步的隐藏状态</strong>。这种结构使得深层RNN能够 <strong>同时捕获长期依赖关系（long-term dependency）和复杂的输入-输出关系</strong>。</p><div align=center><img src=/images/deep-rnn.svg width=300px/></div><p>假设在时间步 <span>\(t\)
</span>，我们有一个小批量输入 <span>\(\mathbf{X}_t \in \mathbb{R}^{n \times d}\)
</span>（样本数：<span>
\(n\)
</span>，每个样本中的输入数： <span>\(d\)
</span>） 同时，将 <span>\(l^\mathrm{th}\)
</span>隐藏层（<span>
\(l=1,\ldots,L\)
</span>）的隐状态设为
<span>\(\mathbf{H}_t^{(l)} \in \mathbb{R}^{n \times h}\)
</span>（隐藏单元数： <span>\(h\)
</span>）， 输出层变量设为 <span>\(\mathbf{O}_t \in \mathbb{R}^{n \times q}\)
</span>（输出数：<span>
\(q\)
</span>）。 设置 <span>\(\mathbf{H}_t^{(0)} = \mathbf{X}_t\)
</span>， 第 <span>\(l\)
</span>个隐藏层的隐状态使用激活函数 <span>\(\phi_l\)
</span>，则：
<span>\[
\mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)} + \mathbf{b}_h^{(l)}),
\]
</span>其中，权重 <span>\(\mathbf{W}_{xh}^{(l)} \in \mathbb{R}^{h \times h}\)
</span>，<span>
\(\mathbf{W}_{hh}^{(l)} \in \mathbb{R}^{h \times h}\)
</span>和偏置 <span>\(\mathbf{b}_h^{(l)} \in \mathbb{R}^{1 \times h}\)
</span>都是第 <span>\(l\)
</span>个隐藏层的模型参数。</p><p>最后，输出层的计算仅基于第<span>
\(l\)
</span>个隐藏层最终的隐状态：
<span>\[
\mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q,
\]</span></p><p>其中，权重 <span>\(\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}\)
</span>和偏置 <span>\(\mathbf{b}_q \in \mathbb{R}^{1 \times q}\)
</span>都是输出层的模型参数。</p><hr><h3 id=双向循环神经网络bidirectional-recurrent-neural-networks><strong>双向循环神经网络（Bidirectional Recurrent Neural Networks）</strong>
<a class=anchor href=#%e5%8f%8c%e5%90%91%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9cbidirectional-recurrent-neural-networks>#</a></h3><p>我们之前使用的例子主要关于是语言建模（language modeling），目标是 <strong>根据序列中所有前面的token（单词或符号）预测下一个token</strong>。在这种情况下，我们只需要依赖左侧的上下文信息，因此使用 <strong>单向RNN（unidirectional RNN）</strong> 是合理的。然而，在某些序列学习任务中，预测每个时间步的结果时可以同时利用左侧和右侧的上下文信息。例如，<strong>词性标注（part of speech detection）</strong> 就是一个典型任务（i.e. 为句子中的每个单词分配其语法类别（词性）。词性反映了单词在句子中的语法功能和作用，例如名词、动词、形容词等。），在判断一个单词的词性时，考虑其两侧的上下文会更加准确。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> 另一个常见任务是 <strong>文本中随机遮盖部分token（masking tokens）</strong>，并训练模型预测这些缺失的token。这种任务通常被用于预训练模型（pretraining），之后再进行特定任务的微调（fine-tuning）。例如，根据句子中缺失位置的上下文，不同的填充值可能有显著变化：</p><ul><li>“I am ___.” （可能是 “happy”）</li><li>“I am ___ hungry.” （可能是 “not” 或 “very”）</li><li>“I am ___ hungry, and I can eat half a pig.” （“not” 在上下文中显然不合适）</li></ul></blockquote><p><strong>双向RNN（Bidirectional RNN）</strong> 是一种简单但有效的方法，它将单向RNN扩展为同时考虑两个方向的上下文信息。具体实现方式如下：</p><ol><li>在同一个输入序列上，<strong>构建两个单向RNN层</strong>：</li></ol><ul><li>第一个RNN层从左到右（forward direction）处理输入序列，第一个输入为 <span>\(X_1\)
</span>，最后一个输入为 <span>\(X_T\)
</span>。</li><li>第二个RNN层从右到左（backward direction）处理输入序列，第一个输入为 <span>\(X_T\)
</span>，最后一个输入为 <span>\(X_1\)
</span>。</li></ul><ol start=2><li>双向RNN层的输出是两个单向RNN层在<strong>每个时间步的输出拼接（concatenate）</strong>。</li></ol><div align=center><img src=/images/birnn.svg width=300px/></div><p>对于任意时间步 <span>\(t\)
</span>，给定一个小批量的输入数据 <span>\(\mathbf{X}_t \in \mathbb{R}^{n \times d}\)
</span>（样本数 <span>\(n\)
</span>，每个示例中的输入数 <span>\(d\)
</span>）， 并且令隐藏层激活函数为 <span>\(\phi\)
</span>。在双向架构中，我们设该时间步的前向和反向隐状态分别为 <span>\(\overrightarrow{\mathbf{H}}_t \in \mathbb{R}^{n \times h}\)
</span>和 <span>\(\overleftarrow{\mathbf{H}}_t \in \mathbb{R}^{n \times h}\)
</span>， 其中 <span>\(h\)
</span>是隐藏单元的数目。前向和反向隐状态的更新如下：
<span>\[
\begin{split}\begin{aligned}
\overrightarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{hh}^{(f)} + \mathbf{b}_h^{(f)}),\\
\overleftarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{hh}^{(b)} + \mathbf{b}_h^{(b)}),
\end{aligned}\end{split}
\]</span></p><p>其中，权重 <span>\(\mathbf{W}_{xh}^{(f)} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh}^{(f)} \in \mathbb{R}^{h \times h}, \mathbf{W}_{xh}^{(b)} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh}^{(b)} \in \mathbb{R}^{h \times h}\)
</span>和偏置 <span>\(\mathbf{b}_h^{(f)} \in \mathbb{R}^{1 \times h}, \mathbf{b}_h^{(b)} \in \mathbb{R}^{1 \times h}\)
</span>都是模型参数。</p><p>接下来，将前向隐状态 <span>\(\overrightarrow{\mathbf{H}}_t\)
</span>和反向隐状态 <span>\(\overleftarrow{\mathbf{H}}_t\)
</span>连接起来，获得需要送入输出层的隐状态 <span>\(\mathbf{H}_t \in \mathbb{R}^{n \times 2h}\)
</span>。在具有多个隐藏层的深度双向循环神经网络中，该信息作为输入传递到下一个双向层。
<span>\[
\mathbf{H}_t = \begin{bmatrix} \overrightarrow{\mathbf{H}}_t \\ \overleftarrow{\mathbf{H}}_t \end{bmatrix}
\]
</span>最后，输出层计算得到的输出为 <span>\(\mathbf{O}_t \in \mathbb{R}^{n \times q}\)
</span>（<span>
\(q\)
</span>是输出单元的数目）：
<span>\[
\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q.
\]</span></p><hr><h3 id=编码器-解码器encoder-decoder架构><strong>编码器-解码器（Encoder-Decoder）架构</strong>
<a class=anchor href=#%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8encoder-decoder%e6%9e%b6%e6%9e%84>#</a></h3><p>在序列到序列（sequence-to-sequence）问题中（如机器翻译），<strong>输入和输出通常具有不同的长度，且无法直接对齐</strong>。为了解决这一问题，通常采用编码器-解码器（encoder-decoder）架构。这个架构包括两个主要组件：</p><ol><li><strong>编码器（Encoder）</strong>：接收一个变长的输入序列，并将其<strong>编码成一个固定长度的状态向量（state）</strong>。</li><li><strong>解码器（Decoder）</strong>：作为一个条件语言模型（conditional language model），根据编码器生成的状态向量以及目标序列的左侧上下文，<strong>逐步预测目标序列中的下一个标记（token）</strong>。</li></ol><blockquote class="book-hint warning"><p><strong>Note：</strong> 固定形状是指该向量的维度是预先设定的，<strong>不依赖于输入序列的长度</strong>。例如，假设我们设定上下文变量的维度为 d，那么无论输入序列包含 5 个、50 个还是 500 个词，最终生成的上下文变量都会是一个 d-维向量。</p></blockquote><p>例如，在将英语翻译成法语的任务中，假设输入序列为：“They”， “are”， “watching”， “.”，编码器会将这个变长的输入序列编码为一个状态向量。随后，解码器利用该状态向量逐步生成翻译后的序列：“Ils”， “regardent”， “.”。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> Encoder 的 <strong>核心目的</strong> 是找到一个能够浓缩输入序列中长期记忆和短期记忆的隐藏状态（hidden state），并将其作为 Decoder 的输入，从而让 Decoder 能够基于这些信息生成合理的输出。</p></blockquote><div align=center><img src=/images/encoder-decoder.svg width=450px/></div><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>RNN与 Encoder-Decoder 架构的区别</strong></p><ol><li><strong>上下文向量（Context Vector）</strong>：<ul><li>在标准的 Encoder-Decoder 架构中，编码器生成一个<strong>全局的上下文向量</strong> c，该向量是解码器生成输出的主要依据。</li><li>在 RNN 中，每个时间步的隐藏状态，既充当上下文向量的角色，也直接作为解码的输入。</li></ul></li><li><strong>输入输出的对齐</strong>：<ul><li>RNN 假设<strong>输入和输出的长度一致</strong>，并且在时间维度上严格对齐（例如时间序列预测、语言模型）。</li><li>Encoder-Decoder 架构专为解决<strong>输入与输出长度不对齐</strong>的问题设计（例如机器翻译）。</li></ul></li><li><strong>信息流方向</strong>：<ul><li>在 RNN 中，信息流是<strong>逐时间步递归</strong>的，依赖于当前时刻的隐藏状态。</li><li>在 Encoder-Decoder 中，编码器<strong>先完成输入序列的处理，生成上下文向量</strong>，解码器<strong>再从上下文向量开始生成输出</strong>。</li></ul></li></ol></blockquote><hr><h3 id=序列到序列学习seq2seq><strong>序列到序列学习（seq2seq）</strong>
<a class=anchor href=#%e5%ba%8f%e5%88%97%e5%88%b0%e5%ba%8f%e5%88%97%e5%ad%a6%e4%b9%a0seq2seq>#</a></h3><p>在序列到序列（sequence-to-sequence）问题中，例如机器翻译（machine translation），输入和输出都是变长的、未对齐的序列。在这种情况下，我们通常依赖编码器-解码器（encoder-decoder）架构来处理这些任务。</p><div align=center><img src=/images/seq2seq.svg width=500px/></div><p>特定的<code>&lt;eos></code>表示序列结束词元。 一旦输出序列生成此词元，模型就会停止预测。在设计中，通常有两个特别的设计决策：首先，每个输入序列开始时都会有一个特殊的序列开始标记（<code>&lt;bos></code>），它是解码器的输入序列的第一个词元；其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。</p><hr><h4 id=编码器encoder-部分><strong>编码器（Encoder） 部分</strong>
<a class=anchor href=#%e7%bc%96%e7%a0%81%e5%99%a8encoder-%e9%83%a8%e5%88%86>#</a></h4><p>Encoder的主要作用是将一个 <strong>长度可变的输入序列</strong> 转换为 <strong>固定形状的上下文变量（context variable）</strong>。假设输入序列为 <span>\(\{x_1, x_2, \dots, x_T\}\)
</span>，其中 <span>\(x_t\)
</span>是第 <span>\(t\)
</span>个时间步的输入标记（token）。在时间步 <span>\(t\)
</span>，RNN 根据输入特征向量 <span>\(x_t\)
</span>和前一个时间步的隐藏状态 <span>\(h_{t-1}\)
</span>来计算当前的隐藏状态 <span>\(h_t\)
</span>。这一过程可表示为：
<span>\[
\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1}).
\]</span></p><p>其中 <span>\(f\)
</span>表示 RNN 的递归计算函数（例如 GRU 或 LSTM 的单元函数）。</p><blockquote class="book-hint warning"><p><strong>Note：</strong> <strong>Encoder 的设计目的</strong>：</p><ol><li><strong>压缩输入信息</strong>：将输入序列的所有信息压缩到一个低维表示中，确保模型能够以固定大小的特征表示处理任意长度的输入。</li><li><strong>捕捉序列的全局语义</strong>： Encoder 会通过递归网络（如 RNN、GRU 或 LSTM）处理输入序列，将序列中的时序依赖关系和语义信息编码到隐藏状态中。</li><li><strong>作为中间表示</strong>： Encoder 的输出（隐藏状态或上下文变量）提供了一种抽象的、高效的输入表示，适合传递给其他模块（如 Decoder）或用于分类、翻译等下游任务。</li></ol></blockquote><p>Encoder 会利用自定义的函数 <span>\(g\)
</span>将所有时间步的隐藏状态 <span>\(\{h_1, h_2, \dots, h_T\}\)
</span>转换为一个固定形状的上下文变量 <span>\(c\)
</span>：
<span>\[
\mathbf{c} = q(\mathbf{h}_1, \ldots, \mathbf{h}_T).
\]</span></p><p>在某些情况下，上下文变量 <span>\(c\)
</span>可以直接选取为最后一个时间步的隐藏状态 <span>\(h_T\)
</span>，即：<span>
\(c = h_T\)
</span>。</p><p>在实现 Encoder 时，常使用 <strong>嵌入层（Embedding Layer）</strong> 来将每个输入标记转换为对应的特征向量：</p><ul><li><p>嵌入层的权重是一个矩阵，行数等于词汇表大小 <span>\(vocab\_size\)
</span>，列数等于嵌入向量维度 <span>\(embed\_size\)
</span>。</p></li><li><p>对于输入标记的索引 <span>\(i\)
</span>，嵌入层返回权重矩阵的第 <span>\(i\)
</span>行，作为该标记的特征向量。</p></li><li><p><strong>Encoder 示例代码实现</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Seq2SeqEncoder</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;用于序列到序列学习的循环神经网络编码器&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, vocab_size, embed_size, num_hiddens, num_layers, 
</span></span><span style=display:flex><span>                dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>, bidirectional<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        <span style=color:#75715e># 嵌入层</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>embedding <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(vocab_size, embed_size)
</span></span><span style=display:flex><span>        <span style=color:#75715e># GRU 层</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>rnn <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>GRU(embed_size, num_hiddens, num_layers, 
</span></span><span style=display:flex><span>                        dropout<span style=color:#f92672>=</span>dropout, bidirectional<span style=color:#f92672>=</span>bidirectional)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 双向 GRU 会将隐藏状态的维度翻倍</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>bidirectional <span style=color:#f92672>=</span> bidirectional
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>num_hiddens <span style=color:#f92672>=</span> num_hiddens
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>num_layers <span style=color:#f92672>=</span> num_layers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, X):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        参数：
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        - X: 输入序列，形状为 (batch_size, num_steps)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        返回：
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        - output: 所有时间步的隐藏状态，形状为 (num_steps, batch_size, num_hiddens * (2 if bidirectional else 1))
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        - state: 最后一层每个方向的隐藏状态，形状为 (num_layers * (2 if bidirectional else 1), batch_size, num_hiddens)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 嵌入层输出形状为 (batch_size, num_steps, embed_size)</span>
</span></span><span style=display:flex><span>        X <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>embedding(X)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 调整输入形状为 (num_steps, batch_size, embed_size)</span>
</span></span><span style=display:flex><span>        X <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># output 的形状为 (num_steps, batch_size, num_hiddens * num_directions)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># state 的形状为 (num_layers * num_directions, batch_size, num_hiddens)</span>
</span></span><span style=display:flex><span>        output, state <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>rnn(X)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> output, state
</span></span></code></pre></div></li></ul><hr><h4 id=解码器decoder-部分><strong>解码器（Decoder） 部分</strong>
<a class=anchor href=#%e8%a7%a3%e7%a0%81%e5%99%a8decoder-%e9%83%a8%e5%88%86>#</a></h4><p>在序列到序列（Seq2Seq）模型中，解码器（decoder）负责根据目标输出序列 <span>\(y_1, y_2, \dots, y_T\)
</span>，在每个时间步 <span>\(t\)
</span>预测下一步的输出 <span>\(y_t\)
</span>。解码器的核心是基于目标序列中前一时间步的输出 <span>\(y_{t-1}\)
</span>、前一时间步的隐藏状态 <span>\(\mathbf{s}_{t-1}\)
</span>和上下文变量 <span>\(\mathbf{c}\)
</span>来计算当前时间步的隐藏状态 <span>\(\mathbf{s}_t\)
</span>。公式如下：
<span>\[
\mathbf{s}_{t} = g(y_{t}, \mathbf{c}, \mathbf{s}_{t}).
\]
</span>在得到当前时间步的隐藏状态 <span>\(\mathbf{s}_t\)
</span>后，通过输出层和 softmax 操作计算下一步的输出 <span>\(y_t\)
</span>的概率分布 <span>\(P(y_{t} \mid y_1, \ldots, y_{t}, \mathbf{c})\)
</span>。</p><div align=center><img src=/images/seq2seq-details.svg width=400px/></div><p>当实现解码器时， 我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。 这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元。 为了进一步包含经过编码的输入序列的信息， 上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。 为了预测输出词元的概率分布， 在循环神经网络解码器的最后一层使用全连接层来变换隐状态。</p><ul><li><strong>Decoder 示例代码实现</strong><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Seq2SeqDecoder</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;用于序列到序列学习的循环神经网络解码器&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>embedding <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(vocab_size, embed_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>rnn <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>GRU(embed_size <span style=color:#f92672>+</span> num_hiddens, num_hiddens, num_layers, dropout<span style=color:#f92672>=</span>dropout)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dense <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(num_hiddens, vocab_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_state</span>(self, enc_outputs, <span style=color:#f92672>*</span>args):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 初始化解码器的隐状态</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> enc_outputs[<span style=color:#ae81ff>1</span>]  <span style=color:#75715e># 通常是编码器的最后一层隐状态</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, X, state):
</span></span><span style=display:flex><span>        <span style=color:#75715e># &#39;X&#39; 的形状: (batch_size, num_steps)</span>
</span></span><span style=display:flex><span>        X <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>embedding(X)<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>)  <span style=color:#75715e># 转换为 (num_steps, batch_size, embed_size)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 获取编码器传来的 context，重复以匹配时间步长</span>
</span></span><span style=display:flex><span>        context <span style=color:#f92672>=</span> state[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>repeat(X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)  <span style=color:#75715e># (num_layers, batch_size, num_hiddens)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 将嵌入向量与 context 拼接</span>
</span></span><span style=display:flex><span>        X_and_context <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat((X, context), dim<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        output, state <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>rnn(X_and_context, state)  <span style=color:#75715e># 使用 GRU 处理序列</span>
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>dense(output)<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>)  <span style=color:#75715e># 转换为 (batch_size, num_steps, vocab_size)</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> output, state
</span></span></code></pre></div></li></ul><hr><h4 id=强制教学teacher-forcing><strong>强制教学（teacher forcing）</strong>
<a class=anchor href=#%e5%bc%ba%e5%88%b6%e6%95%99%e5%ad%a6teacher-forcing>#</a></h4><p>在序列到序列模型中，编码器 (encoder) 的运行相对直接，但解码器 (decoder) 的输入和输出处理需要更加谨慎。最常见的方法是 强制教学（teacher forcing）。在这种方法中，解码器的 <strong>输入使用的是目标序列 (target sequence) 的原始标签</strong>。具体来说，解码器的输入由特殊的起始标记 <code>&lt;bos></code> 和目标序列（去掉最后一个标记）拼接而成，而解码器的输出（用于训练的标签）是原始目标序列 <strong>向右偏移一个标记</strong>。例如：</p><ul><li>输入: <code>&lt;bos></code>, “Ils”, “regardent”, “.”</li><li>输出: “Ils”, “regardent”, “.”, <code>&lt;eos></code></li></ul><p>这种设计确保解码器的每一步输入可以准确地参考目标序列，从而加速训练并提高初期学习效果。</p><hr><h4 id=损失函数><strong>损失函数</strong>
<a class=anchor href=#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0>#</a></h4><p>在序列到序列（sequence-to-sequence）任务中，每个时间步的解码器会为输出标记预测一个概率分布。通过 softmax 可以得到该分布，并使用交叉熵损失（cross-entropy loss）进行优化。为了高效处理长度不同的序列，在小批量中会对齐形状，在序列末尾 <strong>填充特殊的填充标记（padding tokens）</strong>。然而，这些填充标记不应参与损失的计算。</p><p>为了解决这个问题，可以使用 <strong>掩码（masking）技术</strong>，将无关的填充部分设为零，使得这些无关部分在与预测结果相乘时结果仍为零，从而避免其对损失计算的影响。</p><p>假设预测分布为 <span>\(\mathbf{P}\)
</span>，目标分布为 <span>\(\mathbf{T}\)
</span>，掩码为 <span>\(\mathbf{M}\)
</span>，交叉熵损失的计算可以表示为：
<span>\[
\text{Loss} = - \sum_{i} \mathbf{M}_i \cdot \mathbf{T}_i \cdot \log(\mathbf{P}_i)
\]
</span>其中 <span>\(\mathbf{M}_i\)
</span>对应填充部分为零，非填充部分为一。</p><blockquote class="book-hint warning"><p><strong>e.g.</strong> 目标序列（Target Sequences）：</p><ol><li>[“I”, “am”, “happy”, <code>&lt;PAD></code>, <code>&lt;PAD></code>]</li><li>[“You”, “are”, “amazing”, “too”, <code>&lt;PAD></code>]</li><li>[“We”, “are”, “here”, “to”, “learn”]</li></ol><p>&ndash;>
M =
[[1, 1, 1, 0, 0],
[1, 1, 1, 1, 0],
[1, 1, 1, 1, 1]]</p></blockquote><ul><li><strong>seq2seq 训练示例代码实现</strong><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn, optim
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> DataLoader, Dataset
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 定义模型</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Seq2Seq</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;序列到序列模型，集成编码器和解码器&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, encoder, decoder):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>encoder <span style=color:#f92672>=</span> encoder
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>decoder <span style=color:#f92672>=</span> decoder
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, src, tgt, src_valid_len):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 编码器前向传播</span>
</span></span><span style=display:flex><span>        enc_outputs <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>encoder(src)
</span></span><span style=display:flex><span>        enc_state <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>decoder<span style=color:#f92672>.</span>init_state(enc_outputs)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 解码器前向传播</span>
</span></span><span style=display:flex><span>        dec_outputs, _ <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>decoder(tgt, enc_state)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> dec_outputs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 训练过程</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_seq2seq</span>(model, data_iter, loss_fn, optimizer, num_epochs, tgt_vocab, device):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;训练序列到序列模型&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
</span></span><span style=display:flex><span>        model<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>        total_loss <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> src, tgt <span style=color:#f92672>in</span> data_iter:
</span></span><span style=display:flex><span>            src, tgt <span style=color:#f92672>=</span> src<span style=color:#f92672>.</span>to(device), tgt<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>            tgt_input <span style=color:#f92672>=</span> tgt[:, :<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>            tgt_output <span style=color:#f92672>=</span> tgt[:, <span style=color:#ae81ff>1</span>:]
</span></span><span style=display:flex><span>            valid_len <span style=color:#f92672>=</span> (tgt_output <span style=color:#f92672>!=</span> tgt_vocab[<span style=color:#e6db74>&#34;&lt;pad&gt;&#34;</span>])<span style=color:#f92672>.</span>sum(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>            pred <span style=color:#f92672>=</span> model(src, tgt_input, valid_len)
</span></span><span style=display:flex><span>            loss <span style=color:#f92672>=</span> loss_fn(pred, tgt_output, valid_len)
</span></span><span style=display:flex><span>            loss<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>            optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>            total_loss <span style=color:#f92672>+=</span> loss<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Epoch </span><span style=color:#e6db74>{</span>epoch <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74>, Loss: </span><span style=color:#e6db74>{</span>total_loss <span style=color:#f92672>/</span> len(data_iter<span style=color:#f92672>.</span>dataset)<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div></li></ul><hr><h4 id=预测部分><strong>预测部分</strong>
<a class=anchor href=#%e9%a2%84%e6%b5%8b%e9%83%a8%e5%88%86>#</a></h4><p>在序列预测任务中，解码器会在每个时间步中将前一时间步的预测结果作为输入。具体来说，在每一步中，解码器会选择 <strong>预测概率最高的标记（token）作为当前时间步的输出</strong>。这一策略被称为 greedy decoding（贪婪解码）。</p><p>在预测开始时，初始输入是序列的开始标记（<code>&lt;bos></code>）。当解码器输出序列的结束标记（<code>&lt;eos></code>）时，预测过程结束。</p><div align=center><img src=/images/seq2seq-predict.svg width=500px/></div><ul><li><strong>预测示例代码实现</strong></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 简化版的预测函数</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict_seq2seq</span>(net, src_sentence, src_vocab, tgt_vocab, num_steps, device):
</span></span><span style=display:flex><span>    net<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 处理源句子并转为索引</span>
</span></span><span style=display:flex><span>    src_tokens <span style=color:#f92672>=</span> src_vocab[src_sentence<span style=color:#f92672>.</span>lower()<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39; &#39;</span>)] <span style=color:#f92672>+</span> [src_vocab[<span style=color:#e6db74>&#39;&lt;eos&gt;&#39;</span>]]
</span></span><span style=display:flex><span>    src_tokens <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(src_tokens, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>long, device<span style=color:#f92672>=</span>device)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)  <span style=color:#75715e># (1, num_steps)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 编码器的输出</span>
</span></span><span style=display:flex><span>    enc_outputs <span style=color:#f92672>=</span> net<span style=color:#f92672>.</span>encoder(src_tokens)
</span></span><span style=display:flex><span>    dec_state <span style=color:#f92672>=</span> net<span style=color:#f92672>.</span>decoder<span style=color:#f92672>.</span>init_state(enc_outputs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 解码器初始输入是目标语言的 &lt;bos&gt; 标记</span>
</span></span><span style=display:flex><span>    dec_X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([tgt_vocab[<span style=color:#e6db74>&#39;&lt;bos&gt;&#39;</span>]], dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>long, device<span style=color:#f92672>=</span>device)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)  <span style=color:#75715e># (1, 1)</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    output_seq <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(num_steps):
</span></span><span style=display:flex><span>        Y, dec_state <span style=color:#f92672>=</span> net<span style=color:#f92672>.</span>decoder(dec_X, dec_state)
</span></span><span style=display:flex><span>        dec_X <span style=color:#f92672>=</span> Y<span style=color:#f92672>.</span>argmax(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)  <span style=color:#75715e># 选择最大概率的词元作为下一个输入</span>
</span></span><span style=display:flex><span>        pred <span style=color:#f92672>=</span> dec_X<span style=color:#f92672>.</span>squeeze(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>item()  <span style=color:#75715e># 取出预测的词元</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> pred <span style=color:#f92672>==</span> tgt_vocab[<span style=color:#e6db74>&#39;&lt;eos&gt;&#39;</span>]:  <span style=color:#75715e># 遇到 &lt;eos&gt; 时停止</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        output_seq<span style=color:#f92672>.</span>append(pred)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 将预测的词元索引转换回词语</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#39; &#39;</span><span style=color:#f92672>.</span>join(tgt_vocab<span style=color:#f92672>.</span>to_tokens(output_seq))
</span></span></code></pre></div><hr><h4 id=预测序列的评估><strong>预测序列的评估</strong>
<a class=anchor href=#%e9%a2%84%e6%b5%8b%e5%ba%8f%e5%88%97%e7%9a%84%e8%af%84%e4%bc%b0>#</a></h4><p>我们可以通过与真实的标签序列进行比较来评估预测序列。虽然 (Papineni et al., 2002) 提出的 <strong>BLEU（bilingual evaluation understudy）</strong> 最先是用于评估机器翻译的结果，但现在它已经被广泛用于测量许多应用的输出序列的质量。 原则上说，对于预测序列中的任意n元语法（n-grams）， BLEU的评估都是这个n元语法是否出现在标签序列中。我们将BLEU定义为：
<span>\[
\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},\]</span></p><ul><li><strong>惩罚因子（BP, brevity penalty）</strong>：<span>
\(
\exp\left(\min\left(0, 1 - \frac{\mathrm{len}{\text{label}}}{\mathrm{len}{\text{pred}}}\right)\right)
\)</span><ul><li><span>\(\mathrm{len}{\text{label}}\)
</span>：目标序列（ground truth）的长度。</li><li><span>\(\mathrm{len}{\text{pred}}\)
</span>：预测序列的长度。</li><li>当 <span>\(\mathrm{len}{\text{pred}} \geq \mathrm{len}{\text{label}}\)
</span>，此时，<span>
\(\text{BP} = \exp(0) = 1\)
</span>，说明预测序列长度足够，不会被惩罚。</li><li>当 <span>\(\mathrm{len}{\text{pred}} < \mathrm{len}{\text{label}}，\text{BP}\)
</span>会小于 1，表示预测序列过短，从而受到惩罚。</li><li><strong>作用</strong>：防止模型为了提升 n-gram 精确度而倾向于生成不完整的短序列。</li></ul></li><li><strong>精确度分数（n-gram precision）</strong>：<span>
\(\prod_{n=1}^k p_n^{w_n}\)</span><ul><li><span>\(p_n\)
</span>： n-gram 的精确度，即预测序列中与目标序列匹配的 n-gram 个数占预测序列中所有 n-gram 的比例。</li><li><span>\(w_n\)
</span>：权重，通常是 <span>\(\frac{1}{k}\)
</span>，或者特定任务中递减的权重（如 <span>\(w_n = \frac{1}{2^n}\)
</span>）。</li></ul></li></ul><blockquote class="book-hint warning"><p><strong>e.g.</strong> <strong>目标序列</strong>：the cat is on the mat，<strong>预测序列</strong>：the cat the mat</p><ul><li>1-gram 匹配：the, cat, mat（3 个匹配），总共 4 个 1-gram，故 p_1 = 3/4 。</li><li>2-gram 匹配：the cat, the mat（2 个匹配），总共 3 个 2-gram，故 p_2 = 2/3 。</li></ul></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#处理序列working-with-sequences><strong>处理序列（Working with Sequences）</strong></a><ul><li><a href=#自回归模型autoregressive-models><strong>自回归模型（Autoregressive Models）</strong></a></li><li><a href=#序列模型sequence-models><strong>序列模型（Sequence Models）</strong></a></li></ul></li><li><a href=#文本预处理converting-raw-text-into-sequence-data><strong>文本预处理（Converting Raw Text into Sequence Data）</strong></a></li><li><a href=#语言模型和数据集language-models><strong>语言模型和数据集（Language Models）</strong></a><ul><li></li><li><a href=#困惑度perplexity><strong>困惑度（Perplexity）</strong></a></li><li><a href=#读取长序列数据partitioning-sequences><strong>读取长序列数据（Partitioning Sequences）</strong></a></li></ul></li><li><a href=#循环神经网络rnn概述><strong>循环神经网络（RNN）概述</strong></a><ul><li><a href=#rnn的结构与计算><strong>RNN的结构与计算</strong></a></li><li><a href=#基于循环神经网络的字符级语言模型><strong>基于循环神经网络的字符级语言模型</strong></a></li><li><a href=#one-hot-encoding-独热编码><strong>One-Hot Encoding 独热编码</strong></a></li><li><a href=#梯度裁剪gradient-clipping><strong>梯度裁剪（Gradient Clipping）</strong></a></li><li><a href=#解码-decoding><strong>解码 (Decoding)</strong></a></li></ul></li><li><a href=#rnn中的反向传播算法><strong>RNN中的反向传播算法</strong></a><ul><li><a href=#反向传播的细节><strong>反向传播的细节</strong></a></li></ul></li><li><a href=#简单循环神经网络实现><strong>简单循环神经网络实现</strong></a></li><li><a href=#经典循环神经网络modern-recurrent-neural-networks><strong>经典循环神经网络（Modern Recurrent Neural Networks）</strong></a><ul><li><a href=#长短时记忆网络long-short-term-memory-lstm><strong>长短时记忆网络（Long Short-Term Memory, LSTM）</strong></a></li><li><a href=#门控循环单元gated-recurrent-units-gru><strong>门控循环单元（Gated Recurrent Units, GRU）</strong></a></li><li><a href=#深层循环神经网络deep-recurrent-neural-networks-drnn><strong>深层循环神经网络（Deep Recurrent Neural Networks, DRNN）</strong></a></li><li><a href=#双向循环神经网络bidirectional-recurrent-neural-networks><strong>双向循环神经网络（Bidirectional Recurrent Neural Networks）</strong></a></li><li><a href=#编码器-解码器encoder-decoder架构><strong>编码器-解码器（Encoder-Decoder）架构</strong></a></li><li><a href=#序列到序列学习seq2seq><strong>序列到序列学习（seq2seq）</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>