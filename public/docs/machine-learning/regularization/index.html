<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  正则化（Regularization）
  #

Regularization 是一种用于防止机器学习模型过拟合（overfitting）的技术。通过在损失函数中添加正则化项（regularization term）等技术，限制模型的复杂度，从而提高模型的泛化能力（generalization ability）。

  参数范数惩罚（Parameter Norm Penalties）
  #

许多正则化方法都是通过向目标函数 
  \(J\)

 添加参数范数惩罚 
  \(\Omega(\theta)\)

 来限制模型（例如神经网络、线性回归或逻辑回归）的容量。我们将正则化的目标函数表示为 
  \(\tilde{J}\)

：

  \[
\tilde{J}(\theta;X,y) = J(\theta;X,y) + \lambda\Omega(\theta) \\
\]


其中 
  \(\lambda \in [0,\infty)\)

 是一个超参数，它加权范数惩罚项 
  \(\Omega\)

 相对于标准目标函数 
  \(J\)

 的相对贡献。将 
  \(\lambda\)

 设置为 0 会导致无正则化。较大的 
  \(\lambda\)

 值对应更多的正则化。
当我们的训练算法最小化正则化目标函数 
  \(\tilde{J}\)

 时，它将同时降低训练数据上的原始目标 
  \(J\)

 和参数 
  \(\theta\)

（或参数的某些子集）大小的某些度量。参数范数 
  \(\Omega\)

 的不同选择可能导致不同的解决方案被优先考虑。


  L2 正则化（L2 Regularization）
  #

L2 正则化（Ridge 正则化）通过在损失函数中添加权重平方和的惩罚项，限制模型参数的大小，降低模型复杂度。公式如下：

  \[
J(\theta) = Loss(\theta) + \lambda\lVert \theta \rVert_{2}^{2} = Loss(\theta) +  \frac{\lambda}{2}  \sum_{i=1}^n \theta_i^2
\]

"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/machine-learning/regularization/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Regularization"><meta property="og:description" content="正则化（Regularization） # Regularization 是一种用于防止机器学习模型过拟合（overfitting）的技术。通过在损失函数中添加正则化项（regularization term）等技术，限制模型的复杂度，从而提高模型的泛化能力（generalization ability）。
参数范数惩罚（Parameter Norm Penalties） # 许多正则化方法都是通过向目标函数 \(J\) 添加参数范数惩罚 \(\Omega(\theta)\) 来限制模型（例如神经网络、线性回归或逻辑回归）的容量。我们将正则化的目标函数表示为 \(\tilde{J}\) ：
\[ \tilde{J}(\theta;X,y) = J(\theta;X,y) + \lambda\Omega(\theta) \\ \] 其中 \(\lambda \in [0,\infty)\) 是一个超参数，它加权范数惩罚项 \(\Omega\) 相对于标准目标函数 \(J\) 的相对贡献。将 \(\lambda\) 设置为 0 会导致无正则化。较大的 \(\lambda\) 值对应更多的正则化。
当我们的训练算法最小化正则化目标函数 \(\tilde{J}\) 时，它将同时降低训练数据上的原始目标 \(J\) 和参数 \(\theta\) （或参数的某些子集）大小的某些度量。参数范数 \(\Omega\) 的不同选择可能导致不同的解决方案被优先考虑。
L2 正则化（L2 Regularization） # L2 正则化（Ridge 正则化）通过在损失函数中添加权重平方和的惩罚项，限制模型参数的大小，降低模型复杂度。公式如下： \[ J(\theta) = Loss(\theta) + \lambda\lVert \theta \rVert_{2}^{2} = Loss(\theta) + \frac{\lambda}{2} \sum_{i=1}^n \theta_i^2 \]"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Regularization | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/machine-learning/regularization/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.f02cf2dfa58641063a1ee2e2c9d3fb1a132e987a3e32cbb4219477161548ebbc.js integrity="sha256-8Czy36WGQQY6HuLiydP7GhMumHo+Msu0IZR3FhVI67w=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/machine-learning/regularization/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><span>Common Libraries</span><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/ class=active>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a><ul></ul></li><li><input type=checkbox id=section-1d03ca2abc7a4a1911fc57e2cecd1bcf class=toggle>
<label for=section-1d03ca2abc7a4a1911fc57e2cecd1bcf class="flex justify-between"><a href=/docs/deep-learning/computer-vision/>Computer Vision</a></label><ul></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Regularization</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#参数范数惩罚parameter-norm-penalties><strong>参数范数惩罚（Parameter Norm Penalties）</strong></a><ul><li><a href=#l2-正则化l2-regularization><strong>L2 正则化（L2 Regularization）</strong></a></li><li><a href=#l1-正则化l1-regularization><strong>L1 正则化（L1 Regularization）</strong></a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=正则化regularization><strong>正则化（Regularization）</strong>
<a class=anchor href=#%e6%ad%a3%e5%88%99%e5%8c%96regularization>#</a></h1><p>Regularization 是一种用于<strong>防止机器学习模型过拟合（overfitting）<strong>的技术。通过在损失函数中添加正则化项（regularization term）等技术，限制模型的复杂度，从而</strong>提高模型的泛化能力（generalization ability）</strong>。</p><h2 id=参数范数惩罚parameter-norm-penalties><strong>参数范数惩罚（Parameter Norm Penalties）</strong>
<a class=anchor href=#%e5%8f%82%e6%95%b0%e8%8c%83%e6%95%b0%e6%83%a9%e7%bd%9aparameter-norm-penalties>#</a></h2><p>许多正则化方法都是通过向目标函数 <span>\(J\)
</span>添加参数范数惩罚 <span>\(\Omega(\theta)\)
</span>来限制模型（例如神经网络、线性回归或逻辑回归）的容量。我们将正则化的目标函数表示为 <span>\(\tilde{J}\)
</span>：</p><span>\[
\tilde{J}(\theta;X,y) = J(\theta;X,y) + \lambda\Omega(\theta) \\
\]</span><p>其中 <span>\(\lambda \in [0,\infty)\)
</span>是一个超参数，它加权范数惩罚项 <span>\(\Omega\)
</span>相对于标准目标函数 <span>\(J\)
</span>的相对贡献。将 <span>\(\lambda\)
</span>设置为 0 会导致无正则化。较大的 <span>\(\lambda\)
</span>值对应更多的正则化。</p><p>当我们的训练算法最小化正则化目标函数 <span>\(\tilde{J}\)
</span>时，它将同时降低训练数据上的原始目标 <span>\(J\)
</span>和参数 <span>\(\theta\)
</span>（或参数的某些子集）大小的某些度量。参数范数 <span>\(\Omega\)
</span>的不同选择可能导致不同的解决方案被优先考虑。</p><hr><h3 id=l2-正则化l2-regularization><strong>L2 正则化（L2 Regularization）</strong>
<a class=anchor href=#l2-%e6%ad%a3%e5%88%99%e5%8c%96l2-regularization>#</a></h3><p>L2 正则化（Ridge 正则化）通过在损失函数中添加权重平方和的惩罚项，限制模型参数的大小，降低模型复杂度。公式如下：
<span>\[
J(\theta) = Loss(\theta) + \lambda\lVert \theta \rVert_{2}^{2} = Loss(\theta) + \frac{\lambda}{2} \sum_{i=1}^n \theta_i^2
\]</span></p><ul><li><span>\(\lambda\)
</span>：正则化强度，权衡损失函数与正则化项的重要性。</li><li><span>\(\sum_{i=1}^n \theta_i^2\)
</span>：模型权重的 L2 范数（欧几里得范数）。</li><li>正则化项前的 <span>\(\frac{1}{2}\)
</span>是为了后续求导方便。</li></ul><p>优化参数 <span>\(\theta\)
</span>时，我们对目标函数 <span>\(J(\theta) \)
</span>关于 <span>\(\theta\)
</span>求偏导：</p><span>\[
\frac{\partial J(\theta)}{\partial \theta} = \nabla J(\theta) + \lambda \theta
\]</span><p>之后将求导结果用于梯度下降法的参数更新规则：</p><span>\[
\theta := \theta - \alpha \left( \nabla J(\theta) + \lambda \theta \right)
\]</span><hr><h4 id=l2正则化的作用><strong>L2正则化的作用</strong>
<a class=anchor href=#l2%e6%ad%a3%e5%88%99%e5%8c%96%e7%9a%84%e4%bd%9c%e7%94%a8>#</a></h4><ol><li><strong>抑制模型复杂度，防止过拟合</strong>：<ul><li>L2正则化通过惩罚参数的平方和，<strong>限制了参数 <span>\(\theta\)
</span>的大小</strong>，促使模型学习更小的权重。<ul><li><strong>大权重通常意味着模型对输入数据的小变化非常敏感</strong>，容易过拟合。</li><li>小权重会使模型输出<strong>更加平滑，对噪声不敏感</strong>，泛化能力更强。</li></ul></li><li>这使得模型在高噪声或复杂数据下不会对数据点过度拟合。</li></ul></li><li><strong>降低模型的方差</strong>：<ul><li>当输入特征高度相关时（多重共线性），普通的最小二乘法会导致模型参数的方差非常大。<ul><li>L2正则化通过惩罚参数的大小，<strong>有效降低了模型的方差</strong>，方差大，意味着模型过拟合。虽然会略微增加模型的偏差（对训练集拟合得稍差），但整体的泛化误差（训练集与测试集之间的误差）会降低。</li></ul></li></ul></li><li><strong>平滑模型输出</strong>：<ul><li>L2正则化引入了一个约束，使得参数向量 <span>\(\theta\)
</span>不会无限增大，最终可以提高模型在测试集上的表现。<ul><li>普通模型的参数<span>
\(\theta\)
</span>没有任何约束，模型可能会将一些特征的权重学得<strong>非常大</strong>，导致输出对输入变化非常敏感。
• L2正则化的模型参数<span>
\(\theta\)
</span>被<strong>约束在一个较小的范围内</strong>，所有权重都会趋于较小的值，模型的<strong>输出曲线更加平滑</strong>。</li></ul></li></ul></li></ol><div align=center><img src=/images/l2.png width=400px/></div><hr><h4 id=直观理解l2正则化><strong>直观理解L2正则化</strong>
<a class=anchor href=#%e7%9b%b4%e8%a7%82%e7%90%86%e8%a7%a3l2%e6%ad%a3%e5%88%99%e5%8c%96>#</a></h4><ul><li><strong>大权重的危害</strong>：如果某个权重<span>
\(\theta_i\)
</span>特别大，模型输出就会对输入<span>
\(x_i\)
</span>的<strong>微小变化极为敏感</strong>，训练数据的<strong>噪声会被放大</strong>，导致<strong>模型记住了训练集中的噪声（过拟合）</strong>。</li><li><strong>L2正则化后</strong>：通过惩罚<span>
\(\theta_i^2\)
</span>，L2正则化会迫使所有<strong>权重尽量小</strong>，让模型输出更平滑，对噪声更鲁棒。</li><li><strong>例如</strong>：<ul><li>没有正则化时，模型可能拟合出一条剧烈波动的曲线来完全覆盖训练数据。</li><li>加了L2正则化后，模型拟合的曲线会更平滑，不会对每个数据点过分敏感。</li></ul></li></ul><hr><h3 id=l1-正则化l1-regularization><strong>L1 正则化（L1 Regularization）</strong>
<a class=anchor href=#l1-%e6%ad%a3%e5%88%99%e5%8c%96l1-regularization>#</a></h3></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#参数范数惩罚parameter-norm-penalties><strong>参数范数惩罚（Parameter Norm Penalties）</strong></a><ul><li><a href=#l2-正则化l2-regularization><strong>L2 正则化（L2 Regularization）</strong></a></li><li><a href=#l1-正则化l1-regularization><strong>L1 正则化（L1 Regularization）</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>