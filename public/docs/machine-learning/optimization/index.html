<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  优化（Optimization）
  #

机器学习和深度学习中的 优化问题（Optimization） 指的是在给定的模型结构和数据集下，通过调整模型的参数，使目标函数（Objective func-tion, or criterion）达到最小化 或最大化的过程。目标函数通常衡量模型预测值与真实值之间的偏差，例如均方误差或交叉熵。在优化过程中，参数更新依赖于目标函数对参数的梯度信息，通过迭代计算逐步逼近最优解。

凸优化（Convex Optimization） 是指目标函数为凸函数的优化问题，凸函数满足以下性质：

任意两点之间的连线上的函数值不会超过这两点的函数值。
数学形式：对于任意  



  \(x_1, x_2 \in \mathbb{R}^n\)

  和  
  \(\theta \in [0, 1]\)

 ，有

  \[
f(\theta x_1 + (1-\theta)x_2) \leq \theta f(x_1) + (1-\theta)f(x_2)
\]


目标函数特点:

单一的全局最优解（Global Minimum）。
常见目标函数形式：二次函数（如  
  \(f(x) = x^2\)

 ）、对数函数、指数函数等。
使用简单的梯度下降或更高效的二阶方法（如牛顿法）即可快速收敛。




非凸优化（Non-Convex Optimization） 非凸优化是指目标函数为非凸函数的优化问题，非凸函数可能存在多个局部最优解（Local Minimum），不满足凸函数的性质。

目标函数特点:

通常为复杂的多峰形状，可能存在多个局部最优解、鞍点甚至平坦区域。
深度学习中的损失函数（如交叉熵、均方误差）大多属于此类。


解决策略:

启发式算法: 使用随机梯度下降（SGD）及其变种（如 Adam、RMSProp）通过随机性帮助跳出局部最优解。
正则化技巧: 添加 L1/L2 正则项以平滑损失函数，减少极值点的数量。
预训练与迁移学习: 通过初始化参数靠近全局最优区域来提高收敛效率。







  基于梯度的优化（Gradient-Based Optimization）
  #

在函数 
  \(y = f(x)\)

 中（其中 
  \(x\)

 和 
  \(y\)

 都是实数），导数 
  \(f'(x)\)

（或者表示为 
  \(\frac{\partial y}{\partial x}\)

） 表示函数在点 
  \(x\)

 处的斜率（Slope）。它描述了输入 
  \(x\)

 的一个微小变化如何引起输出 
  \(y\)

 的相应变化，用以下公式近似表示：

  \[
f(x + \epsilon) \approx f(x) + \epsilon f'(x)
\]


导数在函数优化中非常有用，因为它指示了如何调整 
  \(x\)

 以使 
  \(y\)

 取得小幅改善。例如，当我们沿着导数的反方向移动时，函数值会减小，即：

  \[
f(x - \epsilon \cdot \text{sign}(f'(x))) < f(x)
\]


这种基于导数的优化技术被称为梯度下降法（Gradient Descent）。"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/machine-learning/optimization/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Optimization"><meta property="og:description" content="优化（Optimization） # 机器学习和深度学习中的 优化问题（Optimization） 指的是在给定的模型结构和数据集下，通过调整模型的参数，使目标函数（Objective func-tion, or criterion）达到最小化 或最大化的过程。目标函数通常衡量模型预测值与真实值之间的偏差，例如均方误差或交叉熵。在优化过程中，参数更新依赖于目标函数对参数的梯度信息，通过迭代计算逐步逼近最优解。
凸优化（Convex Optimization） 是指目标函数为凸函数的优化问题，凸函数满足以下性质： 任意两点之间的连线上的函数值不会超过这两点的函数值。 数学形式：对于任意 \(x_1, x_2 \in \mathbb{R}^n\) 和 \(\theta \in [0, 1]\) ，有 \[ f(\theta x_1 + (1-\theta)x_2) \leq \theta f(x_1) + (1-\theta)f(x_2) \] 目标函数特点: 单一的全局最优解（Global Minimum）。 常见目标函数形式：二次函数（如 \(f(x) = x^2\) ）、对数函数、指数函数等。 使用简单的梯度下降或更高效的二阶方法（如牛顿法）即可快速收敛。 非凸优化（Non-Convex Optimization） 非凸优化是指目标函数为非凸函数的优化问题，非凸函数可能存在多个局部最优解（Local Minimum），不满足凸函数的性质。 目标函数特点: 通常为复杂的多峰形状，可能存在多个局部最优解、鞍点甚至平坦区域。 深度学习中的损失函数（如交叉熵、均方误差）大多属于此类。 解决策略: 启发式算法: 使用随机梯度下降（SGD）及其变种（如 Adam、RMSProp）通过随机性帮助跳出局部最优解。 正则化技巧: 添加 L1/L2 正则项以平滑损失函数，减少极值点的数量。 预训练与迁移学习: 通过初始化参数靠近全局最优区域来提高收敛效率。 基于梯度的优化（Gradient-Based Optimization） # 在函数 \(y = f(x)\) 中（其中 \(x\) 和 \(y\) 都是实数），导数 \(f'(x)\) （或者表示为 \(\frac{\partial y}{\partial x}\) ） 表示函数在点 \(x\) 处的斜率（Slope）。它描述了输入 \(x\) 的一个微小变化如何引起输出 \(y\) 的相应变化，用以下公式近似表示： \[ f(x + \epsilon) \approx f(x) + \epsilon f'(x) \] 导数在函数优化中非常有用，因为它指示了如何调整 \(x\) 以使 \(y\) 取得小幅改善。例如，当我们沿着导数的反方向移动时，函数值会减小，即： \[ f(x - \epsilon \cdot \text{sign}(f'(x))) < f(x) \] 这种基于导数的优化技术被称为梯度下降法（Gradient Descent）。"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Optimization | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/machine-learning/optimization/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a92fa7929255d6b50d4e615691429cf321d08319b585d5de69c2b278f3a48e53.js integrity="sha256-qS+nkpJV1rUNTmFWkUKc8yHQgxm1hdXeacKyePOkjlM=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/machine-learning/optimization/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><span>Common Libraries</span><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/ class=active>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a><ul></ul></li><li><input type=checkbox id=section-1d03ca2abc7a4a1911fc57e2cecd1bcf class=toggle>
<label for=section-1d03ca2abc7a4a1911fc57e2cecd1bcf class="flex justify-between"><a href=/docs/deep-learning/computer-vision/>Computer Vision</a></label><ul></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Optimization</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#基于梯度的优化gradient-based-optimization><strong>基于梯度的优化（Gradient-Based Optimization）</strong></a><ul><li><a href=#关键概念><strong>关键概念</strong></a></li><li><a href=#全批量梯度下降batch-gradient-descent><strong>全批量梯度下降（Batch Gradient Descent）</strong></a></li><li><a href=#随机梯度下降stochastic-gradient-descent><strong>随机梯度下降（Stochastic Gradient Descent）</strong></a></li><li><a href=#小批量随机梯度下降minibatch-stochastic-gradient-descent><strong>小批量随机梯度下降（Minibatch Stochastic Gradient Descent）</strong></a></li></ul></li><li><a href=#优化深度神经网络的挑战challenges-in-neural-network-optimization><strong>优化深度神经网络的挑战（Challenges in Neural Network Optimization）</strong></a><ul><li><a href=#局部极小值local-minima><strong>局部极小值（Local Minima）</strong></a></li><li><a href=#平坦区域和鞍点plateaus-and-saddle-points><strong>平坦区域和鞍点（Plateaus and Saddle Points）</strong></a></li><li><a href=#梯度爆炸exploding-gradients><strong>梯度爆炸（Exploding Gradients）</strong></a></li><li><a href=#梯度消失long-term-dependencies-and-gradient-vanishing><strong>梯度消失（Long-Term Dependencies and Gradient Vanishing）</strong></a></li><li><a href=#不精确的梯度inexact-gradients><strong>不精确的梯度（Inexact Gradients）</strong></a></li></ul></li><li><a href=#momentum><strong>Momentum</strong></a></li><li><a href=#parameter-initialization-strategies><strong>Parameter Initialization Strategies</strong></a></li><li><a href=#algorithms-with-adaptive-learning-rates><strong>Algorithms with Adaptive Learning Rates</strong></a></li><li><a href=#optimization-strategies-and-meta-algorithms><strong>Optimization Strategies and Meta-Algorithms</strong></a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=优化optimization><strong>优化（Optimization）</strong>
<a class=anchor href=#%e4%bc%98%e5%8c%96optimization>#</a></h1><p>机器学习和深度学习中的 <strong>优化问题（Optimization）</strong> 指的是在给定的模型结构和数据集下，通过调整模型的参数，使<strong>目标函数（Objective func-tion, or criterion）达到最小化</strong> 或最大化的过程。目标函数通常衡量模型预测值与真实值之间的偏差，例如均方误差或交叉熵。在优化过程中，参数更新依赖于目标函数对参数的梯度信息，通过迭代计算逐步逼近最优解。</p><ul><li><strong>凸优化（Convex Optimization）</strong> 是指目标函数为凸函数的优化问题，凸函数满足以下性质：<ul><li><strong>任意两点之间的连线上</strong>的函数值不会超过这两点的函数值。</li><li>数学形式：对于任意
<link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\(x_1, x_2 \in \mathbb{R}^n\)
</span>和 <span>\(\theta \in [0, 1]\)
</span>，有
<span>\[
f(\theta x_1 + (1-\theta)x_2) \leq \theta f(x_1) + (1-\theta)f(x_2)
\]</span></li><li><strong>目标函数特点</strong>:<ul><li><strong>单一的全局最优解（Global Minimum）</strong>。</li><li>常见目标函数形式：二次函数（如 <span>\(f(x) = x^2\)
</span>）、对数函数、指数函数等。</li><li>使用简单的梯度下降或更高效的二阶方法（如牛顿法）即可<strong>快速收敛</strong>。</li></ul></li></ul></li><li><strong>非凸优化（Non-Convex Optimization）</strong> 非凸优化是指目标函数为非凸函数的优化问题，非凸函数可能<strong>存在多个局部最优解（Local Minimum）</strong>，不满足凸函数的性质。<ul><li><strong>目标函数特点</strong>:<ul><li>通常为复杂的多峰形状，<strong>可能存在多个局部最优解、鞍点甚至平坦区域</strong>。</li><li><strong>深度学习中的损失函数</strong>（如交叉熵、均方误差）大多属于此类。</li></ul></li><li><strong>解决策略</strong>:<ul><li><strong>启发式算法</strong>: 使用随机梯度下降（SGD）及其变种（如 Adam、RMSProp）通过随机性帮助跳出局部最优解。</li><li><strong>正则化技巧</strong>: 添加 L1/L2 正则项以<strong>平滑损失函数</strong>，减少极值点的数量。</li><li><strong>预训练与迁移学习</strong>: 通过初始化<strong>参数靠近全局最优区域</strong>来提高收敛效率。</li></ul></li></ul></li></ul><hr><h2 id=基于梯度的优化gradient-based-optimization><strong>基于梯度的优化（Gradient-Based Optimization）</strong>
<a class=anchor href=#%e5%9f%ba%e4%ba%8e%e6%a2%af%e5%ba%a6%e7%9a%84%e4%bc%98%e5%8c%96gradient-based-optimization>#</a></h2><p>在函数 <span>\(y = f(x)\)
</span>中（其中 <span>\(x\)
</span>和 <span>\(y\)
</span>都是实数），导数 <span>\(f'(x)\)
</span>（或者表示为 <span>\(\frac{\partial y}{\partial x}\)
</span>） 表示函数在点 <span>\(x\)
</span>处的斜率（Slope）。它描述了输入 <span>\(x\)
</span>的一个<strong>微小变化如何引起输出 <span>\(y\)
</span>的相应变化</strong>，用以下公式近似表示：
<span>\[
f(x + \epsilon) \approx f(x) + \epsilon f'(x)
\]
</span>导数在函数优化中非常有用，因为它指示了如何调整 <span>\(x\)
</span>以使 <span>\(y\)
</span>取得小幅改善。例如，当<strong>我们沿着导数的反方向移动时，函数值会减小</strong>，即：
<span>\[
f(x - \epsilon \cdot \text{sign}(f'(x))) < f(x)
\]
</span>这种基于导数的优化技术被称为<strong>梯度下降法（Gradient Descent）</strong>。</p><div align=center><img src=/images/ML_Basics_03_Gradient_Descent.PNG width=500px/></div><h3 id=关键概念><strong>关键概念</strong>
<a class=anchor href=#%e5%85%b3%e9%94%ae%e6%a6%82%e5%bf%b5>#</a></h3><ol><li><strong>临界点与极值</strong>：<ul><li>当 <span>\(f'(x) = 0\)
</span>时，称为<strong>临界点（Critical points,或者 stationary points）</strong>。</li><li><strong>局部极小值（Local minimum）</strong>：周围点中 <span>\(f(x)\)
</span>最小。</li><li><strong>局部极大值（local maximum）</strong>：周围点中 <span>\(f(x)\)
</span>最大。</li><li><strong>鞍点（Saddle points）</strong>：既非局部极小值也非局部极大值的临界点，其中正交方向的斜率（导数）全部为零（临界点），但<strong>不是函数的局部极值</strong>。在多维空间中，不必具有 0 的特征值才能得到鞍点：只需具有正和负的特征值即可。</li><li><strong>全局极小值（Global minimum）</strong>：函数 <span>\(f(x)\)
</span>在整个<strong>定义域内的最小值</strong>。</li><li><strong>平坦区域（Plateaus）</strong>：平坦区域是指目标函数的梯度几乎为零的区域，网络在这些区域中移动缓慢，几乎没有任何有效的方向引导优化过程。</li><li><strong>悬崖结构区域（Cliffs）</strong>：悬崖结构区域指的是目标函数中梯度变化非常剧烈的区域，即梯度几乎呈现出极为陡峭的下降趋势。在这种区域内，损失函数对于某些参数的变化非常敏感，导致小的参数更新可能引起损失值的剧烈波动。</li></ul><div align=center><img src=/images/ML_Basics_03_Global_Minimum.PNG width=500px/></div></li><li><strong>多维输入优化</strong>：当函数有多个输入（<span>
\(f: \mathbb{R}^n \to \mathbb{R}\)
</span>），优化需要使用<strong>梯度（Gradient）<strong>的概念。梯度是包含</strong>所有偏导数的向量</strong>，定义为：
<span>\[
\nabla_x f(x) = \left[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n}\right]
\]
</span>在多维空间中，临界点是所有梯度分量为零的点，即 <span>\(\nabla_x f(x) = 0\)
</span>。</li><li><strong>学习率的选择</strong>：学习率 <span>\(\epsilon\)
</span>决定了每一步更新的幅度，常见选择方式包括：<ul><li>固定的小常数。</li><li>使用线搜索（Line Search），在多种步长中选择目标函数值最小的步长。</li></ul></li><li><strong>梯度下降的收敛</strong>：当梯度的<strong>所有梯度的分量接近零时</strong>，梯度下降算法收敛。实际上，由于优化问题的复杂性，尤其是在深度学习中，我们<strong>通常寻找“足够低”的函数值，而非严格的全局极小值</strong>。</li></ol><hr><h3 id=全批量梯度下降batch-gradient-descent><strong>全批量梯度下降（Batch Gradient Descent）</strong>
<a class=anchor href=#%e5%85%a8%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dbatch-gradient-descent>#</a></h3><p>Batch Gradient Descent 是一种优化算法，每次使用整个数据集来计算目标函数的梯度，并基于梯度更新模型参数。这种方法适用于目标函数是所有样本损失的平均值或总和的情况。每次迭代<strong>计算所有训练样本的损失函数梯度</strong>。公式为：
<span>\[
\theta = \theta - \eta \cdot \nabla J(\theta)
\]
</span>其中，<span>
\(\nabla J(\theta)\)
</span>是基于整个数据集的梯度。</p><p><strong>工作流程</strong>一般为：</p><ol><li>初始化模型参数 <span>\(\theta\)
</span>为随机值或设定初值。</li><li>重复以下步骤，直到达到停止条件（如梯度足够小或迭代次数用尽）：<ul><li>计算当前所有样本的目标函数值和梯度 <span>\(\nabla J(\theta)\)</span></li><li>使用梯度更新模型参数：
<span>\[
\theta = \theta - \eta \cdot \nabla J(\theta)
\]</span></li></ul></li><li>输出最终优化的参数。</li></ol><ul><li><strong>优点</strong>：<ol><li><strong>精确性高</strong>：每次更新都基于完整的数据集，提供了目标函数梯度的<strong>精确估计</strong>，使得更新过程稳定可靠。</li><li><strong>容易收敛到局部或全局最优</strong>：因为梯度估计噪声较小，参数<strong>更新方向更明确</strong>。在凸优化问题中，Batch Gradient Descent 的收敛轨迹通常表现为朝向最优解的一条平滑路径，梯度的更新方向明确，不会因为噪声而偏离轨道。但是由于梯度计算使用了整个数据集，优化轨迹通常稳定地沿着梯度方向下降，<strong>容易陷入一个局部最优点或停留在鞍点上。</strong></li></ol></li><li><strong>缺点</strong>：<ol><li><strong>计算资源消耗大</strong>：每次迭代需要对整个数据集计算梯度，在数据量大时计算成本高，不适合分布式或在线训练。</li><li><strong>存储限制</strong>：对于大规模数据集，可能需要更多的内存或存储资源来一次性加载数据。</li><li><strong>收敛速度慢</strong>：尤其在每次迭代中，如果数据集中样本的梯度信息存在冗余，则更新过程可能很低效。</li></ol></li></ul><hr><h3 id=随机梯度下降stochastic-gradient-descent><strong>随机梯度下降（Stochastic Gradient Descent）</strong>
<a class=anchor href=#%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dstochastic-gradient-descent>#</a></h3><p>SGD是一种优化算法，用于通过梯度下降更新模型参数，以最小化损失函数。它的核心思想是在每次迭代中，<strong>随机选择一个样本计算梯度，而不是使用整个数据集</strong>。这种方式极大地降低了每次更新的计算成本。更新公式为：
<span>\[
\theta = \theta - \eta \cdot \nabla J(\theta; x_i, y_i)
\]</span></p><ul><li><strong>优点</strong>：<ol><li><strong>高效性</strong>：单次梯度计算只涉及一个样本，<strong>计算速度快</strong>，对内存需求低。</li><li><strong>跳出局部最优解</strong>：随机梯度的噪声可以<strong>避免陷入平滑函数中的局部最优点（local minima）</strong>，尤其适合非凸优化问题。</li><li><strong>在线学习能力</strong>：在模型需要不断更新时（如<strong>实时场景</strong>），SGD可以随着数据流实时调整参数。</li></ol></li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> 随机梯度下降在每次迭代中仅使用一个样本计算梯度，因此梯度估计会带有噪声。这种噪声主要<strong>表现为梯度方向的不确定性</strong>，使得优化过程中的参数更新具有一定的随机性和波动性。随机噪声使得优化路径<strong>不完全按照损失函数表面的梯度方向</strong>前进，而是以一种“抖动”的方式探索参数空间。当优化路径接近某个局部最优点时，全批量梯度可能因所有样本的梯度方向一致而停留在该点；而<strong>随机梯度的波动可能使路径偏离局部最优，继续搜索全局最优解</strong>。</p><p>在深度学习中的目标函数通常具有非凸性质，随机梯度的噪声可以帮助模型训练找到性能更优的解，从而避免陷入次优状态。此外研究表明，在高维空间中，随机梯度的波动尤其有助于突破鞍点，因为鞍点在高维空间中比局部最优点更常见。</p></blockquote><ul><li><strong>缺点</strong>：<ol><li><strong>梯度估计噪声大</strong>：由于每次迭代仅基于单个样本，梯度方向可能<strong>偏离真正的最优方向</strong>，导致优化过程不稳定。</li><li><strong>收敛速度慢</strong>：需要更多迭代次数才能达到较优解，与Batch Gradient Descent相比，<strong>收敛速度可能较慢</strong>。</li><li><strong>对学习率敏感</strong>：<strong>不适当的学习率可能导致震荡或过早停止收敛</strong>，往往可能需要<strong>更小的学习率（ <span>\(\eta \)
</span>）或动态调整以避免振荡</strong>。因此学习率需要仔细调节或动态调整。</li></ol></li></ul><div align=center><img src=/images/batch__stochastic__mini-batch_gradient_descent-Dec-22-2022-04-32-42-4986-AM.png.webp width=700px/></div><hr><h3 id=小批量随机梯度下降minibatch-stochastic-gradient-descent><strong>小批量随机梯度下降（Minibatch Stochastic Gradient Descent）</strong>
<a class=anchor href=#%e5%b0%8f%e6%89%b9%e9%87%8f%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dminibatch-stochastic-gradient-descent>#</a></h3><p>Minibatch SGD 是一种在每次迭代中使用一小批数据（称为 Minibatch）计算梯度并更新参数的优化方法。它结合了 Batch Gradient Descent 和 Stochastic Gradient Descent 的优点，能够在<strong>计算效率和收敛稳定性之间找到平衡</strong>。</p><ul><li>其<strong>核心工作流程</strong>一般为：<ol><li><strong>数据划分</strong>: 将训练数据集分成若干小批量（Minibatches），每个批次包含 <span>\(m\)
</span>个样本。 Minibatch 大小 <span>\(m \)
</span>一般为 16, 32, 64, 128，根据硬件资源和模型大小调节。</li><li><strong>梯度计算与更新</strong>：对于<strong>每个 Minibatch</strong> ( <span>\(X_{minibatch}, Y_{minibatch} \)
</span>)，计算目标函数在该批次上的梯度并更新参数：
<span>\[
\theta = \theta - \eta \cdot \nabla J(\theta; X_{minibatch}, Y_{minibatch})
\]
</span><strong>较大的 Minibatch 通常需要较大的学习率</strong>。可结合学习率衰减策略（如 Step Decay、Exponential Decay、Warm Restarts）来平衡收敛速度与准确性。</li><li><strong>迭代更新</strong>: 对每个 Minibatch 重复上述步骤，<strong>直到遍历整个数据集（称为一个 epoch）</strong>。根据收敛情况执行多个 epoch。</li></ol></li></ul><blockquote class="book-hint warning"><p><strong>Note</strong>：为了确保梯度估计的无偏性（unbiasedness），Minibatch的样本必须独立随机抽样。如果数据集的自然排列存在相关性（如医疗数据按患者排序），在选择Minibatch前需要对数据集进行随机打乱（shuffle）。所以我们需要在每个 <strong>epoch 开始时随机打乱数据</strong>，避免梯度计算因样本顺序产生偏差。</p></blockquote><ul><li><strong>Minibatch大小的选择</strong>：<ul><li><strong>梯度估计的准确性</strong>：批量越大，梯度估计越精确，但收益递减（标准误差与样本数量的平方根成反比）。</li><li><strong>计算资源限制</strong>：较小的批量可能导致多核硬件或GPU利用率不足；较大的批量需要更多内存。</li><li><strong>硬件优化</strong>：GPU通常在批量大小为2的幂（如32, 64, 128）时性能最佳。</li><li><strong>正则化效果</strong>：较小批量可以引入噪声，具有正则化作用，但需要调整较小的学习率。</li></ul></li></ul><blockquote class="book-hint warning"><p><strong>Note</strong>：Minibatch（小批量）在计算上具有优势（相比于大批量）的原因是，小批量数据（如 32 或 64 个样本）可以被完全加载到 GPU 中进行高效的并行计算。<strong>GPU 的计算效率在处理适量数据时达到峰值</strong>。与此同时，在大批量中需要对更多样本进行梯度计算和聚合，梯度计算过程更复杂，占用更多时间。例如，<strong>矩阵计算的开销随数据规模呈非线性增长</strong>。</p></blockquote><h4 id=训练速度比较><strong>训练速度比较</strong>：
<a class=anchor href=#%e8%ae%ad%e7%bb%83%e9%80%9f%e5%ba%a6%e6%af%94%e8%be%83>#</a></h4><ul><li>SGD 的单次更新虽然快，但更新频率极高，导致整体时间长。</li><li><strong>Minibatch SGD 在更新频率和计算量之间取得了平衡</strong>，往往在<strong>一个 epoch 的整体速度最快</strong>，是实际应用中的首选。</li><li>Batch Gradient Descent 由于每次更新都需要遍历整个数据集，在大规模数据上效率低。</li></ul><hr><h2 id=优化深度神经网络的挑战challenges-in-neural-network-optimization><strong>优化深度神经网络的挑战（Challenges in Neural Network Optimization）</strong>
<a class=anchor href=#%e4%bc%98%e5%8c%96%e6%b7%b1%e5%ba%a6%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e6%8c%91%e6%88%98challenges-in-neural-network-optimization>#</a></h2><p>优化深度神经网络面临诸多挑战，因为网络的目标函数通常是非凸函数，即包含多个局部极值点、鞍点和平坦区域等复杂结构。此外，即便是凸优化问题，也会因为高维度和数据特性而复杂化。</p><hr><h3 id=局部极小值local-minima><strong>局部极小值（Local Minima）</strong>
<a class=anchor href=#%e5%b1%80%e9%83%a8%e6%9e%81%e5%b0%8f%e5%80%bclocal-minima>#</a></h3><p>深度神经网络的目标函数（例如分类或回归问题中的损失函数）<strong>通常是非凸的</strong>。非凸函数意味着它可能有<strong>多个局部极小值、鞍点、以及平坦区域</strong>。深度网络的复杂结构使得这些局部极小值的位置和数量变得更加复杂和难以预测。虽然局部极小值可能在低维问题中更常见，但在高维空间中，局部极小值的影响通常被更复杂的平坦区域或鞍点替代。</p><ul><li><strong>局部极小值的影响</strong>：<ol><li><strong>训练停滞</strong>：局部极小值会导致优化算法的停滞，尤其是梯度下降算法。当优化器在一个局部极小值附近时，<strong>梯度变得非常小或几乎为零，参数更新几乎无法继续</strong>，导致训练过程无法继续进行。</li><li><strong>降低训练效率</strong>：即使局部极小值的影响并不完全阻止训练过程，停滞在局部极小值附近也会显著增加训练时间。<strong>优化器可能需要很长时间才能逃脱这些局部极小值</strong>，增加了收敛的时间。</li><li><strong>全局最优解的错失</strong>：如果优化器陷入局部极小值，则无法继续向全局最优解前进。尤其在复杂的神经网络中，局部极小值可能位于一个比较低的损失值附近，因此模型在训练过程中可能会<strong>错过更优的解</strong>。</li></ol></li></ul><hr><h3 id=平坦区域和鞍点plateaus-and-saddle-points><strong>平坦区域和鞍点（Plateaus and Saddle Points）</strong>
<a class=anchor href=#%e5%b9%b3%e5%9d%a6%e5%8c%ba%e5%9f%9f%e5%92%8c%e9%9e%8d%e7%82%b9plateaus-and-saddle-points>#</a></h3><p>深度神经网络包含大量的参数，尤其是深度模型中，每一层的参数都会在目标函数的定义中增加维度。随着维度的增加，非凸优化问题的复杂性大大增加。鞍点和梯度为零的平坦区域在高维空间中更为常见，且它们在这些维度上的“上升”和“下降”趋势很容易相互作用，导致目标函数在这些点附近的行为变得难以预测。鞍点的存在表明，在优化过程中，<strong>目标函数的某些方向上可能是上升的，而其他方向上可能是下降的</strong>。这种局部结构使得优化过程充满不确定性。</p><ul><li><strong>平坦区域和鞍点的影响</strong>：<ol><li><strong>梯度更新变慢</strong>：在平坦区域，梯度接近零，意味着优化算法的<strong>更新非常缓慢</strong>。无论是在平坦区域还是鞍点附近，梯度下降算法（如 SGD）都可能因为梯度过小而在这些区域停滞不前，导致收敛速度变慢，甚至完全停滞。</li><li><strong>优化过程的非稳定性</strong>：鞍点区域的存在导致梯度下降算法可能会**“卡在”某些不理想的位置，无法有效地向全局最优解逼近**。这些点的复杂几何性质使得简单的梯度下降方法无法高效逃离它们，因此可能长时间停留在鞍点区域或平坦区域，无法有效继续优化。</li><li><strong>参数更新难度增加</strong>：在优化过程中，深度神经网络的<strong>参数更新依赖于梯度信息</strong>。如果网络被困在平坦区域或鞍点，它将<strong>无法获得有效的梯度信息</strong>，从而使得参数的更新难以进行。</li></ol></li></ul><hr><h3 id=梯度爆炸exploding-gradients><strong>梯度爆炸（Exploding Gradients）</strong>
<a class=anchor href=#%e6%a2%af%e5%ba%a6%e7%88%86%e7%82%b8exploding-gradients>#</a></h3><p><strong>梯度爆炸（Exploding Gradients）</strong> 是指在反向传播过程中，<strong>梯度值在某些层中异常增大</strong>，导致权重更新时出现非常大的步长，从而导致训练过程中参数的值急剧增大，甚至溢出。</p><ul><li><p><strong>梯度爆炸的原因</strong>：</p><ol><li><strong>链式法则</strong>：在深度神经网络中，反向传播过程通过链式法则计算梯度。当<strong>网络深度较大时，梯度的计算会依赖于多个层的梯度乘积</strong>。如果某些层的梯度值较大，它们会在反向传播过程中不断放大，导致最终梯度值急剧增大，这就造成了梯度爆炸。</li><li><strong>不合理的权重初始化</strong>：如果模型的权重初始化不当（如权重值太大），会使得<strong>每一层的激活值非常大</strong>，导致反向传播时梯度的<strong>放大效应</strong>。</li><li><strong>不适当的激活函数</strong>：某些激活函数，如<strong>ReLU，在特定情况下可能导致梯度爆炸</strong>。特别是当<strong>输入值非常大</strong>时，ReLU激活函数会<strong>直接输出大值</strong>，从而在梯度传播时形成非常大的梯度值。</li></ol></li><li><p><strong>梯度爆炸的影响</strong>：</p><ul><li>梯度爆炸会导致模型参数的值变得异常大，进而导致数值溢出或数值不稳定。具体表现为：训练过程中，损失函数的值出现波动甚至爆炸，模型的参数值可能变得非常大，难以更新，甚至会导致训练停止。</li><li>更严重的是，梯度爆炸可能完全破坏训练过程，导致<strong>模型无法收敛</strong>，并可能使得计算资源的浪费加剧。</li></ul></li></ul><hr><h3 id=梯度消失long-term-dependencies-and-gradient-vanishing><strong>梯度消失（Long-Term Dependencies and Gradient Vanishing）</strong>
<a class=anchor href=#%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1long-term-dependencies-and-gradient-vanishing>#</a></h3><p>梯度消失问题发生在反向传播（backpropagation）过程中，当网络中的<strong>梯度值在传播时逐渐变小，最终接近零</strong>。由于梯度变得非常小，权重<strong>更新的步伐变得非常缓慢</strong>，导致训练变得非常困难甚至无法收敛。</p><ul><li><p><strong>梯度消失的原因</strong>：</p><ol><li><strong>链式法则</strong>：反向传播算法依赖链式法则来计算梯度，即通过每一层的梯度传递，最终得到损失函数对每个参数的梯度。然而，在深层神经网络或者长序列的网络中，梯度的传递通过多个层或时间步进行叠加。<strong>如果每一层或每一步的梯度值都小于1</strong>（通常是通过激活函数计算的），那么多次乘积会导致<strong>梯度指数级下降</strong>，最终变得非常小。</li><li><strong>激活函数的性质</strong>：常见的激活函数（如sigmoid、tanh）会在其饱和区间（即输入非常大或非常小的时候）将梯度压缩到接近零。（e.g. 当<strong>输入值非常大或非常小时，sigmoid函数的梯度接近于零</strong>，这就导致了反向传播过程中梯度的快速衰减。）</li><li><strong>深层网络和长时间序列</strong>：在深层神经网络（特别是RNN或LSTM）中，层数过多或时间步过长时，梯度必须沿着多个路径传播。这使得梯度在每一步都逐渐缩小，导致梯度几乎无法传递到网络的最早层，尤其是在处理长时间依赖时尤为严重。</li></ol></li><li><p><strong>梯度消失的影响</strong>：</p><ul><li><strong>学习效率低下</strong>：当梯度消失时，网络的参数几乎不更新，尤其是接近输入层的参数。这样，网络在训练过程中无法有效地学习到有用的特征，特别是长时间依赖的特征（例如，RNN中用于记忆先前输入的长期依赖关系）。</li><li><strong>训练停滞或失败</strong>：对于长序列数据，梯度消失会导致模型无法捕捉到远程依赖关系。训练可能会停滞，模型的性能无法提高，导致训练过程失败或收敛到不理想的结果。</li></ul></li></ul><hr><h3 id=不精确的梯度inexact-gradients><strong>不精确的梯度（Inexact Gradients）</strong>
<a class=anchor href=#%e4%b8%8d%e7%b2%be%e7%a1%ae%e7%9a%84%e6%a2%af%e5%ba%a6inexact-gradients>#</a></h3><p>在训练深度神经网络时，不精确的梯度指的是在某些情况下，计算出的梯度<strong>并不是目标函数的准确梯度</strong>，而是经过<strong>近似或估算的</strong>。这种不精确的梯度通常出现在<strong>基于小批量（mini-batch）梯度下降方法的训练过程中</strong>，也可能出现在其他优化方法中，如随机梯度下降（SGD）。这些不精确的梯度可能会影响优化的稳定性和收敛速度。</p><ul><li><p><strong>不精确的梯度的原因</strong>：</p><ul><li><strong>小批量梯度计算</strong>：因为每个小批量的数据量有限，因此计算出的梯度只是目标函数在该小批量数据上的估计值，而<strong>不是在整个训练数据集上的真实梯度</strong>。通常情况下，计算出的梯度<strong>存在一些随机噪声</strong>。这意味着每次梯度更新时，参数并未沿着真正的目标函数梯度方向进行更新，而是被噪声扰动，<strong>导致偏离最优方向</strong>。这种偏差特别是在训练初期更为显著，因为网络的参数尚未经过充分训练，梯度计算更容易受到数据和噪声的影响。</li></ul></li><li><p><strong>不精确的梯度的影响</strong>：</p><ol><li><strong>收敛速度减慢</strong>：因为不精确的梯度会引入噪声和偏差，优化过程会变得不稳定，导致梯度下降算法的收敛速度减慢。具体来说，不精确的梯度可能导致模型参数朝错误的方向更新，甚至陷入局部最优解，而不能快速接近全局最优解。</li><li><strong>过度依赖随机性</strong>：不精确的梯度也使得优化算法过度依赖随机性。虽然这种随机性在一定程度上能够帮助优化过程跳出局部最小值，但<strong>过度的随机性可能导致算法无法有效地找到全局最优解</strong>，并且在不稳定的情况下表现得更加差劲。</li></ol></li></ul><hr><h2 id=momentum><strong>Momentum</strong>
<a class=anchor href=#momentum>#</a></h2><h2 id=parameter-initialization-strategies><strong>Parameter Initialization Strategies</strong>
<a class=anchor href=#parameter-initialization-strategies>#</a></h2><h2 id=algorithms-with-adaptive-learning-rates><strong>Algorithms with Adaptive Learning Rates</strong>
<a class=anchor href=#algorithms-with-adaptive-learning-rates>#</a></h2><h2 id=optimization-strategies-and-meta-algorithms><strong>Optimization Strategies and Meta-Algorithms</strong>
<a class=anchor href=#optimization-strategies-and-meta-algorithms>#</a></h2></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#基于梯度的优化gradient-based-optimization><strong>基于梯度的优化（Gradient-Based Optimization）</strong></a><ul><li><a href=#关键概念><strong>关键概念</strong></a></li><li><a href=#全批量梯度下降batch-gradient-descent><strong>全批量梯度下降（Batch Gradient Descent）</strong></a></li><li><a href=#随机梯度下降stochastic-gradient-descent><strong>随机梯度下降（Stochastic Gradient Descent）</strong></a></li><li><a href=#小批量随机梯度下降minibatch-stochastic-gradient-descent><strong>小批量随机梯度下降（Minibatch Stochastic Gradient Descent）</strong></a></li></ul></li><li><a href=#优化深度神经网络的挑战challenges-in-neural-network-optimization><strong>优化深度神经网络的挑战（Challenges in Neural Network Optimization）</strong></a><ul><li><a href=#局部极小值local-minima><strong>局部极小值（Local Minima）</strong></a></li><li><a href=#平坦区域和鞍点plateaus-and-saddle-points><strong>平坦区域和鞍点（Plateaus and Saddle Points）</strong></a></li><li><a href=#梯度爆炸exploding-gradients><strong>梯度爆炸（Exploding Gradients）</strong></a></li><li><a href=#梯度消失long-term-dependencies-and-gradient-vanishing><strong>梯度消失（Long-Term Dependencies and Gradient Vanishing）</strong></a></li><li><a href=#不精确的梯度inexact-gradients><strong>不精确的梯度（Inexact Gradients）</strong></a></li></ul></li><li><a href=#momentum><strong>Momentum</strong></a></li><li><a href=#parameter-initialization-strategies><strong>Parameter Initialization Strategies</strong></a></li><li><a href=#algorithms-with-adaptive-learning-rates><strong>Algorithms with Adaptive Learning Rates</strong></a></li><li><a href=#optimization-strategies-and-meta-algorithms><strong>Optimization Strategies and Meta-Algorithms</strong></a></li></ul></nav></div></aside></main></body></html>