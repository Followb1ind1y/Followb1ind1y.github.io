<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  计算性能（Computational Performance）
  #


  编译器（Compilers）与解释器（Interpreters）
  #

编译器（Compilers）和解释器（Interpreters）是两种不同的程序执行方式。编译器会将整个源代码一次性翻译为目标机器可以直接执行的二进制代码（Machine Code），然后运行该编译后的程序，例如C/C++使用的GCC（GNU Compiler Collection）。相比之下，解释器逐行读取并执行代码，而不是提前转换为二进制文件，例如Python的CPython解释器。
在 Python 中，由于其是一种 解释型语言（Interpreted Language），代码的执行主要依赖解释器。Python源代码（.py 文件）首先被解析为中间字节码（Bytecode），然后由Python虚拟机（Python Virtual Machine, PVM）逐行解释执行。这种方式使Python具有高度的可移植性和灵活性，但也带来了较高的运行时开销（Runtime Overhead），因为每次执行代码时都需要经过解析和解释的过程。Python 代码执行流程一般为：

源代码（Source Code）：Python 程序员编写的 .py 文件
解析（Parsing）：Python 解释器会对代码进行语法分析（Syntax Analysis），构建抽象语法树（Abstract Syntax Tree, AST），并检查代码是否存在语法错误。
编译为字节码（Bytecode Compilation）：Python 并不会直接执行源代码，而是将其转换为 字节码（Bytecode），这是一种低级中间表示，独立于具体的计算机架构。Python 代码的字节码通常存储在 .pyc 文件中（位于 __pycache__ 目录下）。
Python 虚拟机（PVM）执行：Python 的解释器（如 CPython）包含Python 虚拟机（Python Virtual Machine, PVM），它逐条读取字节码并执行相应的操作。例如，当 PVM 读取 add(2, 3) 时，它会调用 add 函数并计算 2 + 3 的结果，然后继续执行 print(result) 语句。


Note：Python 是一种动态语言，源代码首先被编译成字节码（.pyc 文件），但字节码并不是机器代码，它仍然需要 Python 解释器来解释和执行。这意味着，Python 字节码只能在 Python 运行时环境中执行，不能像 C++ 编译的程序那样直接由操作系统和硬件执行。Python 的解释器会读取字节码，并逐条指令解释执行。这个过程并不直接交给机器，而是通过解释器与操作系统交互来实现。"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/machine-learning/computational-performance/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Computational Performance"><meta property="og:description" content="计算性能（Computational Performance） # 编译器（Compilers）与解释器（Interpreters） # 编译器（Compilers）和解释器（Interpreters）是两种不同的程序执行方式。编译器会将整个源代码一次性翻译为目标机器可以直接执行的二进制代码（Machine Code），然后运行该编译后的程序，例如C/C++使用的GCC（GNU Compiler Collection）。相比之下，解释器逐行读取并执行代码，而不是提前转换为二进制文件，例如Python的CPython解释器。
在 Python 中，由于其是一种 解释型语言（Interpreted Language），代码的执行主要依赖解释器。Python源代码（.py 文件）首先被解析为中间字节码（Bytecode），然后由Python虚拟机（Python Virtual Machine, PVM）逐行解释执行。这种方式使Python具有高度的可移植性和灵活性，但也带来了较高的运行时开销（Runtime Overhead），因为每次执行代码时都需要经过解析和解释的过程。Python 代码执行流程一般为：
源代码（Source Code）：Python 程序员编写的 .py 文件 解析（Parsing）：Python 解释器会对代码进行语法分析（Syntax Analysis），构建抽象语法树（Abstract Syntax Tree, AST），并检查代码是否存在语法错误。 编译为字节码（Bytecode Compilation）：Python 并不会直接执行源代码，而是将其转换为 字节码（Bytecode），这是一种低级中间表示，独立于具体的计算机架构。Python 代码的字节码通常存储在 .pyc 文件中（位于 __pycache__ 目录下）。 Python 虚拟机（PVM）执行：Python 的解释器（如 CPython）包含Python 虚拟机（Python Virtual Machine, PVM），它逐条读取字节码并执行相应的操作。例如，当 PVM 读取 add(2, 3) 时，它会调用 add 函数并计算 2 + 3 的结果，然后继续执行 print(result) 语句。 Note：Python 是一种动态语言，源代码首先被编译成字节码（.pyc 文件），但字节码并不是机器代码，它仍然需要 Python 解释器来解释和执行。这意味着，Python 字节码只能在 Python 运行时环境中执行，不能像 C++ 编译的程序那样直接由操作系统和硬件执行。Python 的解释器会读取字节码，并逐条指令解释执行。这个过程并不直接交给机器，而是通过解释器与操作系统交互来实现。"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Computational Performance | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/machine-learning/computational-performance/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.4611f3dc7f6156c2f615195d0c9c031077dddcfe82192d45f7bf5593cfadf7a9.js integrity="sha256-RhHz3H9hVsL2FRldDJwDEHfd3P6CGS1F979Vk8+t96k=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/machine-learning/computational-performance/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li><li><a href=/docs/machine-learning/computational-performance/ class=active>Computational Performance</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><input type=checkbox id=section-d0dd931d60033c220ecd4cd60b7c9170 class=toggle>
<label for=section-d0dd931d60033c220ecd4cd60b7c9170 class="flex justify-between"><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a></label><ul><li><a href=/docs/deep-learning/convolutional-neural-networks/modern-convolutional-neural-networks/>Modern Convolutional Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-a3019bfa8037cc33ed6405d1589b6219 class=toggle>
<label for=section-a3019bfa8037cc33ed6405d1589b6219 class="flex justify-between"><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a></label><ul><li><a href=/docs/deep-learning/recurrent-neural-networks/modern-recurrent-neural-networks/>Modern Recurrent Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-0a43584c16258b228ae9aa8d70efc320 class=toggle>
<label for=section-0a43584c16258b228ae9aa8d70efc320 class="flex justify-between"><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a></label><ul><li><a href=/docs/deep-learning/attention-and-transformers/large-scale-pretraining-with-transformers/>Large-Scale Pretraining with Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/modern-large-language-models-copy/>Modern Large Language Models</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92e8358c45c96009753cf4227e9daea8 class=toggle>
<label for=section-92e8358c45c96009753cf4227e9daea8 class="flex justify-between"><a href=/docs/deep-learning/llm-pipelines/>LLM Pipelines</a></label><ul><li><a href=/docs/deep-learning/llm-pipelines/llm-inference-and-deployment/>LLM Inference and Deployment</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul><li><a href=/docs/others/interview-preparation-guide/>Interview Preparation Guide</a><ul></ul></li></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Computational Performance</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#编译器compilers与解释器interpreters><strong>编译器（Compilers）与解释器（Interpreters）</strong></a><ul><li><a href=#命令式编程imperative-programming><strong>命令式编程（Imperative Programming）</strong></a></li><li><a href=#符号式编程symbolic-programming><strong>符号式编程（Symbolic Programming）</strong></a></li><li><a href=#混合编程hybrid-programming><strong>混合编程（Hybrid Programming）</strong></a></li></ul></li><li><a href=#异步计算-asynchronous-computation><strong>异步计算 (Asynchronous Computation)</strong></a><ul><li><a href=#通过后端实现异步-asynchrony-via-backend><strong>通过后端实现异步 (Asynchrony via Backend)</strong></a></li><li><a href=#提高计算效率-improving-computation><strong>提高计算效率 (Improving Computation)</strong></a></li></ul></li><li><a href=#自动并行化automatic-parallelism><strong>自动并行化（Automatic Parallelism）</strong></a></li><li><a href=#硬件><strong>硬件</strong></a><ul><li><a href=#计算机硬件><strong>计算机硬件</strong></a></li><li><a href=#内存ram><strong>内存（RAM）</strong></a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=计算性能computational-performance><strong>计算性能（Computational Performance）</strong>
<a class=anchor href=#%e8%ae%a1%e7%ae%97%e6%80%a7%e8%83%bdcomputational-performance>#</a></h1><h2 id=编译器compilers与解释器interpreters><strong>编译器（Compilers）与解释器（Interpreters）</strong>
<a class=anchor href=#%e7%bc%96%e8%af%91%e5%99%a8compilers%e4%b8%8e%e8%a7%a3%e9%87%8a%e5%99%a8interpreters>#</a></h2><p>编译器（Compilers）和解释器（Interpreters）是两种不同的程序执行方式。<strong>编译器会将整个源代码一次性翻译为目标机器可以直接执行的二进制代码（Machine Code）</strong>，然后运行该编译后的程序，例如C/C++使用的GCC（GNU Compiler Collection）。相比之下，<strong>解释器逐行读取并执行代码</strong>，而不是提前转换为二进制文件，例如Python的CPython解释器。</p><p>在 <strong>Python</strong> 中，由于其是一种 <strong>解释型语言（Interpreted Language）</strong>，代码的执行主要依赖解释器。Python源代码（<code>.py</code> 文件）首先被解析为中间字节码（Bytecode），然后由Python虚拟机（Python Virtual Machine, PVM）逐行解释执行。这种方式使Python具有高度的可移植性和灵活性，但也带来了较高的运行时开销（Runtime Overhead），因为每次执行代码时都需要经过解析和解释的过程。Python 代码执行流程一般为：</p><ol><li><strong>源代码（Source Code）</strong>：Python 程序员编写的 .py 文件</li><li><strong>解析（Parsing）</strong>：Python 解释器会对代码进行语法分析（Syntax Analysis），构建抽象语法树（Abstract Syntax Tree, AST），并检查代码是否存在语法错误。</li><li><strong>编译为字节码（Bytecode Compilation）</strong>：Python 并不会直接执行源代码，而是将其转换为 <strong>字节码（Bytecode）</strong>，这是一种低级中间表示，独立于具体的计算机架构。Python 代码的字节码通常存储在 .pyc 文件中（位于 <code>__pycache__</code> 目录下）。</li><li><strong>Python 虚拟机（PVM）执行</strong>：Python 的解释器（如 CPython）包含Python 虚拟机（Python Virtual Machine, PVM），它逐条读取字节码并执行相应的操作。例如，当 PVM 读取 <code>add(2, 3)</code> 时，它会调用 <code>add</code> 函数并计算 <code>2 + 3</code> 的结果，然后继续执行 <code>print(result)</code> 语句。</li></ol><blockquote class="book-hint warning"><p><strong>Note</strong>：Python 是一种动态语言，源代码首先被编译成字节码（<code>.pyc</code> 文件），但字节码并不是机器代码，它仍然需要 Python 解释器来解释和执行。这意味着，Python 字节码只能在 Python 运行时环境中执行，<strong>不能像 C++ 编译的程序那样直接由操作系统和硬件执行</strong>。Python 的解释器会读取字节码，并逐条指令解释执行。这个过程并不直接交给机器，<strong>而是通过解释器与操作系统交互来实现</strong>。</p></blockquote><p><strong>C++</strong> 代码在运行前由编译器（Compiler）一次性编译成机器码（Machine Code），然后直接运行：</p><ol><li><strong>编译（Compilation）</strong>：C++ 源代码（<code>.cpp</code>）被编译成目标文件（<code>.o</code>）。</li><li><strong>链接（Linking）</strong>：多个目标文件被链接成最终的可执行文件（<code>.exe</code> / <code>a.out</code>）。</li><li><strong>执行（Execution）</strong>：<ul><li>CPU 直接执行机器码，不需要解释器。</li><li>没有字节码解析或解释的开销，因此执行速度更快。</li></ul></li></ol><blockquote class="book-hint warning"><p><strong>Note</strong>：C++ 是一种静态编译语言，在编译时，源代码会被直接编译成平台特定的机器代码（即可执行文件），这意味着 <strong>编译后的程序可以直接由操作系统加载，并由计算机的 CPU 执行</strong>，不需要额外的解释器。C++ 程序在编译阶段就转化为操作系统特定的二进制机器代码，然后由操作系统调度执行，直接运行在 CPU 上。</p></blockquote><p>虽然 C++ 代码在逻辑上是按照顺序执行的，但 CPU 可能进行自动优化，而 <strong>Python 解释器必须逐条解析并执行</strong>，因此 C++ 不是“逐条执行”的，而是“指令流优化执行”的。</p><ul><li><strong>指令并行（Instruction-Level Parallelism, ILP）</strong>：如果没有数据依赖，可能会同时执行不同的指令。</li><li><strong>指令重排序（Out-of-Order Execution, OOOE）</strong>：如果某条指令需要等待数据，CPU 可能会提前执行后面的指令。</li></ul><hr><h3 id=命令式编程imperative-programming><strong>命令式编程（Imperative Programming）</strong>
<a class=anchor href=#%e5%91%bd%e4%bb%a4%e5%bc%8f%e7%bc%96%e7%a8%8bimperative-programming>#</a></h3><p>我们主要此前关注命令式编程（Imperative Programming），这类编程使用 <code>print</code>、<code>+</code> 和 <code>if</code> 等语句来改变程序的状态。例如，Python 是一种解释型语言（Interpreted Language），当执行 <code>fancy_func</code> 这个函数时，它会按照顺序执行其中的语句，例如 <code>e = add(a, b)</code>，然后将结果存储在变量 <code>e</code> 中。接下来的 <code>f = add(c, d)</code> 和 <code>g = add(e, f)</code> 也是类似的执行方式。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>add</span>(a, b):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> a <span style=color:#f92672>+</span> b
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fancy_func</span>(a, b, c, d):
</span></span><span style=display:flex><span>    e <span style=color:#f92672>=</span> add(a, b)
</span></span><span style=display:flex><span>    f <span style=color:#f92672>=</span> add(c, d)
</span></span><span style=display:flex><span>    g <span style=color:#f92672>=</span> add(e, f)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> g
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(fancy_func(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>))
</span></span></code></pre></div><p>虽然命令式编程较为直观，但它可能并不高效。例如，即使 <code>add</code> 函数在 <code>fancy_func</code> 中被多次调用，<strong>Python 仍然会逐条执行这三个函数调用</strong>。当这些操作在 GPU（甚至多个 GPU）上执行时，由 Python 解释器带来的开销可能会非常大。此外，在 fancy_func 运行结束之前，Python 需要存储 e 和 f 的值，因为无法提前确定这些变量是否会在后续程序中被使用。</p><hr><h3 id=符号式编程symbolic-programming><strong>符号式编程（Symbolic Programming）</strong>
<a class=anchor href=#%e7%ac%a6%e5%8f%b7%e5%bc%8f%e7%bc%96%e7%a8%8bsymbolic-programming>#</a></h3><p>相比之下，<strong>符号式编程（Symbolic Programming）</strong> 通常会等到整个计算过程完全定义后再执行。这种策略被多个深度学习框架采用，例如 Theano 和 TensorFlow（尽管 TensorFlow 现在也支持命令式扩展）。它通常包含以下几个步骤：</p><ol><li>定义要执行的操作。</li><li>将操作编译（Compile）为可执行程序。</li><li>提供必要的输入，并调用已编译的程序进行执行。</li></ol><blockquote class="book-hint warning"><p><strong>Note</strong>：可以理解为先定义并优化整个计算过程，然后一次性执行所有操作，而不是逐步执行每个步骤。操作被 <strong>预定义并优化为更高效的执行计划</strong>，在执行时无需再逐条执行每一条 Python 语句，而是将计算过程整体处理，减轻了 Python 解释器逐条解析和执行的负担。</p><p>在符号式编程中，计算图已经提前优化，并且计算图的执行往往是通过专门的引擎（如 TensorFlow、Theano、PyTorch）来处理，而这些框架通常采用低层次的 C/C++ 实现，并且能够通过多个 GPU 加速计算。因此，虽然在 Python 层面可能仍然会有一些动态开销，但符号式编程通过将 <strong>核心计算任务转移到更高效的底层计算引擎，绕过了 Python 解释器的瓶颈</strong>，极大地提高了性能。</p></blockquote><p>这种方式带来了显著的优化效果：</p><ul><li>减少 Python 解释器的开销。在多个 GPU 配合单个 CPU 线程运行时，Python 解释器可能成为性能瓶颈。</li><li>优化代码执行。编译器可以优化代码，例如将 <code>print((1 + 2) + (3 + 4))</code> 直接转换成 <code>print(10)</code>，因为它在编译阶段可以看到完整的代码。</li><li>高效的内存管理。编译器可以在变量不再需要时释放内存，甚至不分配内存。</li><li>代码转换与优化。编译器可以将代码转换为等效但更高效的版本。</li></ul><hr><h3 id=混合编程hybrid-programming><strong>混合编程（Hybrid Programming）</strong>
<a class=anchor href=#%e6%b7%b7%e5%90%88%e7%bc%96%e7%a8%8bhybrid-programming>#</a></h3><p><strong>混合编程（Hybrid Programming）</strong> 是将不同编程范式、语言或框架结合起来，以利用各自的优点，解决不同层次或任务的计算问题。这种方法能够在性能和灵活性之间找到一个平衡点，通常包括：使用低级语言（如 C、C++）处理性能密集型任务，结合高级语言（如 Python）处理灵活性较强的任务，或者将不同计算模型结合起来。</p><p>Python 是解释型语言，执行时逐行解释，这导致了在 <strong>多 GPU 环境下，Python 本身成为了性能瓶颈</strong>。即使是快速的单个 GPU 也可以顺利运行，但一旦使用多个 GPU（比如 8 个 GPU），<strong>Python 的 GIL（全局解释器锁）和单线程执行限制就会使得 Python 解释器成为性能瓶颈</strong>。</p><p>在这种情况下，混合编程的解决方案是将 Python 层的计算和低级优化代码结合起来，特别是使用 <code>HybridSequential</code> 这样的结构，替代传统的 <code>Sequential</code> 层。</p><p><code>HybridSequential</code> 是在 PyTorch 中使用的一种优化方式，它结合了高层 Python 接口和底层 C++ 代码，使得模型的计算更高效，尤其是在多 GPU 环境下。通过使用混合编程，框架可以将大部分计算推向底层的高效实现，从而减少 Python 解释器的开销，提升并行性。<strong>通过使用 HybridSequential，计算图会被优化，能够利用底层的高效并行计算，这样 Python 的解释器不会成为瓶颈，计算可以被充分分配到多个 GPU 上，从而提升性能。</strong></p><blockquote class="book-hint warning"><p><strong>Note</strong>：在大型语言模型（LLM）中，尤其是像 GPT、BERT 等模型，混合编程也是提高计算效率的关键。LLM 通常涉及大量的矩阵运算和深度神经网络的训练，通常需要 <strong>在多个 GPU 上并行处理</strong>。</p><p>例如，在训练 GPT 等大型模型时，框架（如 HuggingFace 的 Transformers）将模型的计算图分解，并通过低级实现（如 CUDA 核心代码）来高效地在多 GPU 上运行，而这些操作会结合 Python 的高级接口与 C/C++ 的底层优化代码。在模型的计算中，<strong>尽量避免 Python 逐条解释执行的过程，而是通过优化的计算图或批量操作一次性完成</strong>。</p><p><strong>示例</strong>：</p><ul><li>在训练过程中，通过框架的混合编程，将数据传递到 GPU 并行计算的同时，Python 只负责协调工作流程，而 <strong>具体的矩阵运算、权重更新等则交给底层的优化实现（如 CUDA 或 C++ 编写的加速库）来处理</strong>，从而避免 Python 解释器的瓶颈。</li></ul></blockquote><h2 id=异步计算-asynchronous-computation><strong>异步计算 (Asynchronous Computation)</strong>
<a class=anchor href=#%e5%bc%82%e6%ad%a5%e8%ae%a1%e7%ae%97-asynchronous-computation>#</a></h2><p>现代计算机是高度并行的系统，通常包含多个CPU核心（每个核心可能有多个线程）、每个GPU有多个处理单元，并且每个设备通常还配备多个GPU。简而言之，<strong>我们可以同时处理很多不同的任务</strong>，且常常在不同的设备上执行。然而，Python并不是编写并行和异步代码的最佳选择，至少在没有额外帮助的情况下是这样的。毕竟，<strong>Python是单线程的</strong>，这一点未来不太可能发生改变。像MXNet和TensorFlow这样的深度学习框架采用异步编程模型来提高性能，而PyTorch则使用Python自带的调度器，从而带来了不同的性能权衡。对于PyTorch而言，默认情况下，GPU操作是异步的。当调用一个使用GPU的函数时，操作会被加入到指定设备的任务队列中，但不一定立刻执行。这允许我们并行执行更多计算任务，包括CPU或其他GPU上的操作。</p><hr><h3 id=通过后端实现异步-asynchrony-via-backend><strong>通过后端实现异步 (Asynchrony via Backend)</strong>
<a class=anchor href=#%e9%80%9a%e8%bf%87%e5%90%8e%e7%ab%af%e5%ae%9e%e7%8e%b0%e5%bc%82%e6%ad%a5-asynchrony-via-backend>#</a></h3><p>在PyTorch中，前端与用户进行交互，例如通过Python进行编程，后端则用于执行计算任务。无论使用何种前端编程语言（如Python、C++），PyTorch程序的执行主要发生在C++实现的后端中。前端语言发出的操作会传递给后端执行，后端管理自己的线程，持续收集并执行排队的任务。后端需要能够跟踪计算图中各步骤之间的依赖关系，因此，依赖关系密切的操作无法并行执行。</p><p>例如，在PyTorch中，<strong>通过前端语言（Python）执行的计算任务会先加入到后端队列中，而不立即执行</strong>。当需要打印最后一行结果时，前端线程会 <strong>等待C++后端线程完成计算并返回结果</strong>。这种设计的好处是，<strong>Python前端线程不需要执行实际计算</strong>，因此Python的性能对程序整体性能影响较小。</p><hr><h3 id=提高计算效率-improving-computation><strong>提高计算效率 (Improving Computation)</strong>
<a class=anchor href=#%e6%8f%90%e9%ab%98%e8%ae%a1%e7%ae%97%e6%95%88%e7%8e%87-improving-computation>#</a></h3><p>在高度多线程的系统中（即使是普通的笔记本电脑也有4个或更多线程，在多插槽的服务器上这个数字可能超过256），操作调度的开销可能变得非常显著。因此，实现计算和调度的异步和并行化非常重要。</p><p>举个例子，假设我们要将变量递增1 多次，我们可以通过同步和异步两种方式进行对比。通过异步执行，前端线程不必等待每个操作的结果，计算任务可以并行执行，从而显著提高效率。</p><p>简化后的前端（Python）和后端（C++）的交互过程如下：</p><ol><li>前端将计算任务（如 y = x + 1）加入任务队列。</li><li>后端从队列中获取任务并执行实际的计算。</li><li>计算结果返回给前端。</li></ol><p>如果不使用异步编程，执行10000次计算的总时间大约是 t1 + t2 + t3，而如果使用异步编程，前端可以并行执行任务，因此执行10000次计算的总时间可以减少为 t1 + t3（假设 t2 可以并行执行）。</p><hr><h2 id=自动并行化automatic-parallelism><strong>自动并行化（Automatic Parallelism）</strong>
<a class=anchor href=#%e8%87%aa%e5%8a%a8%e5%b9%b6%e8%a1%8c%e5%8c%96automatic-parallelism>#</a></h2><p>深度学习框架（例如 MXNet 和 PyTorch）在后台会自动构建计算图（computational graph）。<strong>通过计算图，系统可以了解所有操作之间的依赖关系，并选择性地并行执行多个相互独立的任务，从而提高计算速度</strong>。</p><p>通常，一个操作会使用所有CPU的计算资源或单个GPU的计算资源。例如，点积（dot）操作会使用所有CPU上的核心（core）和线程（thread），即使在同一台机器上有多个CPU处理器。这同样适用于单个GPU。因此，对于单设备计算机来说，并行化的效果并不显著。多个设备的情况则有所不同。在多个GPU的场景中，并行化尤为重要，同时添加本地CPU也能稍微提高性能。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>核心（Core）</strong>：核心是 <strong>物理处理单元</strong>，也就是CPU内部可以独立执行计算任务的部分。一个CPU可能有多个核心，比如双核（2 cores）、四核（4 cores）、十六核（16 cores）等。每个核心可以独立执行指令，所以多个核心可以 并行执行多个任务，提升计算性能。</p><p><strong>线程（Thread）</strong>：线程是 <strong>操作系统调度的最小单位</strong>，它是运行在 核心上的执行流。一个核心可以支持多个线程，例如 超线程（Hyper-Threading, HT）技术 允许每个物理核心模拟出 两个逻辑线程，从而在一定程度上提高 CPU 利用率。</p></blockquote><p>Automatic Parallelism（自动并行化）指的是深度学习框架（如 PyTorch、MXNet）在后端自动构建计算图（Computational Graph），并根据计算任务之间的依赖关系，<strong>智能地调度和执行多个独立的任务</strong>，使其在多个计算设备（如 CPU、GPU）上并行运行，以提高计算效率。<strong>用户无需手动编写复杂的并行代码，框架会自动管理任务分配和计算资源调度</strong>。自动并行化主要发生在以下几种情况：</p><ol><li><strong>独立任务（Independent Tasks）</strong><ul><li>当多个计算任务之间没有数据依赖关系（即它们的计算结果互不影响），框架可以将它们同时调度执行。例如，在 PyTorch 中，如果两个张量（Tensor）分别初始化且不相互依赖，那么它们可以被并行计算。</li></ul></li><li><strong>单个运算符（Single Operator）</strong><ul><li>一个算子（Operator）本身可能已经进行了多线程或多核心优化。例如，在 CPU 上执行 <code>torch.matmul()</code>（矩阵乘法）时，它会自动使用所有可用的 CPU 核心（cores）和线程（threads）进行计算，而无需用户手动并行化。</li><li>在 GPU 上，许多计算任务会自动分配到多个 CUDA 核心（CUDA cores），如 <code>torch.mm()</code>（矩阵乘法）或 <code>torch.conv2d()</code>（卷积运算）。</li></ul></li><li><strong>多设备计算（Multi-device Computation）</strong><ul><li>如果有多个 GPU，深度学习框架可以自动调度计算任务到多个设备。例如，在数据并行（Data Parallelism）中，模型的不同 mini-batch 可能被分配到不同的 GPU 进行计算。</li><li>同时，部分任务也可以在 CPU 上执行，以进一步优化计算效率（例如 GPU 计算梯度，CPU 负责数据预处理）。</li></ul></li><li><strong>计算与通信并行（Computation and Communication Overlap）</strong><ul><li>在分布式训练或多 GPU 计算时，梯度需要在多个设备之间传输。PyTorch 提供 <code>non_blocking=True</code> 选项，使得数据传输（如 <code>to()</code>、<code>copy_()</code>）可以与计算同时进行，而不会相互阻塞，从而提升效率。</li></ul></li></ol><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>CUDA的内核（kernel）和流（stream）具体指什么？</strong></p><ul><li><p><strong>Kernel（CUDA 内核）</strong>：Kernel（内核） 指的是在 GPU 上执行的并行计算任务，它是一个在 GPU 上运行的函数。GPU 由多个 CUDA 核心（CUDA Cores）组成，每个 Kernel 运行时，会在多个 CUDA 核心上执行多个线程，实现大规模并行计算。</p></li><li><p><strong>Stream（CUDA 流）</strong>：Stream（流） 是 CUDA 任务执行的流水线，表示一系列按顺序执行的计算或数据传输操作。默认情况下，CUDA 计算是在一个流（default stream）中串行执行的，但如果使用多个流（streams），计算可以并行进行，从而提高计算效率</p></li></ul></blockquote><hr><h2 id=硬件><strong>硬件</strong>
<a class=anchor href=#%e7%a1%ac%e4%bb%b6>#</a></h2><p>在学习计算性能（Computational Performance）时，硬件是不可忽视的关键因素，理解计算机的硬件架构和性能特点对于设计高效的算法至关重要。好的系统设计可以带来数量级的性能提升，可能会影响训练一个深度学习模型所需的时间，从几个月缩短到几周甚至几天。</p><hr><h3 id=计算机硬件><strong>计算机硬件</strong>
<a class=anchor href=#%e8%ae%a1%e7%ae%97%e6%9c%ba%e7%a1%ac%e4%bb%b6>#</a></h3><p>大多数深度学习研究者和实践者使用的计算机都配备了大量内存和计算能力，并且常常有某种形式的加速器（如GPU）来提升性能。计算机的关键组件包括：</p><ul><li><strong>处理器（CPU）</strong>：负责执行程序，通常包含8个或更多的核心（Cores）。</li><li><strong>内存（RAM）</strong>：用于存储计算结果，例如权重向量、激活值以及训练数据。</li><li><strong>网络连接</strong>：如以太网（Ethernet），其速度范围从1GB/s到100GB/s。高端服务器可能配备更先进的互联技术。</li><li><strong>高速扩展总线（high speed expansion bus, PCIe）</strong>：将计算机与一个或多个GPU连接。在服务器中，通常有8个加速器，而在桌面计算机中通常有1或2个，具体取决于用户的预算和电源供应。</li><li><strong>持久存储</strong>：如硬盘驱动器（HDD）或固态硬盘（SSD），用于高效传输训练数据和存储中间检查点。</li></ul><p>这些组件通过 PCIe 总线连接到CPU。以AMD的Threadripper 3为例，它有64个PCIe 4.0通道，每个通道可以实现16 Gbit/s的双向数据传输。内存直接连接到CPU，带宽可高达100GB/s。</p><p>为了实现良好的性能，计算任务需要流畅地将数据从存储传输到处理器（CPU或GPU），进行计算，然后再将结果返回到内存和持久存储。为了避免性能瓶颈，需要确保系统中的每个部分都能高效地工作。</p><hr><h3 id=内存ram><strong>内存（RAM）</strong>
<a class=anchor href=#%e5%86%85%e5%ad%98ram>#</a></h3><p>内存的基本作用是存储需要快速访问的数据。当前CPU内存通常采用DDR4内存，每个内存模块的带宽为20–25GB/s。每个模块有64位宽的数据总线，通常使用内存模块对来提供多个内存通道。CPU通常有2到4个内存通道，总带宽可达100GB/s。</p><p>内存访问的成本并不只是带宽问题。访问内存时，需要首先将内存地址发送到内存模块，随后进行读取。第一次读取的成本通常较高，大约为100纳秒，而随后的读取则更为高效，仅需0.2纳秒。为了提高性能，最好避免随机内存访问，而应尽量使用“突发读取”（Burst Read）。这类读写操作一次性传输大量数据，效率远高于单个数据的随机读取。</p><p>对于GPU而言，由于其有更多的计算单元，因此内存的带宽要求更高。常见的解决方案是使用宽总线和高性能内存。例如，NVIDIA的RTX 2080 Ti具有352位宽的总线，能够同时传输更多信息。GPU常用的高性能内存如GDDR6，其带宽可超过500GB/s。高带宽内存（HBM）则通过专用硅片与GPU连接，成本较高，通常只用于高端服务器。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：<strong>总线宽度（Bus Width）和带宽（Bandwidth）具体指什么？</strong></p><ul><li><p><strong>总线宽度（Bus Width）</strong>：总线宽度指的是显卡或计算机内存总线中并行数据传输的“通道”数，也就是一次能够传输多少位的数据。在显卡中，通常用位（bit）来表示总线宽度。例如，352位总线意味着显卡的内存控制器可以同时传输352个比特（bit）的数据。总线宽度越大，意味着每个时钟周期内可以传输的数据量越大。</p></li><li><p><strong>带宽（Bandwidth）</strong>：带宽是指在单位时间内能够传输的数据量，通常以每秒多少字节（GB/s或GB/s）来表示。带宽越大，意味着显卡可以在单位时间内处理更多的数据，这对于图形处理和并行计算任务尤为重要。</p></li></ul><p>假设一款显卡的内存时钟为21GHz（每秒21亿次时钟），总线宽度为352位，那么它的带宽计算如下：</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[
\text{带宽} = 21 \, \text{GHz} \times 352 \, \text{bit} \times 2 = 14,784 \, \text{GB/s}
\]</span><p>如果将其转化为GB/s，可以得到大约 500GB/s 的带宽，表明显卡可以在每秒钟内传输500GB的数据。这对于大规模图形渲染、视频处理、深度学习等高带宽需求的任务至关重要。</p></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#编译器compilers与解释器interpreters><strong>编译器（Compilers）与解释器（Interpreters）</strong></a><ul><li><a href=#命令式编程imperative-programming><strong>命令式编程（Imperative Programming）</strong></a></li><li><a href=#符号式编程symbolic-programming><strong>符号式编程（Symbolic Programming）</strong></a></li><li><a href=#混合编程hybrid-programming><strong>混合编程（Hybrid Programming）</strong></a></li></ul></li><li><a href=#异步计算-asynchronous-computation><strong>异步计算 (Asynchronous Computation)</strong></a><ul><li><a href=#通过后端实现异步-asynchrony-via-backend><strong>通过后端实现异步 (Asynchrony via Backend)</strong></a></li><li><a href=#提高计算效率-improving-computation><strong>提高计算效率 (Improving Computation)</strong></a></li></ul></li><li><a href=#自动并行化automatic-parallelism><strong>自动并行化（Automatic Parallelism）</strong></a></li><li><a href=#硬件><strong>硬件</strong></a><ul><li><a href=#计算机硬件><strong>计算机硬件</strong></a></li><li><a href=#内存ram><strong>内存（RAM）</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>