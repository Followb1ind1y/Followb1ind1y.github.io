<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='
  逻辑回归
  #


  逻辑回归（Logistic Regression）
  #

Logistic Regression（逻辑回归）是一种用于分类问题的统计模型，本质上是一种线性模型，通过 Sigmoid 函数将线性回归的输出映射到 (0,1) 区间，用于预测概率。分类是应用 逻辑回归(Logistic Regression) 的目的和结果, 但中间过程依旧是回归. 通过逻辑回归模型, 我们得到的计算结果是 0-1 之间的连续数字, 可以把它称为"可能性"（概率）. 然后, 给这个可能性加一个阈值, 就成了分类. 例如, 可能性大于 0.5 即记为 1, 可能性小于 0.5 则记为 0。其数学公式可以表达为：




  \[
 \sigma(z) = \frac{1}{1 + e^{-z}} ，z = w^T x + b 
\]




输出概率:

  \[
\begin{align*}  
&P(y=1|x) = \sigma(w^T x &#43; b) \\
&P(y=0|x) = 1 - \sigma(w^T x &#43; b) \\
\end{align*}  
\]

'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/machine-learning/supervised-learning/logistic-regression/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Logistic Regression"><meta property="og:description" content='逻辑回归 # 逻辑回归（Logistic Regression） # Logistic Regression（逻辑回归）是一种用于分类问题的统计模型，本质上是一种线性模型，通过 Sigmoid 函数将线性回归的输出映射到 (0,1) 区间，用于预测概率。分类是应用 逻辑回归(Logistic Regression) 的目的和结果, 但中间过程依旧是回归. 通过逻辑回归模型, 我们得到的计算结果是 0-1 之间的连续数字, 可以把它称为"可能性"（概率）. 然后, 给这个可能性加一个阈值, 就成了分类. 例如, 可能性大于 0.5 即记为 1, 可能性小于 0.5 则记为 0。其数学公式可以表达为：
\[ \sigma(z) = \frac{1}{1 + e^{-z}} ，z = w^T x + b \] 输出概率: \[ \begin{align*} &amp;P(y=1|x) = \sigma(w^T x + b) \\ &amp;P(y=0|x) = 1 - \sigma(w^T x + b) \\ \end{align*} \]'><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Logistic Regression | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/machine-learning/supervised-learning/logistic-regression/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.76b4636a0c4b779407d544c6348e195c736f9397a968e1955e6aba97e85f6bbf.js integrity="sha256-drRjagxLd5QH1UTGNI4ZXHNvk5epaOGVXmq6l+hfa78=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/machine-learning/supervised-learning/logistic-regression/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><span>Common Libraries</span><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle checked>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/ class=active>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li><li><a href=/docs/machine-learning/loss-functions/>Loss Functions</a><ul></ul></li><li><a href=/docs/machine-learning/evaluation-metrics/>Evaluation Metrics</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a><ul></ul></li><li><input type=checkbox id=section-1d03ca2abc7a4a1911fc57e2cecd1bcf class=toggle>
<label for=section-1d03ca2abc7a4a1911fc57e2cecd1bcf class="flex justify-between"><a href=/docs/deep-learning/computer-vision/>Computer Vision</a></label><ul></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Logistic Regression</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#逻辑回归logistic-regression><strong>逻辑回归（Logistic Regression）</strong></a><ul><li><a href=#sigmoid-函数><strong>Sigmoid 函数</strong></a></li><li><a href=#损失函数loss-function><strong>损失函数（Loss Function）</strong></a></li><li><a href=#梯度下降gradient-descent><strong>梯度下降（Gradient Descent）</strong></a></li><li><a href=#性能评估evaluation-metrics><strong>性能评估（Evaluation Metrics）</strong></a></li><li><a href=#logistic-regression-代码实现><strong>Logistic Regression 代码实现</strong></a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=逻辑回归><strong>逻辑回归</strong>
<a class=anchor href=#%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92>#</a></h1><h2 id=逻辑回归logistic-regression><strong>逻辑回归（Logistic Regression）</strong>
<a class=anchor href=#%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92logistic-regression>#</a></h2><p>Logistic Regression（逻辑回归）是一种用于分类问题的统计模型，本质上是一种线性模型，通过 <code>Sigmoid</code> 函数将<strong>线性回归的输出映射到 <code>(0,1)</code> 区间</strong>，用于预测概率。分类是应用 逻辑回归(Logistic Regression) 的目的和结果, 但中间过程依旧是回归. 通过逻辑回归模型, 我们得到的计算结果是 <code>0-1</code> 之间的连续数字, 可以把它称为"可能性"（概率）. 然后, 给这个<strong>可能性加一个阈值, 就成了分类</strong>. 例如, 可能性大于 <code>0.5</code> 即记为 <code>1</code>, 可能性小于 <code>0.5</code> 则记为 <code>0</code>。其数学公式可以表达为：</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[
\sigma(z) = \frac{1}{1 + e^{-z}} ，z = w^T x + b
\]</span><ul><li><p><strong>输出概率</strong>:
<span>\[
\begin{align*}
&P(y=1|x) = \sigma(w^T x + b) \\
&P(y=0|x) = 1 - \sigma(w^T x + b) \\
\end{align*}
\]</span></p></li><li><p><strong>决策边界</strong>：
<strong><span>\(P(y=1|x) \geq 0.5\)
</span>时预测为1</strong>，反之预测为0。</p></li></ul><hr><h3 id=sigmoid-函数><strong>Sigmoid 函数</strong>
<a class=anchor href=#sigmoid-%e5%87%bd%e6%95%b0>#</a></h3><p><code>Sigmoid</code> 函数是一种常用的激活函数，将任意实数映射到区间 <code>(0, 1)</code>。 Logistic回归中，Sigmoid的输出可以帮助解释为<strong>事件发生的概率</strong>。它的数学表达式为：</p><span>\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]</span><div align=center><img src=/images/3_Sigmoid_Function.PNG width=600px/></div><ul><li><strong>值域</strong>：<code>Sigmoid</code> 函数的输出值范围是 <code>(0, 1)</code> ，这使得它特别适合用于概率预测。</li><li><strong>单调递增</strong>：<code>Sigmoid</code> 是单调递增函数，意味着 <strong>输入值越大，输出值越接近 1</strong>。</li><li><strong>中心对称</strong>：以点 <code>(0, 0.5)</code> 为对称中心。</li><li><strong>平滑性</strong>：<code>Sigmoid</code> 函数是光滑的，<strong>具有连续的一阶和二阶导数</strong>。</li></ul><blockquote class="book-hint warning"><p><strong>Note：</strong> 逻辑回归中，<code>Sigmoid</code> 函数的输出是 <strong>分类的概率</strong>，而不是分类的类别。</p></blockquote><hr><h3 id=损失函数loss-function><strong>损失函数（Loss Function）</strong>
<a class=anchor href=#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0loss-function>#</a></h3><p>Logistic 回归的训练目标是通过优化目标函数找到最优的模型参数，使模型能够对输入样本进行概率预测，并最大程度地准确分类数据，最小化训练数据的损失函数（Loss Function）。Logistic 回归的损失函数是基于 <strong>交叉熵损失（Cross-Entropy Loss）</strong> 定义的，它反映了模型预测值与实际值之间的不一致程度。</p><ul><li><p><strong>对单个样本的损失函数</strong>：Logistic 回归的损失函数采用对数似然函数的负值，针对二分类任务的每个样本：
<span>\[
\text{Loss}(y, \hat{y}) = -\left[ y \log \hat{y} + (1 - y) \log (1 - \hat{y}) \right]
\]</span></p><ul><li><span>\(y \in \{0, 1\}\)
</span>是实际标签。</li><li><span>\(\hat{y} = P(y=1|x)\)
</span>是模型预测的概率。</li></ul><p>该损失函数的两种情况：</p><ul><li>当 <span>\(y = 1\)
</span>：损失为 <span>\(-\log(\hat{y})\)
</span>，<strong>鼓励模型将预测概率 <span>\(\hat{y}\)
</span>接近 1</strong>。</li><li>当 <span>\(y = 0\)
</span>：损失为 <span>\(-\log(1 - \hat{y})\)
</span>，<strong>鼓励模型将预测概率 <span>\(\hat{y}\)
</span>接近 0</strong>。</li></ul></li></ul><div align=center><img src=/images/log_function.png width=600px/></div><ul><li><p><strong>总体损失函数</strong>：对整个数据集的损失函数是所有样本损失的平均值：
<span>\[
\mathcal{L}(w, b) = -\frac{1}{n} \sum_{i=1}^n \left[ y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) \right]
\]</span></p><p>这里 <span>\(\hat{y}_i = \sigma(z_i) = \frac{1}{1 + e^{-z_i}}\)
</span>，其中 <span>\(z_i = w^T x_i + b \)
</span>。</p></li></ul><hr><h3 id=梯度下降gradient-descent><strong>梯度下降（Gradient Descent）</strong>
<a class=anchor href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dgradient-descent>#</a></h3><p>Logistic Regression 使用梯度下降（Gradient Descent）优化其损失函数。在优化过程中，需要计算损失函数的梯度以更新模型参数 <span>\(w\)
</span>, <span>\(b\)
</span>：</p><ul><li><p>损失函数对权重的梯度：
<span>\[
\frac{\partial \mathcal{L}}{\partial w} = \frac{1}{n} \sum_{i=1}^n \left[ \sigma(w^T x_i + b) - y_i \right] x_i
\]
</span>其中 <span>\(\sigma(w^T x_i + b) - y_i\)
</span>是预测值与真实值的误差。</p></li><li><p>损失函数对偏置的梯度：
<span>\[
\frac{\partial \mathcal{L}}{\partial b} = \frac{1}{n} \sum_{i=1}^n \left[ \sigma(w^T x_i + b) - y_i \right]
\]</span></p></li><li><p>利用梯度更新参数：
<span>\[
w := w - \alpha \frac{\partial L}{\partial w}, \quad b := b - \alpha \frac{\partial L}{\partial b}
\]</span></p></li></ul><hr><h3 id=性能评估evaluation-metrics><strong>性能评估（Evaluation Metrics）</strong>
<a class=anchor href=#%e6%80%a7%e8%83%bd%e8%af%84%e4%bc%b0evaluation-metrics>#</a></h3><h4 id=混淆矩阵-confusion-matrix><strong>混淆矩阵 (Confusion Matrix)</strong>
<a class=anchor href=#%e6%b7%b7%e6%b7%86%e7%9f%a9%e9%98%b5-confusion-matrix>#</a></h4><p>混淆矩阵是分类模型的基本评价工具，用于总结预测结果的分类情况。对于二分类问题，矩阵包含以下四个元素：</p><div style=text-align:center><table><thead><tr><th></th><th><strong>预测正类 <span>\( (\hat{y} = 1) \)</span></strong></th><th><strong>预测负类 <span>\((\hat{y} = 0)\)</span></strong></th></tr></thead><tbody><tr><td><strong>实际正类 <span>\((y = 1)\)</span></strong></td><td><strong>TP (True Positive)</strong></td><td><strong>FN (False Negative)</strong></td></tr><tr><td><strong>实际负类 <span>\((y = 0)\)</span></strong></td><td><strong>FP (False Positive)</strong></td><td><strong>TN (True Negative)</strong></td></tr></tbody></table></div><ul><li><strong>TP (True Positive)</strong>: 实际为正，预测也为正。</li><li><strong>FN (False Negative)</strong>: 实际为正，但预测为负。</li><li><strong>FP (False Positive)</strong>: 实际为负，但预测为正。</li><li><strong>TN (True Negative)</strong>: 实际为负，预测也为负。</li></ul><hr><h4 id=准确率-accuracy><strong>准确率 (Accuracy)</strong>
<a class=anchor href=#%e5%87%86%e7%a1%ae%e7%8e%87-accuracy>#</a></h4><span>\[
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
\]</span><ul><li><strong>定义</strong>：模型预测正确的样本占总样本的比例。</li><li><strong>优点</strong>：简单直观。</li><li><strong>缺点</strong>：当类别<strong>不平衡</strong>时（正负样本比例悬殊），准确率可能误导。</li></ul><hr><h4 id=精确率-precision><strong>精确率 (Precision)</strong>
<a class=anchor href=#%e7%b2%be%e7%a1%ae%e7%8e%87-precision>#</a></h4><span>\[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
\]</span><ul><li>描述模型预测正类的可靠性。</li><li><strong>适用场景</strong>：当 <strong>FP</strong> 的代价较高时，例如垃圾邮件过滤（<strong>FP</strong> 表示误判正常邮件为垃圾邮件）。</li></ul><hr><h4 id=召回率-recall><strong>召回率 (Recall)</strong>
<a class=anchor href=#%e5%8f%ac%e5%9b%9e%e7%8e%87-recall>#</a></h4><span>\[
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\]</span><ul><li>描述模型对正类样本的捕捉能力。</li><li><strong>适用场景</strong>：当 <strong>FN</strong> 的代价较高时，例如疾病检测（<strong>FN</strong> 表示漏诊病人）。</li></ul><hr><h4 id=f1-score><strong>F1-Score</strong>
<a class=anchor href=#f1-score>#</a></h4><span>\[
\text{F1-Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\]</span><ul><li>F1-Score 是 Precision 和 Recall 的调和平均，用于权衡两者之间的关系。</li><li><strong>适用场景</strong>：当 Precision 和 Recall 同等重要时。</li></ul><hr><h4 id=roc-曲线-和-auc-area-under-the-curve><strong>ROC 曲线 和 AUC (Area Under the Curve)</strong>
<a class=anchor href=#roc-%e6%9b%b2%e7%ba%bf-%e5%92%8c-auc-area-under-the-curve>#</a></h4><div align=center><img src=/images/ROC.png width=300px/></div><ul><li><strong>横轴</strong>：假正率 (<span>
\(FPR = \frac{\text{FP}}{\text{FP} + \text{TN}}\)
</span>)。</li><li><strong>纵轴</strong>：真正率 (<span>
\(TPR = \frac{\text{TP}}{\text{TP} + \text{FN}}\)
</span>)。</li><li>ROC 曲线展示了不同阈值下模型性能的变化。</li></ul><p><strong>阈值（threshold）</strong> 的变化直接影响模型的 TPR（真正例率） 和 FPR（假正例率），从而决定曲线上每个点的位置：</p><ol><li><strong>起点与终点</strong>：<ul><li>当 阈值 = 1.0（极高阈值）：所有样本都被预测为负类，<span>
\(\text{TPR} = 0，\text{FPR} = 0\)
</span>，即曲线起点 <code>(0,0)</code>。</li><li>当 阈值 = 0.0（极低阈值）：所有样本都被预测为正类，<span>
\(\text{TPR} = 1，\text{FPR} = 1\)
</span>，即曲线终点 <code>(1,1)</code>。</li></ul></li><li><strong>中间变化</strong>：<ul><li>随着阈值从高到低移动，曲线从 <code>(0,0)</code> 开始，逐渐向 <code>(1,1)</code> 延展。</li><li>这些点的位置和曲线的形状取决于模型在不同阈值下的 TPR 和 FPR。</li></ul></li><li><strong>关键点</strong>：<ul><li>特定阈值（如 0.5 或其他业务相关的值）对应的 TPR 和 FPR 可通过 ROC 图直接观察，帮助选择最佳阈值。</li></ul></li></ol><hr><h4 id=auc-area-under-the-curve><strong>AUC (Area Under the Curve)</strong>
<a class=anchor href=#auc-area-under-the-curve>#</a></h4><p>ROC 曲线下的面积，取值范围为 <code>[0, 1]</code>。AUC 的意义：</p><ul><li>AUC = 1：完美分类器。</li><li>AUC = 0.5：随机猜测。</li><li>0.5 &lt; AUC &lt; 1：模型有一定的区分能力。</li></ul><h3 id=logistic-regression-代码实现><strong>Logistic Regression 代码实现</strong>
<a class=anchor href=#logistic-regression-%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># &lt;--- From scratch ---&gt;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sigmoid</span>(z):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>exp(<span style=color:#f92672>-</span>z)) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cross_entropy_loss</span>(y, y_pred):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    y: 实际标签 (0 或 1)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    y_pred: 模型预测值 (范围在 0 和 1 之间)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    返回: 平均交叉熵损失
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 防止 log(0) 导致的数值错误，添加一个小的正数 epsilon</span>
</span></span><span style=display:flex><span>    epsilon <span style=color:#f92672>=</span> <span style=color:#ae81ff>1e-15</span>
</span></span><span style=display:flex><span>    y_pred <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>clip(y_pred, epsilon, <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> epsilon)  <span style=color:#75715e># 保证 y_pred 不会等于 0 或 1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span>np<span style=color:#f92672>.</span>mean(y <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>log(y_pred) <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> y) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>log(<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> y_pred))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>Logistic_Regression</span>(X, y, lr, max_iter):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    X: 特征矩阵 (n_samples x n_features)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    y: 标签向量 (n_samples,)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    lr: 学习率
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    max_iter: 最大迭代次数
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    返回: 训练好的权重和偏置
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    n_samples, n_features <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>shape  <span style=color:#75715e># 样本数量和特征数量</span>
</span></span><span style=display:flex><span>    weights <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(n_features)  <span style=color:#75715e># 初始化权重为 0</span>
</span></span><span style=display:flex><span>    bias <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>  <span style=color:#75715e># 初始化偏置为 0</span>
</span></span><span style=display:flex><span>    losses <span style=color:#f92672>=</span> [] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(max_iter):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 计算预测值</span>
</span></span><span style=display:flex><span>        y_pred <span style=color:#f92672>=</span> sigmoid(np<span style=color:#f92672>.</span>dot(X, weights) <span style=color:#f92672>+</span> bias)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 计算梯度</span>
</span></span><span style=display:flex><span>        weight_grad <span style=color:#f92672>=</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> n_samples) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>dot(X<span style=color:#f92672>.</span>T, (y_pred <span style=color:#f92672>-</span> y))  <span style=color:#75715e># 权重的梯度</span>
</span></span><span style=display:flex><span>        bias_grad <span style=color:#f92672>=</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> n_samples) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>sum(y_pred <span style=color:#f92672>-</span> y)  <span style=color:#75715e># 偏置的梯度</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 计算损失并存储</span>
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> cross_entropy_loss(y, y_pred)
</span></span><span style=display:flex><span>        losses<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 使用梯度下降法更新权重和偏置</span>
</span></span><span style=display:flex><span>        weights <span style=color:#f92672>-=</span> lr <span style=color:#f92672>*</span> weight_grad
</span></span><span style=display:flex><span>        bias <span style=color:#f92672>-=</span> lr <span style=color:#f92672>*</span> bias_grad
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> weights, bias
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>Logistic_Regression_Predict</span>(X, weights, bias):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 计算预测概率</span>
</span></span><span style=display:flex><span>    pred_y <span style=color:#f92672>=</span> sigmoid(np<span style=color:#f92672>.</span>dot(X, weights) <span style=color:#f92672>+</span> bias)
</span></span><span style=display:flex><span>    <span style=color:#75715e># 将概率转换为二分类标签</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [<span style=color:#ae81ff>1</span> <span style=color:#66d9ef>if</span> i <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>0.5</span> <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> pred_y]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.datasets <span style=color:#f92672>import</span> make_classification
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 生成模拟分类数据集</span>
</span></span><span style=display:flex><span>X, y <span style=color:#f92672>=</span> make_classification(n_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>, n_features<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>X_train, X_test, y_train, y_test <span style=color:#f92672>=</span> train_test_split(X, y, test_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>  <span style=color:#75715e># 学习率</span>
</span></span><span style=display:flex><span>max_iter <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>  <span style=color:#75715e># 最大迭代次数</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 训练模型</span>
</span></span><span style=display:flex><span>weights, bias <span style=color:#f92672>=</span> Logistic_Regression(X_train, y_train, lr, max_iter)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 使用测试集进行预测</span>
</span></span><span style=display:flex><span>predictions <span style=color:#f92672>=</span> Logistic_Regression_Predict(X_test, weights, bias)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 打印预测结果示例</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Predictions:&#34;</span>, predictions[:<span style=color:#ae81ff>10</span>])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># &lt;--- From scikit-learn ---&gt;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LogisticRegression
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> accuracy_score, confusion_matrix, classification_report
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.datasets <span style=color:#f92672>import</span> make_classification
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X, y <span style=color:#f92672>=</span> make_classification(n_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>, n_features<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 数据划分</span>
</span></span><span style=display:flex><span>X_train, X_test, y_train, y_test <span style=color:#f92672>=</span> train_test_split(X, y, test_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 模型训练</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> LogisticRegression(
</span></span><span style=display:flex><span>    penalty<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;l2&#39;</span>,       <span style=color:#75715e># 正则化类型：&#39;l1&#39;, &#39;l2&#39;, &#39;elasticnet&#39;, 或 &#39;none&#39;（默认 &#39;l2&#39;）</span>
</span></span><span style=display:flex><span>    C<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>,              <span style=color:#75715e># 正则化强度的倒数，值越小正则化越强，默认为 1.0</span>
</span></span><span style=display:flex><span>    solver<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;lbfgs&#39;</span>,     <span style=color:#75715e># 优化算法：如 &#39;lbfgs&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39; 等</span>
</span></span><span style=display:flex><span>    max_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>,       <span style=color:#75715e># 最大迭代次数，防止迭代过多导致训练时间过长</span>
</span></span><span style=display:flex><span>    random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>     <span style=color:#75715e># 随机种子，保证结果可复现</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 预测</span>
</span></span><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(X_test)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 评价指标</span>
</span></span><span style=display:flex><span>accuracy <span style=color:#f92672>=</span> accuracy_score(y_test, y_pred)
</span></span><span style=display:flex><span>conf_matrix <span style=color:#f92672>=</span> confusion_matrix(y_test, y_pred)
</span></span><span style=display:flex><span>report <span style=color:#f92672>=</span> classification_report(y_test, y_pred)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Accuracy: </span><span style=color:#e6db74>{</span>accuracy<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Confusion Matrix:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>conf_matrix<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Classification Report:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>report<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#逻辑回归logistic-regression><strong>逻辑回归（Logistic Regression）</strong></a><ul><li><a href=#sigmoid-函数><strong>Sigmoid 函数</strong></a></li><li><a href=#损失函数loss-function><strong>损失函数（Loss Function）</strong></a></li><li><a href=#梯度下降gradient-descent><strong>梯度下降（Gradient Descent）</strong></a></li><li><a href=#性能评估evaluation-metrics><strong>性能评估（Evaluation Metrics）</strong></a></li><li><a href=#logistic-regression-代码实现><strong>Logistic Regression 代码实现</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>