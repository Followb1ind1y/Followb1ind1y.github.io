<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Machine Learning Basics
  #


  机器学习的定义与类型
  #


  定义
  #

Machine Learning（机器学习） 是一种人工智能技术，核心目标是使计算机能够从数据中自动学习，并根据这些数据改进对特定任务的性能，而无需明确的编程指令。
机器学习通过以下三个要素实现：

数据（Data）：输入的原始数据或特征数据。
模型（Model）：表示学习的假设空间，决定如何从数据中学习模式。
优化目标（Object）：定义如何评估模型好坏并改进其性能（如最小化误差）。



  主要类型
  #





  监督学习 (Supervised Learning)
  #

监督学习 (Supervised Learning) 从标记数据中学习。在这种情况下，输入数据及其对应的输出值被提供给算法，算法学习输入和输出值之间的映射（the mapping between the input and output values）。监督学习的目标是对新的、看不见的输入数据做出准确的预测。


输入（Input）：特征数据  
  \(X\)

  和目标变量  
  \(y\)

 （如标签或真实值）。


输出（Output）：预测模型，用于对新数据进行分类或回归。


应用场景：

分类问题（Classification）：将输入数据划分到预定义类别中。
回归问题（Regression）：预测连续数值的目标变量。





  无监督学习 (Unsupervised Learning)
  #

无监督学习 (Unsupervised Learning) 从未标记的数据中学习。在这种情况下，输入数据没有标记，算法自己学习在数据中寻找模式和结构。无监督学习的目标是发现数据中隐藏的模式和结构。

输入（Input）：仅有特征数据  
  \(X\)

 。
输出（Output）：数据的潜在结构或表示。
应用场景：

聚类 (Clustering)：将数据分组到不同簇中。
降维 (Dimensionality Reduction)：简化数据表示，同时保留主要信息。





  半监督学习 (Semi-Supervised Learning)
  #

半监督学习 (Semi-Supervised Learning) 从标记和未标记数据的组合中进行学习。在这种情况下，算法学习在未标记数据中寻找模式和结构，并使用标记数据来指导学习过程。"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/machine-learning/machine-learning-basics/"><meta property="og:site_name" content="Followblindly"><meta property="og:title" content="Machine Learning Basics"><meta property="og:description" content="Machine Learning Basics # 机器学习的定义与类型 # 定义 # Machine Learning（机器学习） 是一种人工智能技术，核心目标是使计算机能够从数据中自动学习，并根据这些数据改进对特定任务的性能，而无需明确的编程指令。
机器学习通过以下三个要素实现：
数据（Data）：输入的原始数据或特征数据。 模型（Model）：表示学习的假设空间，决定如何从数据中学习模式。 优化目标（Object）：定义如何评估模型好坏并改进其性能（如最小化误差）。 主要类型 # 监督学习 (Supervised Learning) # 监督学习 (Supervised Learning) 从标记数据中学习。在这种情况下，输入数据及其对应的输出值被提供给算法，算法学习输入和输出值之间的映射（the mapping between the input and output values）。监督学习的目标是对新的、看不见的输入数据做出准确的预测。
输入（Input）：特征数据 \(X\) 和目标变量 \(y\) （如标签或真实值）。
输出（Output）：预测模型，用于对新数据进行分类或回归。
应用场景：
分类问题（Classification）：将输入数据划分到预定义类别中。 回归问题（Regression）：预测连续数值的目标变量。 无监督学习 (Unsupervised Learning) # 无监督学习 (Unsupervised Learning) 从未标记的数据中学习。在这种情况下，输入数据没有标记，算法自己学习在数据中寻找模式和结构。无监督学习的目标是发现数据中隐藏的模式和结构。
输入（Input）：仅有特征数据 \(X\) 。 输出（Output）：数据的潜在结构或表示。 应用场景： 聚类 (Clustering)：将数据分组到不同簇中。 降维 (Dimensionality Reduction)：简化数据表示，同时保留主要信息。 半监督学习 (Semi-Supervised Learning) # 半监督学习 (Semi-Supervised Learning) 从标记和未标记数据的组合中进行学习。在这种情况下，算法学习在未标记数据中寻找模式和结构，并使用标记数据来指导学习过程。"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><title>Machine Learning Basics | Followblindly</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=http://localhost:1313/docs/machine-learning/machine-learning-basics/><link rel=stylesheet href=/book.min.bff4c6870ba26abd815329272c8df8231704f9ac54bee84c3ef1f649e394d14f.css integrity="sha256-v/TGhwuiar2BUyknLI34IxcE+axUvuhMPvH2SeOU0U8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.7fcdc0d31daf881e7d05543040042cebd237814b9497cb2d99489429b7d6bc1c.js integrity="sha256-f83A0x2viB59BVQwQAQs69I3gUuUl8stmUiUKbfWvBw=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://localhost:1313/docs/machine-learning/machine-learning-basics/index.xml title=Followblindly></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/As.png alt=Logo class=book-icon><span>Followblindly</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><span>Python Basics</span><ul><li><a href=/docs/python-basics/python-fundamentals/>Python Fundamentals</a><ul></ul></li><li><input type=checkbox id=section-b0810fa42fa69050cb4968ec00fbf282 class=toggle>
<label for=section-b0810fa42fa69050cb4968ec00fbf282 class="flex justify-between"><a href=/docs/python-basics/leetcode/>Leetcode Notes</a></label><ul><li><a href=/docs/python-basics/leetcode/practice-history/>Practice History</a><ul></ul></li></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-7e28d5ac3e9843e0deb580be9504447e class=toggle>
<label for=section-7e28d5ac3e9843e0deb580be9504447e class="flex justify-between"><a role=button>Common Libraries</a></label><ul><li><a href=/docs/common-libraries/numpy/>NumPy</a><ul></ul></li><li><a href=/docs/common-libraries/pandas/>Pandas</a><ul></ul></li><li><a href=/docs/common-libraries/pytorch/>PyTorch</a><ul></ul></li></ul></li><li class=book-section-flat><span>Machine Learning</span><ul><li><a href=/docs/machine-learning/machine-learning-basics/ class=active>Machine Learning Basics</a><ul></ul></li><li><a href=/docs/machine-learning/data-preprocessing/>Data Preprocessing</a><ul></ul></li><li><input type=checkbox id=section-89d4dd5d95507b817cf74368af5982ba class=toggle>
<label for=section-89d4dd5d95507b817cf74368af5982ba class="flex justify-between"><a href=/docs/machine-learning/supervised-learning/>Supervised Learning</a></label><ul><li><a href=/docs/machine-learning/supervised-learning/linear-regression/>Linear Regression</a><ul></ul></li><li><a href=/docs/machine-learning/supervised-learning/logistic-regression/>Logistic Regression</a><ul></ul></li></ul></li><li><input type=checkbox id=section-452d9bf73a55e6b3d947afcc89364ff4 class=toggle>
<label for=section-452d9bf73a55e6b3d947afcc89364ff4 class="flex justify-between"><a href=/docs/machine-learning/unsupervised-learning/>Unsupervised Learning</a></label><ul></ul></li><li><a href=/docs/machine-learning/regularization/>Regularization</a><ul></ul></li><li><a href=/docs/machine-learning/optimization/>Optimization</a><ul></ul></li></ul></li><li class=book-section-flat><span>Deep Learning</span><ul><li><a href=/docs/deep-learning/perceptrons-and-neural-network/>Perceptrons and Neural Network</a><ul></ul></li><li><input type=checkbox id=section-d0dd931d60033c220ecd4cd60b7c9170 class=toggle>
<label for=section-d0dd931d60033c220ecd4cd60b7c9170 class="flex justify-between"><a href=/docs/deep-learning/convolutional-neural-networks/>Convolutional Neural Networks</a></label><ul><li><a href=/docs/deep-learning/convolutional-neural-networks/modern-convolutional-neural-networks/>Modern Convolutional Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-a3019bfa8037cc33ed6405d1589b6219 class=toggle>
<label for=section-a3019bfa8037cc33ed6405d1589b6219 class="flex justify-between"><a href=/docs/deep-learning/recurrent-neural-networks/>Recurrent Neural Networks</a></label><ul><li><a href=/docs/deep-learning/recurrent-neural-networks/modern-recurrent-neural-networks/>Modern Recurrent Neural Networks</a><ul></ul></li></ul></li><li><input type=checkbox id=section-0a43584c16258b228ae9aa8d70efc320 class=toggle>
<label for=section-0a43584c16258b228ae9aa8d70efc320 class="flex justify-between"><a href=/docs/deep-learning/attention-and-transformers/>Attention and Transformers</a></label><ul><li><a href=/docs/deep-learning/attention-and-transformers/large-scale-pretraining-with-transformers/>Large-Scale Pretraining with Transformers</a><ul></ul></li><li><a href=/docs/deep-learning/attention-and-transformers/modern-large-language-models-copy/>Modern Large Language Models</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92e8358c45c96009753cf4227e9daea8 class=toggle>
<label for=section-92e8358c45c96009753cf4227e9daea8 class="flex justify-between"><a href=/docs/deep-learning/llm-pipelines/>LLM Pipelines</a></label><ul><li><a href=/docs/deep-learning/llm-pipelines/llm-inference-and-deployment/>LLM Inference and Deployment</a><ul></ul></li></ul></li><li><input type=checkbox id=section-92c62e5374862501ed6fa038204a433f class=toggle>
<label for=section-92c62e5374862501ed6fa038204a433f class="flex justify-between"><a href=/docs/deep-learning/generative-models/>Generative Models</a></label><ul></ul></li></ul></li><li class=book-section-flat><input type=checkbox id=section-8b0266d7d6ac3da61ec6acf4e97681ca class=toggle>
<label for=section-8b0266d7d6ac3da61ec6acf4e97681ca class="flex justify-between"><a role=button>Others</a></label><ul><li><a href=/docs/others/interview-preparation-guide/>Interview Preparation Guide</a><ul></ul></li></ul></li></ul><ul><li><a href=/posts/>Blog</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Machine Learning Basics</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><link rel=stylesheet href=/css/prism-one-dark.css><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#机器学习的定义与类型><strong>机器学习的定义与类型</strong></a><ul><li><a href=#定义><strong>定义</strong></a></li><li><a href=#主要类型><strong>主要类型</strong></a></li></ul></li><li><a href=#显式explicit-和-隐式implicit><strong>显式（Explicit） 和 隐式（Implicit）</strong></a></li><li><a href=#泛化能力generalization><strong>泛化能力（Generalization）</strong></a><ul><li><a href=#数据集datasets分类和误差errors><strong>数据集（Datasets）分类和误差（Errors）</strong></a></li><li><a href=#欠拟合-underfitting><strong>欠拟合 (Underfitting)</strong></a></li><li><a href=#过拟合-overfitting><strong>过拟合 (Overfitting)</strong></a></li><li><a href=#偏差-方差权衡bias-variance-tradeoﬀ><strong>偏差-方差权衡（Bias-Variance Tradeoﬀ）</strong></a></li></ul></li><li><a href=#交叉验证cross-validation><strong>交叉验证（Cross Validation）</strong></a><ul><li><a href=#k折交叉验证-k-fold-cross-validation><strong>K折交叉验证 (K-Fold Cross-Validation)</strong></a></li><li><a href=#留一法-leave-one-out-cross-validation-loocv><strong>留一法 (Leave-One-Out Cross-Validation, LOOCV)</strong></a></li></ul></li><li><a href=#常见的机器学习完整流程><strong>常见的机器学习完整流程</strong></a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=machine-learning-basics><strong>Machine Learning Basics</strong>
<a class=anchor href=#machine-learning-basics>#</a></h1><h2 id=机器学习的定义与类型><strong>机器学习的定义与类型</strong>
<a class=anchor href=#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%ae%9a%e4%b9%89%e4%b8%8e%e7%b1%bb%e5%9e%8b>#</a></h2><h3 id=定义><strong>定义</strong>
<a class=anchor href=#%e5%ae%9a%e4%b9%89>#</a></h3><p><strong>Machine Learning（机器学习）</strong> 是一种人工智能技术，核心目标是使计算机能够从数据中自动学习，并根据这些数据改进对特定任务的性能，而无需明确的编程指令。</p><p>机器学习通过以下三个要素实现：</p><ol><li><strong>数据（Data）</strong>：输入的原始数据或特征数据。</li><li><strong>模型（Model）</strong>：表示学习的假设空间，决定如何从数据中学习模式。</li><li><strong>优化目标（Object）</strong>：定义如何评估模型好坏并改进其性能（如最小化误差）。</li></ol><hr><h3 id=主要类型><strong>主要类型</strong>
<a class=anchor href=#%e4%b8%bb%e8%a6%81%e7%b1%bb%e5%9e%8b>#</a></h3><div align=center><img src=/images/The-main-types-of-machine-learning-Main-approaches-include-classification-and-regression.tif.png width=600px/></div><h4 id=监督学习-supervised-learning><strong>监督学习 (Supervised Learning)</strong>
<a class=anchor href=#%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0-supervised-learning>#</a></h4><p>监督学习 (Supervised Learning) 从<strong>标记数据</strong>中学习。在这种情况下，输入数据及<strong>其对应的输出值</strong>被提供给算法，算法<strong>学习输入和输出值之间的映射（the mapping between the input and output values）</strong>。监督学习的目标是对新的、看不见的输入数据做出准确的预测。</p><ul><li><p><strong>输入（Input）</strong>：特征数据 <span>\(X\)
</span>和目标变量 <span>\(y\)
</span>（如标签或真实值）。</p></li><li><p><strong>输出（Output）</strong>：预测模型，用于对新数据进行分类或回归。</p></li><li><p><strong>应用场景</strong>：</p><ol><li><strong>分类问题（Classification）</strong>：将输入数据划分到预定义类别中。</li><li><strong>回归问题（Regression）</strong>：预测连续数值的目标变量。</li></ol></li></ul><hr><h4 id=无监督学习-unsupervised-learning><strong>无监督学习 (Unsupervised Learning)</strong>
<a class=anchor href=#%e6%97%a0%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0-unsupervised-learning>#</a></h4><p>无监督学习 (Unsupervised Learning) 从<strong>未标记的数据</strong>中学习。在这种情况下，输入数据没有标记，算法自己学习在数据中<strong>寻找模式和结构</strong>。无监督学习的目标是<strong>发现数据中隐藏的模式和结构</strong>。</p><ul><li><strong>输入（Input）</strong>：仅有特征数据 <span>\(X\)
</span>。</li><li><strong>输出（Output）</strong>：数据的潜在结构或表示。</li><li><strong>应用场景</strong>：<ol><li><strong>聚类 (Clustering)</strong>：将数据分组到不同簇中。</li><li><strong>降维 (Dimensionality Reduction)</strong>：简化数据表示，同时保留主要信息。</li></ol></li></ul><hr><h4 id=半监督学习-semi-supervised-learning><strong>半监督学习 (Semi-Supervised Learning)</strong>
<a class=anchor href=#%e5%8d%8a%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0-semi-supervised-learning>#</a></h4><p>半监督学习 (Semi-Supervised Learning) 从<strong>标记和未标记数据的组合中</strong>进行学习。在这种情况下，算法学习在未标记数据中寻找模式和结构，并使用标记数据来指导学习过程。</p><ul><li><strong>输入（Input）</strong>：部分标注的数据和<strong>大量未标注的数据</strong>。</li><li><strong>输出（Output）</strong>：用于分类或回归的预测模型。</li><li><strong>应用场景</strong>：<ol><li>在标注数据有限或获取标签成本高昂的情况下（如医学影像标注）。</li></ol></li></ul><hr><h4 id=强化学习-reinforcement-learning><strong>强化学习 (Reinforcement Learning)</strong>
<a class=anchor href=#%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0-reinforcement-learning>#</a></h4><p>强化学习 (Reinforcement Learning) 通过与 <strong>环境（environment）</strong> 交互进行学习。在这种情况下，算法学习采取 <strong>行动（action）</strong> 来最大化环境提供的奖励信号。强化学习的目标是学习最大化长期 <strong>奖励（reward）</strong> 的 <strong>策略（policy）</strong>。</p><ul><li><strong>输入（Input）</strong>：状态 <span>\(S\)
</span>、动作 <span>\(A\)
</span>、奖励 <span>\(R\)
</span>。</li><li><strong>输出（Output）</strong>：一个策略 <span>\(\pi\)
</span>，指引在不同状态下的最佳行动。</li><li><strong>应用场景</strong>：<ol><li>游戏 AI：如 AlphaGo 使用强化学习在围棋中击败人类选手。</li><li>机器人导航：训练机器人在环境中找到最佳路径。</li></ol></li></ul><hr><h2 id=显式explicit-和-隐式implicit><strong>显式（Explicit） 和 隐式（Implicit）</strong>
<a class=anchor href=#%e6%98%be%e5%bc%8fexplicit-%e5%92%8c-%e9%9a%90%e5%bc%8fimplicit>#</a></h2><p>在机器学习中，显式（Explicit） 和 隐式（Implicit） 通常用于描述模型、方法或表示的不同特性。它们反映了信息是直接表达出来还是通过间接方式体现。</p><ul><li><p><strong>显式（Explicit）</strong> 是指信息或结构是 直接表示 的，通常可以被明确地解释或观察到。</p><ul><li><strong>直接性</strong>：显式方法通常有<strong>清晰的数学公式或逻辑规则</strong>。</li><li><strong>可解释性</strong>：显式模型的内部机制容易被理解。</li><li><strong>可见性</strong>：输入到输出之间的关系是明确可见的。</li><li><strong>示例</strong>：线性回归，逻辑回归，规则模型（决策树）。</li></ul></li><li><p><strong>隐式（Implicit）</strong> 是指信息或结构是通过 间接方式表示 的，不直接显现。</p><ul><li><strong>间接性</strong>：隐式方法通常<strong>不提供明确的公式</strong>，而是通过<strong>训练或优化过程间接捕获关系</strong>。</li><li><strong>不可解释性</strong>：隐式模型的内部机制较难理解，通常被认为是“黑箱”。</li><li><strong>抽象性</strong>：信息的表示可能分布在多个特征或参数中，而不是单一表达。</li><li><strong>示例</strong>：神经网络，SVM，隐变量模型（Latent Variable Models）。</li></ul></li></ul><hr><h2 id=泛化能力generalization><strong>泛化能力（Generalization）</strong>
<a class=anchor href=#%e6%b3%9b%e5%8c%96%e8%83%bd%e5%8a%9bgeneralization>#</a></h2><p>机器学习的核心挑战在于，我们必须在新的、以前未见过的输入上表现良好——而不仅仅是我们的模型所训练的输入。在以前未观察到的输入上表现良好的能力称为<strong>泛化（generalization）</strong>。在训练过程中，我们通过降低 <strong>训练误差 (Training Error)</strong> 优化模型。但模型不仅需要在训练集上表现优异，还需要降低泛化误差 (Generalization Error)，即 <strong>测试误差（Test Error）</strong>。</p><hr><h3 id=数据集datasets分类和误差errors><strong>数据集（Datasets）分类和误差（Errors）</strong>
<a class=anchor href=#%e6%95%b0%e6%8d%ae%e9%9b%86datasets%e5%88%86%e7%b1%bb%e5%92%8c%e8%af%af%e5%b7%aeerrors>#</a></h3><h4 id=训练集train-set><strong>训练集（Train Set）</strong>
<a class=anchor href=#%e8%ae%ad%e7%bb%83%e9%9b%86train-set>#</a></h4><ul><li><strong>定义</strong>：训练集是模型学习的主要数据来源（占大多数），包括 <strong>输入特征</strong> 和对应的 <strong>目标输出</strong>（对于监督学习）。</li><li><strong>功能</strong>：用于训练模型（的<strong>参数</strong>），通过优化算法最小化训练误差。</li></ul><p><strong>训练误差（Training Error）</strong>：训练误差是模型在训练数据上的错误率。它是通过测量每个训练示例的预测输出（predicted output）与实际输出（actual output）之间的差异来计算的。由于模型是在此数据上训练的，因此预计它会在此数据上表现良好，并且训练误差通常较低。</p><hr><h4 id=验证集validation-set><strong>验证集（Validation Set）</strong>
<a class=anchor href=#%e9%aa%8c%e8%af%81%e9%9b%86validation-set>#</a></h4><ul><li><strong>定义</strong>：验证集是从<strong>训练数据中分离出来的一部分</strong>，用于评估模型在未见数据上的表现。</li><li><strong>功能</strong>：<ol><li>帮助调整超参数（如学习率、正则化系数、模型结构等）。</li><li>用于选择最佳模型，例如在多次训练后选择验证误差最低的模型。</li></ol></li></ul><p><strong>验证误差（Validation Error）</strong>：验证误差是模型在验证数据上的错误率。用于评估训练期间模型的性能，目标是找到验证误差最低的模型。</p><hr><h4 id=测试集test-set><strong>测试集（Test Set）</strong>
<a class=anchor href=#%e6%b5%8b%e8%af%95%e9%9b%86test-set>#</a></h4><ul><li><strong>定义</strong>：测试集是<strong>完全独立</strong>于训练和验证的数据，模型在训练和验证过程中从未接触过。</li><li><strong>功能</strong>：用于评估模型的<strong>最终泛化性能</strong>，反映模型在实际场景中的表现。</li></ul><p><strong>测试误差（Test Error）</strong>：测试误差是模型在测试数据上的错误率。测试数据是与训练和验证数据完全独立的数据集，用于评估模型的最终性能。测试误差是 <strong>最重要的误差指标</strong>，因为它告诉我们模型在新的、未见过的数据上的表现如何。</p><hr><h3 id=欠拟合-underfitting><strong>欠拟合 (Underfitting)</strong>
<a class=anchor href=#%e6%ac%a0%e6%8b%9f%e5%90%88-underfitting>#</a></h3><ul><li><strong>定义</strong>：当<strong>模型过于简单而无法捕捉数据中的底层模式</strong>时，就会发生欠拟合。该模型具有高偏差和低方差，这意味着它在训练和测试数据上的表现都很差。</li><li><strong>表现</strong>：<ol><li>模型复杂度低，无法很好地拟合训练数据。</li><li><strong>训练误差与测试误差都较大</strong>，模型性能较差。</li></ol></li><li><strong>原因</strong>：<ol><li>模型过于简单，无法学习到数据中的复杂关系或特征。</li><li>特征不足，数据无法充分表达问题。</li><li>训练时间不足，模型未完全收敛。</li></ol></li><li><strong>解决方法</strong>：<ol><li>增加模型复杂度：选择更复杂的模型（如从线性模型切换到非线性模型）。</li><li>增加特征：引入更多特征或通过特征工程提取更有效的特征。</li><li>延长训练时间：确保模型充分训练直到收敛。</li></ol></li></ul><div align=center><img src=/images/1*_7OPgojau8hkiPUiHoGK_w.png width=650px/></div><hr><h3 id=过拟合-overfitting><strong>过拟合 (Overfitting)</strong>
<a class=anchor href=#%e8%bf%87%e6%8b%9f%e5%90%88-overfitting>#</a></h3><ul><li><strong>定义</strong>：当模型过于复杂，<strong>与训练数据的拟合度过高</strong>时，就会发生过度拟合，从而捕获数据中的噪声和随机波动。因此，该模型<strong>在新的、未见过的数据上表现不佳</strong>。</li><li><strong>表现</strong>：<ol><li>模型对训练数据拟合良好，训练误差很低。</li><li>测试误差较高，模型泛化能力差。</li></ol></li><li><strong>原因</strong>：<ol><li>模型复杂度过高，学习到了训练数据中的噪声或无意义模式。</li><li>训练数据过少，噪声占比高。</li><li>缺乏正则化约束，模型自由度太高。</li></ol></li><li><strong>解决方法</strong>：<ol><li>减少模型复杂度：降低模型自由度（如减少神经网络层数或节点数）。</li><li>增加数据量：收集更多样本，减少模型对噪声的敏感性。</li><li>正则化：</li></ol><ul><li><span>\(L_1\)
</span>正则化：鼓励稀疏性，减少不重要的参数。</li><li><span>\(L_2\)
</span>正则化：限制参数的幅度，防止过大权重。</li></ul><ol start=4><li>交叉验证：通过交叉验证选择模型或超参数，避免过拟合。</li></ol></li></ul><hr><h3 id=偏差-方差权衡bias-variance-tradeoﬀ><strong>偏差-方差权衡（Bias-Variance Tradeoﬀ）</strong>
<a class=anchor href=#%e5%81%8f%e5%b7%ae-%e6%96%b9%e5%b7%ae%e6%9d%83%e8%a1%a1bias-variance-tradeo%ef%ac%80>#</a></h3><h4 id=偏差-bias>偏差 (Bias)
<a class=anchor href=#%e5%81%8f%e5%b7%ae-bias>#</a></h4><ul><li>定义：偏差衡量模型预测值的期望值与真实值之间的偏离程度。</li><li>公式：
<span>\[
\text{Bias} = E[f(x)] - f^*(x)
\]</span><ul><li><span>\(f(x)\)
</span>：模型的预测值</li><li><span>\(f^*(x)\)
</span>：真实值或目标函数</li></ul></li></ul><hr><h4 id=方差-variance>方差 (Variance)
<a class=anchor href=#%e6%96%b9%e5%b7%ae-variance>#</a></h4><ul><li>定义：方差衡量模型在不同训练数据集上的预测值的变化幅度。</li><li>公式：
<span>\[
\text{Variance} = E[(f(x) - E[f(x)])^2]
\]</span></li><li>方差的平方根称为标准差，表示为 <span>\(SE(x)\)</span></li></ul><hr><h4 id=总误差分解><strong>总误差分解</strong>
<a class=anchor href=#%e6%80%bb%e8%af%af%e5%b7%ae%e5%88%86%e8%a7%a3>#</a></h4><p>模型的总误差（Expected Error）可以分解为三部分：偏差、方差和噪声。在实际应用中，<strong>我们需要平衡 Bias 和 Variance 以此来找到最小的 Expected Error</strong>，目标是找到偏差和方差的最佳平衡点，既能保证低训练误差，又能有良好的泛化能力。</p><span>\[
\text{Total Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Noise}
\]</span><ul><li><span>\(\text{Bias}^2\)
</span>: 表示系统误差，与模型的表达能力有关。</li><li><span>\(\text{Variance}\)
</span>: 表示模型对训练数据的敏感程度。</li><li><span>\(\text{Irreducible Noise}\)
</span>: 数据中固有的随机噪声，无法通过任何模型降低。</li></ul><hr><h4 id=bias-variance-tradeoff-理解>Bias-Variance Tradeoff 理解
<a class=anchor href=#bias-variance-tradeoff-%e7%90%86%e8%a7%a3>#</a></h4><p>偏差-方差权衡指的是模型很好地<strong>拟合训练数据的能力（低偏差）</strong> 与其<strong>推广到新数据的能力（低方差）</strong> 之间的权衡。</p><div align=center><img src=/images/bias-variance-tradeoff.png width=600px/></div><p>随着模型<strong>复杂度的增加，偏差趋于减小，方差趋于增大</strong>。如果模型太简单，它可能具有高偏差，这意味着它无法捕捉数据中的潜在模式，训练误差和测试误差会很高。<strong>如果模型太复杂，它可能具有高方差，这意味着它对训练数据中的噪声过于敏感</strong>，并且可能无法很好地推广到新数据，从而导致过度拟合。为了在偏差和方差之间取得平衡，我们需要找到模型的最佳复杂度。</p><ol><li><strong>低Bias但高Variance</strong>：复杂模型，过度拟合，表现为<strong>训练误差低</strong>但<strong>测试误差高</strong>。</li><li><strong>高Bias但低Variance</strong>：简单模型，欠拟合，表现为<strong>训练误差和测试误差都高</strong>。</li></ol><hr><h5 id=选择模型评估方法><strong>选择模型评估方法</strong>
<a class=anchor href=#%e9%80%89%e6%8b%a9%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0%e6%96%b9%e6%b3%95>#</a></h5><ul><li><strong>交叉验证 (Cross-Validation)</strong>：通过分割数据集来更好地估计模型的偏差和方差。</li><li>学习曲线 (Learning Curve)：观察训练集误差和验证集误差随样本数量或模型复杂度变化的趋势，帮助分析模型的偏差和方差问题。</li></ul><hr><h2 id=交叉验证cross-validation><strong>交叉验证（Cross Validation）</strong>
<a class=anchor href=#%e4%ba%a4%e5%8f%89%e9%aa%8c%e8%af%81cross-validation>#</a></h2><p>交叉验证是机器学习中用于评估模型在独立数据集上性能的一种技术。交叉验证的基本思想是将可用数据分成两个或多个部分，其中一个部分用于训练模型，另一个部分用于验证模型。交叉验证用于通过提供模型对新数据的泛化程度的估计来<strong>防止过度拟合</strong>。它有效解决了仅用单一验证集或测试集可能导致的评估结果不稳定或偏差的问题。它也可以用于<strong>调整模型的超参数</strong>。</p><blockquote class="book-hint warning"><p><strong>Note</strong>：在交叉验证（Cross-Validation）中，<strong>每一次计算的是 验证误差（validation error）</strong>，而不是训练误差（training error）。交叉验证的目的是估计模型在未见数据上的性能，<strong>因此重点在于验证集的误差</strong>。</p></blockquote><div align=center><img src=/images/grid_search_cross_validation.png width=450px/></div><hr><h3 id=k折交叉验证-k-fold-cross-validation><strong>K折交叉验证 (K-Fold Cross-Validation)</strong>
<a class=anchor href=#k%e6%8a%98%e4%ba%a4%e5%8f%89%e9%aa%8c%e8%af%81-k-fold-cross-validation>#</a></h3><ol><li>将数据集划分为 <strong>K 个不重叠的子集</strong>（folds）。每次取一个子集作为验证集，其余 K-1 个子集作为训练集。重复 K 次，每次更换验证集，最终对<strong>所有验证结果（Validation Error）取平均</strong>。</li><li>在 <code>Cross-Validation</code> 的框架内，调节模型的 <code>hyperparameters</code>，对使用不同参数的模型进行 <code>Cross-Validation</code> 验证。根据最终结果选择最佳模型。</li><li>在选择好最佳的 <code>hyperparameters</code> 和模型后，用全部的数据进行训练。</li><li>使用完全独立的 <code>Testset</code> 来评估模型的泛化能力。</li></ol><h4 id=优缺点>优缺点：
<a class=anchor href=#%e4%bc%98%e7%bc%ba%e7%82%b9>#</a></h4><ul><li>优点：可靠性高，适合数据量较大的情况。</li><li>缺点：当 K 较大时，计算成本较高。</li></ul><h4 id=k-fold-cross-validation-代码实现><strong>K-Fold Cross-Validation 代码实现</strong>：
<a class=anchor href=#k-fold-cross-validation-%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># &lt;--- From scratch ---&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LinearRegression
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> mean_squared_error
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 简单的数据集生成</span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[i] <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>100</span>)])
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([i<span style=color:#f92672>*</span><span style=color:#ae81ff>2</span> <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>100</span>)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> LinearRegression()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>K_fold_CrossValidation</span>(X, y, model, k<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 数据集划分，按顺序划分为K个子集</span>
</span></span><span style=display:flex><span>    fold <span style=color:#f92672>=</span> len(X) <span style=color:#f92672>//</span> k
</span></span><span style=display:flex><span>    error <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(k):
</span></span><span style=display:flex><span>        val_start_idx <span style=color:#f92672>=</span> i <span style=color:#f92672>*</span> fold
</span></span><span style=display:flex><span>        val_end_idx <span style=color:#f92672>=</span> (i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> fold
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 创建训练集和验证集</span>
</span></span><span style=display:flex><span>        X_train <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate(X[:val_start_idx], X[val_end_idx:]),
</span></span><span style=display:flex><span>        y_train <span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>concatenate(y[:val_start_idx], y[val_end_idx:])
</span></span><span style=display:flex><span>        X_val <span style=color:#f92672>=</span> X[val_start_idx:val_end_idx]
</span></span><span style=display:flex><span>        y_val <span style=color:#f92672>=</span> y[val_start_idx:val_end_idx]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 使用模型进行训练和预测</span>
</span></span><span style=display:flex><span>        model<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>        y_pred <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(X_val)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 计算误差</span>
</span></span><span style=display:flex><span>        curr_error <span style=color:#f92672>=</span> mean_squared_error(y_val, y_pred)
</span></span><span style=display:flex><span>        error<span style=color:#f92672>.</span>append(curr_error)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>mean(error)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 执行交叉验证</span>
</span></span><span style=display:flex><span>avg_error <span style=color:#f92672>=</span> K_fold_CrossValidation(X, y, model, k<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Average cross-validation error: </span><span style=color:#e6db74>{</span>avg_error<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># &lt;--- From scikit-learn ---&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> cross_val_score
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LinearRegression
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.datasets <span style=color:#f92672>import</span> make_regression
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 生成数据集</span>
</span></span><span style=display:flex><span>X, y <span style=color:#f92672>=</span> make_regression(n_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>, n_features<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, noise<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 初始化模型</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> LinearRegression()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 执行5折交叉验证，评分标准为负均方误差（neg_mean_squared_error）</span>
</span></span><span style=display:flex><span>scores <span style=color:#f92672>=</span> cross_val_score(
</span></span><span style=display:flex><span>    estimator<span style=color:#f92672>=</span>model,        <span style=color:#75715e># estimator 类型对象，必须实现 fit 和 predict 方法。</span>
</span></span><span style=display:flex><span>    X<span style=color:#f92672>=</span>X,                    <span style=color:#75715e># 特征数据</span>
</span></span><span style=display:flex><span>    y<span style=color:#f92672>=</span>y,                    <span style=color:#75715e># 标签数据</span>
</span></span><span style=display:flex><span>    cv<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>,                   <span style=color:#75715e># 指定使用5折交叉验证</span>
</span></span><span style=display:flex><span>    scoring<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;neg_mean_squared_error&#39;</span>,  
</span></span><span style=display:flex><span>    <span style=color:#75715e># 表示使用的评分方法（如 accuracy, neg_mean_squared_error, f1, roc_auc 等）</span>
</span></span><span style=display:flex><span>    return_train_score<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span> <span style=color:#75715e># 不返回训练集得分，只返回验证集得分</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 输出每次折的误差</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Cross-validation errors for each fold: </span><span style=color:#e6db74>{</span><span style=color:#f92672>-</span>scores<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 输出平均误差</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Average cross-validation error: </span><span style=color:#e6db74>{</span><span style=color:#f92672>-</span>scores<span style=color:#f92672>.</span>mean()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><hr><h3 id=留一法-leave-one-out-cross-validation-loocv><strong>留一法 (Leave-One-Out Cross-Validation, LOOCV)</strong>
<a class=anchor href=#%e7%95%99%e4%b8%80%e6%b3%95-leave-one-out-cross-validation-loocv>#</a></h3><ul><li>数据集中<strong>每个样本单独作为一次验证集</strong>，其余样本作为训练集。</li><li>模型训练次数等于样本数 N，最后计算所有验证集的误差平均值。</li></ul><h4 id=优缺点-1>优缺点：
<a class=anchor href=#%e4%bc%98%e7%bc%ba%e7%82%b9-1>#</a></h4><ul><li>优点：不浪费数据，最全面的评估方法。</li><li>缺点：计算代价极高，尤其是数据集较大时。</li></ul><hr><h2 id=常见的机器学习完整流程><strong>常见的机器学习完整流程</strong>
<a class=anchor href=#%e5%b8%b8%e8%a7%81%e7%9a%84%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%ae%8c%e6%95%b4%e6%b5%81%e7%a8%8b>#</a></h2><ol><li><strong>数据的准备和预处理</strong><ul><li>从数据库、API、文件等来源<strong>收集数据</strong>。</li><li>对所有的数据进行<strong>数据清洗</strong> (e.g 处理缺失值，处理异常值，数据格式转换)</li><li><strong>数据分割</strong>（训练集80%、验证集10%、测试集10%）</li><li><strong>数据预处理</strong> - 仅针对训练集（标准化/归一化，特征工程/选择/变换，Label Encoding，One-Hot Encoding）</li></ul></li><li><strong>模型训练与评估</strong><ul><li><strong>选择模型</strong>：根据任务性质选择初始模型</li><li>设置交叉验证策略，<strong>交叉验证中的模型训练</strong> - 使用训练集。</li><li><strong>评估指标</strong>：不仅<strong>观察平均值（Validation Error）</strong>，还需关注标准差，评估模型的稳定性。</li></ul></li><li><strong>超参数调优</strong><ul><li><strong>网格搜索 (Grid Search)</strong>：枚举所有可能的超参数组合，使用交叉验证评估每一组参数的表现。选择评估结果最优的参数组合。</li><li>随机搜索 (Random Search)：从参数空间中随机采样一定数量的超参数组合进行评估。</li></ul></li><li><strong>测试集上的最终评估</strong><ul><li><strong>固定最佳模型</strong>：在交叉验证确定的最佳超参数和模型结构上，重新训练模型，<strong>使用全体训练数据</strong>。</li><li><strong>在测试集上评估</strong>：用测试集数据评估最终模型的性能，作为模型实际泛化能力的最终指标。Test Set 仅在最终测试时使用一次以防止数据泄漏（Data Leakage）</li></ul></li></ol></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#机器学习的定义与类型><strong>机器学习的定义与类型</strong></a><ul><li><a href=#定义><strong>定义</strong></a></li><li><a href=#主要类型><strong>主要类型</strong></a></li></ul></li><li><a href=#显式explicit-和-隐式implicit><strong>显式（Explicit） 和 隐式（Implicit）</strong></a></li><li><a href=#泛化能力generalization><strong>泛化能力（Generalization）</strong></a><ul><li><a href=#数据集datasets分类和误差errors><strong>数据集（Datasets）分类和误差（Errors）</strong></a></li><li><a href=#欠拟合-underfitting><strong>欠拟合 (Underfitting)</strong></a></li><li><a href=#过拟合-overfitting><strong>过拟合 (Overfitting)</strong></a></li><li><a href=#偏差-方差权衡bias-variance-tradeoﬀ><strong>偏差-方差权衡（Bias-Variance Tradeoﬀ）</strong></a></li></ul></li><li><a href=#交叉验证cross-validation><strong>交叉验证（Cross Validation）</strong></a><ul><li><a href=#k折交叉验证-k-fold-cross-validation><strong>K折交叉验证 (K-Fold Cross-Validation)</strong></a></li><li><a href=#留一法-leave-one-out-cross-validation-loocv><strong>留一法 (Leave-One-Out Cross-Validation, LOOCV)</strong></a></li></ul></li><li><a href=#常见的机器学习完整流程><strong>常见的机器学习完整流程</strong></a></li></ul></nav></div></aside></main></body></html>