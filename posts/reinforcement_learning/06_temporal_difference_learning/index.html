<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Temporal-Difference Learning | Followb1ind1y</title>
<meta name="keywords" content="TD Learning" />
<meta name="description" content="If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap).">
<meta name="author" content="Followb1ind1y">
<link rel="canonical" href="https://followb1ind1y.github.io/posts/reinforcement_learning/06_temporal_difference_learning/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.d70d9be7236724828e9dc04b26c5a1f3b228217674a56d00269f53e0d3db96f0.css" integrity="sha256-1w2b5yNnJIKOncBLJsWh87IoIXZ0pW0AJp9T4NPblvA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://followb1ind1y.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://followb1ind1y.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://followb1ind1y.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://followb1ind1y.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://followb1ind1y.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.83.1" />
<meta property="og:title" content="Temporal-Difference Learning" />
<meta property="og:description" content="If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://followb1ind1y.github.io/posts/reinforcement_learning/06_temporal_difference_learning/" /><meta property="og:image" content="https://followb1ind1y.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-11T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2022-06-11T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://followb1ind1y.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Temporal-Difference Learning"/>
<meta name="twitter:description" content="If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap)."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://followb1ind1y.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Temporal-Difference Learning",
      "item": "https://followb1ind1y.github.io/posts/reinforcement_learning/06_temporal_difference_learning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Temporal-Difference Learning",
  "name": "Temporal-Difference Learning",
  "description": "If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap).",
  "keywords": [
    "TD Learning"
  ],
  "articleBody": "If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap). The relationship between TD, DP, and Monte Carlo methods is a recurring theme in the theory of reinforcement learning\nTD Prediction Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy $\\pi$, both methods update their estimate $V$ of $v_{\\pi}$ for the nonterminal states $S_{t}$ occurring in that experience. Roughly speaking, Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_{t})$. A simple every-visit Monte Carlo method suitable for non-stationary environments is\n$$ V(S_{t})\\leftarrow V(S_{t})+\\alpha[G_{t}-V(S_{t})] \\\\ $$\nwhere $G_{t}$ is the actual return following time $t$, and $\\alpha$ is a constant step-size parameter. Let us call this method constant-$\\alpha$ MC. Whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_{t})$ (only then is $G_{t}$ known), TD methods need to wait only until the next time step. At time $t+1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$. The simplest TD method makes the update\n$$ V(S_{t})\\leftarrow V(S_{t})+\\alpha[R_{t+1}+\\gamma V(S_{t+1})-V(S_{t})] \\\\ $$\nimmediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. In effect, the target for the Monte Carlo update is $G_{t}$, whereas the target for the TD update is $R_{t+1}+\\gamma V(S_{t+1})$. This TD method is called TD(0), or one-step TD. The box below specifies TD(0) completely in procedural form.\n Because TD(0) bases its update in part on an existing estimate, we say that it is a bootstrapping method. Note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of $S_{t}$ and the better estimate $R_{t+1}+\\gamma V(S_{t+1})$. This quantity, called the TD error, arises in various forms throughout reinforcement learning:\n$$ \\delta_{t} \\doteq R_{t+1}+\\gamma V(S_{t+1}) -V(S_{t}) \\\\ $$\nNotice that the TD error at each time is the error in the estimate made at that time. Because the TD error depends on the next state and next reward, it is not actually available until one time step later. That is, $\\delta_{t}$ is the error in $V(S_{t})$, available at time $t+1$. Also note that if the array $V$ does not change during the episode (as it does not in Monte Carlo methods), then the Monte Carlo error can be written as a sum of TD errors:\n$$ \\begin{align} G_{t}-V(S_{t}) \u0026= R_{t+1}+\\gamma V(S_{t+1}) -V(S_{t}) + \\gamma V(S_{t+1}-\\gamma V(S_{t+1}) \\\\ \u0026=\\delta_{t}+\\gamma(G_{t+1}-V(S_{t+1})) \\\\ \u0026=\\delta_{t}+\\gamma\\delta_{t+1}+\\gamma^{2}(G_{t+2}-V(S_{t+2})) \\\\ \u0026= \\delta_{t}+\\gamma\\delta_{t+1}+\\gamma^{2}\\delta_{t+2}+\\cdots+\\gamma^{T-t-1}\\delta_{T-1}+\\gamma^{T-t}(G_{T}-V(S_{T})) \\\\ \u0026= \\delta_{t}+\\gamma\\delta_{t+1}+\\gamma^{2}\\delta_{t+2}+\\cdots+\\gamma^{T-t-1}\\delta_{T-1}+\\gamma^{T-t}(0-0) \\\\ \u0026= \\sum^{T-1}_{k=t}\\gamma^{k-t}\\delta_{k} \\end{align} $$\nThis identity is not exact if $V$ is updated during the episode (as it is in TD(0)), but if the step size is small then it may still hold approximately. Generalizations of this identity play an important role in the theory and algorithms of temporal-difference learning.\nAdvantages of TD Prediction Methods TD methods update their estimates based in part on other estimates. They learn a guess from a guess—they bootstrap. Is this a good thing to do? Obviously, TD methods have an advantage over DP methods in that they do not require a model of the environment, of its reward and next-state probability distributions.\nThe next most obvious advantage of TD methods over Monte Carlo methods is that they are naturally implemented in an online, fully incremental fashion. With Monte Carlo methods one must wait until the end of an episode, because only then is the return known, whereas with TD methods one need wait only one time step. Surprisingly often this turns out to be a critical consideration. Some applications have very long episodes, so that delaying all learning until the end of the episode is too slow. Other applications are continuing tasks and have no episodes at all. Finally, as we noted in the previous chapter, some Monte Carlo methods must ignore or discount episodes on which experimental actions are taken, which can greatly slow learning. TD methods are much less susceptible to these problems because they learn from each transition regardless of what subsequent actions are taken.\nIf both TD and Monte Carlo methods converge asymptotically to the correct predictions, then a natural next question is “Which gets there first?” In other words, which method learns faster? Which makes the more efficient use of limited data? At the current time this is an open question in the sense that no one has been able to prove mathematically that one method converges faster than the other. In fact, it is not even clear what is the most appropriate formal way to phrase this question! In practice, however, TD methods have usually been found to converge faster than constant-$\\alpha$ MC methods on stochastic tasks.\nSarsa: On-policy TD Control We turn now to the use of TD prediction methods for the control problem. As usual, we follow the pattern of generalized policy iteration (GPI), only this time using TD methods for the evaluation or prediction part. As with Monte Carlo methods, we face the need to trade off exploration and exploitation, and again approaches fall into two main classes: on-policy and off-policy.\nThe first step is to learn an action-value function rather than a state-value function. In particular, for an on-policy method we must estimate $q_{\\pi}(s,a)$ for the current behavior policy $\\pi$ and for all states $s$ and actions $a$. This can be done using essentially the same TD method described above for learning $v_{\\pi}$. Recall that an episode consists of an alternating sequence of states and state–action pairs:\n In the previous section we considered transitions from state to state and learned the values of states. Now we consider transitions from state–action pair to state–action pair, and learn the values of state–action pairs. Formally these cases are identical: they are both Markov chains with a reward process. The theorems assuring the convergence of state values under TD(0) also apply to the corresponding algorithm for action values:\n$$ Q(S_{t},A_{t})\\leftarrow Q(S_{t},A_{t})+\\alpha[R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})-Q(S_{t},A_{t})] \\\\ $$\nThis update is done after every transition from a nonterminal state $S_{t}$. If $S_{t+1}$ is terminal, then $Q(S_{t+1},A_{t+1})$ is defined as zero. This rule uses every element of the quintuple of events, $(S_{t},A_{t},R_{t+1},S_{t+1},A_{t+1})$, that make up a transition from one state–action pair to the next. This quintuple gives rise to the name Sarsa for the algorithm. The backup diagram for Sarsa is as shown to the right.\nIt is straightforward to design an on-policy control algorithm based on the Sarsa prediction method. As in all on-policy methods, we continually estimate $q_{\\pi}$ for the behavior policy $\\pi$, and at the same time change $\\pi$ toward greediness with respect to $q_{\\pi}$. The general form of the Sarsa control algorithm is given in the box on the next page. The convergence properties of the Sarsa algorithm depend on the nature of the policy’s dependence on $Q$. For example, one could use “$\\varepsilon$-greedy or “$\\varepsilon$-soft policies.\n $Q$-learning: Off-policy TD Control One of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as $Q$-learning (Watkins, 1989), defined by\n$$ Q(S_{t},A_{t})\\leftarrow Q(S_{t},A_{t})+\\alpha[R_{t+1}+\\gamma \\max_{a}Q(S_{t+1},a)-Q(S_{t},A_{t})] \\\\ $$\nIn this case, the learned action-value function, $Q$, directly approximates $q_{*}$, the optimal action-value function, independent of the policy being followed. This dramatically simplifies the analysis of the algorithm and enabled early convergence proofs. The policy still has an effect in that it determines which state–action pairs are visited and updated. However, all that is required for correct convergence is that all pairs continue to be updated. This is a minimal requirement in the sense that any method guaranteed to find optimal behavior in the general case must require it. Under this assumption and a variant of the usual stochastic approximation conditions on the sequence of step-size parameters, $Q$ has been shown to converge with probability 1 to $q_{*}$. The $Q$-learning algorithm is shown below in procedural form.\n Expected Sarsa Consider the learning algorithm that is just like $Q$-learning except that instead of the maximum over next state–action pairs it uses the expected value, taking into account how likely each action is under the current policy. That is, consider the algorithm with the update rule\n$$ \\begin{align} Q(S_{t},A_{t})\u0026\\leftarrow Q(S_{t},A_{t})+\\alpha[R_{t+1}+\\gamma \\mathbb{E}_{\\pi}[Q(S_{t+1},A_{t+1})|S_{t+1}]-Q(S_{t},A_{t})] \\\\ \u0026\\leftarrow Q(S_{t},A_{t})+\\alpha[R_{t+1}+\\gamma\\sum_{a}\\pi(a|S_{t+1})Q(S_{t+1},a) -Q(S_{t},A_{t})] \\\\ \\end{align} $$\nbut that otherwise follows the schema of $Q$-learning. Given the next state, $S_{t+1}$, this algorithm moves deterministiacally in the same direction as Sarsa moves in expectation, and accordingly it is called Expected Sarsa.\n Reference [1] Sutton, R. S., Bach, F., \u0026 Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press Ltd.\n",
  "wordCount" : "1481",
  "inLanguage": "en",
  "datePublished": "2022-06-11T00:00:00Z",
  "dateModified": "2022-06-11T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Followb1ind1y"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://followb1ind1y.github.io/posts/reinforcement_learning/06_temporal_difference_learning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Followb1ind1y",
    "logo": {
      "@type": "ImageObject",
      "url": "https://followb1ind1y.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://followb1ind1y.github.io/" accesskey="h" title="Followb1ind1y (Alt + H)">Followb1ind1y</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://followb1ind1y.github.io/archives" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://followb1ind1y.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://followb1ind1y.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Temporal-Difference Learning
    </h1>
    <div class="post-meta">June 11, 2022&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Followb1ind1y
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#td-prediction" aria-label="TD Prediction">TD Prediction</a></li>
                <li>
                    <a href="#advantages-of-td-prediction-methods" aria-label="Advantages of TD Prediction Methods">Advantages of TD Prediction Methods</a></li>
                <li>
                    <a href="#sarsa-on-policy-td-control" aria-label="Sarsa: On-policy TD Control">Sarsa: On-policy TD Control</a></li>
                <li>
                    <a href="#q-learning-off-policy-td-control" aria-label="$Q$-learning: Off-policy TD Control"><code>$Q$</code>-learning: Off-policy TD Control</a></li>
                <li>
                    <a href="#expected-sarsa" aria-label="Expected Sarsa">Expected Sarsa</a></li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be <em><strong>temporal-difference (TD)</strong></em> learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can <strong>learn directly from raw experience without a model of the environment’s dynamics</strong>. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap). The relationship between TD, DP, and Monte Carlo methods is a recurring theme in the theory of reinforcement learning</p>
<h3 id="td-prediction">TD Prediction<a hidden class="anchor" aria-hidden="true" href="#td-prediction">#</a></h3>
<p>Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy <code>$\pi$</code>, both methods update their estimate <code>$V$</code> of <code>$v_{\pi}$</code> for the nonterminal states <code>$S_{t}$</code> occurring in that experience. Roughly speaking, Monte Carlo methods wait until the return following the visit is known, then use that return as a target for <code>$V(S_{t})$</code>. A simple every-visit Monte Carlo method suitable for non-stationary environments is</p>
<p><code>$$ V(S_{t})\leftarrow V(S_{t})+\alpha[G_{t}-V(S_{t})] \\ $$</code></p>
<p>where <code>$G_{t}$</code> is the actual return following time <code>$t$</code>, and <code>$\alpha$</code> is a constant step-size parameter. Let us call this method constant-<code>$\alpha$</code> MC. Whereas Monte Carlo methods must wait until the end of the episode to determine the increment to <code>$V(S_{t})$</code> (only then is <code>$G_{t}$</code> known), TD methods need to wait only until the next time step. At time <code>$t+1$</code> they immediately form a target and make a useful update using the observed reward <code>$R_{t+1}$</code> and the estimate <code>$V(S_{t+1})$</code>. The simplest TD method makes the update</p>
<p><code>$$ V(S_{t})\leftarrow V(S_{t})+\alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_{t})] \\ $$</code></p>
<p>immediately on transition to <code>$S_{t+1}$</code> and receiving <code>$R_{t+1}$</code>. In effect, the target for the Monte Carlo update is <code>$G_{t}$</code>, whereas the target for the TD update is <code>$R_{t+1}+\gamma V(S_{t+1})$</code>. This TD method is called TD(0), or one-step TD.  The box below specifies TD(0) completely in procedural form.</p>
<div align="center">
  <img src="/img_RL/06_TD_est.PNG" width=650px/>
</div>
<br>
<p>Because TD(0) bases its update in part on an existing estimate, we say that it is a bootstrapping method. Note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of <code>$S_{t}$</code> and the better estimate <code>$R_{t+1}+\gamma V(S_{t+1})$</code>. This quantity, called the TD error, arises in various forms throughout reinforcement learning:</p>
<p><code>$$ \delta_{t} \doteq R_{t+1}+\gamma V(S_{t+1}) -V(S_{t}) \\ $$</code></p>
<p>Notice that the TD error at each time is the error in the estimate made at that time. Because the TD error depends on the next state and next reward, it is not actually available until one time step later. That is, <code>$\delta_{t}$</code> is the error in <code>$V(S_{t})$</code>, available at time <code>$t+1$</code>. Also note that if the array <code>$V$</code> does not change during the episode (as it does not in Monte Carlo methods), then the Monte Carlo error can be written as a sum of TD errors:</p>
<p><code>$$ \begin{align} G_{t}-V(S_{t}) &amp;=  R_{t+1}+\gamma V(S_{t+1}) -V(S_{t}) + \gamma V(S_{t+1}-\gamma V(S_{t+1}) \\ &amp;=\delta_{t}+\gamma(G_{t+1}-V(S_{t+1})) \\ &amp;=\delta_{t}+\gamma\delta_{t+1}+\gamma^{2}(G_{t+2}-V(S_{t+2})) \\ &amp;= \delta_{t}+\gamma\delta_{t+1}+\gamma^{2}\delta_{t+2}+\cdots+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-t}(G_{T}-V(S_{T})) \\ &amp;= \delta_{t}+\gamma\delta_{t+1}+\gamma^{2}\delta_{t+2}+\cdots+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-t}(0-0) \\ &amp;= \sum^{T-1}_{k=t}\gamma^{k-t}\delta_{k} \end{align} $$</code></p>
<p>This identity is not exact if <code>$V$</code> is updated during the episode (as it is in TD(0)), but if the step size is small then it may still hold approximately. Generalizations of this identity play an important role in the theory and algorithms of temporal-difference learning.</p>
<h3 id="advantages-of-td-prediction-methods">Advantages of TD Prediction Methods<a hidden class="anchor" aria-hidden="true" href="#advantages-of-td-prediction-methods">#</a></h3>
<p>TD methods update their estimates based in part on other estimates. They learn a guess from a guess—they bootstrap. Is this a good thing to do? Obviously, TD methods have an advantage over DP methods in that <em><strong>they do not require a model of the environment, of its reward and next-state probability distributions</strong></em>.</p>
<p><em><strong>The next most obvious advantage of TD methods over Monte Carlo methods is that they are naturally implemented in an online, fully incremental fashion.</strong></em> With Monte Carlo methods one must wait until the end of an episode, because only then is the return known, whereas with TD methods one need wait only one time step. Surprisingly often this turns out to be a critical consideration. Some applications have very long episodes, so that delaying all learning until the end of the episode is too slow. Other applications are continuing tasks and have no episodes at all. Finally, as we noted in the previous chapter, some Monte Carlo methods must ignore or discount episodes on which experimental actions are taken, which can greatly slow learning. TD methods are much less susceptible to these problems because they learn from each transition regardless of what subsequent actions are taken.</p>
<p>If both TD and Monte Carlo methods converge asymptotically to the correct predictions, then a natural next question is “Which gets there first?” In other words, which method learns faster? Which makes the more efficient use of limited data? At the current time this is an open question in the sense that no one has been able to prove mathematically that one method converges faster than the other. In fact, it is not even clear what is the most appropriate formal way to phrase this question! In practice, however, <em><strong>TD methods have usually been found to converge faster than constant-<code>$\alpha$</code> MC methods on stochastic tasks</strong></em>.</p>
<h3 id="sarsa-on-policy-td-control">Sarsa: On-policy TD Control<a hidden class="anchor" aria-hidden="true" href="#sarsa-on-policy-td-control">#</a></h3>
<p>We turn now to the use of TD prediction methods for the control problem. As usual, we follow the pattern of generalized policy iteration (GPI), only this time using TD methods for the evaluation or prediction part. As with Monte Carlo methods, we face the need to trade off exploration and exploitation, and again approaches fall into two main classes: on-policy and off-policy.</p>
<p>The first step is to learn an action-value function rather than a state-value function. In particular, for an on-policy method we must estimate <code>$q_{\pi}(s,a)$</code> for the current behavior policy <code>$\pi$</code> and for all states <code>$s$</code> and actions <code>$a$</code>. This can be done using essentially the same TD method described above for learning <code>$v_{\pi}$</code>. Recall that an episode consists of an alternating sequence of states and state–action pairs:</p>
<div align="center">
  <img src="/img_RL/06_on_policy_sarsa_progress.PNG" width=550px/>
</div>
<br>
<p>In the previous section we considered transitions from state to state and learned the values of states. Now we consider transitions from state–action pair to state–action pair, and learn the values of state–action pairs. Formally these cases are identical: they are both Markov chains with a reward process. The theorems assuring the convergence of state values under TD(0) also apply to the corresponding algorithm for action values:</p>
<p><code>$$ Q(S_{t},A_{t})\leftarrow Q(S_{t},A_{t})+\alpha[R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_{t},A_{t})] \\ $$</code></p>
<p>This update is done after every transition from a nonterminal state <code>$S_{t}$</code>. If <code>$S_{t+1}$</code> is terminal, then <code>$Q(S_{t+1},A_{t+1})$</code> is defined as zero. This rule uses every element of the quintuple of events, <code>$(S_{t},A_{t},R_{t+1},S_{t+1},A_{t+1})$</code>, that make up a transition from one state–action pair to the next. This quintuple gives rise to the name Sarsa for the algorithm. The backup diagram for Sarsa is as shown to the right.</p>
<p>It is straightforward to design an on-policy control algorithm based on the Sarsa prediction method. As in all on-policy methods, we continually estimate <code>$q_{\pi}$</code> for the behavior policy <code>$\pi$</code>, and at the same time change <code>$\pi$</code> toward greediness with respect to <code>$q_{\pi}$</code>. The general form of the Sarsa control algorithm is given in the box on the next page. The convergence properties of the Sarsa algorithm depend on the nature of the policy’s dependence on <code>$Q$</code>. For example, one could use &ldquo;<code>$\varepsilon$</code>-greedy or &ldquo;<code>$\varepsilon$</code>-soft policies.</p>
<div align="center">
  <img src="/img_RL/06_On_policy_sarsa_alg.PNG" width=650px/>
</div>
<br>
<h3 id="q-learning-off-policy-td-control"><code>$Q$</code>-learning: Off-policy TD Control<a hidden class="anchor" aria-hidden="true" href="#q-learning-off-policy-td-control">#</a></h3>
<p>One of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as <code>$Q$</code>-learning (Watkins, 1989), defined by</p>
<p><code>$$ Q(S_{t},A_{t})\leftarrow Q(S_{t},A_{t})+\alpha[R_{t+1}+\gamma \max_{a}Q(S_{t+1},a)-Q(S_{t},A_{t})] \\ $$</code></p>
<p>In this case, the learned action-value function, <code>$Q$</code>, directly approximates <code>$q_{*}$</code>, the optimal action-value function, independent of the policy being followed. This dramatically simplifies the analysis of the algorithm and enabled early convergence proofs. The policy still has an effect in that it determines which state–action pairs are visited and updated. However, all that is required for correct convergence is that all pairs continue to be updated. This is a minimal requirement in the sense that any method guaranteed to find optimal behavior in the general case must require it. Under this assumption and a variant of the usual stochastic approximation conditions on the sequence of step-size parameters, <code>$Q$</code> has been shown to converge with probability 1 to <code>$q_{*}$</code>. The <code>$Q$</code>-learning algorithm is shown below in procedural form.</p>
<div align="center">
  <img src="/img_RL/06_QLearning_alg.PNG" width=650px/>
</div>
<br>
<h3 id="expected-sarsa">Expected Sarsa<a hidden class="anchor" aria-hidden="true" href="#expected-sarsa">#</a></h3>
<p>Consider the learning algorithm that is just like <code>$Q$</code>-learning except that instead of the maximum over next state–action pairs it uses the expected value, taking into account how likely each action is under the current policy. That is, consider the algorithm with the update rule</p>
<p><code>$$ \begin{align} Q(S_{t},A_{t})&amp;\leftarrow Q(S_{t},A_{t})+\alpha[R_{t+1}+\gamma \mathbb{E}_{\pi}[Q(S_{t+1},A_{t+1})|S_{t+1}]-Q(S_{t},A_{t})] \\ &amp;\leftarrow Q(S_{t},A_{t})+\alpha[R_{t+1}+\gamma\sum_{a}\pi(a|S_{t+1})Q(S_{t+1},a) -Q(S_{t},A_{t})] \\ \end{align} $$</code></p>
<p>but that otherwise follows the schema of <code>$Q$</code>-learning. Given the next state, <code>$S_{t+1}$</code>, this algorithm moves deterministiacally in the same direction as Sarsa moves in expectation, and accordingly it is called Expected Sarsa.</p>
<div align="center">
  <img src="/img_RL/06_exp_sarsa.PNG" width=450px/>
</div>
<br>
<h3 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h3>
<p>[1] Sutton, R. S., Bach, F., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press Ltd.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://followb1ind1y.github.io/tags/td-learning/">TD Learning</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://followb1ind1y.github.io/posts/reinforcement_learning/05_-monte_carlo_methods/">
    <span class="title">Next Page »</span>
    <br>
    <span>Monte Carlo Methods</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Temporal-Difference Learning on twitter"
        href="https://twitter.com/intent/tweet/?text=Temporal-Difference%20Learning&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f06_temporal_difference_learning%2f&amp;hashtags=TDLearning">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Temporal-Difference Learning on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f06_temporal_difference_learning%2f&amp;title=Temporal-Difference%20Learning&amp;summary=Temporal-Difference%20Learning&amp;source=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f06_temporal_difference_learning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Temporal-Difference Learning on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f06_temporal_difference_learning%2f&title=Temporal-Difference%20Learning">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Temporal-Difference Learning on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f06_temporal_difference_learning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Temporal-Difference Learning on whatsapp"
        href="https://api.whatsapp.com/send?text=Temporal-Difference%20Learning%20-%20https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f06_temporal_difference_learning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Temporal-Difference Learning on telegram"
        href="https://telegram.me/share/url?text=Temporal-Difference%20Learning&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f06_temporal_difference_learning%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2022 <a href="https://followb1ind1y.github.io/">Followb1ind1y</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script src="//yihui.org/js/math-code.js"></script>


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>

</html>
