<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Finite Markov Decision Processes | Followb1ind1y</title>
<meta name="keywords" content="Finite Markov Decision Processes" />
<meta name="description" content="MDPs are a mathematically idealized form of the reinforcement learning problem for which precise theoretical statements can be made. We introduce key elements of the problem&rsquo;s mathematical structure, such as returns, value functions, and Bellman equations. We try to convey the wide range of applications that can be formulated as finite MDPs. As in all of artificial intelligence, there is a tension between breadth of applicability and mathematical tractability.
The Agent–Environment Interface MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.">
<meta name="author" content="Followb1ind1y">
<link rel="canonical" href="https://followb1ind1y.github.io/posts/reinforcement_learning/03_finite_markov_decision_processes/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.d70d9be7236724828e9dc04b26c5a1f3b228217674a56d00269f53e0d3db96f0.css" integrity="sha256-1w2b5yNnJIKOncBLJsWh87IoIXZ0pW0AJp9T4NPblvA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://followb1ind1y.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://followb1ind1y.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://followb1ind1y.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://followb1ind1y.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://followb1ind1y.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.83.1" />
<meta property="og:title" content="Finite Markov Decision Processes" />
<meta property="og:description" content="MDPs are a mathematically idealized form of the reinforcement learning problem for which precise theoretical statements can be made. We introduce key elements of the problem&rsquo;s mathematical structure, such as returns, value functions, and Bellman equations. We try to convey the wide range of applications that can be formulated as finite MDPs. As in all of artificial intelligence, there is a tension between breadth of applicability and mathematical tractability.
The Agent–Environment Interface MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://followb1ind1y.github.io/posts/reinforcement_learning/03_finite_markov_decision_processes/" /><meta property="og:image" content="https://followb1ind1y.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-24T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2022-05-24T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://followb1ind1y.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Finite Markov Decision Processes"/>
<meta name="twitter:description" content="MDPs are a mathematically idealized form of the reinforcement learning problem for which precise theoretical statements can be made. We introduce key elements of the problem&rsquo;s mathematical structure, such as returns, value functions, and Bellman equations. We try to convey the wide range of applications that can be formulated as finite MDPs. As in all of artificial intelligence, there is a tension between breadth of applicability and mathematical tractability.
The Agent–Environment Interface MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://followb1ind1y.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Finite Markov Decision Processes",
      "item": "https://followb1ind1y.github.io/posts/reinforcement_learning/03_finite_markov_decision_processes/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Finite Markov Decision Processes",
  "name": "Finite Markov Decision Processes",
  "description": "MDPs are a mathematically idealized form of the reinforcement learning problem for which precise theoretical statements can be made. We introduce key elements of the problem\u0026rsquo;s mathematical structure, such as returns, value functions, and Bellman equations. We try to convey the wide range of applications that can be formulated as finite MDPs. As in all of artificial intelligence, there is a tension between breadth of applicability and mathematical tractability.\nThe Agent–Environment Interface MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.",
  "keywords": [
    "Finite Markov Decision Processes"
  ],
  "articleBody": "MDPs are a mathematically idealized form of the reinforcement learning problem for which precise theoretical statements can be made. We introduce key elements of the problem’s mathematical structure, such as returns, value functions, and Bellman equations. We try to convey the wide range of applications that can be formulated as finite MDPs. As in all of artificial intelligence, there is a tension between breadth of applicability and mathematical tractability.\nThe Agent–Environment Interface MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal. The learner and decision maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment. These interact continually, the agent selecting actions and the environment responding to these actions and presenting new situations to the agent.1 The environment also gives rise to rewards, special numerical values that the agent seeks to maximize over time through its choice of actions.\n More specifically, the agent and environment interact at each of a sequence of discrete time steps, $t=0,1,2,3,...$ At each time step $t$, the agent receives some representation of the environment’s state, $S_{t}\\in \\mathcal{S}$, and on that basis selects an action, At $A_{t}\\in\\mathcal{A}(s)$. One time step later, in part as a consequence of its action, the agent receives a numerical reward, $R_{t+1}\\in \\mathbb{R}$, and finds itself in a new state, $S_{t+1}$. The MDP and agent together thereby give rise to a sequence or trajectory that begins like this:\n$$ S_{0},A_{0},R_{1},S_{1},A_{1},R_{2},S_{2},A_{2},R_{3},... \\\\ $$\nIn a finite MDP, the sets of states, actions, and rewards ($\\mathcal{S}$, $\\mathcal{A}$, and $\\mathcal{R}$) all have a finite number of elements. In this case, the random variables $R_{t}$ and $S_{t}$ have well defined discrete probability distributions dependent only on the preceding state and action. That is, for particular values of these random variables, $s'\\in \\mathcal{S}$ and $r\\in\\mathcal{R}$, there is a probability of those values occurring at time $t$, given particular values of the preceding state and action:\n$$ p(s',r|s,a)\\doteq \\mathrm{Pr}({S_{t}=s',R_{t}=r}|S_{t-1}=s,A_{t-1}=a) \\\\ $$\nfor all $s',s\\in\\mathcal{S},r\\in\\mathcal{R},$ and $a \\in \\mathcal{A}(s)$. The function $p$ defines the dynamics of the MDP. The dot over the equals sign in the equation reminds us that it is a definition (in this case of the function $p$) rather than a fact that follows from previous definitions. The dynamics function $p:\\mathcal{S}\\times\\mathcal{R}\\times\\mathcal{S}\\times\\mathcal{R} \\to [0,1]$ is an ordinary deterministic function of four arguments. The $|$ in the middle of it comes from the notation for conditional probability, but here it just reminds us that $p$ specifies a probability distribution for each choice of $s$ and $a$, that is, that\n$$ \\sum_{s'\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{R}}p(s',r|s,a)=1, \\mathrm{for} \\ \\mathrm{all} \\ s \\in \\mathcal{S}, a \\in \\mathcal{A}(s) \\\\ $$\nIn a Markov decision process, the probabilities given by $p$ completely characterize the environment’s dynamics. That is, the probability of each possible value for $S_{t}$ and $R_{t}$ depends on the immediately preceding state and action, $S_{t-1}$ and $A_{t-1}$, and, given them, not at all on earlier states and actions. This is best viewed as a restriction not on the decision process, but on the state. The state must include information about all aspects of the past agent–environment interaction that make a difference for the future. If it does, then the state is said to have the Markov property.\nFrom the four-argument dynamics function, $p$, one can compute anything else one might want to know about the environment, such as the state-transition probabilities (which we denote, with a slight abuse of notation, as a three-argument function $p:\\mathcal{S}\\times\\mathcal{S}\\times\\mathcal{A} \\to [0,1]$),\n$$ p(s'|s,a)\\doteq \\mathrm{Pr}\\{S_{t}=s'|S_{t-1}=s,A_{t-1}=a\\}=\\sum_{r\\in\\mathcal{R}}p(s',r|s,a) \\\\ $$\nWe can also compute the expected rewards for state–action pairs as a two-argument function $r:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}$:\n$$ r(s,a)\\doteq \\mathbb{E}[R_{t}|S_{t-1}=s,A_{t-1}=a]=\\sum_{r\\in\\mathcal{R}}r\\sum_{s'\\in\\mathcal{S}}p(s',r|s,a) \\\\ $$\nand the expected rewards for state–action–next-state triples as a three-argument function $r:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S} \\to \\mathbb{R}$,\n$$ r(s,a,s')\\doteq \\mathbb{E}[R_{t}|S_{t-1}=s,A_{t-1}=a,S_{t}=s]=\\sum_{r\\in\\mathcal{R}}r\\frac{p(s',r|s,a)}{p(s'|s,a)} \\\\ $$\nIn particular, the boundary between agent and environment is typically not the same as the physical boundary of a robot’s or an animal’s body. Usually, the boundary is drawn closer to the agent than that. The general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment. We do not assume that everything in the environment is unknown to the agent.\nThe MDP framework is a considerable abstraction of the problem of goal-directed learning from interaction. It proposes that whatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: one signal to represent the choices made by the agent (the actions), one signal to represent the basis on which the choices are made (the states), and one signal to define the agent’s goal (the rewards). This framework may not be sufficient to represent all decision-learning problems usefully, but it has proved to be widely useful and applicable.\nGoals and Rewards In reinforcement learning, the purpose or goal of the agent is formalized in terms of a special signal, called the reward, passing from the environment to the agent. At each time step, the reward is a simple number, $R_{t} \\in \\mathbb{R}$. Informally, the agent’s goal is to maximize the total amount of reward it receives. This means maximizing not immediate reward, but cumulative reward in the long run. We can clearly state this informal idea as the reward hypothesis:\n That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\n The use of a reward signal to formalize the idea of a goal is one of the most distinctive features of reinforcement learning.\nAlthough formulating goals in terms of reward signals might at first appear limiting, in practice it has proved to be flexible and widely applicable. We can see the agent always learns to maximize its reward. If we want it to do something for us, we must provide rewards to it in such a way that in maximizing them the agent will also achieve our goals. It is thus critical that the rewards we set up truly indicate what we want accomplished. In particular, the reward signal is not the place to impart to the agent prior knowledge about how to achieve what we want it to do.\nReturns and Episodes We have said that the agent’s goal is to maximize the cumulative reward it receives in the long run. How might this be defined formally? If the sequence of rewards received after time step $t$ is denoted $R_{t+1},R_{t+2},R_{t+3},...,$ then what precise aspect of this sequence do we wish to maximize? In general, we seek to maximize the expected return, where the return, denoted $G_{t}$, is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards:\n$$ G_{t}\\doteq R_{t+1}+R_{t+2}+R_{t+3}+...+R_{T}, \\\\ $$\nwhere $T$ is a final time step. This approach makes sense in applications in which there is a natural notion of final time step, that is, when the agent–environment interaction breaks naturally into subsequences, which we call episodes. Each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. Even if you think of episodes as ending in different ways, such as winning and losing a game, the next episode begins independently of how the previous one ended. Thus the episodes can all be considered to end in the same terminal state, with different rewards for the different outcomes. Tasks with episodes of this kind are called episodic tasks. In episodic tasks we sometimes need to distinguish the set of all nonterminal states, denoted $\\mathcal{S}$, from the set of all states plus the terminal state, denoted $\\mathcal{S}^{+}$. The time of termination, $T$, is a random variable that normally varies from episode to episode. On the other hand, in many cases the agent-environment interaction does not break naturally into identifiable episodes, but goes on continually without limit.\nThe additional concept that we need is that of discounting. According to this approach, the agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses $A_{t}$ to maximize the expected discounted return:\n$$ G_{t}\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+...=\\sum^{\\infty}_{k=0}\\gamma^{k}R_{t+k+1}, \\\\ $$\nwhere $\\gamma$ is a parameter, $0\\leq\\gamma\\leq1$, called the discount rate.\nThe discount rate determines the present value of future rewards: a reward received $k$ time steps in the future is worth only $\\gamma^{k-1}$ times what it would be worth if it were received immediately. If $\\gamma , the infinite sum in the equation has a finite value as long as the reward sequence $\\{R_{k}\\}$ is bounded. If $\\gamma=0$, the agent is “myopic” in being concerned only with maximizing immediate rewards: its objective in this case is to learn how to choose $A_{t}$ so as to maximize only $R_{t+1}$. But in general, acting to maximize immediate reward can reduce access to future rewards so that the return is reduced. As $\\gamma$ approaches 1, the return objective takes future rewards into account more strongly; the agent becomes more farsighted.\nReturns at successive time steps are related to each other in a way that is important for the theory and algorithms of reinforcement learning:\n$$ \\begin{align} G_{t}\u0026\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\gamma^{3}R_{t+4}+...\\\\ \u0026=R_{t+1}+\\gamma (R_{t+2}+\\gamma R_{t+3}+\\gamma^{2}R_{t+4}+...)\\\\ \u0026=R_{t+1}+\\gamma G_{t+1} \\\\ \\end{align} $$\nNote that this works for all time steps $t , even if termination occurs at $t+1$, provided we define $G_{T}=0$. This often makes it easy to compute returns from reward sequences.\nNote that although the return of the equation is a sum of an infinite number of terms, it is still finite if the reward is nonzero and constant — if $\\gamma. For example, if the reward is a constant +1, then the return is\n$$ G_{t}=\\sum^{\\infty}_{k=0}\\gamma^{k}=\\frac{1}{1-\\gamma} \\\\ $$\nPolicies and Value Functions Almost all reinforcement learning algorithms involve estimating value functions—functions of states (or of state–action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of “how good” here is defined in terms of future rewards that can be expected, or, to be precise, in terms of expected return. Of course the rewards the agent can expect to receive in the future depend on what actions it will take. Accordingly, value functions are defined with respect to particular ways of acting, called policies.\nFormally, a policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy $\\pi$ at time $t$, then $\\pi(a|s)$ is the probability that $A_{t}=a$ if $S_{t}=s$. Like $p$, $\\pi$ is an ordinary function; the $|$ in the middle of $\\pi(a|s)$ merely reminds us that it defines a probability distribution over $a\\in\\mathcal{A}(s)$ for each $s\\in\\mathcal{S}$. Reinforcement learning methods specify how the agent’s policy is changed as a result of its experience.\nThe value function of a state s under a policy $\\pi$, denoted $v_{\\pi}(s)$, is the expected return when starting in $s$ and following $\\pi$ thereafter. For MDPs, we can define $v_{\\pi}$ formally by\n$$ v_{\\pi}(s)\\doteq \\mathbb{E}_{\\pi}[G_{t}|S_{t}=s]=\\mathbb{E}_{\\pi}[\\sum^{\\infty}_{k=0}\\gamma^{k}R_{t+k+1}|S_{t}=s], \\mathrm{for} \\ \\mathrm{all} \\ s \\in \\mathcal{S}, \\\\ $$\nwhere $\\mathbb{E}_{\\pi}[\\cdot]$ denotes the expected value of a random variable given that the agent follows policy $\\pi$, and $t$ is any time step. Note that the value of the terminal state, if any, is always zero. We call the function $v_{\\pi}$ the state-value function for policy $\\pi$.\nSimilarly, we define the value of taking action $a$ in state $s$ under a policy $\\pi$, denoted $q_{\\pi}(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\\pi$:\n$$ q_{\\pi}(s,a)\\doteq \\mathbb{E}_{\\pi}[G_{t}|S_{t}=s,A_{t}=a]=\\mathbb{E}_{\\pi}[\\sum^{\\infty}_{k=0}\\gamma^{k}R_{t+k+1}|S_{t}=s,A_{t}=a], \\\\ $$\nWe call $q_{\\pi}(s,a)$ the action-value function for policy $\\pi$.\nA fundamental property of value functions used throughout reinforcement learning and dynamic programming is that they satisfy recursive relationships similar to that which we have already established for the return. For any policy $\\pi$ and any state $s$, the following consistency condition holds between the value of $s$ and the value of its possible successor states:\n$$ \\begin{align} v_{\\pi}(s)\u0026\\doteq\\mathbb{E}[G_{t}|S_{t}=s] \\\\ \u0026=\\mathbb{E}[R_{t+1}+\\gamma G_{t+1}|S_{t}=s] \\\\ \u0026= \\sum_{a}\\pi(a|s)\\sum_{s'}\\sum_{r}p(s',r|s,a)[r+\\gamma\\mathbb{E}_{\\pi}[G_{t+1}|S_{t+1}=s']] \\\\ \u0026= \\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')], \\mathrm{\\ for \\ all \\ }s\\in\\mathcal{S} \\\\ \\end{align} $$\nwhere it is implicit that the actions, $a$, are taken from the set $\\mathcal{A}(s)$, that the next states, $s'$, are taken from the set $\\mathcal{S}$, and that the rewards, $r$, are taken from the set $\\mathcal{R}$. Note also how in the last equation we have merged the two sums, one over all the values of $s'$ and the other over all the values of $r$, into one sum over all the possible values of both. We use this kind of merged sum often to simplify formulas. Note how the final expression can be read easily as an expected value. It is really a sum over all values of the three variables, $a$, $s'$, and $r$. For each triple, we compute its probability, $\\pi(a,s)p(s',r|s,a)$, weight the quantity in brackets by that probability, then sum over all possibilities to get an expected value.\nPrevious equation is the Bellman equation for $v_{\\pi}$. It expresses a relationship between the value of a state and the values of its successor states. Think of looking ahead from a state to its possible successor states, as suggested by the diagram to the right. Each open circle represents a state and each solid circle represents a state–action pair. Starting from state $s$, the root node at the top, the agent could take any of some set of actions — three are shown in the diagram — based on its policy $\\pi$. From each of these, the environment could respond with one of several next states, $s'$ (two are shown in the figure), along with a reward, $r$, depending on its dynamics given by the function $p$. The Bellman equation averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way. The value function $v_{\\pi}$ is the unique solution to its Bellman equation.\n Optimal Policies and Optimal Value Functions Solving a reinforcement learning task means, roughly, finding a policy that achieves a lot of reward over the long run. For finite MDPs, we can precisely define an optimal policy in the following way. Value functions define a partial ordering over policies. A policy $\\pi$ is defined to be better than or equal to a policy $\\pi'$ if its expected return is greater than or equal to that of $\\pi'$ for all states. In other words, $\\pi\\geq\\pi'$ if and only if $v_{\\pi}(s)\\geq v_{\\pi'}(s)$ for all $s\\in\\mathcal{S}$. There is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by $\\pi_{*}$. They share the same state-value function, called the optimal state-value function, denoted $v_{*}$, and defined as\n$$ v_{*}(s)\\doteq\\max_{\\pi}v_{\\pi}(s), \\mathrm{\\ for \\ all \\ } s\\in\\mathcal{S} \\\\ $$\nOptimal policies also share the same optimal action-value function, denoted $q_{*}$, and defined as\n$$ q_{*}(s,a)\\doteq\\max_{\\pi}q_{\\pi}(s,a), \\mathrm{\\ for \\ all \\ } s\\in\\mathcal{S}\\mathrm{\\ and\\ } a \\in \\mathcal{A} \\\\ $$\nFor the state–action pair $(s,a)$, this function gives the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. Thus, we can write $q_{*}$ in terms of $v_{*}$ as follows:\n$$ q_{*}(s,a)=\\mathbb{E}[R_{t+1}+\\gamma v_{*}(S_{t+1})|S_{t}=s, A_{t}=a] \\\\ $$\nReference [1] Sutton, R. S., Bach, F., \u0026 Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press Ltd.\n",
  "wordCount" : "2631",
  "inLanguage": "en",
  "datePublished": "2022-05-24T00:00:00Z",
  "dateModified": "2022-05-24T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Followb1ind1y"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://followb1ind1y.github.io/posts/reinforcement_learning/03_finite_markov_decision_processes/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Followb1ind1y",
    "logo": {
      "@type": "ImageObject",
      "url": "https://followb1ind1y.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://followb1ind1y.github.io/" accesskey="h" title="Followb1ind1y (Alt + H)">Followb1ind1y</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://followb1ind1y.github.io/archives" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://followb1ind1y.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://followb1ind1y.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Finite Markov Decision Processes
    </h1>
    <div class="post-meta">May 24, 2022&nbsp;·&nbsp;13 min&nbsp;·&nbsp;Followb1ind1y
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#the-agentenvironment-interface" aria-label="The Agent–Environment Interface">The Agent–Environment Interface</a></li>
                <li>
                    <a href="#goals-and-rewards" aria-label="Goals and Rewards">Goals and Rewards</a></li>
                <li>
                    <a href="#returns-and-episodes" aria-label="Returns and Episodes">Returns and Episodes</a></li>
                <li>
                    <a href="#policies-and-value-functions" aria-label="Policies and Value Functions">Policies and Value Functions</a></li>
                <li>
                    <a href="#optimal-policies-and-optimal-value-functions" aria-label="Optimal Policies and Optimal Value Functions">Optimal Policies and Optimal Value Functions</a></li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>MDPs are a mathematically idealized form of the reinforcement learning problem for which precise theoretical statements can be made. We introduce key elements of the problem&rsquo;s mathematical structure, such as returns, value functions, and Bellman equations. We try to convey the wide range of applications that can be formulated as finite MDPs. As in all of artificial intelligence, there is a tension between breadth of applicability and mathematical tractability.</p>
<h3 id="the-agentenvironment-interface">The Agent–Environment Interface<a hidden class="anchor" aria-hidden="true" href="#the-agentenvironment-interface">#</a></h3>
<p>MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal. The learner and decision maker is called the <em><strong>agent</strong></em>. The thing it interacts with, comprising everything outside the agent, is called the <em><strong>environment</strong></em>. These interact continually, the agent selecting actions and the environment responding to these actions and presenting new situations to the agent.1 The environment also gives rise to rewards, special numerical values that the agent seeks to maximize over time through its choice of actions.</p>
<div align="center">
  <img src="/img_RL/03_Agent_envir.PNG" width=650px/>
</div>
<br>
<p>More specifically, the agent and environment interact at each of a sequence of discrete time steps, <code>$t=0,1,2,3,...$</code> At each time step <code>$t$</code>, the agent receives some representation of the environment’s state, <code>$S_{t}\in \mathcal{S}$</code>, and on that basis selects an action, At <code>$A_{t}\in\mathcal{A}(s)$</code>. One time step later, in part as a consequence of its action, the agent receives a numerical reward, <code>$R_{t+1}\in \mathbb{R}$</code>, and finds itself in a new state, <code>$S_{t+1}$</code>. The MDP and agent together thereby give rise to a sequence or trajectory that begins like this:</p>
<p><code>$$ S_{0},A_{0},R_{1},S_{1},A_{1},R_{2},S_{2},A_{2},R_{3},... \\ $$</code></p>
<p>In a finite MDP, the sets of states, actions, and rewards (<code>$\mathcal{S}$</code>, <code>$\mathcal{A}$</code>, and <code>$\mathcal{R}$</code>) all have a finite number of elements. In this case, the random variables <code>$R_{t}$</code> and <code>$S_{t}$</code> have well defined discrete probability distributions dependent only on the preceding state and action. That is, for particular values of these random variables, <code>$s'\in \mathcal{S}$</code> and <code>$r\in\mathcal{R}$</code>, there is a probability of those values occurring at time <code>$t$</code>, given particular values of the preceding state and action:</p>
<p><code>$$ p(s',r|s,a)\doteq \mathrm{Pr}({S_{t}=s',R_{t}=r}|S_{t-1}=s,A_{t-1}=a) \\ $$</code></p>
<p>for all <code>$s',s\in\mathcal{S},r\in\mathcal{R},$</code> and <code>$a \in \mathcal{A}(s)$</code>. The function <code>$p$</code> defines the dynamics of the MDP. The dot over the equals sign in the equation reminds us that it is a definition (in this case of the function <code>$p$</code>) rather than a fact that follows from previous definitions. The dynamics function <code>$p:\mathcal{S}\times\mathcal{R}\times\mathcal{S}\times\mathcal{R} \to [0,1]$</code> is an ordinary deterministic function of four arguments. The <code>$|$</code> in the middle of it comes from the notation for conditional probability, but here it just reminds us that <code>$p$</code> specifies a probability distribution for each choice of <code>$s$</code> and <code>$a$</code>, that is, that</p>
<p><code>$$ \sum_{s'\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s',r|s,a)=1, \mathrm{for} \ \mathrm{all} \ s \in \mathcal{S}, a \in \mathcal{A}(s) \\ $$</code></p>
<p>In a <em><strong>Markov decision process</strong></em>, the probabilities given by <code>$p$</code> completely characterize the environment’s dynamics. That is, the probability of each possible value for <code>$S_{t}$</code> and <code>$R_{t}$</code> depends on the immediately preceding state and action, <code>$S_{t-1}$</code> and <code>$A_{t-1}$</code>, and, given them, not at all on earlier states and actions. This is best viewed as a restriction not on the decision process, but on the <em><strong>state</strong></em>. The state must include information about all aspects of the past agent–environment interaction that make a difference for the future. If it does, then the state is said to have the <em><strong>Markov property</strong></em>.</p>
<p>From the four-argument dynamics function, <code>$p$</code>, one can compute anything else one might want to know about the environment, such as the <em><strong>state-transition probabilities</strong></em> (which we denote, with a slight abuse of notation, as a three-argument function <code>$p:\mathcal{S}\times\mathcal{S}\times\mathcal{A} \to [0,1]$</code>),</p>
<p><code>$$ p(s'|s,a)\doteq \mathrm{Pr}\{S_{t}=s'|S_{t-1}=s,A_{t-1}=a\}=\sum_{r\in\mathcal{R}}p(s',r|s,a) \\ $$</code></p>
<p>We can also compute the expected rewards for state–action pairs as a two-argument function <code>$r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$</code>:</p>
<p><code>$$ r(s,a)\doteq \mathbb{E}[R_{t}|S_{t-1}=s,A_{t-1}=a]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r|s,a) \\ $$</code></p>
<p>and the expected rewards for state–action–next-state triples as a three-argument function <code>$r:\mathcal{S}\times\mathcal{A}\times\mathcal{S} \to \mathbb{R}$</code>,</p>
<p><code>$$ r(s,a,s')\doteq \mathbb{E}[R_{t}|S_{t-1}=s,A_{t-1}=a,S_{t}=s]=\sum_{r\in\mathcal{R}}r\frac{p(s',r|s,a)}{p(s'|s,a)} \\ $$</code></p>
<p>In particular, the boundary between agent and environment is typically not the same as the physical boundary of a robot’s or an animal’s body. Usually, the boundary is drawn closer to the agent than that. The general rule we follow is that anything that <em>cannot</em> be changed arbitrarily by the agent is considered to be <strong>outside of it and thus part of its environment</strong>. We do not assume that everything in the environment is unknown to the agent.</p>
<p>The MDP framework is a considerable abstraction of the problem of goal-directed learning from interaction. It proposes that whatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be reduced to <strong>three signals passing back and forth between an agent and its environment</strong>: one signal to represent the choices made by the <strong>agent</strong> (the actions), one signal to represent the basis on which the choices are made (the states), and one <strong>signal</strong> to define the agent’s goal (the rewards). This framework may not be sufficient to represent all decision-learning problems usefully, but it has proved to be widely useful and applicable.</p>
<h3 id="goals-and-rewards">Goals and Rewards<a hidden class="anchor" aria-hidden="true" href="#goals-and-rewards">#</a></h3>
<p>In reinforcement learning, the purpose or goal of the agent is formalized in terms of a special signal, called the <em><strong>reward</strong></em>, passing from the environment to the agent. At each time step, the reward is a simple number, <code>$R_{t} \in \mathbb{R}$</code>. Informally, the agent’s goal is to maximize the total amount of reward it receives. <strong>This means maximizing not immediate reward, but cumulative reward in the long run.</strong> We can clearly state this informal idea as the <em><strong>reward hypothesis</strong></em>:</p>
<blockquote>
<p><em>That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).</em></p>
</blockquote>
<p>The use of a reward signal to formalize the idea of a goal is one of the most distinctive features of reinforcement learning.</p>
<p>Although formulating goals in terms of reward signals might at first appear limiting, in practice it has proved to be flexible and widely applicable. We can see the agent always learns to maximize its reward. If we want it to do something for us, we must provide rewards to it in such a way that in maximizing them the agent will also achieve our goals. It is thus critical that the rewards we set up truly indicate what we want accomplished. In particular, the reward signal is not the place to impart to the agent prior knowledge about how to achieve what we want it to do.</p>
<h3 id="returns-and-episodes">Returns and Episodes<a hidden class="anchor" aria-hidden="true" href="#returns-and-episodes">#</a></h3>
<p>We have said that the agent’s goal is to <strong>maximize the cumulative reward</strong> it receives in the long run. How might this be defined formally? If the sequence of rewards received after time step <code>$t$</code> is denoted <code>$R_{t+1},R_{t+2},R_{t+3},...,$</code> then what precise aspect of this sequence do we wish to maximize? In general, we seek to maximize the <em><strong>expected return</strong></em>, where the return, denoted <code>$G_{t}$</code>, is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards:</p>
<p><code>$$ G_{t}\doteq R_{t+1}+R_{t+2}+R_{t+3}+...+R_{T}, \\ $$</code></p>
<p>where <code>$T$</code> is a final time step. This approach makes sense in applications in which there is a natural notion of final time step, that is, when the agent–environment interaction breaks naturally into subsequences, which we call <em><strong>episodes</strong></em>. Each episode ends in a special state called the <em><strong>terminal state</strong></em>, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. Even if you think of episodes as ending in different ways, such as winning and losing a game, the next episode begins independently of how the previous one ended. Thus the episodes can all be considered to end in the same terminal state, with different rewards for the different outcomes. Tasks with episodes of this kind are called <em><strong>episodic tasks</strong></em>. In episodic tasks we sometimes need to distinguish the set of all nonterminal states, denoted <code>$\mathcal{S}$</code>, from the set of all states plus the terminal state, denoted <code>$\mathcal{S}^{+}$</code>. The time of termination, <code>$T$</code>, is a random variable that normally varies from episode to episode. On the other hand, in many cases the agent-environment interaction does not break naturally into identifiable episodes, but goes on continually without limit.</p>
<p>The additional concept that we need is that of <em><strong>discounting</strong></em>. According to this approach, the agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses <code>$A_{t}$</code> to maximize the expected <em><strong>discounted return</strong></em>:</p>
<p><code>$$ G_{t}\doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+...=\sum^{\infty}_{k=0}\gamma^{k}R_{t+k+1}, \\ $$</code></p>
<p>where <code>$\gamma$</code> is a parameter, <code>$0\leq\gamma\leq1$</code>, called the <em><strong>discount rate</strong></em>.</p>
<p>The discount rate determines the present value of future rewards: a reward received <code>$k$</code> time steps in the future is worth only <code>$\gamma^{k-1}$</code> times what it would be worth if it were received immediately. If <code>$\gamma &lt;1$</code>, the infinite sum in the equation has a finite value as long as the reward sequence <code>$\{R_{k}\}$</code> is bounded. If <code>$\gamma=0$</code>, the agent is “<strong>myopic</strong>” in being concerned only with maximizing immediate rewards: its objective in this case is to learn how to choose <code>$A_{t}$</code> so as to maximize only <code>$R_{t+1}$</code>. But in general, acting to maximize immediate reward can reduce access to future rewards so that the return is reduced. As <code>$\gamma$</code> approaches 1, the return objective takes future rewards into account more strongly; the agent becomes more <strong>farsighted</strong>.</p>
<p>Returns at successive time steps are related to each other in a way that is important for the theory and algorithms of reinforcement learning:</p>
<p><code>$$ \begin{align} G_{t}&amp;\doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\gamma^{3}R_{t+4}+...\\ &amp;=R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3}+\gamma^{2}R_{t+4}+...)\\ &amp;=R_{t+1}+\gamma G_{t+1} \\ \end{align} $$</code></p>
<p>Note that this works for all time steps <code>$t&lt;T$</code> , even if termination occurs at <code>$t+1$</code>, provided we define <code>$G_{T}=0$</code>. This often makes it easy to compute returns from reward sequences.</p>
<p>Note that although the return of the equation is a sum of an infinite number of terms, it is still finite if the reward is nonzero and constant — if <code>$\gamma&lt;1$</code>. For example, if the reward is a constant +1, then the return is</p>
<p><code>$$ G_{t}=\sum^{\infty}_{k=0}\gamma^{k}=\frac{1}{1-\gamma} \\ $$</code></p>
<h3 id="policies-and-value-functions">Policies and Value Functions<a hidden class="anchor" aria-hidden="true" href="#policies-and-value-functions">#</a></h3>
<p>Almost all reinforcement learning algorithms involve estimating <em><strong>value functions</strong></em>—functions of states (or of state–action pairs) that estimate <em>how good</em> it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of “how good” here is defined in terms of future rewards that can be expected, or, to be precise, in terms of expected return. Of course the rewards the agent can expect to receive in the future depend on what actions it will take. Accordingly, value functions are defined with respect to particular ways of acting, called <em><strong>policies</strong></em>.</p>
<p>Formally, a policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy <code>$\pi$</code> at time <code>$t$</code>, then <code>$\pi(a|s)$</code> is the probability that <code>$A_{t}=a$</code> if <code>$S_{t}=s$</code>. Like <code>$p$</code>, <code>$\pi$</code> is an ordinary function; the <code>$|$</code> in the middle of <code>$\pi(a|s)$</code> merely reminds us that it defines a probability distribution over <code>$a\in\mathcal{A}(s)$</code> for each <code>$s\in\mathcal{S}$</code>. Reinforcement learning methods specify how the agent’s policy is changed as a result of its experience.</p>
<p>The value function of a state s under a policy <code>$\pi$</code>, denoted <code>$v_{\pi}(s)$</code>, is the expected return when starting in <code>$s$</code> and following <code>$\pi$</code> thereafter. For MDPs, we can define <code>$v_{\pi}$</code> formally by</p>
<p><code>$$ v_{\pi}(s)\doteq \mathbb{E}_{\pi}[G_{t}|S_{t}=s]=\mathbb{E}_{\pi}[\sum^{\infty}_{k=0}\gamma^{k}R_{t+k+1}|S_{t}=s], \mathrm{for} \ \mathrm{all} \ s \in \mathcal{S}, \\ $$</code></p>
<p>where <code>$\mathbb{E}_{\pi}[\cdot]$</code> denotes the expected value of a random variable given that the agent follows policy <code>$\pi$</code>, and <code>$t$</code> is any time step. Note that the value of the terminal state, if any, is always zero. We call the function <code>$v_{\pi}$</code> the <em>state-value function for policy</em> <code>$\pi$</code>.</p>
<p>Similarly, we define the value of taking action <code>$a$</code> in state <code>$s$</code> under a policy <code>$\pi$</code>, denoted <code>$q_{\pi}(s,a)$</code>, as the expected return starting from <code>$s$</code>, taking the action <code>$a$</code>, and thereafter following policy <code>$\pi$</code>:</p>
<p><code>$$ q_{\pi}(s,a)\doteq \mathbb{E}_{\pi}[G_{t}|S_{t}=s,A_{t}=a]=\mathbb{E}_{\pi}[\sum^{\infty}_{k=0}\gamma^{k}R_{t+k+1}|S_{t}=s,A_{t}=a], \\ $$</code></p>
<p>We call <code>$q_{\pi}(s,a)$</code> the action-value function for policy <code>$\pi$</code>.</p>
<p>A fundamental property of value functions used throughout reinforcement learning and dynamic programming is that they satisfy recursive relationships similar to that which we have already established for the return. For any policy <code>$\pi$</code> and any state <code>$s$</code>, the following consistency condition holds between the value of <code>$s$</code> and the value of its possible successor states:</p>
<p><code>$$ \begin{align} v_{\pi}(s)&amp;\doteq\mathbb{E}[G_{t}|S_{t}=s] \\ &amp;=\mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_{t}=s] \\ &amp;= \sum_{a}\pi(a|s)\sum_{s'}\sum_{r}p(s',r|s,a)[r+\gamma\mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s']] \\ &amp;= \sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_{\pi}(s')], \mathrm{\ for \ all \ }s\in\mathcal{S} \\ \end{align} $$</code></p>
<p>where it is implicit that the actions, <code>$a$</code>, are taken from the set <code>$\mathcal{A}(s)$</code>, that the next states, <code>$s'$</code>, are taken from the set <code>$\mathcal{S}$</code>, and that the rewards, <code>$r$</code>, are taken from the set <code>$\mathcal{R}$</code>. Note also how in the last equation we have merged the two sums, one over all the values of <code>$s'$</code> and the other over all the values of <code>$r$</code>, into one sum over all the possible values of both. We use this kind of merged sum often to simplify formulas. Note how the final expression can be read easily as an expected value. It is really a sum over all values of the three variables, <code>$a$</code>, <code>$s'$</code>, and <code>$r$</code>. For each triple, we compute its probability, <code>$\pi(a,s)p(s',r|s,a)$</code>, weight the quantity in brackets by that probability, then sum over all possibilities to get an expected value.</p>
<p>Previous equation is the <em><strong>Bellman equation</strong></em> for <code>$v_{\pi}$</code>. It expresses a relationship between the value of a state and the values of its successor states. Think of looking ahead from a state to its possible successor states, as suggested by the diagram to the right. Each open circle represents a state and each solid circle represents a state–action pair. Starting from state $s$, the root node at the top, the agent could take any of some set of actions — three are shown in the diagram — based on its policy <code>$\pi$</code>. From each of these, the environment could respond with one of several next states, <code>$s'$</code> (two are shown in the figure), along with a reward, <code>$r$</code>, depending on its dynamics given by the function $p$. The Bellman equation averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way. The value function <code>$v_{\pi}$</code> is the <strong>unique solution to its Bellman equation</strong>.</p>
<div align="center">
  <img src="/img_RL/03_Backup.PNG" width=250px/>
</div>
<br>
<h3 id="optimal-policies-and-optimal-value-functions">Optimal Policies and Optimal Value Functions<a hidden class="anchor" aria-hidden="true" href="#optimal-policies-and-optimal-value-functions">#</a></h3>
<p>Solving a reinforcement learning task means, roughly, finding a policy that achieves a lot of reward over the long run. For finite MDPs, we can precisely define an optimal policy in the following way. Value functions define a partial ordering over policies. A policy <code>$\pi$</code> is defined to be better than or equal to a policy <code>$\pi'$</code> if its expected return is greater than or equal to that of <code>$\pi'$</code> for all states. In other words, <code>$\pi\geq\pi'$</code> if and only if <code>$v_{\pi}(s)\geq v_{\pi'}(s)$</code> for all <code>$s\in\mathcal{S}$</code>. There is always at least one policy that is better than or equal to all other policies. This is an <em><strong>optimal policy</strong></em>. Although there may be more than one, we denote all the optimal policies by <code>$\pi_{*}$</code>. They share the same state-value function, called the <em><strong>optimal state-value function</strong></em>, denoted <code>$v_{*}$</code>, and defined as</p>
<p><code>$$ v_{*}(s)\doteq\max_{\pi}v_{\pi}(s), \mathrm{\ for \ all \ } s\in\mathcal{S} \\ $$</code></p>
<p>Optimal policies also share the same <em><strong>optimal action-value function</strong></em>, denoted <code>$q_{*}$</code>, and defined as</p>
<p><code>$$ q_{*}(s,a)\doteq\max_{\pi}q_{\pi}(s,a), \mathrm{\ for \ all \ } s\in\mathcal{S}\mathrm{\ and\ } a \in \mathcal{A}  \\ $$</code></p>
<p>For the state–action pair <code>$(s,a)$</code>, this function gives the expected return for taking action <code>$a$</code> in state <code>$s$</code> and thereafter following an optimal policy. Thus, we can write <code>$q_{*}$</code> in terms of <code>$v_{*}$</code> as follows:</p>
<p><code>$$ q_{*}(s,a)=\mathbb{E}[R_{t+1}+\gamma v_{*}(S_{t+1})|S_{t}=s, A_{t}=a] \\ $$</code></p>
<h3 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h3>
<p>[1] Sutton, R. S., Bach, F., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press Ltd.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://followb1ind1y.github.io/tags/finite-markov-decision-processes/">Finite Markov Decision Processes</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://followb1ind1y.github.io/posts/reinforcement_learning/04_dynamic_programming/">
    <span class="title">« Prev Page</span>
    <br>
    <span>Dynamic Programming</span>
  </a>
  <a class="next" href="https://followb1ind1y.github.io/posts/reinforcement_learning/02_multi-armed_bandits/">
    <span class="title">Next Page »</span>
    <br>
    <span>Multi-armed Bandits</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Finite Markov Decision Processes on twitter"
        href="https://twitter.com/intent/tweet/?text=Finite%20Markov%20Decision%20Processes&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f03_finite_markov_decision_processes%2f&amp;hashtags=FiniteMarkovDecisionProcesses">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Finite Markov Decision Processes on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f03_finite_markov_decision_processes%2f&amp;title=Finite%20Markov%20Decision%20Processes&amp;summary=Finite%20Markov%20Decision%20Processes&amp;source=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f03_finite_markov_decision_processes%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Finite Markov Decision Processes on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f03_finite_markov_decision_processes%2f&title=Finite%20Markov%20Decision%20Processes">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Finite Markov Decision Processes on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f03_finite_markov_decision_processes%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Finite Markov Decision Processes on whatsapp"
        href="https://api.whatsapp.com/send?text=Finite%20Markov%20Decision%20Processes%20-%20https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f03_finite_markov_decision_processes%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Finite Markov Decision Processes on telegram"
        href="https://telegram.me/share/url?text=Finite%20Markov%20Decision%20Processes&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f03_finite_markov_decision_processes%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2022 <a href="https://followb1ind1y.github.io/">Followb1ind1y</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script src="//yihui.org/js/math-code.js"></script>


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>

</html>
