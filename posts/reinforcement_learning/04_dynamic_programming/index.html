<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Dynamic Programming | Followb1ind1y</title>
<meta name="keywords" content="Dynamic Programming" />
<meta name="description" content="The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically.
Starting with this chapter, we usually assume that the environment is a finite MDP.">
<meta name="author" content="Followb1ind1y">
<link rel="canonical" href="https://followb1ind1y.github.io/posts/reinforcement_learning/04_dynamic_programming/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.d70d9be7236724828e9dc04b26c5a1f3b228217674a56d00269f53e0d3db96f0.css" integrity="sha256-1w2b5yNnJIKOncBLJsWh87IoIXZ0pW0AJp9T4NPblvA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://followb1ind1y.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://followb1ind1y.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://followb1ind1y.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://followb1ind1y.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://followb1ind1y.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.83.1" />
<meta property="og:title" content="Dynamic Programming" />
<meta property="og:description" content="The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically.
Starting with this chapter, we usually assume that the environment is a finite MDP." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://followb1ind1y.github.io/posts/reinforcement_learning/04_dynamic_programming/" /><meta property="og:image" content="https://followb1ind1y.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-28T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2022-05-28T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://followb1ind1y.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Dynamic Programming"/>
<meta name="twitter:description" content="The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically.
Starting with this chapter, we usually assume that the environment is a finite MDP."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://followb1ind1y.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Dynamic Programming",
      "item": "https://followb1ind1y.github.io/posts/reinforcement_learning/04_dynamic_programming/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Dynamic Programming",
  "name": "Dynamic Programming",
  "description": "The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically.\nStarting with this chapter, we usually assume that the environment is a finite MDP.",
  "keywords": [
    "Dynamic Programming"
  ],
  "articleBody": "The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically.\nStarting with this chapter, we usually assume that the environment is a finite MDP. That is, we assume that its state, action, and reward sets, $\\mathcal{S}$, $\\mathcal{A}$, and $\\mathcal{R}$ are finite, and that its dynamics are given by a set of probabilities $p(s',r|s,a)$, for all $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}(s)$, $r \\in \\mathcal{R}$, and $s'\\in\\mathcal{S}^{+}$ ($S^{+}$ is $\\mathcal{S}$ plus a terminal state if the problem is episodic). Although DP ideas can be applied to problems with continuous state and action spaces, exact solutions are possible only in special cases. A common way of obtaining approximate solutions for tasks with continuous states and actions is to quantize the state and action spaces and then apply finite-state DP methods.\nThe key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. We know that we can easily obtain optimal policies once we have found the optimal value functions, $v_{*}$ or $q_{*}$, which satisfy the Bellman optimality equations:\n$$ v_{*}(s,a)=\\max_{a}\\mathbb{E}[R_{t+1}+\\gamma v_{*}(S_{t+1})|S_{t}=s, A_{t}=a] \\\\ =\\max_{a}\\sum_{s',r}p(s',r|s,a)[r+ \\gamma v_{*}(s')] \\\\ $$\nor\n$$ q_{*}(s,a)=\\mathbb{E}[R_{t+1}+\\gamma \\max_{a'}q_{*}(S_{t+1},a')|S_{t}=s, A_{t}=a] \\\\ =\\sum_{s',r}p(s',r|s,a)[r+ \\gamma \\max_{a'}q_{*}(s',a')] \\\\ $$\nall $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}(s)$, and $s'\\in\\mathcal{S}^{+}$. As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, into update rules for improving approximations of the desired value functions.\nPolicy Evaluation (Prediction) First we consider how to compute the state-value function $v_{\\pi}$ for an arbitrary policy $\\pi$. This is called policy evaluation in the DP literature. We also refer to it as the prediction problem. For all $s \\in \\mathcal{S}$,\n$$ \\begin{align} v_{\\pi}(s)\u0026\\doteq\\mathbb{E}[G_{t}|S_{t}=s] \\\\ \u0026=\\mathbb{E}[R_{t+1}+\\gamma G_{t+1}|S_{t}=s] \\\\ \u0026= \\sum_{a}\\pi(a|s)\\sum_{s'}\\sum_{r}p(s',r|s,a)[r+\\gamma\\mathbb{E}_{\\pi}[G_{t+1}|S_{t+1}=s']] \\\\ \u0026= \\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')] \\\\ \\end{align} $$\nwhere $\\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\\pi$, and the expectations are subscripted by $\\pi$ to indicate that they are conditional on $\\pi$ being followed. The existence and uniqueness of $v_{\\pi}$ are guaranteed as long as either $\\gamma  or eventual termination is guaranteed from all states under the policy $\\pi$.\nIf the environment’s dynamics are completely known, then the previous equation is a system of $|\\mathcal{S}|$ simultaneous linear equations in $|\\mathcal{S}|$ unknowns (the $v_{\\pi}(s)$, $s \\in \\mathcal{S}$). In principle, its solution is a straightforward, if tedious, computation. For our purposes, iterative solution methods are most suitable. Consider a sequence of approximate value functions $v_{0},v_{1},v_{2}.\\cdots,$ each mapping $\\mathcal{S}^{+}$ to $\\mathbb{R}$ (the real numbers). The initial approximation, $v_{0}$, is chosen arbitrarily (except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the Bellman equation for $v_{\\pi}$ as an update rule:\n$$ \\begin{align} v_{k+1}(s) \u0026=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma v_{k}S_{t+1}|S_{t}=s] \\\\ \u0026= \\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{k}(s')] \\\\ \\end{align} $$\nfor all $s\\in \\mathcal{S}$. Clearly, $v_{k}=v_{\\pi}$ is a fixed point for this update rule because the Bellman equation for $v_{\\pi}$ assures us of equality in this case. Indeed, the sequence {$v_{k}$} can be shown in general to converge to $v_{\\pi}$ as $k \\to \\infty$ under the same conditions that guarantee the existence of $v_{\\pi}$. This algorithm is called iterative policy evaluation.\n  Policy Improvement Our reason for computing the value function for a policy is to help find better policies. Suppose we have determined the value function $v_{\\pi}$ for an arbitrary deterministic policy $\\pi$. For some state $s$ we would like to know whether or not we should change the policy to deterministically choose an action $a \\neq \\pi(s)$. We know how good it is to follow the current policy from $s$— that is $v_{\\pi}(s)$ — but would it be better or worse to change to the new policy? One way to answer this question is to consider selecting a in $s$ and thereafter following the existing policy, $\\pi$. The value of this way of behaving is\n$$ q_{\\pi}(s,a)=\\mathbb{E}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})|S_{t}=s, A_{t}=a] \\\\ =\\sum_{s',r}p(s',r|s,a)[r+ \\gamma v_{\\pi}(s')] \\\\ $$\nThe key criterion is whether this is greater than or less than $v_{\\pi}(s)$. If it is greater — that is, if it is better to select $a$ once in $s$ and thereafter follow $\\pi$ than it would be to follow $\\pi$ all the time—then one would expect it to be better still to select a every time $s$ is encountered, and that the new policy would in fact be a better one overall.\nThat this is true is a special case of a general result called the policy improvement theorem. Let $\\pi$ and $\\pi'$ be any pair of deterministic policies such that, for all $s\\in \\mathcal{S}$,\n$$ q_{\\pi}(s,\\pi'(s)) \\geq v_{\\pi}(s) \\\\ $$\nThen the policy $\\pi'$ must be as good as, or better than, $\\pi$. That is, it must obtain greater or equal expected return from all states $s \\in\\mathcal{S}$:\n$$ v_{\\pi'}(s) \\geq v_{\\pi}(s) \\\\ $$\nSo far we have seen how, given a policy and its value function, we can easily evaluate a change in the policy at a single state. It is a natural extension to consider changes at all states, selecting at each state the action that appears best according to $q_{\\pi}(s,a)$. In other words, to consider the new greedy policy, $\\pi'$, given by\n$$ \\begin{align} \\pi'(s)\u0026\\doteq \\arg\\max_{a}q_{\\pi}(s,a) \\\\ \u0026= \\arg\\max_{a} \\mathbb{E}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})|S_{t}=s, A_{t}=a] \\\\ \u0026= \\arg\\max_{a}\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')] \\\\ \\end{align} $$\nwhere $\\arg\\max_{a}$ denotes the value of $a$ at which the expression that follows is maximized (with ties broken arbitrarily). The greedy policy takes the action that looks best in the short term—after one step of lookahead—according to $v_{\\pi}$. By construction, the greedy policy meets the conditions of the policy improvement theorem, so we know that it is as good as, or better than, the original policy. The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called policy improvement.\nSuppose the new greedy policy, $\\pi'$, is as good as, but not better than, the old policy $\\pi$. Then $v_{\\pi}=v_{\\pi'}$ , and it follows that for all $s \\in \\mathcal{S}$:\n$$ \\begin{align} v_{\\pi'}(s)\u0026= \\max_{a} \\mathbb{E}[R_{t+1}+\\gamma v_{\\pi'}(S_{t+1})|S_{t}=s, A_{t}=a] \\\\ \u0026= \\max_{a}\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')] \\\\ \\end{align} $$\nBut this is the same as the Bellman optimality equation, and therefore, $v_{\\pi'}$ must be $v_{*}$, and both $\\pi$ and $\\pi'$ must be optimal policies. Policy improvement thus must give us a strictly better policy except when the original policy is already optimal.\nPolicy Iteration Once a policy, $\\pi$, has been improved using $v_{\\pi}$ to yield a better policy, $\\pi'$, we can then compute $v_{\\pi'}$ and improve it again to yield an even better $\\pi''$. We can thus obtain a sequence of monotonically improving policies and value functions:\n where $\\to^{E}$ denotes a policy evaluation and $\\to^{I}$ denotes a policy improvement. Each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal). Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and the optimal value function in a finite number of iterations.\nThis way of finding an optimal policy is called policy iteration. Note that each policy evaluation, itself an iterative computation, is started with the value function for the previous policy. This typically results in a great increase in the speed of convergence of policy evaluation (presumably because the value function changes little from one policy to the next).\n Value Iteration One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to $v_{\\pi}$ occurs only in the limit. Must we wait for exact convergence, or can we stop short of that?\nIn fact, the policy evaluation step of policy iteration can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm is called value iteration. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps:\n$$ \\begin{align} v_{k+1}(s)\u0026\\doteq \\max_{a} \\mathbb{E}[R_{t+1}+\\gamma v_{k}(S_{t+1})|S_{t}=s, A_{t}=a] \\\\ \u0026= \\max_{a}\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{k}(s')] \\\\ \\end{align} $$\nfor all $s \\in \\mathcal{S}$. For arbitrary $v_{0}$, the sequence $\\{v_{k}\\}$ can be shown to converge to $v_{*}$ under the same conditions that guarantee the existence of $v_{*}$.\nAnother way of understanding value iteration is by reference to the Bellman optimality equation. Note that value iteration is obtained simply by turning the Bellman optimality equation into an update rule. Also note how the value iteration update is identical to the policy evaluation update except that it requires the maximum to be taken over all actions. Another way of seeing this close relationship is to compare the backup diagrams for these algorithms (policy evaluation) and (value iteration). These two are the natural backup operations for computing $v_{\\pi}$ and $v_{*}$.\nFinally, let us consider how value iteration terminates. Like policy evaluation, value iteration formally requires an infinite number of iterations to converge exactly to $v_{*}$. In practice, we stop once the value function changes by only a small amount in a sweep. The box below shows a complete algorithm with this kind of termination condition.\n Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement. Faster convergence is often achieved by interposing multiple policy evaluation sweeps between each policy improvement sweep. In general, the entire class of truncated policy iteration algorithms can be thought of as sequences of sweeps, some of which use policy evaluation updates and some of which use value iteration updates. Because the max operation is the only difference between these updates, this just means that the max operation is added to some sweeps of policy evaluation. All of these algorithms converge to an optimal policy for discounted finite MDPs.\nReference [1] Sutton, R. S., Bach, F., \u0026 Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press Ltd.\n",
  "wordCount" : "1717",
  "inLanguage": "en",
  "datePublished": "2022-05-28T00:00:00Z",
  "dateModified": "2022-05-28T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Followb1ind1y"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://followb1ind1y.github.io/posts/reinforcement_learning/04_dynamic_programming/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Followb1ind1y",
    "logo": {
      "@type": "ImageObject",
      "url": "https://followb1ind1y.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://followb1ind1y.github.io/" accesskey="h" title="Followb1ind1y (Alt + H)">Followb1ind1y</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://followb1ind1y.github.io/archives" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://followb1ind1y.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://followb1ind1y.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Dynamic Programming
    </h1>
    <div class="post-meta">May 28, 2022&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Followb1ind1y
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#policy-evaluation-prediction" aria-label="Policy Evaluation (Prediction)">Policy Evaluation (Prediction)</a></li>
                <li>
                    <a href="#policy-improvement" aria-label="Policy Improvement">Policy Improvement</a></li>
                <li>
                    <a href="#policy-iteration" aria-label="Policy Iteration">Policy Iteration</a></li>
                <li>
                    <a href="#value-iteration" aria-label="Value Iteration">Value Iteration</a></li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>The term <em><strong>dynamic programming</strong></em> (DP) refers to a collection of algorithms that can be used to <strong>compute optimal policies</strong> given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically.</p>
<p>Starting with this chapter, we usually assume that the environment is a finite MDP. That is, we assume that its state, action, and reward sets, <code>$\mathcal{S}$</code>, <code>$\mathcal{A}$</code>, and <code>$\mathcal{R}$</code> are finite, and that its dynamics are given by a set of probabilities <code>$p(s',r|s,a)$</code>, for all <code>$s \in \mathcal{S}$</code>, <code>$a \in \mathcal{A}(s)$</code>, <code>$r \in \mathcal{R}$</code>, and <code>$s'\in\mathcal{S}^{+}$</code> (<code>$S^{+}$</code> is <code>$\mathcal{S}$</code> plus a terminal state if the problem is episodic). Although DP ideas can be applied to problems with continuous state and action spaces, exact solutions are possible only in special cases. A common way of obtaining approximate solutions for tasks with continuous states and actions is to quantize the state and action spaces and then apply finite-state DP methods.</p>
<p>The key idea of DP, and of reinforcement learning generally, <em><strong>is the use of value functions to organize and structure the search for good policies.</strong></em> We know that we can easily obtain optimal policies once we have found the optimal value functions, <code>$v_{*}$ or $q_{*}$</code>, which satisfy the Bellman optimality equations:</p>
<p><code>$$ v_{*}(s,a)=\max_{a}\mathbb{E}[R_{t+1}+\gamma v_{*}(S_{t+1})|S_{t}=s, A_{t}=a] \\ =\max_{a}\sum_{s',r}p(s',r|s,a)[r+ \gamma v_{*}(s')] \\ $$</code></p>
<p>or</p>
<p><code>$$ q_{*}(s,a)=\mathbb{E}[R_{t+1}+\gamma \max_{a'}q_{*}(S_{t+1},a')|S_{t}=s, A_{t}=a] \\ =\sum_{s',r}p(s',r|s,a)[r+ \gamma \max_{a'}q_{*}(s',a')] \\ $$</code></p>
<p>all <code>$s \in \mathcal{S}$</code>, <code>$a \in \mathcal{A}(s)$</code>, and <code>$s'\in\mathcal{S}^{+}$</code>. As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, into update rules for improving approximations of the desired value functions.</p>
<h3 id="policy-evaluation-prediction">Policy Evaluation (Prediction)<a hidden class="anchor" aria-hidden="true" href="#policy-evaluation-prediction">#</a></h3>
<p>First we consider how to compute the state-value function <code>$v_{\pi}$</code> for an arbitrary policy <code>$\pi$</code>. This is called <em><strong>policy evaluation</strong></em> in the DP literature. We also refer to it as the <em><strong>prediction problem</strong></em>. For all <code>$s \in \mathcal{S}$</code>,</p>
<p><code>$$ \begin{align} v_{\pi}(s)&amp;\doteq\mathbb{E}[G_{t}|S_{t}=s] \\ &amp;=\mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_{t}=s] \\ &amp;= \sum_{a}\pi(a|s)\sum_{s'}\sum_{r}p(s',r|s,a)[r+\gamma\mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s']] \\ &amp;= \sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_{\pi}(s')] \\ \end{align} $$</code></p>
<p>where <code>$\pi(a|s)$</code> is the probability of taking action <code>$a$</code> in state <code>$s$</code> under policy <code>$\pi$</code>, and the expectations are subscripted by <code>$\pi$</code> to indicate that they are conditional on <code>$\pi$</code> being followed. The existence and uniqueness of <code>$v_{\pi}$</code> are guaranteed as long as either <code>$\gamma &lt;1$</code> or eventual termination is guaranteed from all states under the policy <code>$\pi$</code>.</p>
<p>If the environment’s dynamics are completely known, then the previous equation is a system of <code>$|\mathcal{S}|$</code> simultaneous linear equations in <code>$|\mathcal{S}|$</code> unknowns (the <code>$v_{\pi}(s)$</code>, <code>$s \in \mathcal{S}$</code>). In principle, its solution is a straightforward, if tedious, computation. For our purposes, iterative solution methods are most suitable. Consider a sequence of approximate value functions <code>$v_{0},v_{1},v_{2}.\cdots,$</code> each mapping <code>$\mathcal{S}^{+}$</code> to <code>$\mathbb{R}$</code> (the real numbers). The initial approximation, <code>$v_{0}$</code>, is chosen arbitrarily (except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the Bellman equation for <code>$v_{\pi}$</code> as an update rule:</p>
<p><code>$$ \begin{align} v_{k+1}(s) &amp;=\mathbb{E}_{\pi}[R_{t+1}+\gamma v_{k}S_{t+1}|S_{t}=s] \\ &amp;= \sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_{k}(s')] \\ \end{align} $$</code></p>
<p>for all <code>$s\in \mathcal{S}$</code>. Clearly, <code>$v_{k}=v_{\pi}$</code> is a fixed point for this update rule because the Bellman equation for <code>$v_{\pi}$</code> assures us of equality in this case. Indeed, the sequence {<code>$v_{k}$</code>} can be shown in general to converge to <code>$v_{\pi}$</code> as <code>$k \to \infty$</code> under the same conditions that guarantee the existence of <code>$v_{\pi}$</code>. This algorithm is called iterative policy evaluation.</p>
<div align="center">
  <img src="/img_RL/04_policy_evl_alg.PNG" width=650px/>
</div>
<br>
<div align="center">
  <img src="/img_RL/04_Vs_exp.PNG" width=650px/>
</div>
<br>
<h3 id="policy-improvement">Policy Improvement<a hidden class="anchor" aria-hidden="true" href="#policy-improvement">#</a></h3>
<p>Our reason for computing the value function for a policy is to help find better policies. Suppose we have determined the value function <code>$v_{\pi}$</code> for an arbitrary deterministic policy <code>$\pi$</code>. For some state <code>$s$</code> we would like to know whether or not we should change the policy to deterministically choose an action <code>$a \neq \pi(s)$</code>. We know how good it is to follow the current policy from <code>$s$</code>— that is <code>$v_{\pi}(s)$</code> — but would it be better or worse to change to the new policy? One way to answer this question is to consider selecting a in <code>$s$</code> and thereafter following the existing policy, <code>$\pi$</code>. The value of this way of behaving is</p>
<p><code>$$ q_{\pi}(s,a)=\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s, A_{t}=a] \\ =\sum_{s',r}p(s',r|s,a)[r+ \gamma v_{\pi}(s')] \\ $$</code></p>
<p>The key criterion is whether this is greater than or less than <code>$v_{\pi}(s)$</code>. If it is greater — that is, if it is better to select <code>$a$</code> once in <code>$s$</code> and thereafter follow <code>$\pi$</code> than it would be to follow <code>$\pi$</code> all the time—then one would expect it to be better still to select a every time <code>$s$</code> is encountered, and that the new policy would in fact be a better one overall.</p>
<p>That this is true is a special case of a general result called the <em><strong>policy improvement</strong></em> theorem. Let <code>$\pi$</code> and <code>$\pi'$</code> be any pair of deterministic policies such that, for all <code>$s\in \mathcal{S}$</code>,</p>
<p><code>$$ q_{\pi}(s,\pi'(s)) \geq v_{\pi}(s) \\ $$</code></p>
<p>Then the policy <code>$\pi'$</code> must be as good as, or better than, <code>$\pi$</code>. That is, it must obtain greater or equal expected return from all states <code>$s \in\mathcal{S}$</code>:</p>
<p><code>$$ v_{\pi'}(s) \geq v_{\pi}(s) \\ $$</code></p>
<p>So far we have seen how, given a policy and its value function, we can easily evaluate a change in the policy at a single state. It is a natural extension to consider changes at all states, selecting at each state the action that appears best according to <code>$q_{\pi}(s,a)$</code>. In other words, to consider the new greedy policy, <code>$\pi'$</code>, given by</p>
<p><code>$$ \begin{align} \pi'(s)&amp;\doteq \arg\max_{a}q_{\pi}(s,a) \\ &amp;= \arg\max_{a} \mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s, A_{t}=a] \\ &amp;= \arg\max_{a}\sum_{s',r}p(s',r|s,a)[r+\gamma v_{\pi}(s')] \\ \end{align} $$</code></p>
<p>where <code>$\arg\max_{a}$</code> denotes the value of <code>$a$</code> at which the expression that follows is maximized (with ties broken arbitrarily). The greedy policy takes the action that looks best in the short term—after one step of lookahead—according to <code>$v_{\pi}$</code>. By construction, the greedy policy meets the conditions of the <em><strong>policy improvement theorem</strong></em>, so we know that it is as good as, or better than, the original policy. The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called <em><strong>policy improvement</strong></em>.</p>
<p>Suppose the new greedy policy, <code>$\pi'$</code>, is as good as, but not better than, the old policy <code>$\pi$</code>. Then <code>$v_{\pi}=v_{\pi'}$</code> , and it follows that for all <code>$s \in \mathcal{S}$</code>:</p>
<p><code>$$ \begin{align} v_{\pi'}(s)&amp;= \max_{a} \mathbb{E}[R_{t+1}+\gamma v_{\pi'}(S_{t+1})|S_{t}=s, A_{t}=a] \\ &amp;= \max_{a}\sum_{s',r}p(s',r|s,a)[r+\gamma v_{\pi}(s')] \\ \end{align} $$</code></p>
<p>But this is the same as the Bellman optimality equation, and therefore, <code>$v_{\pi'}$</code> must be <code>$v_{*}$</code>, and both <code>$\pi$</code> and <code>$\pi'$</code> must be optimal policies. Policy improvement thus must give us a strictly better policy except when the original policy is already optimal.</p>
<h3 id="policy-iteration">Policy Iteration<a hidden class="anchor" aria-hidden="true" href="#policy-iteration">#</a></h3>
<p>Once a policy, <code>$\pi$</code>, has been improved using <code>$v_{\pi}$</code> to yield a better policy, <code>$\pi'$</code>, we can then compute <code>$v_{\pi'}$</code> and improve it again to yield an even better <code>$\pi''$</code>. We can thus obtain a sequence of monotonically improving policies and value functions:</p>
<div align="center">
  <img src="/img_RL/04_Iter.PNG" width=500px/>
</div>
<br>
<p>where <code>$\to^{E}$</code> denotes a <em><strong>policy evaluation</strong></em> and <code>$\to^{I}$</code> denotes a <em><strong>policy improvement</strong></em>. Each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal). Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and the optimal value function in a finite number of iterations.</p>
<p>This way of finding an optimal policy is called <em><strong>policy iteration</strong></em>. Note that each policy evaluation, itself an iterative computation, is started with the value function for the previous policy. This typically results in a great increase in the speed of convergence of policy evaluation (presumably because the value function changes little from one policy to the next).</p>
<div align="center">
  <img src="/img_RL/04_Iter_alg.PNG" width=650px/>
</div>
<br>
<h3 id="value-iteration">Value Iteration<a hidden class="anchor" aria-hidden="true" href="#value-iteration">#</a></h3>
<p>One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to <code>$v_{\pi}$</code> occurs only in the limit. Must we wait for exact convergence, or can we stop short of that?</p>
<p>In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm is called <em><strong>value iteration</strong></em>. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps:</p>
<p><code>$$ \begin{align} v_{k+1}(s)&amp;\doteq \max_{a} \mathbb{E}[R_{t+1}+\gamma v_{k}(S_{t+1})|S_{t}=s, A_{t}=a] \\ &amp;= \max_{a}\sum_{s',r}p(s',r|s,a)[r+\gamma v_{k}(s')] \\ \end{align} $$</code></p>
<p>for all <code>$s \in \mathcal{S}$</code>. For arbitrary <code>$v_{0}$</code>, the sequence <code>$\{v_{k}\}$</code> can be shown to converge to <code>$v_{*}$</code> under the same conditions that guarantee the existence of <code>$v_{*}$</code>.</p>
<p>Another way of understanding value iteration is by reference to the Bellman optimality equation. Note that value iteration is obtained simply by turning the Bellman optimality equation into an update rule. Also note how the value iteration update is identical to the policy evaluation update except that it requires the maximum to be taken over all actions. Another way of seeing this close relationship is to compare the backup diagrams for these algorithms (policy evaluation) and (value iteration). These two are the natural backup operations for computing <code>$v_{\pi}$</code> and <code>$v_{*}$</code>.</p>
<p>Finally, let us consider how value iteration terminates. Like policy evaluation, value iteration formally requires an infinite number of iterations to converge exactly to <code>$v_{*}$</code>. In practice, we stop once the value function changes by only a small amount in a sweep. The box below shows a complete algorithm with this kind of termination condition.</p>
<div align="center">
  <img src="/img_RL/04_Value_iter.PNG" width=650px/>
</div>
<br>
<p>Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement. Faster convergence is often achieved by interposing multiple policy evaluation sweeps between each policy improvement sweep. In general, the entire class of truncated policy iteration algorithms can be thought of as sequences of sweeps, some of which use policy evaluation updates and some of which use value iteration updates. Because the max operation is the only difference between these updates, this just means that the max operation is added to some sweeps of policy evaluation. All of these algorithms converge to an optimal policy for discounted finite MDPs.</p>
<h3 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h3>
<p>[1] Sutton, R. S., Bach, F., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press Ltd.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://followb1ind1y.github.io/tags/dynamic-programming/">Dynamic Programming</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://followb1ind1y.github.io/posts/reinforcement_learning/05_-monte_carlo_methods/">
    <span class="title">« Prev Page</span>
    <br>
    <span>Monte Carlo Methods</span>
  </a>
  <a class="next" href="https://followb1ind1y.github.io/posts/reinforcement_learning/03_finite_markov_decision_processes/">
    <span class="title">Next Page »</span>
    <br>
    <span>Finite Markov Decision Processes</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Dynamic Programming on twitter"
        href="https://twitter.com/intent/tweet/?text=Dynamic%20Programming&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f04_dynamic_programming%2f&amp;hashtags=DynamicProgramming">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Dynamic Programming on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f04_dynamic_programming%2f&amp;title=Dynamic%20Programming&amp;summary=Dynamic%20Programming&amp;source=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f04_dynamic_programming%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Dynamic Programming on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f04_dynamic_programming%2f&title=Dynamic%20Programming">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Dynamic Programming on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f04_dynamic_programming%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Dynamic Programming on whatsapp"
        href="https://api.whatsapp.com/send?text=Dynamic%20Programming%20-%20https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f04_dynamic_programming%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Dynamic Programming on telegram"
        href="https://telegram.me/share/url?text=Dynamic%20Programming&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2freinforcement_learning%2f04_dynamic_programming%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2022 <a href="https://followb1ind1y.github.io/">Followb1ind1y</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script src="//yihui.org/js/math-code.js"></script>


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>

</html>
